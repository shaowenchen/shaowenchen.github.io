<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:sy="http://purl.org/rss/1.0/modules/syndication/" xmlns:media="http://search.yahoo.com/mrss/"><channel><title>Triton on 陈少文的网站</title><link>https://www.chenshaowen.com/tags/triton/</link><description>Recent content in Triton on 陈少文的网站</description><generator>Hugo -- gohugo.io</generator><language>zh</language><copyright>&amp;copy;2016 - {year}, All Rights Reserved.</copyright><lastBuildDate>Sat, 03 Feb 2024 08:05:48 +0000</lastBuildDate><sy:updatePeriod>weekly</sy:updatePeriod><atom:link href="https://www.chenshaowen.com/tags/triton/atom.xml" rel="self" type="application/rss+xml"/><item><title>容器下使用 Triton Server 和 TensorRT-LLM 进行大模型推理</title><link>https://www.chenshaowen.com/blog/using-triton-server-and-tensorrt-llm-under-container.html</link><pubDate>Sat, 03 Feb 2024 08:05:48 +0000</pubDate><atom:modified>Sat, 03 Feb 2024 08:05:48 +0000</atom:modified><guid>https://www.chenshaowen.com/blog/using-triton-server-and-tensorrt-llm-under-container.html</guid><description>1. TensorRT-LLM 编译模型 1.1 TensorRT-LLM 简介 使用 TensorRT 时，通常需要将模型转换为 ONNX 格式，再将 ONNX 转换为 TensorRT 格式，然后在 TensorRT、Triton Server 中进行推理。 但这个转换过程并不简单，经常会遇到各种报错，需要对模型结构、平台算子有一定的掌握，具备转换和调试能力。而 TensorRT-LLM 的目标</description><dc:creator>微信公众号</dc:creator><category>博文</category><category>AI</category><category>Triton</category><category>TensorRT</category></item></channel></rss>