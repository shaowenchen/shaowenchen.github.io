<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:sy="http://purl.org/rss/1.0/modules/syndication/" xmlns:media="http://search.yahoo.com/mrss/"><channel><title>Kubernetes on 陈少文的网站</title><link>https://www.chenshaowen.com/tags/kubernetes/</link><description>Recent content in Kubernetes on 陈少文的网站</description><generator>Hugo -- gohugo.io</generator><language>zh</language><copyright>&amp;copy;2016 - {year}, All Rights Reserved.</copyright><lastBuildDate>Sat, 22 Feb 2025 00:00:00 +0000</lastBuildDate><sy:updatePeriod>weekly</sy:updatePeriod><atom:link href="https://www.chenshaowen.com/tags/kubernetes/atom.xml" rel="self" type="application/rss+xml"/><item><title>kubectl logs 无法查看 Pod 日志报错 NotFound</title><link>https://www.chenshaowen.com/blog/kubectl-logs-not-found-error.html</link><pubDate>Sat, 22 Feb 2025 00:00:00 +0000</pubDate><atom:modified>Sat, 22 Feb 2025 00:00:00 +0000</atom:modified><guid>https://www.chenshaowen.com/blog/kubectl-logs-not-found-error.html</guid><description>1. 现象 能查看 Pod 的信息 1 2 3 4 kubectl -n my-testns get pod my-testpod NAME READY STATUS RESTARTS AGE my-testpod 1/1 Running 0 2d13h 不能查看 Pod 的日志 1 2 3 kubectl -n my-testns logs my-testpod -f Error from server (NotFound): the server could not find the requested resource ( pods/log my-testpod) 在 Pod 所在主机上可以通过 docker logs 查看容器日志。 测试 Kubelet 的健康状态 OK 1 curl -k https://x.x.x.x:10250/healthz 这里要使用主机的 IP 地址，kubectl logs 命名会直接</description><dc:creator>微信公众号</dc:creator><category>博文</category><category>Kubernetes</category><category>kubectl</category><category>Pod</category><category>运维故障</category></item><item><title>在 Kubernetes 部署 Jumpserver 跳板机</title><link>https://www.chenshaowen.com/blog/how-to-deploy-jumpserver-on-kubernetes.html</link><pubDate>Thu, 09 Jan 2025 00:00:00 +0000</pubDate><atom:modified>Thu, 09 Jan 2025 00:00:00 +0000</atom:modified><guid>https://www.chenshaowen.com/blog/how-to-deploy-jumpserver-on-kubernetes.html</guid><description>1. 部署 Jumpserver 需要提前准备好 StorageClass，用于存储 Jumpserver 的数据。除了下面提到的数据库，各个组件 jms-core、jms-web、jms-koko、jms-lion、jms-chen 都需要一个 PV 存储。 1.1 部署 MySQL 参考 https://github.com/shaowenchen/hubimage/blob/main/database/mysql8.yaml ，部署 MySQL。 需要调整</description><dc:creator>微信公众号</dc:creator><category>博文</category><category>Kubernetes</category><category>Jumpserver</category><category>基础运维</category></item><item><title>使用 Iceberg 和 Spark 在 Kubernetes 上处理数据</title><link>https://www.chenshaowen.com/blog/use-iceberg-and-spark-on-kubernetes.html</link><pubDate>Thu, 12 Sep 2024 00:00:00 +0000</pubDate><atom:modified>Thu, 12 Sep 2024 00:00:00 +0000</atom:modified><guid>https://www.chenshaowen.com/blog/use-iceberg-and-spark-on-kubernetes.html</guid><description>1. 数据处理架构 主要分为四层： 处理能力层，Spark on Kubernetes 提供流式的数据处理能力 数据管理层，Iceberg 提供 ACID、table 等数据集访问操作能力 存储层，Hive MetaStore 管理 Iceberg 表元数据，Postgresql 作为 Hive MetaStore 存储后端，S3 作为数据存储后端 资</description><dc:creator>微信公众号</dc:creator><category>博文</category><category>Spark</category><category>Iceberg</category><category>Kubernetes</category></item><item><title>Kubernetes 下的 DLRover 工作流程分析</title><link>https://www.chenshaowen.com/blog/kubernetes-dlrover-workflow-analysis.html</link><pubDate>Tue, 27 Aug 2024 00:00:00 +0000</pubDate><atom:modified>Tue, 27 Aug 2024 00:00:00 +0000</atom:modified><guid>https://www.chenshaowen.com/blog/kubernetes-dlrover-workflow-analysis.html</guid><description>本文使用的 DLRover 版本是 0.3.7 1. DLRover Operator 1.1 启动 ElasticJob 和 ScalePlan 的控制器 实现代码： 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 // 创建 ElasticJob 的控制器 if err = controllers.NewElasticJobReconciler(mgr, masterImage).SetupWithManager(mgr); err != nil { setupLog.Error(err, &amp;#34;unable to create controller&amp;#34;, &amp;#34;controller&amp;#34;, &amp;#34;ElasticJob&amp;#34;) os.Exit(1) } // 创建 ScalePlan 的控制器 if err = controllers.NewScalePlanReconciler(mgr).SetupWithManager(mgr); err != nil { setupLog.Error(err, &amp;#34;unable to create controller&amp;#34;, &amp;#34;controller&amp;#34;, &amp;#34;ScalePlan&amp;#34;) os.Exit(1) } // 启动控制器 if err := mgr.Start(ctrl.SetupSignalHandler()); err != nil { setupLog.Error(err, &amp;#34;problem running manager&amp;#34;) os.Exit(1) } 这部分代码是</description><dc:creator>微信公众号</dc:creator><category>博文</category><category>DLRover</category><category>Kubernetes</category><category>训练</category><category>故障自愈</category><category>源码</category></item><item><title>使用 DLRover 托管作业进行弹性、容错训练</title><link>https://www.chenshaowen.com/blog/use-dlrover-to-manage-training-job.html</link><pubDate>Sat, 17 Aug 2024 00:00:00 +0000</pubDate><atom:modified>Sat, 17 Aug 2024 00:00:00 +0000</atom:modified><guid>https://www.chenshaowen.com/blog/use-dlrover-to-manage-training-job.html</guid><description>1. 分布式训练面临的问题 预估训练资源困难，无法自动化 需要多少算力、需要多少时间、需要多少带宽、需要多少 CPU、需要多少内存，如果没有足够的积累，很难估算准确。导致的结果就是，超额申请、超额分配，造成极大的资源浪费。 需要去沉淀和提供解决方案。 故</description><dc:creator>微信公众号</dc:creator><category>博文</category><category>DLRover</category><category>训练</category><category>Kubernetes</category><category>弹性训练</category><category>容错训练</category></item><item><title>为什么 top node、free、Grafana 的数据对不上</title><link>https://www.chenshaowen.com/blog/why-top-node-free-grafana-data-not-match.html</link><pubDate>Fri, 26 Jul 2024 01:00:00 +0000</pubDate><atom:modified>Fri, 26 Jul 2024 01:00:00 +0000</atom:modified><guid>https://www.chenshaowen.com/blog/why-top-node-free-grafana-data-not-match.html</guid><description>1. top 查看节点资源使用率超过 100% 1 2 3 4 5 6 kubectl top node NAME CPU(cores) CPU% MEMORY(bytes) MEMORY% master-1 995m 16% 13760Mi 118% master-2 827m 13% 10672Mi 92% master-3 889m 14% 10244Mi 88% 这是由于在计算使用率时，默认使用的是可分配的资源，排除了 Kubelet 保留的部分。在 kubectl 源码中可以看到: 1 2 3 4 5 6 7 for _, n := range nodes { if !o.ShowCapacity { availableResources[n.Name] = n.Status.Allocatable } else { availableResources[n.Name] = n.Status.Capacity } } 如果需要</description><dc:creator>微信公众号</dc:creator><category>博文</category><category>监控</category><category>内存</category><category>Kubernetes</category></item><item><title>使用 JuiceFS 存储 Elasticsearch 数据</title><link>https://www.chenshaowen.com/blog/store-elasticsearch-data-in-juicefs.html</link><pubDate>Wed, 22 May 2024 00:00:00 +0000</pubDate><atom:modified>Wed, 22 May 2024 00:00:00 +0000</atom:modified><guid>https://www.chenshaowen.com/blog/store-elasticsearch-data-in-juicefs.html</guid><description>1. 存储方案 三种存储方案： 基于目录隔离公用一个 JuiceFS Elasticsearch 的节点共用一个 JuiceFS，通过子目录挂载不同的 Elasticsearch 节点。 /0/ 对应节点 Node-0 /1/ 对应节点 Node-1 /2/ 对应节点 Node-2 这种方式的好处主要是，易于扩展、配置方便。 基于 JuiceFS 隔离节点数据 Elasticsearch 每个节点都对接一个独立的 JuiceF</description><dc:creator>微信公众号</dc:creator><category>博文</category><category>Kubernetes</category><category>JuiceFS</category><category>Elasticsearch</category><category>数据存储</category></item><item><title>Volcano 使用基础</title><link>https://www.chenshaowen.com/blog/the-basic-of-volcano.html</link><pubDate>Sun, 24 Mar 2024 00:00:00 +0000</pubDate><atom:modified>Sun, 24 Mar 2024 00:00:00 +0000</atom:modified><guid>https://www.chenshaowen.com/blog/the-basic-of-volcano.html</guid><description>1. Volcano 简介 Volcano 是华为开源的一个基于 Kubernetes 的资源调度系统，相较于原生的调度器，具有的显著特点有： 支持 gang scheduling 对于批量作业的调度，容易碰到死锁的问题，比如两个作业都需要同时运行 10 个 Pod 才能启动，当两个作业同时提交时，可能都只有部分 Pod 被调度，两个作业都无法正常</description><dc:creator>微信公众号</dc:creator><category>博文</category><category>Volcano</category><category>AI</category><category>Kubernetes</category><category>调度</category></item><item><title>Ops 新增 Server 及 UI 服务</title><link>https://www.chenshaowen.com/blog/ops-added-server-and-ui-services.html</link><pubDate>Wed, 14 Feb 2024 00:00:00 +0000</pubDate><atom:modified>Wed, 14 Feb 2024 00:00:00 +0000</atom:modified><guid>https://www.chenshaowen.com/blog/ops-added-server-and-ui-services.html</guid><description>1. 什么是 Ops 项目 我在之前的文章中介绍过一个常用的 Ops 工具。 Ops 的设计理念在于，运维工具的核心在于文本分发和脚本执行，实现了这两种能力就能够满足运维的功能诉求。 目前我主要的运维对象是 Host 主机、Kubernetes 集群，因此在 OpsObject 层实现了 Host 和 Cluster 对象，分别</description><dc:creator>微信公众号</dc:creator><category>博文</category><category>Ops</category><category>Copilot</category><category>Kubernetes</category><category>运维</category></item><item><title>kind 实用指南</title><link>https://www.chenshaowen.com/blog/practice-guide-to-kind.html</link><pubDate>Mon, 05 Feb 2024 00:00:00 +0000</pubDate><atom:modified>Mon, 05 Feb 2024 00:00:00 +0000</atom:modified><guid>https://www.chenshaowen.com/blog/practice-guide-to-kind.html</guid><description>1. 项目简介 kind 是使用容器管理 Kubernetes 集群的工具。项目地址 https://github.com/kubernetes-sigs/kind 。 主要用在: 本地开发环境 学习时的临时环境 自动化测试 2. 安装 kind macOS 1 brew install kind Linux 1 2 curl -Lo /usr/local/bin/kind https://kind.sigs.k8s.io/dl/v0.21.0/kind-linux-amd64 chmod +x /usr/local/bin/kind 3. 创建 kind 集群 如果你本地配置有 PROXY，在创建之间建议重新设置一下环境变量： 1 2 export https_proxy=http://x.x.x.x:7890 export http_proxy=http://x.x.x.x:7890 本地代理通常设</description><dc:creator>微信公众号</dc:creator><category>博文</category><category>Kubernetes</category><category>kind</category><category>实践</category></item><item><title>Kubernetes 集群中 AI 相关的采集器</title><link>https://www.chenshaowen.com/blog/ai-related-exporters-in-kubernetes.html</link><pubDate>Sun, 04 Feb 2024 00:00:00 +0000</pubDate><atom:modified>Sun, 04 Feb 2024 00:00:00 +0000</atom:modified><guid>https://www.chenshaowen.com/blog/ai-related-exporters-in-kubernetes.html</guid><description>1. dcgm-exporter dcgm-exporter 是 NVIDIA 官方社区提供的 GPU 监控工具。 项目地址 https://github.com/NVIDIA/dcgm-exporter 1.1 安装方式 添加 Helm 镜像仓库 1 helm repo add gpu-helm-charts https://nvidia.github.io/dcgm-exporter/helm-charts 1 helm repo update 安装 1 2 3 4 5 6 helm install dcgm-exporter gpu-helm-charts/dcgm-exporter --namespace monitor --create-namespace \ --set serviceMonitor.enabled=false \ --set image.repository=hubimage/nvidia-dcgm-exporter \ --set image.tag=3.3.3-3.3.0-ubuntu22.04 \ --set nodeSelector.&amp;#34;accelerator\/provider&amp;#34;=nvidia-gpu \ --version 3.3.1 需要给 NVIDIA GPU 节点打上标签 1 kubectl label node &amp;lt;node-name&amp;gt; accelerator/provider=nvidia-gpu 1.2 指标 GPU 利用率 指标名称 指标类型 单位 描述 DCGM_FI_DEV_GPU_UTIL Gauge % GPU 利用率 DCGM_FI_DEV_MEM_COPY_UTIL Gauge</description><dc:creator>微信公众号</dc:creator><category>博文</category><category>Kubernetes</category><category>AI</category><category>指标</category><category>采集</category></item><item><title>源码分析 Kubernetes 对 Pod IP 的管理</title><link>https://www.chenshaowen.com/blog/source-analysis-kubernetes-management-of-pod-ip.html</link><pubDate>Thu, 02 Nov 2023 19:45:31 +0000</pubDate><atom:modified>Thu, 02 Nov 2023 19:45:31 +0000</atom:modified><guid>https://www.chenshaowen.com/blog/source-analysis-kubernetes-management-of-pod-ip.html</guid><description>1. kube-controller-manager 对网段的管理 在 kube-controller-manager 有众多控制器，与 Pod IP 相关的是 NodeIpamController。 NodeIpamController 控制器主要是管理节点的 podcidr，当有新节点加入集群时，分配一个子网段给节点；当节点删除时，回收子网段。 每个节点的子网段不会重叠，每个节点都能够独立</description><dc:creator>微信公众号</dc:creator><category>源码分析</category><category>Kubernetes</category><category>Pod</category><category>IP</category><category>网络</category><category>博文</category></item><item><title>流水线构建时，凭证作用域问题</title><link>https://www.chenshaowen.com/blog/the-scope-of-credential-in-building.html</link><pubDate>Thu, 06 Jul 2023 00:00:00 +0000</pubDate><atom:modified>Thu, 06 Jul 2023 00:00:00 +0000</atom:modified><guid>https://www.chenshaowen.com/blog/the-scope-of-credential-in-building.html</guid><description>在 client 中已经看到 Docker CLI 在给 Docker Daemon 发生构建上下文时，通过设置 X-Registry-Config 传递凭证，但在最近的构建反馈中，还是会出现一些无法解释的现象，本篇主要是进行一些基础的测试，以便于更好排查问题。 1. 宿主机 Docker 下构建 Docker Daemon 以 root 用户权限启动。 未登录任何账户 1 2 3 4 su ansible echo &amp;#34;FROM harbor.chenshaowen.com/private/test:v1&amp;#34; | sudo</description><dc:creator>微信公众号</dc:creator><category>博文</category><category>CICD</category><category>DevOps</category><category>凭证</category><category>Kubernetes</category></item><item><title>如何给 Kubernetes 应用设置 HPA 以及相关参数</title><link>https://www.chenshaowen.com/blog/how-to-set-hpa-for-kubernetes-app.html</link><pubDate>Thu, 08 Jun 2023 00:00:00 +0000</pubDate><atom:modified>Thu, 08 Jun 2023 00:00:00 +0000</atom:modified><guid>https://www.chenshaowen.com/blog/how-to-set-hpa-for-kubernetes-app.html</guid><description>1. 业务背景 当企业达到一定规模时，完全依赖于公有云基础设施，IT 成本会很高。 采购物理机器的成本可以摊薄到未来 3~5 年，之后机器并不会报废，而是会继续超期服役。私有云需要配比一定运维人员、购买专线带宽、机房费用等，IT 服务达到一定规模才能有效降低成</description><dc:creator>微信公众号</dc:creator><category>博文</category><category>Kubernetes</category><category>HPA</category><category>Pod</category></item><item><title>使用 KEDA 自动伸缩 Kubernetes 应用</title><link>https://www.chenshaowen.com/blog/autoscale-kubernetes-applications-with-keda.html</link><pubDate>Thu, 18 May 2023 00:00:00 +0000</pubDate><atom:modified>Thu, 18 May 2023 00:00:00 +0000</atom:modified><guid>https://www.chenshaowen.com/blog/autoscale-kubernetes-applications-with-keda.html</guid><description>1. HPA VS KEDA HPA 也实现了: 自定义指标的弹性 Scale to Zero 这些与 KEDA 相比较，并不算劣势了。 真正的差别在于 HPA 只能利用监控数据进行伸缩，而 KEDA 可以利用更多数据来源进行伸缩，比如队列消息、数据库、Redis 等，当然也包括监控数据。 从 Kubernetes-based Event Driven Autoscaler (KEDA) 项目的名字就可以看出，K</description><dc:creator>微信公众号</dc:creator><category>博文</category><category>Kubernetes</category><category>KEDA</category><category>集群弹性</category></item><item><title>WebAssembly Serverless 飞入寻常百姓家</title><link>https://www.chenshaowen.com/blog/webassembly-serverless-will-be-a-common-arch-in-the-future.html</link><pubDate>Tue, 09 May 2023 00:00:00 +0000</pubDate><atom:modified>Tue, 09 May 2023 00:00:00 +0000</atom:modified><guid>https://www.chenshaowen.com/blog/webassembly-serverless-will-be-a-common-arch-in-the-future.html</guid><description>1. 基于容器的 Serverless 无法支撑下一代应用的形态 如上图，我们正经历着一次运行时态的变革。 从裸金属机到虚拟机，应用不在受限于本地服务器的数量、机房稳定性，具有更好的弹性和可用性。 从虚拟机到容器，应用不再受限于操作系统、配置漂移，具有更好的可移植性和可扩</description><dc:creator>微信公众号</dc:creator><category>WebAssembly</category><category>Serverless</category><category>Kubernetes</category><category>应用</category></item><item><title>优化 Tekton 执行克隆任务慢问题，节省约 30 秒</title><link>https://www.chenshaowen.com/blog/optimizing-the-slow-of-tekton-clone-task.html</link><pubDate>Thu, 16 Feb 2023 00:00:00 +0000</pubDate><atom:modified>Thu, 16 Feb 2023 00:00:00 +0000</atom:modified><guid>https://www.chenshaowen.com/blog/optimizing-the-slow-of-tekton-clone-task.html</guid><description>1. 现象 - Tekton 克隆代码任务慢 在执行克隆任务时，Tekton 很费时间，多仓库下一般都需要 2 分 30 秒左右。如下图: 仅克隆的流水线就需要执行 2 分钟 16 秒，而克隆脚本实际上仅执行 1-3 秒。其中大部分时间花在了哪里？能不能减少？这是本篇主要想讨论的问题。 2. 分析克</description><dc:creator>微信公众号</dc:creator><category>博文</category><category>Tekton</category><category>Kubernetes</category><category>CI</category><category>CICD</category><category>DevOps</category></item><item><title>如何给 Kubernetes 服务添加 Basic 认证访问</title><link>https://www.chenshaowen.com/blog/how-to-add-basic-auth-to-kubernetes-service.html</link><pubDate>Sun, 05 Feb 2023 00:00:00 +0000</pubDate><atom:modified>Sun, 05 Feb 2023 00:00:00 +0000</atom:modified><guid>https://www.chenshaowen.com/blog/how-to-add-basic-auth-to-kubernetes-service.html</guid><description>1. 部署 Ingress Controller 查看 Kubernetes 版本 1 2 3 4 kubectl version --short Client Version: v1.21.4 Server Version: v1.21.4 查找兼容的 Nginx Ingress 版本 Helm Chart version Helm Chart 最高可用版本 K8s 适配版本 3.x.x 3.36.0 1.16+ 4.x.x 4.4.2 1.19+ 参考: https://github.com/kubernetes/ingress-nginx 安装 Nginx Ingress Controller 1 2 3 helm upgrade --install ingress-nginx ingress-nginx \ --repo https://kubernetes.github.io/ingress-nginx \ --namespace ingress-nginx --create-namespace --version v4.4.2 查看服务 1 2 3 4 5 6 7 8 9 10 11 12 kubectl -n ingress-nginx get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE ingress-nginx-controller LoadBalancer 10.233.11.232 &amp;lt;pending&amp;gt; 80:30914/TCP,443:31493/TCP 14m ingress-nginx-controller-admission ClusterIP 10.233.56.67 &amp;lt;none&amp;gt; 443/TCP 14m kae@node1:~$</description><dc:creator>微信公众号</dc:creator><category>博文</category><category>Kubernetes</category><category>认证</category><category>Ingress</category></item><item><title>如何修复重装系统后的 Kubernetes Master 节点</title><link>https://www.chenshaowen.com/blog/how-to-repair-k8s-master-after-reinstalling-os.html</link><pubDate>Tue, 17 Jan 2023 00:00:00 +0000</pubDate><atom:modified>Tue, 17 Jan 2023 00:00:00 +0000</atom:modified><guid>https://www.chenshaowen.com/blog/how-to-repair-k8s-master-after-reinstalling-os.html</guid><description>最近碰到两次，因故障需要重装主机系统。其中一次 Etcd 只剩下一个节点，导致整个集群宕机半个小时才恢复。本篇主要记录的是新系统 Ubuntu 20.04 初始化的过程，完成初始化之后采用优秀的集群安装工具 Kubekey 的 add nodes 命令，无需修改配置文件，一键就将节点重新加入了集群。 1. 恢复 Etcd</description><dc:creator>微信公众号</dc:creator><category>博文</category><category>Kubernetes</category><category>修复</category><category>运维</category></item><item><title>Tekton 压力测试及构建集群参数优化</title><link>https://www.chenshaowen.com/blog/tekton-stress-test-and-optimize-k8s-cluster.html</link><pubDate>Thu, 10 Nov 2022 00:00:00 +0000</pubDate><atom:modified>Thu, 10 Nov 2022 00:00:00 +0000</atom:modified><guid>https://www.chenshaowen.com/blog/tekton-stress-test-and-optimize-k8s-cluster.html</guid><description>1. 测试目的 调优构建集群的参数 探测 Tekton 并发流水线数量上限 给出单个集群最佳并发上限值 2. 相关组件及机器配置 Kubernetes 版本 v1.21.4 Tekton 版本 v0.24.1，与生产版本保持一致 OpenEBS 版本 localpv 版本 3.3.0，与生产版本保持一致 集群节点配置，共五个节点，四个用于构建 node1 master</description><dc:creator>微信公众号</dc:creator><category>Tekton</category><category>Kubernetes</category><category>大集群</category><category>调优</category><category>博文</category></item><item><title>Kubernetes 应用 troubleshooting</title><link>https://www.chenshaowen.com/blog/kubernetes-app-troubleshooting.html</link><pubDate>Tue, 01 Nov 2022 03:00:00 +0000</pubDate><atom:modified>Tue, 01 Nov 2022 03:00:00 +0000</atom:modified><guid>https://www.chenshaowen.com/blog/kubernetes-app-troubleshooting.html</guid><description>设置合理的 Req 和 Limit 不设置 Req 和 Limit，当应用的 CPU、MEM 暴涨时，会危害同一节点上的其他 Pod，甚至导致集群节点一个接一个被压垮。 Req 和 Limit 一共有四个值，如果只设置部分值，当节点资源使用率达到 Kubelet 预设值时，Kubelet 会驱逐 Pod，驱逐的顺序</description><dc:creator>微信公众号</dc:creator><category>博文</category><category>Kubernetes</category></item><item><title>Kubernetes 集群 troubleshooting</title><link>https://www.chenshaowen.com/blog/kubernetes-cluster-troubleshooting.html</link><pubDate>Tue, 01 Nov 2022 01:00:00 +0000</pubDate><atom:modified>Tue, 01 Nov 2022 01:00:00 +0000</atom:modified><guid>https://www.chenshaowen.com/blog/kubernetes-cluster-troubleshooting.html</guid><description>FailedCreatePodSandBox 错误 Error response from daemon: OCI runtime create failed: container_linux.go:380: starting container process caused: process_linux.go:402: getting the final child's pid from pipe caused: EOF: unknown 处理 清理 cache 1 echo 3 &amp;gt; /proc/sys/vm/drop_caches 原因 内存碎片过多 calico-node 不停重启 increase max user 错误 runtime: failed to create new OS thread (have 11 already; errno=11)，runtime: may need to increase max user processes (ulimit -u) 处理 增加 ulimit 限制额度 1 ulimit -u unlimited 原因 用户进程数耗尽 calico-node BIRD is not ready 错</description><dc:creator>微信公众号</dc:creator><category>博客</category><category>Kubernetes</category></item><item><title>如何修复变更 IP 之后的 Kubernetes 集群</title><link>https://www.chenshaowen.com/blog/how-to-repair-the-kubernetes-cluster-after-changing-ip.html</link><pubDate>Tue, 25 Oct 2022 00:00:00 +0000</pubDate><atom:modified>Tue, 25 Oct 2022 00:00:00 +0000</atom:modified><guid>https://www.chenshaowen.com/blog/how-to-repair-the-kubernetes-cluster-after-changing-ip.html</guid><description>记录一次因为 IP 变更导致集群故障的修复过程。有两个集群，一个是单节点(allinone)，另一个是四节点(3 master 1 node)的集群。 1. 更新 Etcd 证书 【在每个 Etcd 节点】备份 Etcd 证书 1 cp -R /etc/ssl/etcd/ssl /etc/ssl/etcd/ssl-bak 查看 Etcd 证书中的域 1 2 3 openssl x509 -in /etc/ssl/etcd/ssl/node-node1.pem -noout -text|grep DNS DNS:etcd, DNS:etcd.kube-system, DNS:etcd.kube-system.svc, DNS:etcd.kube-system.svc.cluster.local, DNS:localhost, DNS:node1, IP Address:127.0.0.1, IP Address:0:0:0:0:0:0:0:1, IP Address:x.x.x.1 需要</description><dc:creator>微信公众号</dc:creator><category>博文</category><category>Kubernetes</category><category>运维</category><category>修复</category></item><item><title>Kubernetes 网络流量转发详解</title><link>https://www.chenshaowen.com/blog/kubernetes-network-packets.html</link><pubDate>Sat, 17 Sep 2022 00:00:00 +0000</pubDate><atom:modified>Sat, 17 Sep 2022 00:00:00 +0000</atom:modified><guid>https://www.chenshaowen.com/blog/kubernetes-network-packets.html</guid><description>本文翻译自 https://learnk8s.io/kubernetes-network-packets，并没有逐字翻译，带入了些自己的理解。 阅读本文，你可以了解在 Kubernetes 内外，数据包是如何转发的，从原始的 Web 请求开始，到托管应用程序的容器。 Kubernetes 网络要</description><dc:creator>微信公众号</dc:creator><category>翻译</category><category>Kubernetes</category><category>网络</category></item><item><title>拉取大镜像报错</title><link>https://www.chenshaowen.com/blog/pull-a-large-image-and-get-an-error.html</link><pubDate>Sat, 10 Sep 2022 02:00:00 +0000</pubDate><atom:modified>Sat, 10 Sep 2022 02:00:00 +0000</atom:modified><guid>https://www.chenshaowen.com/blog/pull-a-large-image-and-get-an-error.html</guid><description>1，接上一回，共享存储优化海外镜像的拉取 在基于 Harbor 和 Registry 的镜像管理分发方案的基础上，最近又做了一个优化。 之前的方案是，在每个区域，使用一台低配大磁盘的机器，部署一个 Mirror Cache 缓存镜像。这样带来一个问题，就是每个区域都需要拉取一个镜像，如果有 N 个区域，</description><dc:creator>微信公众号</dc:creator><category>博文</category><category>Kubernetes</category><category>镜像仓库</category><category>镜像</category></item><item><title>使用 Kindling 观测 Kubernetes 的网络连接</title><link>https://www.chenshaowen.com/blog/insight-kubernetes-network-by-kindling.html</link><pubDate>Sat, 10 Sep 2022 00:00:00 +0000</pubDate><atom:modified>Sat, 10 Sep 2022 00:00:00 +0000</atom:modified><guid>https://www.chenshaowen.com/blog/insight-kubernetes-network-by-kindling.html</guid><description>最近有一个需求，收集 Kubernetes 的外网访问情况。因此对相关项目进行了调用和试用，本篇主要是介绍如何安装 Kindling，配置 Grafana 查看 Kubernetes 网络连接数据。 1. 什么是 Kindling Kindling 解决的是，在不入侵应用的前提下，如何观测网络的问题，其功能主要是通过暴露内核事件来实现观测。</description><dc:creator>微信公众号</dc:creator><category>博文</category><category>Kubernetes</category><category>Kindling</category><category>网络</category></item><item><title>OpenEBS 证书过期导致服务不可用</title><link>https://www.chenshaowen.com/blog/expiration-of-the-openebs-cert.html</link><pubDate>Fri, 09 Sep 2022 00:00:00 +0000</pubDate><atom:modified>Fri, 09 Sep 2022 00:00:00 +0000</atom:modified><guid>https://www.chenshaowen.com/blog/expiration-of-the-openebs-cert.html</guid><description>1. 安装方式 1 2 kubectl apply -f https://openebs.github.io/charts/openebs-operator.yaml kubectl patch storageclass openebs-hostpath -p &amp;#39;{&amp;#34;metadata&amp;#34;: {&amp;#34;annotations&amp;#34;:{&amp;#34;storageclass.kubernetes.io/is-default-class&amp;#34;:&amp;#34;true&amp;#34;}}}&amp;#39; OpenEBS 主要用来给 Tekton 流水线作为默认的存储使用。之前，我也试过 Longhorn，但是高峰期扛不住，流水线 Pending。而卸载 Longhorn 之后有残留，导致 kube-apiserver 一直报错，最后花了很大力气才删除。 2. Kubernetes 集群证书过期之后，OpenE</description><dc:creator>微信公众号</dc:creator><category>博文</category><category>OpenEBS</category><category>存储</category><category>Kubernetes</category><category>Tekton</category></item><item><title>如何预估 Kubernetes 集群中监控组件的资源消耗</title><link>https://www.chenshaowen.com/blog/how-to-estimate-the-resource-request-at-monitoring-kubernetes.html</link><pubDate>Tue, 23 Aug 2022 00:00:00 +0000</pubDate><atom:modified>Tue, 23 Aug 2022 00:00:00 +0000</atom:modified><guid>https://www.chenshaowen.com/blog/how-to-estimate-the-resource-request-at-monitoring-kubernetes.html</guid><description>本文描述的监控指标，仅包含 Kubernetes 基础的指标，不包含业务相关指标，相关组件为 prometheus-server、kube-state-metrics、node-exporter，数据的保存周期为 3 天。 1. 集群中监控相关组件 1 2 3 4 helm -n monitor list NAME NAMESPACE REVISION UPDATED STATUS</description><dc:creator>微信公众号</dc:creator><category>博文</category><category>Kubernetes</category><category>计算</category><category>预估</category></item><item><title>Kubernetes Job 创建了近 3W Pod，差点导致严重事故</title><link>https://www.chenshaowen.com/blog/kubernetes-job-create-3w-pod-attack-cluster.html</link><pubDate>Wed, 17 Aug 2022 00:00:00 +0000</pubDate><atom:modified>Wed, 17 Aug 2022 00:00:00 +0000</atom:modified><guid>https://www.chenshaowen.com/blog/kubernetes-job-create-3w-pod-attack-cluster.html</guid><description>1. 相关背景 早上 10:00 因同事需求，我通过工具在集群上创建 Kubernetes Job 执行任务。 工具创建 Job 时，会拿到集群上的全部节点，然后逐个绑定节点创建 Job。例如，如下集群: 1 2 3 4 5 6 7 8 9 10 kubectl get node NAME STATUS ROLES AGE VERSION node2 Ready control-plane,master,worker 64d v1.16.11 node3 Ready control-plane,master,worker 64d v1.16.11 node4 Ready control-plane,master,worker 64d v1.16.11 node5 Ready worker 64d v1.16.11 node6 Ready worker 64d v1.16.11 node7 NotReady,SchedulingDisabled worker 64d</description><dc:creator>微信公众号</dc:creator><category>Kubernetes</category><category>Job</category><category>Pod</category><category>事故</category><category>博文</category></item><item><title>如何劫持 docker.io 的镜像流量到私有仓库</title><link>https://www.chenshaowen.com/blog/hijack-docker-io-req-to-private-repository.html</link><pubDate>Mon, 18 Jul 2022 00:00:00 +0000</pubDate><atom:modified>Mon, 18 Jul 2022 00:00:00 +0000</atom:modified><guid>https://www.chenshaowen.com/blog/hijack-docker-io-req-to-private-repository.html</guid><description>1. 自签 *.docker.io 域名证书 1.1 创建 CA 证书 生成 CA 证书私钥 1 openssl genrsa -out ca.key 4096 生成 CA 证书 1 2 3 4 openssl req -x509 -new -nodes -sha512 -days 3650 \ -subj &amp;#34;/C=CN/ST=Beijing/L=Beijing/O=example/OU=Personal/CN=chenshaowen.com&amp;#34; \ -key ca.key \ -out ca.crt 1.2 创建 *.docker.io 域名证书 生成私钥 1 openssl genrsa -out docker.io.key 4096 生成证书签名请求 CSR 1 2 3 4 openssl req -sha512 -new \ -subj &amp;#34;/C=CN/ST=Beijing/L=Beijing/O=example/OU=Personal/CN=*.docker.io&amp;#34; \ -key docker.io.key \ -out docker.io.csr 生成 x509 v3 扩展 1 2 3 4 5 6 7 8 9 10 11 cat &amp;gt; v3.ext &amp;lt;&amp;lt;-EOF authorityKeyIdentifier=keyid,issuer basicConstraints=CA:FALSE</description><dc:creator>微信公众号</dc:creator><category>博文</category><category>Jenkins</category><category>Kubernetes</category></item><item><title>在 Kubernetes 集群上部署 Elasticsearch 栈</title><link>https://www.chenshaowen.com/blog/how-to-deploy-the-elasticsearch-stack-on-kubernetes.html</link><pubDate>Wed, 06 Jul 2022 00:00:00 +0000</pubDate><atom:modified>Wed, 06 Jul 2022 00:00:00 +0000</atom:modified><guid>https://www.chenshaowen.com/blog/how-to-deploy-the-elasticsearch-stack-on-kubernetes.html</guid><description>如果采用 Logstash 集中接收 Filebeat 的日志输入，容易造成单点瓶颈；如果采用 Kafka 接收 Filebeat 的日志输入，日志的时效性又得不到保障。这里直接将 Filebeat 采集的日志直接输出到 Elasticsearch。 1. 准备工作 节点规划 这里没有区分 master、data、client 节点，而是</description><dc:creator>微信公众号</dc:creator><category>博文</category><category>Kubernetes</category><category>Elasticsearch</category><category>日志</category></item><item><title>如何设置 Pod 到指定节点运行</title><link>https://www.chenshaowen.com/blog/how-to-set-up-pod-to-run-to-a-specified-node.html</link><pubDate>Mon, 27 Jun 2022 00:00:00 +0000</pubDate><atom:modified>Mon, 27 Jun 2022 00:00:00 +0000</atom:modified><guid>https://www.chenshaowen.com/blog/how-to-set-up-pod-to-run-to-a-specified-node.html</guid><description>1. 创建负载时，通过 nodeSelector 指定 Node 给节点添加标签 1 kubectl label node node2 project=A 指定 nodeSelector 创建工作负载 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 cat &amp;lt;&amp;lt;EOF | kubectl apply -f - apiVersion: apps/v1 kind: Deployment metadata: name: nginx-nodeselector spec: replicas: 1 selector: matchLabels: app: nginx-nodeselector template: metadata: labels: app: nginx-nodeselector spec: nodeSelector: project: A containers: - name: nginx image: nginx EOF 查看工作负载 1 2 3 4 kubectl get pod -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES</description><dc:creator>微信公众号</dc:creator><category>博文</category><category>Pod</category><category>Kubernetes</category><category>调度</category></item><item><title>多机房下的 Kubernetes 演进</title><link>https://www.chenshaowen.com/blog/evolution-of-kubernetes-under-multipl-location.html</link><pubDate>Sat, 18 Jun 2022 12:09:53 +0000</pubDate><atom:modified>Sat, 18 Jun 2022 12:09:53 +0000</atom:modified><guid>https://www.chenshaowen.com/blog/evolution-of-kubernetes-under-multipl-location.html</guid><description>1. 应用架构与业务发展、运维能力匹配 在行业会议、文档博客中，我们时常能见到各种优秀的解决方案，但是如果直接照搬到自己的业务，却又频频碰壁。因为，这些技术方案是特定的业务场景孵化出来的，不同的业务形态、不同的业务规模、不同的业务发展阶段都会影响</description><dc:creator>微信公众号</dc:creator><category>博文</category><category>多机房</category><category>Kubernetes</category><category>架构</category><category>拓扑</category></item><item><title>descheduler 二次调度让 Kubernetes 负载更均衡</title><link>https://www.chenshaowen.com/blog/descheduler-makes-kubernetes-load-more-balanced.html</link><pubDate>Sat, 11 Jun 2022 00:00:00 +0000</pubDate><atom:modified>Sat, 11 Jun 2022 00:00:00 +0000</atom:modified><guid>https://www.chenshaowen.com/blog/descheduler-makes-kubernetes-load-more-balanced.html</guid><description>1. 为什么需要二次调度 Kubernetes 调度器的作用是将 Pod 绑定到某一个最佳的节点。为了实现这一功能，调度器会需要进行一系列的筛选和打分。 Kubernetes 的调度是基于 Request，但是每个 Pod 的实际使用值是动态变化的。经过一段时间的运行之后，节点的负载并不均衡。一些节点负载</description><dc:creator>微信公众号</dc:creator><category>博文</category><category>Kubernetes</category><category>descheduler</category><category>调度</category></item><item><title>如何更新 Kubernetes 证书</title><link>https://www.chenshaowen.com/blog/how-to-renew-kubernetes-certs-manually.html</link><pubDate>Fri, 10 Jun 2022 00:00:00 +0000</pubDate><atom:modified>Fri, 10 Jun 2022 00:00:00 +0000</atom:modified><guid>https://www.chenshaowen.com/blog/how-to-renew-kubernetes-certs-manually.html</guid><description>在默认情况下，Kubernetes 的证书每隔一年需要 renew 一次，下面是记录的一次证书更新过程。 1. 查看证书 在 Master 节点上查看证书过期时间: 1 2 3 4 5 6 7 8 9 10 11 12 13 kubeadm certs check-expiration CERTIFICATE EXPIRES RESIDUAL TIME CERTIFICATE AUTHORITY EXTERNALLY MANAGED admin.conf Apr 02, 2023 09:53 UTC 296d no apiserver Apr 02, 2023 09:53 UTC 296d ca no apiserver-kubelet-client Apr 02, 2023 09:53 UTC 296d ca no controller-manager.conf Apr 02, 2023 09:53 UTC</description><dc:creator>微信公众号</dc:creator><category>博文</category><category>Kubernetes</category><category>证书</category><category>运维</category></item><item><title>如何查看 Tekton 的流水线指标</title><link>https://www.chenshaowen.com/blog/how-to-insight-the-pipeline-of-tekton.html</link><pubDate>Tue, 07 Jun 2022 00:00:00 +0000</pubDate><atom:modified>Tue, 07 Jun 2022 00:00:00 +0000</atom:modified><guid>https://www.chenshaowen.com/blog/how-to-insight-the-pipeline-of-tekton.html</guid><description>1. 抓取 Tekton Metrics 新增 ConfigMap 配置文件 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 cat &amp;lt;&amp;lt;EOF | kubectl apply -f - apiVersion: v1 kind: ConfigMap metadata: name: config-observability namespace: tekton-pipelines labels: app.kubernetes.io/instance: default app.kubernetes.io/part-of: tekton-pipelines data: metrics.backend-destination: prometheus metrics.taskrun.level: &amp;#34;task&amp;#34; metrics.taskrun.duration-type: &amp;#34;histogram&amp;#34; metrics.pipelinerun.level: &amp;#34;pipeline&amp;#34; metrics.pipelinerun.duration-type: &amp;#34;histogram&amp;#34; EOF 修改 data 中的配置，会改变上报指标的粒度，甚至会严重影响 Prometheus 的性能，需要谨慎修改。 重启 Tekton 1 kubectl -n tekton-pipelines rollout restart deployment tekton-pipelines-controller [可选] 将 tekton-pipelines-controller 设置为 NodePort</description><dc:creator>微信公众号</dc:creator><category>博文</category><category>Kubernetes</category><category>Tekton</category><category>监控</category><category>指标</category></item><item><title>如何采集 Kubernetes 对象的 labels 和 annotations</title><link>https://www.chenshaowen.com/blog/how-to-collect-labels-and-annotations-of-kubernetes-objects.html</link><pubDate>Thu, 02 Jun 2022 01:00:00 +0000</pubDate><atom:modified>Thu, 02 Jun 2022 01:00:00 +0000</atom:modified><guid>https://www.chenshaowen.com/blog/how-to-collect-labels-and-annotations-of-kubernetes-objects.html</guid><description>1. 为什么需要 kube-status-metrics Kubernetes 的监控主要关注两类指标: 基础性能指标 CPU、内存、磁盘、网络等指标，可以通过 DaemonSet 部署 node-exporter，由 Prometheus 抓取相关指标。 资源对象指标 Deployment 的副本数量、Pod 的运行状态等。这些指标需要 kube-status-metrics 轮询 Kubernetes 的 API 查询，并暴露给 Prometheus 才能够看到</description><dc:creator>微信公众号</dc:creator><category>博文</category><category>Kubernetes</category><category>监控</category><category>指标</category><category>采集</category></item><item><title>给 Kubernetes 添加 imagePullSecrets</title><link>https://www.chenshaowen.com/blog/how-to-add-imagepullsecrets-for-kubernetes.html</link><pubDate>Fri, 15 Apr 2022 00:00:00 +0000</pubDate><atom:modified>Fri, 15 Apr 2022 00:00:00 +0000</atom:modified><guid>https://www.chenshaowen.com/blog/how-to-add-imagepullsecrets-for-kubernetes.html</guid><description>1. 通过 kubectl create 添加 1 kubectl create secret docker-registry mypullsecret --docker-server=harbor.chenshaowen.com --docker-username=robot-test --docker-password=xxxxxx 通过 kubectl create 可以直接添加拉取镜像的凭证。 2. 通过 ~/.docker/config.json 添加 使用账户密码登录镜像仓库 1 docker login harbor.chenshaowen.com:5000 1 docker login harbor.chenshaowen.com 可以添加多个。 查看本地保存的凭证 1 2 3 4 5 6 7 8 9 10 11 12 cat ~/.docker/config.json { &amp;#34;auths&amp;#34;: { &amp;#34;harbor.chenshaowen.com:5000&amp;#34;: { &amp;#34;auth&amp;#34;: &amp;#34;xxxxxx&amp;#34; }, &amp;#34;harbor.chenshaowen.com&amp;#34;: { &amp;#34;auth&amp;#34;: &amp;#34;xxxxxx&amp;#34; } } } 对凭证进行 base64 编码 1 2 3 cat ~/.docker/config.json |base64 -w</description><dc:creator>微信公众号</dc:creator><category>博文</category><category>Kubernetes</category></item><item><title>面向全球的镜像分发网络</title><link>https://www.chenshaowen.com/blog/a-global-images-distribution-network.html</link><pubDate>Sun, 27 Mar 2022 14:00:00 +0000</pubDate><atom:modified>Sun, 27 Mar 2022 14:00:00 +0000</atom:modified><guid>https://www.chenshaowen.com/blog/a-global-images-distribution-network.html</guid><description>1. 全球的网络规划 很多面向全球的多区域基础设施，在设计之初并没有在网络规划上花费太多心思。当业务复杂到一定程度时，又被逼着进行网络调整和优化。而任何网络上的大调整，都将对业务产生巨大影响。最终会陷入进退两难之地，只能投入更多人力，背上历史包袱</description><dc:creator>微信公众号</dc:creator><category>博文</category><category>镜像</category><category>Habor</category><category>网络</category><category>Kubernetes</category></item><item><title>如何升级 Kubernetes 集群</title><link>https://www.chenshaowen.com/blog/how-upgrade-kubernetes-cluster.html</link><pubDate>Mon, 07 Mar 2022 00:00:00 +0000</pubDate><atom:modified>Mon, 07 Mar 2022 00:00:00 +0000</atom:modified><guid>https://www.chenshaowen.com/blog/how-upgrade-kubernetes-cluster.html</guid><description>升级思路是，驱逐负载、摘除流量之后，先升级控制节点，后升级工作节点。 1. 查看集群版本 1 2 3 4 kubectl version Client Version: version.Info{Major:&amp;#34;1&amp;#34;, Minor:&amp;#34;20&amp;#34;, GitVersion:&amp;#34;v1.20.4&amp;#34;, GitCommit:&amp;#34;e87da0bd6e03ec3fea7933c4b5263d151aafd07c&amp;#34;, GitTreeState:&amp;#34;clean&amp;#34;, BuildDate:&amp;#34;2021-02-18T16:12:00Z&amp;#34;, GoVersion:&amp;#34;go1.15.8&amp;#34;, Compiler:&amp;#34;gc&amp;#34;, Platform:&amp;#34;linux/amd64&amp;#34;} Server Version: version.Info{Major:&amp;#34;1&amp;#34;, Minor:&amp;#34;22&amp;#34;, GitVersion:&amp;#34;v1.22.0&amp;#34;, GitCommit:&amp;#34;c2b5237ccd9c0f1d600d3072634ca66cefdf272f&amp;#34;, GitTreeState:&amp;#34;clean&amp;#34;, BuildDate:&amp;#34;2021-08-04T17:57:25Z&amp;#34;, GoVersion:&amp;#34;go1.16.6&amp;#34;, Compiler:&amp;#34;gc&amp;#34;, Platform:&amp;#34;linux/amd64&amp;#34;} 当前版本是 1.22，由于 kubeadm 不允许跨版本升级，这里准备升级到 1.23 。 2. 添加 Kubernetes 安装源 CentOS 操作系统: 1 2 3 4 5 6 7</description><dc:creator>微信公众号</dc:creator><category>博文</category><category>Kubernetes</category><category>升级</category><category>运维</category></item><item><title>如何在 Kubernetes Pod 中注入环境变量及优先级问题</title><link>https://www.chenshaowen.com/blog/injecting-env-vars-to-kubernetes-pod-and-priority.html</link><pubDate>Fri, 04 Mar 2022 00:00:00 +0000</pubDate><atom:modified>Fri, 04 Mar 2022 00:00:00 +0000</atom:modified><guid>https://www.chenshaowen.com/blog/injecting-env-vars-to-kubernetes-pod-and-priority.html</guid><description>1. Kubernetes Pod 引用环境变量的几种方式 1.1 直接 Key/Value 可以直接设置 Value 值，也可以将当前 Pod 的信息作为 Value 值。 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 apiVersion: v1 kind: Pod metadata: name: envar-demo labels: purpose: demonstrate-envars spec: containers: - name: envar-demo-container image: gcr.io/google-samples/node-hello:1.0 env: - name: DEMO_GREETING value: &amp;#34;Hello from the environment&amp;#34; - name: DEMO_FAREWELL value: &amp;#34;Such a sweet sorrow&amp;#34; - name: MY_NODE_NAME valueFrom: fieldRef: fieldPath: spec.nodeName - name: MY_POD_NAME valueFrom: fieldRef: fieldPath: metadata.name 1.2 从 Secret 引用 有两</description><dc:creator>微信公众号</dc:creator><category>博文</category><category>Kubernetes</category><category>环境变量</category></item><item><title>给 Kubernetes 集群新增外部 DNS 服务</title><link>https://www.chenshaowen.com/blog/add-outer-dns-server-to-kubernetes-cluster.html</link><pubDate>Thu, 24 Feb 2022 01:00:00 +0000</pubDate><atom:modified>Thu, 24 Feb 2022 01:00:00 +0000</atom:modified><guid>https://www.chenshaowen.com/blog/add-outer-dns-server-to-kubernetes-cluster.html</guid><description>1. 给主机添加 DNS 1.1 CentOS 第一种方法: /etc/resolv.conf 管理 DNS 禁用 NetworkManager 如果不禁用 NetworkManager，在重启 NetworkManager 组件之后，直接在 /etc/resolv.conf 中添加的 DNS 记录会丢失。 1 2 3 4 vim /etc/NetworkManager/NetworkManager.conf [main] dns=none 在 [main] 部分添加 dns=none。 此时重启 NetworkManager，对已经添加到 /etc/resolv.conf 的记录无影</description><dc:creator>微信公众号</dc:creator><category>博文</category><category>Kubernetes</category><category>DNS</category><category>运维</category></item><item><title>几种开源的 Kubernetes Web 端管理工具</title><link>https://www.chenshaowen.com/blog/several-open-source-kubernetes-web-management-tools.html</link><pubDate>Mon, 27 Dec 2021 01:00:00 +0000</pubDate><atom:modified>Mon, 27 Dec 2021 01:00:00 +0000</atom:modified><guid>https://www.chenshaowen.com/blog/several-open-source-kubernetes-web-management-tools.html</guid><description>最近在调研开源的 Kubernetes 管理平台，需求是能够管理内网的上百个集群。功能定位是辅助运维、向应用层提供能力，而非直接面向终端用户。 1. Kubernetes Dashboard 项目地址： https://github.com/kubernetes/dashboard 技术栈：Angular + Go 关键字： 单集群 K8s 资源管理 2. Kuboard 项目地址： https://github.com/eip-work/kuboard-press 技术栈：Vue 关键字： 多集群 K8s 资源管</description><dc:creator>微信公众号</dc:creator><category>博文</category><category>Kubernetes</category><category>对比</category><category>管理工具</category></item><item><title>常用的清理 Kubernetes 集群资源命令</title><link>https://www.chenshaowen.com/blog/common-commands-for-cleaning-up-kubernetes-cluster-resources.html</link><pubDate>Wed, 08 Dec 2021 00:00:00 +0000</pubDate><atom:modified>Wed, 08 Dec 2021 00:00:00 +0000</atom:modified><guid>https://www.chenshaowen.com/blog/common-commands-for-cleaning-up-kubernetes-cluster-resources.html</guid><description>长时间运行的集群，常会面临各种资源耗尽的问题，另外磁盘不足时 Kubelet 还会主动清理镜像增加不确定因素，本文提供了一些命令片段用于清理工作。 1. Kubernetes 基础对象清理 清理 Evicted 状态的 Pod 1 sudo kubectl get pods --all-namespaces -o wide | grep Evicted | awk &amp;#39;{print $1,$2}&amp;#39; | sudo xargs -L1 kubectl delete pod -n 清理 Error 状态的 Pod 1 sudo kubectl get pods --all-namespaces -o wide | grep</description><dc:creator>微信公众号</dc:creator><category>博文</category><category>Kubernetes</category><category>实践</category><category>清理</category><category>运维</category></item><item><title>容器下的两地三中心建设</title><link>https://www.chenshaowen.com/blog/the-construction-of-two-places-and-three-centers-under-the-container.html</link><pubDate>Sun, 10 Oct 2021 00:00:00 +0000</pubDate><atom:modified>Sun, 10 Oct 2021 00:00:00 +0000</atom:modified><guid>https://www.chenshaowen.com/blog/the-construction-of-two-places-and-three-centers-under-the-container.html</guid><description>1. 关于两地三中心 如上图，两地三中心的架构，是为了提高系统的容错、容灾的能力。当一个数据中心不可用时，能够将关键业务的流量切换到其他数据中心，可以抵御城市级的自然灾害。 两地指的是，地理上不同的两座城市，而三中心指的是: 生产中心 同城灾备中心 异地</description><dc:creator>微信公众号</dc:creator><category>博文</category><category>两地三中心</category><category>容器</category><category>Kubernetes</category></item><item><title>Kubevela 下的多集群应用</title><link>https://www.chenshaowen.com/blog/multi-cluster-applications-under-kubevela.html</link><pubDate>Fri, 17 Sep 2021 00:00:00 +0000</pubDate><atom:modified>Fri, 17 Sep 2021 00:00:00 +0000</atom:modified><guid>https://www.chenshaowen.com/blog/multi-cluster-applications-under-kubevela.html</guid><description>Kubevela 目前处于 1.1 版本。通常，我们认为 1.x 的版本是相对稳定的，可以尝试引入生产。在不断地跟踪和学习过程中，也感受到 Kubevela 的一些好的地方，这是一篇小结性的文档。 1. Kubevela 能解决什么问题 面向平台开发者 需要区分几个角色: 开发、运维、运维开发。开发面向的是业务需求，</description><dc:creator>微信公众号</dc:creator><category>博文</category><category>Kubevela</category><category>Kubernetes</category><category>应用</category></item><item><title>使用 Cilium 替换 Calico</title><link>https://www.chenshaowen.com/blog/how-to-use-cilium-to-replace-calico.html</link><pubDate>Thu, 16 Sep 2021 08:01:28 +0000</pubDate><atom:modified>Thu, 16 Sep 2021 08:01:28 +0000</atom:modified><guid>https://www.chenshaowen.com/blog/how-to-use-cilium-to-replace-calico.html</guid><description>1. 系统要求 Kubernetes， &amp;gt;= 1.16.0-0 Helm， &amp;gt;= 3.0 Linux 内核 &amp;gt;= 4.9.17 CentOS 7 升级内核过程，可以参考 Calico 下如何切换数据面到 eBPF 。 2. 卸载 Calico 删除集群资源 1 2 3 4 5 6 kubectl -n kube-system delete ds calico-node kubectl -n kube-system delete deploy calico-kube-controllers kubectl -n kube-system delete sa calico-node kubectl -n kube-system delete sa calico-kube-controllers kubectl -n kube-system delete cm calico-config kubectl -n kube-system delete secret calico-config 1 kubectl get crd | grep calico | awk &amp;#39;{print $1}&amp;#39; | xargs</description><dc:creator>微信公众号</dc:creator><category>博文</category><category>Cilium</category><category>Calico</category><category>Kubernetes</category><category>容器</category><category>网络</category></item><item><title>Calico 下如何切换数据面到 eBPF</title><link>https://www.chenshaowen.com/blog/how-to-switch-data-plane-to-ebpf.html</link><pubDate>Wed, 15 Sep 2021 07:33:55 +0000</pubDate><atom:modified>Wed, 15 Sep 2021 07:33:55 +0000</atom:modified><guid>https://www.chenshaowen.com/blog/how-to-switch-data-plane-to-ebpf.html</guid><description>1. 环境准备 1.1 Calico eBPF 要求 系统要求 Ubuntu 18.04.4+ Red Hat v8.2 Linux kernel v5.3+ 如果 Calico 没有检测到兼容的内核，将会回退到标准模式。 每个节点的 /sys/fs/bpf 都需要挂载有 BPF 文件系统 Calico 版本不低于 3.13 1.2 升级内核 这里使用的是 CentOS 7 操作系统： 1 2 3 uname -rv 3.10.0-957.el7.x86_64 #1 SMP Thu Nov 8 23:39:32 UTC 2018 内核版本不满足要求，因此需要升级内核</description><dc:creator>微信公众号</dc:creator><category>博文</category><category>网络</category><category>Calico</category><category>eBPF</category><category>Kubernetes</category></item><item><title>Kubernetes 安装问题 QA</title><link>https://www.chenshaowen.com/blog/the-question-and-answer-of-kubernetes-installation.html</link><pubDate>Fri, 10 Sep 2021 00:00:00 +0000</pubDate><atom:modified>Fri, 10 Sep 2021 00:00:00 +0000</atom:modified><guid>https://www.chenshaowen.com/blog/the-question-and-answer-of-kubernetes-installation.html</guid><description>1. Calico 1.1 BIRD is not ready 1 kubectl -n kube-system get pod calico-node-xxx 0/1 一直起不来，报错 calico/node is not ready: BIRD is not ready: BGP not established with 解决办法: Calico 默认使用 first-found，也就是从第一个找到的网卡中获取 NodeIP。虽然排除了 lo、docker0 等网卡，但是依然有一定概率会识别失败。需要手动修改，</description><dc:creator>微信公众号</dc:creator><category>整理</category><category>Kubernetes</category></item><item><title>如何给 Kubernetes Apiserver 新增访问入口</title><link>https://www.chenshaowen.com/blog/how-to-add-entrance-to-kubernetes-apiserver.html</link><pubDate>Wed, 08 Sep 2021 00:00:00 +0000</pubDate><atom:modified>Wed, 08 Sep 2021 00:00:00 +0000</atom:modified><guid>https://www.chenshaowen.com/blog/how-to-add-entrance-to-kubernetes-apiserver.html</guid><description>1. 本地怎么访问远程集群 在研发时，需要直接连接远端 Kubernetes 集群。通常的做法是，将 /etc/kubernetes/admin.conf 拷贝到本地 ~/.kube/kubeconfig。 但是 kubeconfig 的 server 地址是 kubernetes.default.svc。因此，我们需要配置一个 hosts: 1 1.1.1.1 kubernetes.default.svc 如果需要在不同集群之间切换</description><dc:creator>微信公众号</dc:creator><category>博文</category><category>Kubernetes</category><category>研发</category><category>证书</category></item><item><title>Kubernetes 下的网关服务 - APISIX</title><link>https://www.chenshaowen.com/blog/a-gateway-under-kubernetes-named-apisix.html</link><pubDate>Wed, 01 Sep 2021 00:00:00 +0000</pubDate><atom:modified>Wed, 01 Sep 2021 00:00:00 +0000</atom:modified><guid>https://www.chenshaowen.com/blog/a-gateway-under-kubernetes-named-apisix.html</guid><description>1. 几种常见网关的比较 Nginx, 模块化设计的反向代理软件，C 语言开发 OpenResty, 以 Nginx 为核心的 Web 开发平台，可以解析执行 Lua 脚本 Kong, OpenResty 的一个应用，是一个 API 网关，具有 API 管理和请求代理的功能，使用 PostgreSQL 存储 APISIX, 替换了 Kong 的 PostgreSQL 为 Etcd，基于 Nginx 的核心库实现 APISIX 的优势在于提供了 API 的管理</description><dc:creator>微信公众号</dc:creator><category>博文</category><category>Kubernetes</category><category>APISIX</category><category>微服务</category></item><item><title>多集群下的 Tekton 流水线</title><link>https://www.chenshaowen.com/blog/using-kubefed-to-distribute-tekton-resource-cross-cluster.html</link><pubDate>Sat, 26 Jun 2021 00:00:00 +0000</pubDate><atom:modified>Sat, 26 Jun 2021 00:00:00 +0000</atom:modified><guid>https://www.chenshaowen.com/blog/using-kubefed-to-distribute-tekton-resource-cross-cluster.html</guid><description>1. 多集群构建 Tekton 的优势 借助于 Kubernetes, Tekton 已经具备很好的弹性, 能够支持大规模构建。同时, 开发 Task 主要使用 Yaml 和 Shell, 这扩大了 Tekton 的各种场景适配范围。 上面是一张 Tekton 在多集群下的示意图。为什么 Tekton 需要多集群执行流水线？ 随时可变的 Kubernetes 集群。单一的 Kubernetes 集群, 无法满足运维的要求,</description><dc:creator>微信公众号</dc:creator><category>Tekton</category><category>KubeFed</category><category>Kubernetes</category><category>多集群</category><category>博文</category></item><item><title>在 Tekton 中如何实现审批功能</title><link>https://www.chenshaowen.com/blog/how-to-implement-approval-function-in-tekton.html</link><pubDate>Thu, 24 Jun 2021 00:00:00 +0000</pubDate><atom:modified>Thu, 24 Jun 2021 00:00:00 +0000</atom:modified><guid>https://www.chenshaowen.com/blog/how-to-implement-approval-function-in-tekton.html</guid><description>1. CICD 平台的基本功能 常见的 CICD 引擎并不适合直接提供给业务方使用。主要原因在于用户学习成本高、缺乏必要的鉴权、维护升级难度大。 我们通常会基于流程引擎，针对业务进行适配提高易用性，针对场景进行封装收敛复杂度，那么一个 CICD 平台需要具备哪些基本的功能呢？</description><dc:creator>微信公众号</dc:creator><category>博文</category><category>Tekton</category><category>Kubernetes</category><category>CICD</category><category>DevOps</category></item><item><title>如何修改 Kubelet 的启动参数</title><link>https://www.chenshaowen.com/blog/how-to-change-kubelet-config.html</link><pubDate>Wed, 23 Jun 2021 00:00:00 +0000</pubDate><atom:modified>Wed, 23 Jun 2021 00:00:00 +0000</atom:modified><guid>https://www.chenshaowen.com/blog/how-to-change-kubelet-config.html</guid><description>1. 编辑 Kubelet 配置文件 1 vim /etc/systemd/system/kubelet.service.d/10-kubeadm.conf 修改 Kubelet 相关参数 ExecStart=/usr/local/bin/kubelet $KUBELET_KUBECONFIG_ARGS $KUBELET_CONFIG_ARGS $KUBELET_KUBEADM_ARGS $KUBELET_EXTRA_ARGS 可以通过如下命令查看可选的参数: 1 kubelet --help 例如需要修改，KUBELET_KUBECONFIG_ARGS 的 --sync-frequency ， 同步容器配置的时钟周期，默认值是 1 min。其含义是，更新了容器挂载的配置文件 1 min 之后，容器</description><dc:creator>微信公众号</dc:creator><category>博文</category><category>Kubernetes</category><category>Kubelet</category></item><item><title>在 Kubernetes 中如何给 NodePort 配置 NetworkPolicy</title><link>https://www.chenshaowen.com/blog/how-to-configure-networkpolicy-for-nodeport.html</link><pubDate>Wed, 23 Jun 2021 00:00:00 +0000</pubDate><atom:modified>Wed, 23 Jun 2021 00:00:00 +0000</atom:modified><guid>https://www.chenshaowen.com/blog/how-to-configure-networkpolicy-for-nodeport.html</guid><description>1. 需求背景 如上图，业务方需要隔离 namespae 的服务，禁止 bar 空间的负载访问，而允许用户从 Load Balancer (LB) 通过 NodePort 访问服务。可以很容易地写出一个网络策略: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: test-network-policy namespace: foo spec: podSelector: matchLabels: {} policyTypes: - Ingress ingress: - from: - ipBlock: cidr: 10.2.3.4/32 - namespaceSelector: matchExpressions: - key: region operator: NotIn values: - bar 然而</description><dc:creator>微信公众号</dc:creator><category>博文</category><category>NetworkPolicy</category><category>网络</category><category>Kubernetes</category></item><item><title>/var/lib/docker 能不能挂载远端存储</title><link>https://www.chenshaowen.com/blog/can-we-mount-var-lib-docker-to-remote-storage.html</link><pubDate>Tue, 22 Jun 2021 00:00:00 +0000</pubDate><atom:modified>Tue, 22 Jun 2021 00:00:00 +0000</atom:modified><guid>https://www.chenshaowen.com/blog/can-we-mount-var-lib-docker-to-remote-storage.html</guid><description>不能 1. 问题背景 基于 Kubernetes 构建可靠、稳定的运维系统时，虚拟机 (VM) 的销毁和新建是一种常态。VM 提供的是计算和内存资源，而使用外部存储，通过 StorageClass 提供给集群中的 PVC 消费。 在这样的背景下，如何快速初始化 VM 成为新的挑战。常见的思路是制作 Node 节点的 VM 镜像，提前将依赖</description><dc:creator>微信公众号</dc:creator><category>博文</category><category>Docker</category><category>容器</category><category>存储</category><category>Kubernetes</category><category>能不能</category></item><item><title>Kubernetes 之网络隔离(内附十多种使用场景)</title><link>https://www.chenshaowen.com/blog/network-policy-of-kubernetes.html</link><pubDate>Tue, 18 May 2021 00:00:00 +0000</pubDate><atom:modified>Tue, 18 May 2021 00:00:00 +0000</atom:modified><guid>https://www.chenshaowen.com/blog/network-policy-of-kubernetes.html</guid><description>1. Kubernetes 中的网络隔离 Kuberntes 自 1.3 引入了 Network Policy（网络策略） ，通过 ipBlock、podSelector、namespaceSelector 定义实体，控制其 From（Ingress）、To（Egress）的流量行为。 但 Kubernetes 只是定义了网络策略，具体实</description><dc:creator>微信公众号</dc:creator><category>整理</category><category>Kubernetes</category><category>网络隔离</category></item><item><title>Tekton 如何接入物理机进行构建</title><link>https://www.chenshaowen.com/blog/how-to-add-physical-machines-to-tekton.html</link><pubDate>Wed, 28 Apr 2021 00:00:00 +0000</pubDate><atom:modified>Wed, 28 Apr 2021 00:00:00 +0000</atom:modified><guid>https://www.chenshaowen.com/blog/how-to-add-physical-machines-to-tekton.html</guid><description>1. 为什么需要物理构建机 在文章《如何接入远程 macOS 物理机进行 Jenkins 流水线构建》中，我描述了在 Jenkins 中添加物理构建机的方法。这并不是我拍脑袋想的需求，而是当时真的有 ToB 的商业客户在咨询方案。 对于多端开发商来说，构建 Android、IOS、macOS、Arm 、</description><dc:creator>微信公众号</dc:creator><category>博文</category><category>Tekton</category><category>云原生</category><category>Kubernetes</category><category>CICD</category><category>DevOps</category></item><item><title>在 Dell OptiPlex 7080MT 上安装 macOS 操作系统</title><link>https://www.chenshaowen.com/blog/how-to-install-macos-on-dell-optiplex-7080.html</link><pubDate>Mon, 26 Apr 2021 00:00:00 +0000</pubDate><atom:modified>Mon, 26 Apr 2021 00:00:00 +0000</atom:modified><guid>https://www.chenshaowen.com/blog/how-to-install-macos-on-dell-optiplex-7080.html</guid><description>大概是十年前，我在笔记本上安装过 macOS。当时最头疼的是只有特定的硬件才能安装成功，而且还缺各种驱动程序。后来自己买了 Mac 笔记本，很长时间没有关注如何在通用机器上安装 macOS 。最近拿到了一台 Dell 台式机，配置还不错，又尝试了一下。本文主要是记录这一过</description><dc:creator>微信公众号</dc:creator><category>博文</category><category>Kubernetes</category><category>Docker</category><category>PaaS</category></item><item><title>在 Kubernetes 中面向虚拟机节点分发文件、执行脚本</title><link>https://www.chenshaowen.com/blog/how-to-distribute-files-and-scripts-to-vm-under-kubernetes.html</link><pubDate>Sat, 24 Apr 2021 00:00:00 +0000</pubDate><atom:modified>Sat, 24 Apr 2021 00:00:00 +0000</atom:modified><guid>https://www.chenshaowen.com/blog/how-to-distribute-files-and-scripts-to-vm-under-kubernetes.html</guid><description>1. 本文主要讨论什么 勿在浮沙筑高台。业务量的增长、业务形态的进化都需要坚实强劲的 IT 系统支撑。业务内容对市场是透明的，但是 IT 系统不是一朝一夕能建设完善的。未来公司之间的竞争主要也会来自于 IT 系统之间的竞争，能不能快速响应业务需求是决胜的关键。 IT 系</description><dc:creator>微信公众号</dc:creator><category>博文</category><category>Kubernetes</category><category>Docker</category><category>PaaS</category></item><item><title>ToB 创业公司的开源之路 - KubeSphere</title><link>https://www.chenshaowen.com/blog/the-road-to-open-source-for-tob.html</link><pubDate>Tue, 30 Mar 2021 00:00:00 +0000</pubDate><atom:modified>Tue, 30 Mar 2021 00:00:00 +0000</atom:modified><guid>https://www.chenshaowen.com/blog/the-road-to-open-source-for-tob.html</guid><description>1. 以开源为核心的商业模式 开源的魅力之一在于其包容性。它能接受怀揣各种意图的人，无论是执着技术的的工程师，还是心怀鬼胎的商人，亦或是热心公益的志愿者，甚至茶余饭后的看客，都能在这里碰撞、交融，形成一股力量。 围绕开源做商业，应该被允许和接受。开</description><dc:creator>微信公众号</dc:creator><category>博文</category><category>开源</category><category>ToB</category><category>思考</category><category>KubeSphere</category><category>Kubernetes</category></item><item><title>DevOps 工具链之 Lighthouse 介绍</title><link>https://www.chenshaowen.com/blog/lighthouse-of-devops-tool-chain.html</link><pubDate>Fri, 05 Mar 2021 00:00:00 +0000</pubDate><atom:modified>Fri, 05 Mar 2021 00:00:00 +0000</atom:modified><guid>https://www.chenshaowen.com/blog/lighthouse-of-devops-tool-chain.html</guid><description>本文介绍一个 ChatOps 工具 Lighthouse, 主要内容来自官方文档 。Kubernetes 社区使用 Prow 驱动其在 GitHub 上的协作, 但是不适用于其他仓库。Lighthouse 普适于更多类型的 Git 仓库。 1. 什么是 Lighthouse Lighthouse 是一个基于 webhooks 的轻量级 ChatOps 工具 , 通过 Git 仓库的 webhooks 可以触发 Jenkins X 流水线 、Tekt</description><dc:creator>微信公众号</dc:creator><category>翻译</category><category>DevOps</category><category>Prow</category><category>Kubernetes</category><category>CICD</category><category>Lighthouse</category></item><item><title>基于 Kubernetes 的 Jenkins 服务也可以去 Docker 了</title><link>https://www.chenshaowen.com/blog/using-podman-to-build-images-under-kubernetes-and-jenkins.html</link><pubDate>Thu, 25 Feb 2021 00:00:00 +0000</pubDate><atom:modified>Thu, 25 Feb 2021 00:00:00 +0000</atom:modified><guid>https://www.chenshaowen.com/blog/using-podman-to-build-images-under-kubernetes-and-jenkins.html</guid><description>从原理上看，在 Kubernetes 集群中，Jenkins 都可以使用 Podman 进行镜像构建，本文主要以 Containerd 为例。 1. 去 Docker 给 CICD 带来新的挑战 在 CICD 场景下, 我们经常需要在流水线中构建和推送镜像。 在之前的文档 《在 Kubernetes 上动态创建 Jenkins Slave》 中, 我描述了通过挂载 /var/run/docker.sock 文件, 允许在 Docker 驱动的 Kubernetes</description><dc:creator>微信公众号</dc:creator><category>博文</category><category>Jenkins</category><category>DevOps</category><category>CICD</category><category>Docker</category><category>Kubernetes</category></item><item><title>DevOps 工具链之 Argo CD</title><link>https://www.chenshaowen.com/blog/argocd-of-devops-tool-chain.html</link><pubDate>Thu, 04 Feb 2021 00:00:00 +0000</pubDate><atom:modified>Thu, 04 Feb 2021 00:00:00 +0000</atom:modified><guid>https://www.chenshaowen.com/blog/argocd-of-devops-tool-chain.html</guid><description>1. Argo CD 能解决什么问题 1.1 从 GitOps 说起 GitOps 起源于 Weaveworks 公司在 2017 年发表的一篇博客， GitOps - Operations by Pull Request 。在文中，Alexis 介绍了一种以 Git 为唯一事实来源的部署方式。 在 GitOps 实践中，我们需要将软件设施定义在 Git 仓库中进行管理。其中的软件设施，包括 IaaS、Kubernet</description><dc:creator>微信公众号</dc:creator><category>博文</category><category>DevOps</category><category>ArgoCD</category><category>Kubernetes</category><category>CICD</category></item><item><title>Kuberntes 系统下的 `rm -rf /`，执行完就可以跑路了</title><link>https://www.chenshaowen.com/blog/attack-vectors-under-kubernetes.html</link><pubDate>Fri, 22 Jan 2021 00:00:00 +0000</pubDate><atom:modified>Fri, 22 Jan 2021 00:00:00 +0000</atom:modified><guid>https://www.chenshaowen.com/blog/attack-vectors-under-kubernetes.html</guid><description>本文档主要用于展示 Docker 特权模式的危害，请谨慎操作。对于没有 CLI 操作权限的用户，可以拷贝示例的 Yaml，直接创建集群负载 Pod、Job、DaemonSet 等进行操作。 1. 直接删除全部资源 如果能登陆机器，收拾好东西，执行命令: 1 kubectl delete all --all --all-namespaces 但是也有可能</description><dc:creator>微信公众号</dc:creator><category>博文</category><category>Kubernetes</category><category>安全</category><category>攻击向量</category></item><item><title>如何在主机上调试容器、在容器中操作主机</title><link>https://www.chenshaowen.com/blog/operate-host-in-container-and-debug-container-on-host.html</link><pubDate>Mon, 11 Jan 2021 00:00:00 +0000</pubDate><atom:modified>Mon, 11 Jan 2021 00:00:00 +0000</atom:modified><guid>https://www.chenshaowen.com/blog/operate-host-in-container-and-debug-container-on-host.html</guid><description>1. 一个奇怪的需求 老板有个奇怪的需求，通过一个 kubeconfig 文件，获取主机的各种状态信息，比如进程列表、进程状态等。 第一反应就是，老板是不是不懂容器，容器怎么能这样玩，这样玩还要什么容器，内心万马奔腾。 直到最近遇到了一个命令行工具，才发现原来小丑是我自己</description><dc:creator>微信公众号</dc:creator><category>博文</category><category>容器</category><category>安全</category><category>Kubernetes</category><category>Docker</category></item><item><title>使用 Terraform 和 GitHub Actions 对基础设施进行自动化安装测试</title><link>https://www.chenshaowen.com/blog/using-terraform-and-github-actions-to-test-iac.html</link><pubDate>Sun, 13 Dec 2020 00:00:00 +0000</pubDate><atom:modified>Sun, 13 Dec 2020 00:00:00 +0000</atom:modified><guid>https://www.chenshaowen.com/blog/using-terraform-and-github-actions-to-test-iac.html</guid><description>1. 测试是海上的航标 项目越复杂、规模越大，越能体现测试的价值和重要性。 测试保证了方向的正确性。就像航行时，海上出现的航标，可以用来检验、纠正路线。便于掌舵人，随时了解动态，做出调整。 测试决定了迭代的速度。随着 Scrum 等敏捷开发方法的实践，交付的节奏</description><dc:creator>微信公众号</dc:creator><category>博文</category><category>Terraform</category><category>GitHub</category><category>CICD</category><category>Kubernetes</category></item><item><title>Daemon-less 镜像构建工具 - Kaniko</title><link>https://www.chenshaowen.com/blog/the-daemon-less-tools-of-kaniko.html</link><pubDate>Fri, 11 Dec 2020 08:00:00 +0000</pubDate><atom:modified>Fri, 11 Dec 2020 08:00:00 +0000</atom:modified><guid>https://www.chenshaowen.com/blog/the-daemon-less-tools-of-kaniko.html</guid><description>1. daemon-less 镜像构建工具 1.1 什么是 daemon-less 镜像构建工具 在 CICD 流程中，经常会涉及镜像构建，常规的做法是使用 Docker in Docker 或者 Docker out of Docker 进行构建。详情可以参考文档：如何在 Docker 中使用 Docker 实际上，为了避免垄断，促进行业发展，基于 Docker 的镜像格式，早就指定了统一的 OCI 镜像格式规范。也就是</description><dc:creator>微信公众号</dc:creator><category>博文</category><category>DevOps</category><category>镜像</category><category>CICD</category><category>Kubernetes</category></item><item><title>如何在 Docker 中使用 Docker</title><link>https://www.chenshaowen.com/blog/how-to-use-docker-in-docker.html</link><pubDate>Sat, 21 Nov 2020 00:00:00 +0000</pubDate><atom:modified>Sat, 21 Nov 2020 00:00:00 +0000</atom:modified><guid>https://www.chenshaowen.com/blog/how-to-use-docker-in-docker.html</guid><description>1. 典型适用场景 在 CI 中，通常会有一个 CI Engine 负责解析流程，控制整个构建过程，而将真正的构建交给 Agent 去完成。例如，Jenkins 、GitLab 均是如此。 如下图, 连接 CI Engine 的 Agent, 种类很多。这是为了满足不同项目对构建环境的要求。 同时 Agent 是动态的，构建时才需要，</description><dc:creator>微信公众号</dc:creator><category>博文</category><category>Docker</category><category>Kubernetes</category><category>Containers</category><category>CI</category></item><item><title>在 Kubernetes 中如何获取客户端真实 IP</title><link>https://www.chenshaowen.com/blog/how-to-get-the-real-ip-of-client-in-kubernetes.html</link><pubDate>Fri, 20 Nov 2020 17:22:45 +0000</pubDate><atom:modified>Fri, 20 Nov 2020 17:22:45 +0000</atom:modified><guid>https://www.chenshaowen.com/blog/how-to-get-the-real-ip-of-client-in-kubernetes.html</guid><description>Kubernetes 依靠 kube-proxy 组件实现 Service 的通信与负载均衡。在这个过程中，由于使用了 SNAT 对源地址进行了转换，导致 Pod 中的服务拿不到真实的客户端 IP 地址信息。本篇主要解答了在 Kubernetes 集群中负载如何获取客户端真实 IP 地址这个问题。 1. 创建一个后端服务 1.1 服务选择 这里选择 containous/whoami 作为后端服务镜</description><dc:creator>微信公众号</dc:creator><category>博文</category><category>Kubernetes</category><category>IP</category><category>Demo</category></item><item><title>白板分享 - Jenkins on Kubernetes</title><link>https://www.chenshaowen.com/blog/whiteboard-jenkins-on-kubernetes.html</link><pubDate>Mon, 19 Oct 2020 00:08:00 +0000</pubDate><atom:modified>Mon, 19 Oct 2020 00:08:00 +0000</atom:modified><guid>https://www.chenshaowen.com/blog/whiteboard-jenkins-on-kubernetes.html</guid><description/><dc:creator>微信公众号</dc:creator><category>博文</category><category>Jenkins</category><category>Kubernetes</category><category>白板分享</category></item><item><title>Kubernetes网络权威指南：基础、原理与实践</title><link>https://www.chenshaowen.com/blog/book/the-guide-to-kubernetes-network.html</link><pubDate>Fri, 16 Oct 2020 00:00:00 +0000</pubDate><atom:modified>Fri, 16 Oct 2020 00:00:00 +0000</atom:modified><guid>https://www.chenshaowen.com/blog/book/the-guide-to-kubernetes-network.html</guid><description>作者: 杜军 出版社: 电子工业出版社 出版年: 2019-10 ISBN: 9787121373398 Notes: 网络是 Kubernetes 中不易掌握的一个难点。网络故障会直接影响现有的负载，通常是十分紧急的问题。而网络相关的知识相较于应用开发更底层，很多的细节，需要长期的积累。 书中相关的要点之前陆续都有所接触，通过阅读这</description><dc:creator>微信公众号</dc:creator><category>书籍</category><category>Kubernetes</category><category>网络</category></item><item><title>Kubernetes 平台管理软件压力测试方案</title><link>https://www.chenshaowen.com/blog/stress-testing-plan-of-kubernetes-platform-management-software.html</link><pubDate>Thu, 17 Sep 2020 00:00:00 +0000</pubDate><atom:modified>Thu, 17 Sep 2020 00:00:00 +0000</atom:modified><guid>https://www.chenshaowen.com/blog/stress-testing-plan-of-kubernetes-platform-management-software.html</guid><description>Kubernetes 平台管理软件运行在 Kubernetes ，用于管理运行在 Kubernetes 上的资源对象。 1. 测试思路 测试在一定负载一定集群规模下，平台软件的管理能力，而不是 Kubernetes 的管理能力。平台软件的管理能力主要体现在能通过 UI 对负载、PV 进行增删改查，在 UI 上能够直接查看负载的监控和日志。 明确测试</description><dc:creator>微信公众号</dc:creator><category>博文</category><category>Kubernetes</category><category>压力测试</category><category>方案</category><category>Etcd</category><category>负载</category></item><item><title>如何在 Kubernetes 集群集成 Kata</title><link>https://www.chenshaowen.com/blog/how-to-integrate-kata-in-kubernetes-cluster.html</link><pubDate>Sun, 30 Aug 2020 00:00:00 +0000</pubDate><atom:modified>Sun, 30 Aug 2020 00:00:00 +0000</atom:modified><guid>https://www.chenshaowen.com/blog/how-to-integrate-kata-in-kubernetes-cluster.html</guid><description>1. Kata 解决什么问题 安全性和隔离性是 Kata Container 显著区别于 Docker Container 的地方。 Kata Container 来源于 Intel Clear Containers 和 Hyper runV 项目的合并。Intel Clear Containers 借助 Intel VT-x 技术使用轻量级虚拟机提供容器，解决安全性问题，同时性能优异。而 Hyper runV 对标的是 Docker 的 runc ，提供容器的运行时，遵循 OCI runtime 规范。 2. Kubernetes 中的 Kata 2.1</description><dc:creator>微信公众号</dc:creator><category>博文</category><category>Kubernetes</category><category>Kata</category><category>Containers</category></item><item><title>如何使用 Terraform Provider 提供 Iac 级别的应用</title><link>https://www.chenshaowen.com/blog/how-to-use-terraform-to-provide-iac-platform.html</link><pubDate>Sat, 22 Aug 2020 00:00:00 +0000</pubDate><atom:modified>Sat, 22 Aug 2020 00:00:00 +0000</atom:modified><guid>https://www.chenshaowen.com/blog/how-to-use-terraform-to-provide-iac-platform.html</guid><description>1. Terraform Vs Kubernetes 基础架构即代码（Iac) 基于不可变的基础架构，使用编排工具将基础架构文本化，允许像管理代码一样管理基础设施。 2018 年，我在从事 SaaS 开发，使用 Kubernetes 平台进行部署，这一年 Terraform 很火。2019 年，我开始从事 Kubernetes 的二次开发，才听说 Terraform 。现在网上 Terraform 相关的文档增</description><dc:creator>微信公众号</dc:creator><category>博文</category><category>Terraform</category><category>Iac</category><category>Kubernetes</category><category>DevOps</category><category>CICD</category><category>研发</category></item><item><title>Etcd、Etcdctl 应用实践</title><link>https://www.chenshaowen.com/blog/the-use-of-etcd-and-etcdctl.html</link><pubDate>Tue, 11 Aug 2020 00:00:00 +0000</pubDate><atom:modified>Tue, 11 Aug 2020 00:00:00 +0000</atom:modified><guid>https://www.chenshaowen.com/blog/the-use-of-etcd-and-etcdctl.html</guid><description>1. Etcd 基本介绍 Etcd 是一个分布式 Key/Value 的存储系统，通过分布式锁、leader 选举、写屏障(write barriers) 实现了分布式协作，提供高可用、持久化数据存储和检索服务。 工作原理 每个 Etcd 节点都存储了一份完整的数据，任意时刻至多存在一个主节点。主节点处理所有来自客户</description><dc:creator>微信公众号</dc:creator><category>博文</category><category>Etcd</category><category>Etcdctl</category><category>Kubernetes</category></item><item><title>Kubernetes 动态创建 Jenkins Agent 压力测试</title><link>https://www.chenshaowen.com/blog/the-stress-test-about-kubernetes-dynamically-creates-jenkins-agent.html</link><pubDate>Sun, 02 Aug 2020 00:00:00 +0000</pubDate><atom:modified>Sun, 02 Aug 2020 00:00:00 +0000</atom:modified><guid>https://www.chenshaowen.com/blog/the-stress-test-about-kubernetes-dynamically-creates-jenkins-agent.html</guid><description>前面的文档中，我们利用 Kubernetes 提供的弹性，在 Kubernetes 上动态创建 Jenkins Slave 。本篇文档主要是对 Jenkins 进行大规模构建的压力测试。 1. 集群配置 1.1 Kubernetes 版本 这里使用的是 v1.16.7 1 2 3 4 kubectl version Client Version: version.Info{Major:&amp;#34;1&amp;#34;, Minor:&amp;#34;16&amp;#34;, GitVersion:&amp;#34;v1.16.7&amp;#34;, GitCommit:&amp;#34;be3d344ed06bff7a4fc60656200a93c74f31f9a4&amp;#34;, GitTreeState:&amp;#34;clean&amp;#34;, BuildDate:&amp;#34;2020-02-11T19:34:02Z&amp;#34;, GoVersion:&amp;#34;go1.13.6&amp;#34;, Compiler:&amp;#34;gc&amp;#34;, Platform:&amp;#34;linux/amd64&amp;#34;} Server Version: version.Info{Major:&amp;#34;1&amp;#34;, Minor:&amp;#34;16&amp;#34;, GitVersion:&amp;#34;v1.16.7&amp;#34;, GitCommit:&amp;#34;be3d344ed06bff7a4fc60656200a93c74f31f9a4&amp;#34;, GitTreeState:&amp;#34;clean&amp;#34;, BuildDate:&amp;#34;2020-02-11T19:24:46Z&amp;#34;, GoVersion:&amp;#34;go1.13.6&amp;#34;, Compiler:&amp;#34;gc&amp;#34;, Platform:&amp;#34;linux/amd64&amp;#34;} 1.2 节点数量 集群节点总数， 16 个 1 2 3 kubectl get node |grep &amp;#34;Ready&amp;#34; | wc -l 16</description><dc:creator>微信公众号</dc:creator><category>博文</category><category>Jenkins</category><category>Kubernetes</category><category>DevOps</category><category>测试</category></item><item><title>Kubernetes Windows 节点动态提供 Jenkins Agent</title><link>https://www.chenshaowen.com/blog/windows-node-to-dynamicly-provide-jenkins-agent-on-k8s.html</link><pubDate>Sun, 14 Jun 2020 00:00:00 +0000</pubDate><atom:modified>Sun, 14 Jun 2020 00:00:00 +0000</atom:modified><guid>https://www.chenshaowen.com/blog/windows-node-to-dynamicly-provide-jenkins-agent-on-k8s.html</guid><description>在前面两篇文档，在 Kubernetes 上动态创建 Jenkins Slave 和 Kubernetes 添加 Windows 节点提供 Jenkins 构建动态 Agent 的基础之上，本篇文档主要尝试在 Kubernetes 上动态提供 Windows 构建 Agent 。 1. 新增流水线 Kubernetes 与 Jenkins 集成部分可以参考上面的两篇文档，这里直接新建两条流水线进行测试。 windows - jenkins 内置的流水线示例 1 2 3 4 5 6 7 8 9 10</description><dc:creator>微信公众号</dc:creator><category>博文</category><category>Jenkins</category><category>Kubernetes</category><category>Windows</category><category>DevOps</category></item><item><title>Kubernetes 添加 Windows 节点</title><link>https://www.chenshaowen.com/blog/add-windows-node-for-k8s.html</link><pubDate>Sat, 13 Jun 2020 00:00:00 +0000</pubDate><atom:modified>Sat, 13 Jun 2020 00:00:00 +0000</atom:modified><guid>https://www.chenshaowen.com/blog/add-windows-node-for-k8s.html</guid><description>这里主要使用 Windows 节点作为 Worker，而 Master 控制平面依然在 Linux 。 1. 系统配置 1.1 Kubernetes 控制平面 Kubernetes 自 1.14 版本，增加了对 Windows 节点生产级的支持。由于微软官方文档主要提供的是 flannel 网络插件的安装方式，这里建议 Kubernetes 也采用 flannel 插件。 查看当前集群 Kubernetes 版本 1 2 3 4 kubectl version Client Version: version.Info{Major:&amp;#34;1&amp;#34;, Minor:&amp;#34;17&amp;#34;, GitVersion:&amp;#34;v1.17.6&amp;#34;, GitCommit:&amp;#34;d32e40e20d167e103faf894261614c5b45c44198&amp;#34;, GitTreeState:&amp;#34;clean&amp;#34;, BuildDate:&amp;#34;2020-05-20T13:16:24Z&amp;#34;,</description><dc:creator>微信公众号</dc:creator><category>博文</category><category>Kubernetes</category><category>Windows</category><category>节点</category><category>安装</category></item><item><title>Kubernetes 批量操作命令</title><link>https://www.chenshaowen.com/blog/batch-command-of-kubernetes.html</link><pubDate>Sun, 07 Jun 2020 00:00:00 +0000</pubDate><atom:modified>Sun, 07 Jun 2020 00:00:00 +0000</atom:modified><guid>https://www.chenshaowen.com/blog/batch-command-of-kubernetes.html</guid><description>1. 批量删除 Evicted 状态的 Pod 1 kubectl get pods --all-namespaces -o wide | grep Evicted | awk &amp;#39;{print $1,$2}&amp;#39; | xargs -L1 kubectl delete pod -n 2. 批量删除指定空间指定状态的 Pod 根据 field-selectors ，可以删除指定空间指定状态的 Pod 。 1 kubectl get pods --field-selector=status.phase!=Running -n default | cut -d&amp;#39; &amp;#39; -f 1 | xargs kubectl delete pod -n default</description><dc:creator>微信公众号</dc:creator><category>博文</category><category>Kubernetes</category></item><item><title>Kubernetes 调度器之亲和性</title><link>https://www.chenshaowen.com/blog/affinity-of-kubernetes-scheduler.html</link><pubDate>Sat, 30 May 2020 00:00:00 +0000</pubDate><atom:modified>Sat, 30 May 2020 00:00:00 +0000</atom:modified><guid>https://www.chenshaowen.com/blog/affinity-of-kubernetes-scheduler.html</guid><description>1. Kubernetes 中的调度器 kube-scheduler 是 Kubernetes 中决定 Pending 状态的 Pod 运行在哪个 Node 的组件，被称之为调度器。 Kubernetes 中内置了大量的调度策略，也提供了一些高级调度策略（nodeAffinity、podAffinity 等），以供用户使用，基本能够满足绝大部分的业务需求。 前面的文档 Kubernetes 之 L</description><dc:creator>微信公众号</dc:creator><category>博文</category><category>Kubernetes</category><category>调度</category><category>亲和性</category></item><item><title>如何配置高效的 Kubernetes 命令行终端</title><link>https://www.chenshaowen.com/blog/how-to-configure-efficient-k8s-terminal.html</link><pubDate>Sat, 16 May 2020 00:00:00 +0000</pubDate><atom:modified>Sat, 16 May 2020 00:00:00 +0000</atom:modified><guid>https://www.chenshaowen.com/blog/how-to-configure-efficient-k8s-terminal.html</guid><description>磨刀不误砍柴工，无论什么时候，花点时间在工具链上都是值得的。 1. 自动补全 - kubectl OS X 安装命令： 1 brew install bash-complete@2 不仅仅是 kubectl ，也给其他命令行提供自动补全的命令提示。 在 .zshrc 中添加如下内容： 1 2 # kubectl complete source &amp;lt;(kubectl completion zsh) 在输入 kubectl get pod 命令时，键入 Tab 会自动列举当前类型下的资源，如</description><dc:creator>微信公众号</dc:creator><category>博文</category><category>Kubernetes</category><category>命令行</category><category>工具</category></item><item><title>给 Kubernetes 配置 Proxy</title><link>https://www.chenshaowen.com/blog/how-to-set-proxy-for-kubernetes.html</link><pubDate>Sun, 03 May 2020 00:00:00 +0000</pubDate><atom:modified>Sun, 03 May 2020 00:00:00 +0000</atom:modified><guid>https://www.chenshaowen.com/blog/how-to-set-proxy-for-kubernetes.html</guid><description>通常，我们在主机上执行 export http_proxy/https_proxy 格式的命令，即可设置 Proxy 。但是主机上的设置在容器中并不会生效，下面提供了几种配置方法。 1. 配置 Docker 的代理 - Node 级 在需要使用 Proxy 的节点进行配置，下面以 Docker 为例： 创建配置文件 1 2 mkdir -p /etc/systemd/system/docker.service.d touch /etc/systemd/system/docker.service.d/https-proxy.conf 编辑配置文件，配置代理 [Service] Environment=&amp;#34;HTTP_PROXY=http://proxy.example.com:80/&amp;#34; Environment=&amp;#34;HTTPS_PROXY=https://proxy.example.com:443/&amp;#34; Environment=&amp;#34;NO_PROXY=localhost,127.0.0.1&amp;#34; 重启 Docker 1 2 systemctl</description><dc:creator>微信公众号</dc:creator><category>博文</category><category>Docker</category><category>Kubernetes</category><category>代理</category></item><item><title>Helm 2 、Helm 3 比较</title><link>https://www.chenshaowen.com/blog/helm-2-vs-helm-3.html</link><pubDate>Sat, 25 Apr 2020 00:00:00 +0000</pubDate><atom:modified>Sat, 25 Apr 2020 00:00:00 +0000</atom:modified><guid>https://www.chenshaowen.com/blog/helm-2-vs-helm-3.html</guid><description>Helm 3 终于发布了。我们可以告别 Tiller 了，但 Helm 3 的改变不仅于此。让我们继续探讨其他的变化。 1. 告别 Tiller Helm 3 移除了 Tiller ，是个不错的决定。但是要理解为什么不错，我们还需要了解一下 Tiller 产生的背景。Tiller 是 Helm 的服务端组件（运行在 Kubernetes 集群上），主要目的是为了让多</description><dc:creator>微信公众号</dc:creator><category>翻译</category><category>Helm</category><category>Kubernetes</category></item><item><title>Kubernetes 中的 DNS 服务</title><link>https://www.chenshaowen.com/blog/dns-server-in-kubernetes.html</link><pubDate>Fri, 24 Apr 2020 00:00:00 +0000</pubDate><atom:modified>Fri, 24 Apr 2020 00:00:00 +0000</atom:modified><guid>https://www.chenshaowen.com/blog/dns-server-in-kubernetes.html</guid><description>1. 关于 DNS 1.1 DNS 服务的用途 DNS 提供的是域名到 IP 的映射服务。例如，在浏览器输入 https://www.chenshaowen.com 访问页面，但数据链路是基于 IP 的通信，无法识别 www.chenshaowen.com 。这时就需要进行 DNS 查询，输入参数是 www.chenshaowen.com ，返回结果是 IP 地址。 可以看到 DNS 提供了一种助记方法，我们不必关注 IP 地址以及其变动，而只需</description><dc:creator>微信公众号</dc:creator><category>博文</category><category>Kubernetes</category><category>DNS</category><category>域名</category></item><item><title>国内的 Helm 镜像源</title><link>https://www.chenshaowen.com/blog/configure-helm-mirror-in-china.html</link><pubDate>Tue, 21 Apr 2020 00:00:00 +0000</pubDate><atom:modified>Tue, 21 Apr 2020 00:00:00 +0000</atom:modified><guid>https://www.chenshaowen.com/blog/configure-helm-mirror-in-china.html</guid><description>helm 官方源 https://charts.helm.sh/stable ，国内的某些机器无法访问，需要配置镜像源。 1. 官方镜像源 1 helm repo add stable https://charts.helm.sh/stable 2. Git Pages 镜像 1 helm repo add stable https://burdenbear.github.io/kube-charts-mirror/ 可以参考 kube-charts-mirror ，搭建一个自主可控的镜像源。 3. Aliyun 镜像 长时间未更新，版本较旧 1 helm repo add stable https://kubernetes.oss-cn-hangzhou.aliyuncs.com/charts/ 4. Azure 镜像源 已经不可用，2021.07.08 更新。 1 2 helm repo add stable http://mirror.azure.cn/kubernetes/charts/</description><dc:creator>微信公众号</dc:creator><category>博文</category><category>Helm</category><category>Kubernetes</category><category>国内</category></item><item><title>Harbor 使用自签证书支持 Https 访问</title><link>https://www.chenshaowen.com/blog/support-https-access-harbor-using-self-signed-cert.html</link><pubDate>Sat, 18 Apr 2020 00:00:00 +0000</pubDate><atom:modified>Sat, 18 Apr 2020 00:00:00 +0000</atom:modified><guid>https://www.chenshaowen.com/blog/support-https-access-harbor-using-self-signed-cert.html</guid><description>在之前的文章 使用 Helm 安装 harbor 中，我已经详细描述了安装 Ingress 、Harbor ，最后成功推送镜像的步骤。其中的域名是公网可以访问的，证书是认证机构签发的。但是在内网环境下，我们需要使用内网域名进行访问。本文主要解决使用自签证书通过 Https 访问 Harbor 的问题。 1. 生成自</description><dc:creator>微信公众号</dc:creator><category>博文</category><category>Harbor</category><category>Kubernetes</category><category>Https</category><category>镜像</category></item><item><title>DevOps 工具链之 Prow</title><link>https://www.chenshaowen.com/blog/prow-of-devops-tool-chain.html</link><pubDate>Fri, 17 Apr 2020 13:17:44 +0000</pubDate><atom:modified>Fri, 17 Apr 2020 13:17:44 +0000</atom:modified><guid>https://www.chenshaowen.com/blog/prow-of-devops-tool-chain.html</guid><description>1. 关于 Prow 在 Kubernetes、Istio 等知名项目的 Github 仓库中，我们经常会看到 xxx-bot 用户，给 issues 添加标签、合并 PR 。这个机器人账户就是被 Prow 驱动的。 Prow 是 Kubernetes 测试特别兴趣小组的项目，目前是 kubernetes/test-infra 的一部分。Prow 是一个基于 Kubernetes 使用各类事件驱动执行 Job 的 CI/CD 系统。 除</description><dc:creator>微信公众号</dc:creator><category>博文</category><category>DevOps</category><category>Prow</category><category>Kubernetes</category><category>CICD</category></item><item><title>如何使用 kfctl 安装 Kubeflow</title><link>https://www.chenshaowen.com/blog/how-to-install-kubeflow-using-kfctl.html</link><pubDate>Sun, 12 Apr 2020 00:00:00 +0000</pubDate><atom:modified>Sun, 12 Apr 2020 00:00:00 +0000</atom:modified><guid>https://www.chenshaowen.com/blog/how-to-install-kubeflow-using-kfctl.html</guid><description>1. 安装基础环境 安装 Kubernetes 参考链接：使用 Kubeadm 安装 Kubernetes 集群 。值得注意的是 Kubeflow 并不是对每个版本的 Kubernetes 兼容，system-requirements。 1 2 3 4 kubectl version Client Version: version.Info{Major:&amp;#34;1&amp;#34;, Minor:&amp;#34;15&amp;#34;, GitVersion:&amp;#34;v1.15.12&amp;#34;, GitCommit:&amp;#34;e2a822d9f3c2fdb5c9bfbe64313cf9f657f0a725&amp;#34;, GitTreeState:&amp;#34;clean&amp;#34;, BuildDate:&amp;#34;2020-05-06T05:17:59Z&amp;#34;, GoVersion:&amp;#34;go1.12.17&amp;#34;, Compiler:&amp;#34;gc&amp;#34;, Platform:&amp;#34;linux/amd64&amp;#34;} Server Version: version.Info{Major:&amp;#34;1&amp;#34;, Minor:&amp;#34;15&amp;#34;, GitVersion:&amp;#34;v1.15.12&amp;#34;, GitCommit:&amp;#34;e2a822d9f3c2fdb5c9bfbe64313cf9f657f0a725&amp;#34;, GitTreeState:&amp;#34;clean&amp;#34;, BuildDate:&amp;#34;2020-05-06T05:09:48Z&amp;#34;, GoVersion:&amp;#34;go1.12.17&amp;#34;, Compiler:&amp;#34;gc&amp;#34;, Platform:&amp;#34;linux/amd64&amp;#34;} 安装 kustomize 1 2 curl -s &amp;#34;https://raw.githubusercontent.com/kubernetes-sigs/kustomize/master/hack/install_kustomize.sh&amp;#34; | bash mv kustomize /usr/local/bin/ 1 2 3 kustomize version {Version:kustomize/v3.5.5 GitCommit:897e7b6e61e65188d846c32bd3af9ef68b0f746a BuildDate:2020-05-11T16:51:33Z GoOs:linux GoArch:amd64} 2. 安装 Kubefolow 确</description><dc:creator>微信公众号</dc:creator><category>博文</category><category>Kubeflow</category><category>Kubernetes</category><category>机器学习</category></item><item><title>开发 Tips（19）</title><link>https://www.chenshaowen.com/blog/developing-tips-19.html</link><pubDate>Fri, 10 Jan 2020 00:00:00 +0000</pubDate><atom:modified>Fri, 10 Jan 2020 00:00:00 +0000</atom:modified><guid>https://www.chenshaowen.com/blog/developing-tips-19.html</guid><description>主要记录最近遇到的一些开发问题，解决方法。 1. macOS 快速切换不同 Kubernetes 环境 涉及 Kubernetes 相关开发时，经常需要在多个集群之间切换。配置多集群 context 是一个选择，但是如果集群在不断重置，可以试下如下方法： 在 ~/.profile 文件中定义一系列相关 function，切换时只需要执行 on_cluster_name 即可</description><dc:creator>微信公众号</dc:creator><category>博文</category><category>Tips</category><category>Kubernetes</category><category>环境</category><category>Docker</category></item><item><title>使用 Velero 备份 Kubernetes 集群</title><link>https://www.chenshaowen.com/blog/backup-kubernetes-cluster-using-velero.html</link><pubDate>Wed, 25 Dec 2019 00:00:00 +0000</pubDate><atom:modified>Wed, 25 Dec 2019 00:00:00 +0000</atom:modified><guid>https://www.chenshaowen.com/blog/backup-kubernetes-cluster-using-velero.html</guid><description>1. Velero 简介 Velero 是 heptio 团队（被 VMWare 收购）开源的 Kubernetes 集群备份、迁移工具。 Velero 使用对象存储保存集群资源。默认支持的对象存储有 AWS、Azure、GCP ，兼容 S3 协议，也可以通过插件来扩展到其他平台，比如 Aliyun OSS。 目前，Velero 不具备版本管理功能，只能进行增</description><dc:creator>微信公众号</dc:creator><category>博文</category><category>Kubernetes</category><category>Velero</category><category>备份</category></item><item><title>Kubernetes Cheat Sheet</title><link>https://www.chenshaowen.com/blog/kubernetes-cheat-sheet.html</link><pubDate>Tue, 24 Dec 2019 01:00:00 +0000</pubDate><atom:modified>Tue, 24 Dec 2019 01:00:00 +0000</atom:modified><guid>https://www.chenshaowen.com/blog/kubernetes-cheat-sheet.html</guid><description/><dc:creator>微信公众号</dc:creator><category>整理</category><category>Kubernetes</category></item><item><title>KubeSpray 安装 Kubernetes 报错 ip in ansible_all_ipv4_addresses</title><link>https://www.chenshaowen.com/blog/ip-not-in-ansible-all-ipv4-addresses-while-using-kubespray.html</link><pubDate>Wed, 18 Dec 2019 00:00:00 +0000</pubDate><atom:modified>Wed, 18 Dec 2019 00:00:00 +0000</atom:modified><guid>https://www.chenshaowen.com/blog/ip-not-in-ansible-all-ipv4-addresses-while-using-kubespray.html</guid><description>使用 KubeSpray 安装 Kubernetes 时，报错 1 2 3 4 5 6 fatal: [node0]: FAILED! =&amp;gt; { &amp;#34;assertion&amp;#34;: &amp;#34;ip in ansible_all_ipv4_addresses&amp;#34;, &amp;#34;changed&amp;#34;: false, &amp;#34;evaluated_to&amp;#34;: false, &amp;#34;failed&amp;#34;: true } 查看 inventory.ini 配置 1 2 3 4 5 6 7 cat inventory.ini # ## Configure &amp;#39;ip&amp;#39; variable to bind kubernetes services on a # ## different ip than the default iface # ## We should set etcd_member_name for etcd cluster. The node that is not a etcd member do not need to set the value, or can set the empty string value. [all] node0 ansible_host=139.168.12.4 ip=139.168.12.4 ... 查找报错信息来源 kubespray/extra_playbooks/roles/network_plugin/calico/tasks/pre.yml 1 2 3 4 5 6 7 8 9 ---</description><dc:creator>微信公众号</dc:creator><category>博文</category><category>Kubernetes</category><category>错误</category><category>安装</category></item><item><title>基于 Kubernetes 和 Jenkins 搭建自动化测试系统</title><link>https://www.chenshaowen.com/blog/build-an-automated-test-system-using-kubernetes-and-jenkins.html</link><pubDate>Thu, 12 Dec 2019 00:00:00 +0000</pubDate><atom:modified>Thu, 12 Dec 2019 00:00:00 +0000</atom:modified><guid>https://www.chenshaowen.com/blog/build-an-automated-test-system-using-kubernetes-and-jenkins.html</guid><description>1. 测试分层 测试的目的是为了验证预期的功能，发现潜在的缺陷。测试增强了交付合格产品的信心，也给敏捷迭代带来了可能。可以说，测试决定了产品的开发进度。 网络模型有七层的 OSI 、四层的 TCP，而开发模式有 MTV、MVC、MVP、MVVM 等。高内聚、低耦</description><dc:creator>微信公众号</dc:creator><category>博文</category><category>测试</category><category>自动化</category><category>Kubernetes</category><category>Jenkins</category><category>DevOps</category></item><item><title>在 Kubernetes 上动态创建 Jenkins Slave</title><link>https://www.chenshaowen.com/blog/creating-jenkins-slave-dynamically-on-kubernetes.html</link><pubDate>Fri, 06 Dec 2019 00:00:00 +0000</pubDate><atom:modified>Fri, 06 Dec 2019 00:00:00 +0000</atom:modified><guid>https://www.chenshaowen.com/blog/creating-jenkins-slave-dynamically-on-kubernetes.html</guid><description>1. Jenkins 的工作模式 Jenkins 是一个单 Master，多 Slave 架构。Master 负责分配任务、管理服务。 Slave 负责执行具体任务。 即使部署了多个 Master，这些 Master 之间依然相互独立，无法协同调度。在高可用的 Jenkins 方案中，需要借助外部的任务分发框架，协调多 Master 之间的调度，比</description><dc:creator>微信公众号</dc:creator><category>博文</category><category>Jenkins</category><category>Kubernetes</category></item><item><title>开发 Tips（16）</title><link>https://www.chenshaowen.com/blog/developing-tips-16.html</link><pubDate>Wed, 27 Nov 2019 00:00:00 +0000</pubDate><atom:modified>Wed, 27 Nov 2019 00:00:00 +0000</atom:modified><guid>https://www.chenshaowen.com/blog/developing-tips-16.html</guid><description>主要记录最近遇到的一些开发问题，解决方法。 1. Kubernetes 服务仅在负载节点可用 正常情况下 NodePort 类型的 Service ，任意 Node 节点 IP + 端口，都可以访问。但是，也有可能仅负载的 Node 节点 IP + 端口可以访问。 首先，可以尝试配置转发相关参数: 1 2 3 4 5 6 cat &amp;lt;&amp;lt;EOF &amp;gt; /etc/sysctl.d/k8s.conf net.bridge.bridge-nf-call-ip6tables = 1 net.bridge.bridge-nf-call-iptables = 1 net.ipv4.ip_forward=1 vm.swappiness=0 EOF 1 sysctl</description><dc:creator>微信公众号</dc:creator><category>博文</category><category>Tips</category><category>Kubernetes</category></item><item><title>Kubernetes 签发 Ingress 证书及日常故障运维</title><link>https://www.chenshaowen.com/blog/kubernetes-ingress-certificates-and-ops.html</link><pubDate>Thu, 10 Oct 2019 00:00:00 +0000</pubDate><atom:modified>Thu, 10 Oct 2019 00:00:00 +0000</atom:modified><guid>https://www.chenshaowen.com/blog/kubernetes-ingress-certificates-and-ops.html</guid><description>1. 自动签发 Ingress 证书 安装 cert-manager 1 2 3 4 5 6 7 8 9 10 kubectl apply -f https://raw.githubusercontent.com/jetstack/cert-manager/release-0.10/deploy/manifests/00-crds.yaml kubectl create namespace cert-manager kubectl label namespace cert-manager certmanager.k8s.io/disable-validation=true helm repo add jetstack https://charts.jetstack.io helm repo update helm install \ --name cert-manager \ --namespace cert-manager \ --version v0.10.0 \ jetstack/cert-manager 创建一个全局的签发机构 新建文件 issuer.yaml 1 2 3 4 5 6 7 8 9 10 11 12 apiVersion: certmanager.k8s.io/v1alpha1 kind: ClusterIssuer metadata: name: letsencrypt-prod namespace: cert-manager spec: acme: server: https://acme-v02.api.letsencrypt.org/directory email: admin@domain.com privateKeySecretRef: name: letsencrypt-prod http01: {} 创建签发机构 1 kubectl apply -f issuer.yaml 签发证书 新建</description><dc:creator>微信公众号</dc:creator><category>博文</category><category>Kubernetes</category><category>证书</category><category>故障</category><category>Ingress</category></item><item><title>Kubernetes 中 Deployment 的基本操作</title><link>https://www.chenshaowen.com/blog/basic-operation-of-deployment-in-kubernetes.html</link><pubDate>Fri, 27 Sep 2019 00:00:00 +0000</pubDate><atom:modified>Fri, 27 Sep 2019 00:00:00 +0000</atom:modified><guid>https://www.chenshaowen.com/blog/basic-operation-of-deployment-in-kubernetes.html</guid><description>Deployment 通过创建 ReplicaSet 控制 Pod 的数量、状态。本篇主要介绍一些 Deployment 常用的操作。 1. Deployment yaml 格式 带上 --dry-run 参数表示并不执行命令，仅生成 yaml 输出： 1 kubectl create deployment nginx --image=nginx --dry-run -o yaml apiVersion: apps/v1 kind: Deployment metadata: creationTimestamp: null labels: app: nginx name: nginx spec: replicas: 1 selector: matchLabels: app: nginx strategy: {} template: metadata: creationTimestamp: null labels: app: nginx spec: containers: - image: nginx name: nginx resources: {} status: {} 2. 创建 使用 run 参数 1 kubectl run nginx --image=nginx 使用</description><dc:creator>微信公众号</dc:creator><category>博文</category><category>Kubernetes</category><category>Deployment</category></item><item><title>Kubernetes 中的证书</title><link>https://www.chenshaowen.com/blog/certificates-in-kubernetes.html</link><pubDate>Thu, 26 Sep 2019 00:00:00 +0000</pubDate><atom:modified>Thu, 26 Sep 2019 00:00:00 +0000</atom:modified><guid>https://www.chenshaowen.com/blog/certificates-in-kubernetes.html</guid><description>我使用的是 Kubernetes 1.15.3 ，不同版本的处理方法可能会有不同。 1. 关于证书 根证书是自签的 根证书是由自己签发的。在浏览器中，内置了常见的证书服务商的 CA 证书。因此，浏览器才会信任这些证书服务商签发的下一级证书。 我们也可以生成根证书，但是需要将根证书添加到系统信</description><dc:creator>微信公众号</dc:creator><category>博文</category><category>Kubernetes</category><category>证书</category></item><item><title>如何使用 KubeBuilder 开发一个 Operator</title><link>https://www.chenshaowen.com/blog/how-to-develop-a-operator-using-kubebuilder.html</link><pubDate>Wed, 25 Sep 2019 00:00:00 +0000</pubDate><atom:modified>Wed, 25 Sep 2019 00:00:00 +0000</atom:modified><guid>https://www.chenshaowen.com/blog/how-to-develop-a-operator-using-kubebuilder.html</guid><description>通过 Operator 的方案，可以对 Kubernetes 的功能进行友好地扩展。Operatpr = CRD + Controller。首先通过 yaml 定义，生成 CRD ，然后 Controller 不断地监听 etcd 中的数据，执行相应动作。开发 Operator 时，有很多繁琐且重复的事情。KubeBuilder 可以帮助我们快速生成骨架代码，</description><dc:creator>微信公众号</dc:creator><category>博文</category><category>Kubernetes</category><category>Go</category><category>Operator</category></item><item><title>Kubernetes 1.6.0 安装问题汇总</title><link>https://www.chenshaowen.com/blog/summary-of-installation-problems-for-kubernetes-1.6.0.html</link><pubDate>Fri, 20 Sep 2019 00:00:00 +0000</pubDate><atom:modified>Fri, 20 Sep 2019 00:00:00 +0000</atom:modified><guid>https://www.chenshaowen.com/blog/summary-of-installation-problems-for-kubernetes-1.6.0.html</guid><description>1. CNI 问题 错误日志 1 2 journalctl -u kubelet ...Unable to update cni config: No networks found in /etc/cni/net.d 由于没有安装 CNI ，需要移除 /var/lib/kubelet/kubeadm-flags.env 参数中的--network-plugin=cni 1 2 cat /var/lib/kubelet/kubeadm-flags.env KUBELET_KUBEADM_ARGS=&amp;#34;--cgroup-driver=systemd --pod-infra-container-image=k8s.gcr.io/pause:3.1&amp;#34; 2. 节点 NotReady 节点 NotReady 可能的原因有很多。通常会是网络、容器配置错误导致，需要逐一排查。 这里使用的是 使用 Kubeadm 安装 Kubernetes 集群 文</description><dc:creator>微信公众号</dc:creator><category>博文</category><category>Kubernetes</category><category>安装</category></item><item><title>开发 Tips（15）</title><link>https://www.chenshaowen.com/blog/developing-tips-15.html</link><pubDate>Wed, 11 Sep 2019 00:00:00 +0000</pubDate><atom:modified>Wed, 11 Sep 2019 00:00:00 +0000</atom:modified><guid>https://www.chenshaowen.com/blog/developing-tips-15.html</guid><description>主要记录最近遇到的一些开发问题，解决方法。 1. Ingress 开启 HTTPS 准备好证书，domain.com.crt、domain.com.key 创建 Secret 1 kubectl create secret tls {SECRET_NAME} --key domain.com.key --cert domain.com.crt -n {NAMESPACE} 更新 Ingress 配置 1 2 3 4 5 spec: tls: - hosts: - domain.com secretName: {SECRET_NAME} 2. SSH 登陆失败，提示 ssh-dss SSH 登陆提示: 1 Unable to negotiate with 10.10.10.10 port 22: no matching</description><dc:creator>微信公众号</dc:creator><category>博文</category><category>Tips</category><category>Kubernetes</category><category>Ingress</category></item><item><title>开发 Tips（14）</title><link>https://www.chenshaowen.com/blog/developing-tips-14.html</link><pubDate>Fri, 06 Sep 2019 00:00:00 +0000</pubDate><atom:modified>Fri, 06 Sep 2019 00:00:00 +0000</atom:modified><guid>https://www.chenshaowen.com/blog/developing-tips-14.html</guid><description>主要记录最近遇到的一些开发问题，解决方法。 1. Kubernetes 集群添加新的 Node 节点 在执行 kubeadm init 时，Console 会打印添加 Node 的命令。Token 默认的有效期为 24h 。当超过有效期时，需要重新创建 Token ，执行命令： 1 2 kubeadm token create --print-join-command kubeadm join 192.168.10.2:6443 --token ocyzce.3hv8y7w60lrvulir --discovery-token-ca-cert-hash sha256:7a86632f54de1004bb3f38124b663f837399d6ba9aa803d58c6707a76c02a6cb 使用 Console 输出的命令，即可将 Node 节点添</description><dc:creator>微信公众号</dc:creator><category>博文</category><category>Tips</category><category>Kubernetes</category><category>CentOS</category><category>Tcpdump</category></item><item><title>使用 Helm 安装 harbor</title><link>https://www.chenshaowen.com/blog/install-harbor-using-helm.html</link><pubDate>Fri, 30 Aug 2019 00:00:00 +0000</pubDate><atom:modified>Fri, 30 Aug 2019 00:00:00 +0000</atom:modified><guid>https://www.chenshaowen.com/blog/install-harbor-using-helm.html</guid><description>前提准备，（1）已经安装 Helm ，参考 Helm 安装 ，（2）集群有默认的动态存储可用，参数 使用StorageClass提供PV动态存储 1. 使用 Helm 安装 Ingress Ingress 由 Ingress 和 Ingress Controller 两部分组成。 在 Kubernetes 中，Ingress 对象描述路由规则；Ingress Controller 通过与 Apiserver 交互，将 Ingress 规则写入</description><dc:creator>微信公众号</dc:creator><category>博文</category><category>Kubernetes</category><category>Harbor</category><category>Ingress</category><category>Helm</category><category>镜像仓库</category><category>镜像</category></item><item><title>开发 Tips（13）</title><link>https://www.chenshaowen.com/blog/developing-tips-13.html</link><pubDate>Tue, 27 Aug 2019 00:00:00 +0000</pubDate><atom:modified>Tue, 27 Aug 2019 00:00:00 +0000</atom:modified><guid>https://www.chenshaowen.com/blog/developing-tips-13.html</guid><description>主要记录最近遇到的一些开发问题，解决方法。 1. NodePort 服务仅指定 Node 可以访问 通过 NodePort 暴露的服务，在集群外可以使用 Kubernetes 任意 Node IP 加端口的形式访问。kube-proxy 会将访问流量以轮询的方式转发给 service 中的每个 Pod。 但是，发现并不是每一个 Node IP 加端口都可以访问，</description><dc:creator>微信公众号</dc:creator><category>博文</category><category>Tips</category><category>NodePort</category><category>Kubernetes</category><category>StorageClass</category><category>存储</category></item><item><title>在 Kubernetes 中使用 emptyDir、hostPath、localVolume</title><link>https://www.chenshaowen.com/blog/using-emptydir-hostpath-localvolume-in-kubernetes.html</link><pubDate>Sat, 24 Aug 2019 00:00:00 +0000</pubDate><atom:modified>Sat, 24 Aug 2019 00:00:00 +0000</atom:modified><guid>https://www.chenshaowen.com/blog/using-emptydir-hostpath-localvolume-in-kubernetes.html</guid><description>之前通过 Kubernetes 之 Volumes ，对 Volumes 有了一定的了解。本篇主要侧重实践，学习如何使用 emptydir、hostpath、localvolume 三种本地存储方案。 1. PV 的基本属性 1.1 PV 的生命周期 PV 的状态： Available：可用，还未被任何 PVC 绑定 Bound：已经被</description><dc:creator>微信公众号</dc:creator><category>博文</category><category>Kubernetes</category><category>Volume</category><category>存储</category></item><item><title>开发 Tips（12）</title><link>https://www.chenshaowen.com/blog/developing-tips-12.html</link><pubDate>Fri, 23 Aug 2019 00:00:00 +0000</pubDate><atom:modified>Fri, 23 Aug 2019 00:00:00 +0000</atom:modified><guid>https://www.chenshaowen.com/blog/developing-tips-12.html</guid><description>主要记录最近遇到的一些开发问题，解决方法。 1. Kubernetes 中给 Node 增加 Role: worker 1 2 3 4 kubectl get nodes NAME STATUS ROLES AGE VERSION i-6fns0nua Ready master 6d3h v1.15.2 i-m69skuyd Ready &amp;lt;none&amp;gt; 6d2h v1.15.2 1 2 kubectl label node i-m69skuyd node-role.kubernetes.io/worker= node/i-m69skuyd labeled 1 2 3 4 kubectl get node NAME STATUS ROLES AGE VERSION i-6fns0nua Ready master 6d3h v1.15.2 i-m69skuyd Ready worker 6d2h v1.15.2 2. 删除 Kubernetes 的一个节点 查看当前节点： 1 2 3 4 kubectl get node NAME STATUS ROLES AGE VERSION i-6fns0nua Ready master 6d3h v1.15.2 i-m69skuyd Ready worker 6d2h</description><dc:creator>微信公众号</dc:creator><category>博文</category><category>Tips</category><category>Kubernetes</category></item><item><title>使用 Kubeadm 安装 Kubernetes 集群</title><link>https://www.chenshaowen.com/blog/using-kubeadm-to-install-the-kubernetes-cluster.html</link><pubDate>Thu, 15 Aug 2019 00:00:00 +0000</pubDate><atom:modified>Thu, 15 Aug 2019 00:00:00 +0000</atom:modified><guid>https://www.chenshaowen.com/blog/using-kubeadm-to-install-the-kubernetes-cluster.html</guid><description>1. 集群规划 准备三个主机，一个 Master ，两个 Node。 操作系统，CentOS 7 配置，2 Core 4 GB Docker 版本，18.06.3 Kubernetes 版本，1.15.3 如果是购买的云主机，请将以下端口打开: 1 2 3 4 5 6 7 8 9 10 11 12 # Master TCP 6443* Kubernetes API Server TCP 2379-2380 etcd server client API TCP 10250 Kubelet API TCP 10251 kube-scheduler TCP 10252 kube-controller-manager TCP 10255 Read-Only</description><dc:creator>微信公众号</dc:creator><category>博文</category><category>Kubernetes</category><category>Kubeadm</category><category>安装</category></item><item><title>Kubernetes 中的 Ceph</title><link>https://www.chenshaowen.com/blog/ceph-in-kubernetes.html</link><pubDate>Wed, 07 Aug 2019 00:00:00 +0000</pubDate><atom:modified>Wed, 07 Aug 2019 00:00:00 +0000</atom:modified><guid>https://www.chenshaowen.com/blog/ceph-in-kubernetes.html</guid><description>1. 不同种类的存储 1.1 文件存储 文件存储是，基于文件的存储。在访问数据时，需要提供相应的查找路径。 适用于，FTP、NFS 等服务。 1.2 块存储 块存储是，将数据拆分成块，并单独存储各个部分。在访问数据时，底层存储软件会将这些分散的数据组装起来。 块存储，通</description><dc:creator>微信公众号</dc:creator><category>整理</category><category>Kubernetes</category><category>Ceph</category><category>存储</category></item><item><title>Kubernetes 之 Volumes</title><link>https://www.chenshaowen.com/blog/volumes-of-kubernetes.html</link><pubDate>Mon, 05 Aug 2019 00:00:00 +0000</pubDate><atom:modified>Mon, 05 Aug 2019 00:00:00 +0000</atom:modified><guid>https://www.chenshaowen.com/blog/volumes-of-kubernetes.html</guid><description>1. Docker 的存储卷 1.2 Docker 中的 Volume Docker Volume 将宿主机目录，挂载到容器中。在容器中修改的文件内容，将会被持久化到宿主机中。即时容器被删除，宿主机中的文件也会被保留。 Docker 使用 /var/lib/docker/volumes/ 存储容器的 Volume。 查看本地 Volume ： 1 2 3 4 5 6 7 8 9 tree /var/lib/docker/volumes/ -L 3 /var/lib/docker/volumes/ |-- 714450f353b26b5aa57aa352766c201c0851685e0e28c2e67ae1631f29c465b4 | `-- _data | |-- access.log -&amp;gt; /dev/stdout | `--</description><dc:creator>微信公众号</dc:creator><category>整理</category><category>Kubernetes</category><category>学习</category></item><item><title>Kubernetes 之 Labels、Selectors</title><link>https://www.chenshaowen.com/blog/labels-and-selectors-of-kubernetes.html</link><pubDate>Sun, 04 Aug 2019 00:00:00 +0000</pubDate><atom:modified>Sun, 04 Aug 2019 00:00:00 +0000</atom:modified><guid>https://www.chenshaowen.com/blog/labels-and-selectors-of-kubernetes.html</guid><description>1. Labels 1.1 什么是 Labels Labels 是一对关联到对象的键值对。可以在创建对象时，直接添加 Labels ，也可以在创建之后动态修改。 Labels 格式: 1 2 3 4 &amp;#34;labels&amp;#34;: { &amp;#34;key1&amp;#34; : &amp;#34;value1&amp;#34;, &amp;#34;key2&amp;#34; : &amp;#34;value2&amp;#34; } 格式要求： Key，不能重复 Value，须以字母或数字开头，可以使用字母、数字、连字符、点和下划线，最长63个</description><dc:creator>微信公众号</dc:creator><category>整理</category><category>Kubernetes</category><category>学习</category></item><item><title>Kubernetes 之网络</title><link>https://www.chenshaowen.com/blog/networks-of-kubernetes.html</link><pubDate>Sun, 28 Jul 2019 00:00:00 +0000</pubDate><atom:modified>Sun, 28 Jul 2019 00:00:00 +0000</atom:modified><guid>https://www.chenshaowen.com/blog/networks-of-kubernetes.html</guid><description>1. Docker 的网络模型 1.1 bridge 模式 默认使用 bridge 模式，也可以使用 --net=bridge 指定 bridge 模式。 bridge 模式下，容器连接到同一个虚拟网桥 docker0 上。docker0 通常会占用 172.17.0.1/16 网段。同一个网桥上的容器之间，可以通过 ip 直接通信。 1.2 host 模式 使用 --net=host 指定 host 模式。 host 模式，容器与主机共享 Network Namesp</description><dc:creator>微信公众号</dc:creator><category>整理</category><category>Kubernetes</category><category>API</category><category>对象</category></item><item><title>Kubernetes 之 API</title><link>https://www.chenshaowen.com/blog/api-of-kubernetes.html</link><pubDate>Fri, 26 Jul 2019 00:00:00 +0000</pubDate><atom:modified>Fri, 26 Jul 2019 00:00:00 +0000</atom:modified><guid>https://www.chenshaowen.com/blog/api-of-kubernetes.html</guid><description>1. Kubernetes 中的对象 Kubernetes 对象是系统中的持久实体，用于表示集群的状态。用户通过操作对象，与 Kubernetes 进行交互，告诉系统自己期望的工作负载情况。 对象的操作是通过 Kubernetes API 来实现的。每个 Kubernetes 对象包含两个嵌套的对象字段，Spec 和 Status。Spec 描述了期望的对象状态，</description><dc:creator>微信公众号</dc:creator><category>整理</category><category>Kubernetes</category><category>API</category><category>对象</category></item><item><title>使用 Helm 和 Operator 快速部署 Prometheus</title><link>https://www.chenshaowen.com/blog/quickly-deploy-prometheus-using-helm-and-operator.html</link><pubDate>Fri, 26 Jul 2019 00:00:00 +0000</pubDate><atom:modified>Fri, 26 Jul 2019 00:00:00 +0000</atom:modified><guid>https://www.chenshaowen.com/blog/quickly-deploy-prometheus-using-helm-and-operator.html</guid><description>Helm 是 Kubernetes 的包管理工具；Operator 用于管理 Kubernetes 的有状态分布式应用。本文主要描述如何使用 Helm、Operator 在 Minikube 集群上快速部署 Prometheus，并使用 Grafana 查看监控数据。Minikube 安装可以参考，搭建远程 Kubernetes 开发环境，Helm 配置可以</description><dc:creator>微信公众号</dc:creator><category>博文</category><category>Kubernetes</category><category>Helm</category><category>Operator</category></item><item><title>Kubernetes 复杂有状态应用管理框架 -- Operator</title><link>https://www.chenshaowen.com/blog/complex-application-management-framework-operator-for-kubernetes.html</link><pubDate>Thu, 25 Jul 2019 00:00:00 +0000</pubDate><atom:modified>Thu, 25 Jul 2019 00:00:00 +0000</atom:modified><guid>https://www.chenshaowen.com/blog/complex-application-management-framework-operator-for-kubernetes.html</guid><description>1. 缘起 最初，有两名 CoreOS 的员工，为了更方便部署 etcd 集群，在 etcdCluster 对象的增、删、改事件上绑定了相应的逻辑操作，借助 Kubernetes 来自动化管理 etcd 集群。 在几个月之后的 KubeCon 大会上，他们分享了这种称之为 Operator 的方案，得到社区的强烈回响。随后，大量项目宣布支持以 Operator 的方式进行运行和</description><dc:creator>微信公众号</dc:creator><category>整理</category><category>Kubernetes</category><category>Helm</category><category>实践</category></item><item><title>Kubernetes 的包管理器 -- Helm</title><link>https://www.chenshaowen.com/blog/package-manager-helm-of-kubernetes.html</link><pubDate>Wed, 24 Jul 2019 00:00:00 +0000</pubDate><atom:modified>Wed, 24 Jul 2019 00:00:00 +0000</atom:modified><guid>https://www.chenshaowen.com/blog/package-manager-helm-of-kubernetes.html</guid><description>1. 为什么需要 Helm Kubernetes 中一个重要的设计理念就是，声明式的操作。用户通过设置系统的预期状态来改变系统。例如，现在的副本数量是 2 ，需要调整为 3。声明式的处理方式是，修改配置文件中副本数量为 3 ；命令式的处理方式是，发送增加一个副本的命令，+1。 使用申明</description><dc:creator>微信公众号</dc:creator><category>博文</category><category>Kubernetes</category><category>Helm</category><category>实践</category></item><item><title>kubectl 实用指南</title><link>https://www.chenshaowen.com/blog/practice-guide-to-kubectl.html</link><pubDate>Fri, 19 Jul 2019 01:00:00 +0000</pubDate><atom:modified>Fri, 19 Jul 2019 01:00:00 +0000</atom:modified><guid>https://www.chenshaowen.com/blog/practice-guide-to-kubectl.html</guid><description>1. 什么是 kubectl kubectl 是 Kubernetes 的命令行工具，通过 API server 与集群进行交互。 2. 配置 kubectl kubectl 可以通过 ~/.kube/config 配置连接到一个或多个集群。 具体如何配置可以参考: 配置对多集群的访问 。如果需要配置远程集群，可以参考: 搭建远程 Kubernetes 开发环境。 查看配置的集群: 1 2 3 kubectl config get-contexts CURRENT NAME CLUSTER AUTHINFO NAMESPACE * minikube minikube minikube</description><dc:creator>微信公众号</dc:creator><category>整理</category><category>Kubernetes</category><category>kubectl</category><category>实践</category></item><item><title>Kubernetes 中的基本概念</title><link>https://www.chenshaowen.com/blog/basic-concepts-in-kubernetes.html</link><pubDate>Fri, 19 Jul 2019 00:00:00 +0000</pubDate><atom:modified>Fri, 19 Jul 2019 00:00:00 +0000</atom:modified><guid>https://www.chenshaowen.com/blog/basic-concepts-in-kubernetes.html</guid><description>1. 集群 1.1 Master Master 负责管理和维护 Kubernetes 集群信息，并向 Node 下放任务和接收反馈信息。 Master 上运行的组件有 kube-apiserver、kube-scheduler、kube-controller-manager、cloud-controller-manager</description><dc:creator>微信公众号</dc:creator><category>整理</category><category>Kubernetes</category><category>概念</category><category>学习</category></item><item><title>搭建远程 Kubernetes 开发环境</title><link>https://www.chenshaowen.com/blog/building-a-remote-kubernetes-development-environment.html</link><pubDate>Thu, 18 Jul 2019 00:00:00 +0000</pubDate><atom:modified>Thu, 18 Jul 2019 00:00:00 +0000</atom:modified><guid>https://www.chenshaowen.com/blog/building-a-remote-kubernetes-development-environment.html</guid><description>Minikube 是 Kubernetes 的单机发行版本，适用于产品体验和日常开发。这里使用 Minikube 搭建开发环境，将 Kubernetes 搭建在 CentOS 云服务器，本地使用 OS X 进行远程开发。 1. 云服务器安装 Minikube 在 Minikube 的 GitHub 版本页面，找到合适的版本，进行安装。 以 CentOS 为例，执行命令: 1 curl -Lo minikube https://storage.googleapis.com/minikube/releases/v1.2.0/minikube-linux-amd64 &amp;amp;&amp;amp; chmod +x minikube &amp;amp;&amp;amp; sudo cp minikube /usr/local/bin/ &amp;amp;&amp;amp; rm minikube minikube 是一</description><dc:creator>微信公众号</dc:creator><category>博文</category><category>Kubernetes</category><category>研发</category><category>环境</category><category>实践</category></item><item><title>开启 Kubernetes 监控并实施压力测试</title><link>https://www.chenshaowen.com/blog/open-k8s-monitoring-and-stress-testing..html</link><pubDate>Mon, 24 Sep 2018 00:00:00 +0000</pubDate><atom:modified>Mon, 24 Sep 2018 00:00:00 +0000</atom:modified><guid>https://www.chenshaowen.com/blog/open-k8s-monitoring-and-stress-testing..html</guid><description>1. Kubectl 基本命令 1.1 创建对象 1 2 3 4 5 6 7 8 # 创建资源，也可以使用远程 URL kubectl create -f ./my.yaml # 使用多个文件创建资源 kubectl create -f ./my1.yaml -f ./my2.yaml # 使用目录下的所有清单文件来创建资源 kubectl create -f ./dir # 启动一个 nginx 实例 kubectl run nginx --image=nginx 1.2 显示和查找资源 1 2 3 4 5 6 7 8 # 列出所有 namespace 中的 service kubectl get services # 列出所</description><dc:creator>微信公众号</dc:creator><category>博文</category><category>Kubernetes</category><category>测试</category><category>Apache</category></item><item><title>Windows 7 下使用 MiniKube 学习 Kubernetes</title><link>https://www.chenshaowen.com/blog/how-to-use-minikube-under-windows7.html</link><pubDate>Sun, 23 Sep 2018 00:00:00 +0000</pubDate><atom:modified>Sun, 23 Sep 2018 00:00:00 +0000</atom:modified><guid>https://www.chenshaowen.com/blog/how-to-use-minikube-under-windows7.html</guid><description>1. 基本概念 1.1 Kubernetes Kubernetes（简称，K8s），前身是 Google 的 Borg，是用于自动部署、扩展和管理容器化应用程序的开源系统。 提供的功能有： 容器的自动化部署 自动化扩缩容 自动化应用/服务升级 容器成组，对外提供服务，支持负载均衡 服务的健康检查，自</description><dc:creator>微信公众号</dc:creator><category>博文</category><category>Docker</category><category>Kubernetes</category><category>Windows</category><category>MiniKube</category></item></channel></rss>