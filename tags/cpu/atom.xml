<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:sy="http://purl.org/rss/1.0/modules/syndication/" xmlns:media="http://search.yahoo.com/mrss/"><channel><title>CPU on 陈少文的网站</title><link>https://www.chenshaowen.com/tags/cpu/</link><description>Recent content in CPU on 陈少文的网站</description><generator>Hugo -- gohugo.io</generator><language>zh</language><copyright>&amp;copy;2016 - {year}, All Rights Reserved.</copyright><lastBuildDate>Wed, 08 Nov 2023 00:00:00 +0000</lastBuildDate><sy:updatePeriod>weekly</sy:updatePeriod><atom:link href="https://www.chenshaowen.com/tags/cpu/atom.xml" rel="self" type="application/rss+xml"/><item><title>从 CPU 到网络记录一次排查应用慢的过程</title><link>https://www.chenshaowen.com/blog/record-a-troubleshooting-process-for-application-slowness.html</link><pubDate>Wed, 08 Nov 2023 00:00:00 +0000</pubDate><atom:modified>Wed, 08 Nov 2023 00:00:00 +0000</atom:modified><guid>https://www.chenshaowen.com/blog/record-a-troubleshooting-process-for-application-slowness.html</guid><description>1. 现象 业务反馈应用 app-a 的接口慢，查看日志发现是某一个 Pod 慢，删除该 Pod 让其更换节点就好。 从监控指标可以看到，Pod 的 CPU 使用率确实有剧增。 但该 Pod 没有达到 Limit 的限制，没有被限流 CPU。 接着看节点的 CPU 监控，发现节点的 CPU 使用率也有剧增。 并且增加的部分是 System C</description><dc:creator>微信公众号</dc:creator><category>博文</category><category>CPU</category><category>网络</category><category>应用</category><category>排查</category><category>故障</category></item><item><title>使用 CPU 推理 llama 结构的大模型</title><link>https://www.chenshaowen.com/blog/how-to-run-llama-on-cpu.html</link><pubDate>Sat, 16 Sep 2023 00:00:00 +0000</pubDate><atom:modified>Sat, 16 Sep 2023 00:00:00 +0000</atom:modified><guid>https://www.chenshaowen.com/blog/how-to-run-llama-on-cpu.html</guid><description>1. 本地容器运行 启动 LLM 1 docker run --rm -p 8000:8000 shaowenchen/chinese-alpaca-2-7b-gguf:Q2_K 在 http://localhost:8000/docs 页面即可看到接口文档，如下图: 部署一个简单的 Chat UI 这里需要注意的是 OPENAI_API_HOST 参数，需要设置为你的宿主机 IP 地址，而不是 localhost 127.0.0.1，否则无法访问。 1 docker run -e OPENAI_API_HOST=http://{YOUR_HOST_IP}:8000 -e OPENAI_API_KEY=random -p 3000:3000 hubimage/chatbot-ui:main 页面效果如下: 2. K8s 快速部署 部署 LLM 应用 kubectl create</description><dc:creator>微信公众号</dc:creator><category>博文</category><category>CPU</category><category>大模型</category><category>LLM</category><category>推理</category></item></channel></rss>