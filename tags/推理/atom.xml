<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:sy="http://purl.org/rss/1.0/modules/syndication/" xmlns:media="http://search.yahoo.com/mrss/"><channel><title>推理 on 陈少文的网站</title><link>https://www.chenshaowen.com/tags/%E6%8E%A8%E7%90%86/</link><description>Recent content in 推理 on 陈少文的网站</description><generator>Hugo -- gohugo.io</generator><language>zh</language><copyright>&amp;copy;2016 - {year}, All Rights Reserved.</copyright><lastBuildDate>Sun, 09 Feb 2025 10:00:00 +0000</lastBuildDate><sy:updatePeriod>weekly</sy:updatePeriod><atom:link href="https://www.chenshaowen.com/tags/%E6%8E%A8%E7%90%86/atom.xml" rel="self" type="application/rss+xml"/><item><title>分布式计算框架 Ray</title><link>https://www.chenshaowen.com/blog/what-is-ray.html</link><pubDate>Sun, 09 Feb 2025 10:00:00 +0000</pubDate><atom:modified>Sun, 09 Feb 2025 10:00:00 +0000</atom:modified><guid>https://www.chenshaowen.com/blog/what-is-ray.html</guid><description>1. 什么是 Ray 2016 年，UC Berkeley 的 RISELab 发布了一个新的分布式计算框架 Ray。 2017 年，发布 Ray 相关论文之后，受到业内的广泛关注，国内主要是蚂蚁集团采用并贡献了 Ray。 2020 年，Ray 发布了 1.0 版本，引入 Placement Group 特性，增加了用户自定义任务编排的灵活性，为后续的 Ray AI Libraries 和 vLLM 等</description><dc:creator>微信公众号</dc:creator><category>博文</category><category>分布式</category><category>计算框架</category><category>Ray</category><category>训练</category><category>推理</category></item><item><title>使用 vLLM 进行模型推理</title><link>https://www.chenshaowen.com/blog/use-vllm-for-inference.html</link><pubDate>Sat, 18 Jan 2025 00:00:00 +0000</pubDate><atom:modified>Sat, 18 Jan 2025 00:00:00 +0000</atom:modified><guid>https://www.chenshaowen.com/blog/use-vllm-for-inference.html</guid><description>1. 环境准备 下载 Miniforge 1 wget &amp;#34;https://github.com/conda-forge/miniforge/releases/latest/download/Miniforge3-$(uname)-$(uname -m).sh&amp;#34; 安装 Miniforge 1 bash Miniforge3-$(uname)-$(uname -m).sh 1 2 echo &amp;#34;export PATH=$HOME/miniforge3/bin:$PATH&amp;#34; &amp;gt;&amp;gt; ~/.bashrc source ~/.bashrc 创建环境 1 conda create -n vllm python=3.12 目前 vllm 要求 Python 3.9+ 激活环境 1 conda activate vllm 安装依赖 1 conda install vllm 2. 推理测试 2.1 模型准备 设置模型地址 海外 1 export MODEL_REPO=https://huggingface.co/Qwen/Qwen1.5-1.8B-Chat 国内 1 export MODEL_REPO=https://hf-mirror.com/Qwen/Qwen1.5-1.8B-Chat 下载模型 1 nerdctl run --rm -v ./:/runtime registry.cn-beijing.aliyuncs.com/shaowenchen/git lfs clone $MODEL_REPO 2.2 Offline Batched Inference 这种推理方式适用于离线场景，比</description><dc:creator>微信公众号</dc:creator><category>博文</category><category>vLLM</category><category>AI</category><category>推理</category></item><item><title>使用 vLLM 应用验证推理节点</title><link>https://www.chenshaowen.com/blog/use-vllm-verify-inference-node.html</link><pubDate>Thu, 16 Jan 2025 00:00:00 +0000</pubDate><atom:modified>Thu, 16 Jan 2025 00:00:00 +0000</atom:modified><guid>https://www.chenshaowen.com/blog/use-vllm-verify-inference-node.html</guid><description>1. 制作镜像 为了方便测试，这里将模型文件打包到镜像中。 下载模型 1 2 3 4 git clone https://huggingface.co/Qwen/Qwen1.5-1.8B-Chat cd Qwen1.5-1.8B-Chat &amp;amp;&amp;amp; git lfs pull rm -rf .git cd .. 编写 Dockerfile 1 2 3 4 5 cat &amp;lt;&amp;lt;EOF &amp;gt; Dockerfile FROM vllm/vllm-openai:latest RUN mkdir -p /models/Qwen1.5-1.8B-Chat COPY Qwen1.5-1.8B-Chat/* /models/Qwen1.5-1.8B-Chat EOF 编译镜像 1 nerdctl build --platform=amd64 -t registry-1.docker.io/shaowenchen/demo-vllm-qwen:1.5-1.8b-chat-amd64 . 推送镜像 1 nerdctl push --platform=amd64 registry-1.docker.io/shaowenchen/demo-vllm-qwen:1.5-1.8b-chat-amd64 为了方便国内的集群测试，我将镜像推送到了阿里云的容器镜像服务</description><dc:creator>微信公众号</dc:creator><category>博文</category><category>vLLM</category><category>推理</category><category>AI</category></item><item><title>使用 TensorRT 加速模型推理</title><link>https://www.chenshaowen.com/blog/speeding-up-model-inference-with-tensorrt.html</link><pubDate>Tue, 06 Feb 2024 00:00:00 +0000</pubDate><atom:modified>Tue, 06 Feb 2024 00:00:00 +0000</atom:modified><guid>https://www.chenshaowen.com/blog/speeding-up-model-inference-with-tensorrt.html</guid><description>1. 什么是 TensorRT TensorRT 是一个 C++ 库，主要用在 NVIDIA GPU 进行高性能的推理加速上，提供了 C++ API 和 Python API 用于集成。 TensorRT 支持的主流深度学习框架有: Caffe，TensorRT 可以直接读取 prototxt 格式 TensorFlow，需要将 TensorFlow 的 pb 转换为 uff 格式 PyTorch，需要将 PyTorch 的 pth 格式转</description><dc:creator>微信公众号</dc:creator><category>博文</category><category>AI</category><category>TensorRT</category><category>NVIDIA</category><category>推理</category></item><item><title>使用 CPU 推理 llama 结构的大模型</title><link>https://www.chenshaowen.com/blog/how-to-run-llama-on-cpu.html</link><pubDate>Sat, 16 Sep 2023 00:00:00 +0000</pubDate><atom:modified>Sat, 16 Sep 2023 00:00:00 +0000</atom:modified><guid>https://www.chenshaowen.com/blog/how-to-run-llama-on-cpu.html</guid><description>1. 本地容器运行 启动 LLM 1 docker run --rm -p 8000:8000 shaowenchen/chinese-alpaca-2-7b-gguf:Q2_K 在 http://localhost:8000/docs 页面即可看到接口文档，如下图: 部署一个简单的 Chat UI 这里需要注意的是 OPENAI_API_HOST 参数，需要设置为你的宿主机 IP 地址，而不是 localhost 127.0.0.1，否则无法访问。 1 docker run -e OPENAI_API_HOST=http://{YOUR_HOST_IP}:8000 -e OPENAI_API_KEY=random -p 3000:3000 hubimage/chatbot-ui:main 页面效果如下: 2. K8s 快速部署 部署 LLM 应用 kubectl create</description><dc:creator>微信公众号</dc:creator><category>博文</category><category>CPU</category><category>大模型</category><category>LLM</category><category>推理</category></item></channel></rss>