<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:sy="http://purl.org/rss/1.0/modules/syndication/" xmlns:media="http://search.yahoo.com/mrss/"><channel><title>大模型 on 陈少文的网站</title><link>https://www.chenshaowen.com/tags/%E5%A4%A7%E6%A8%A1%E5%9E%8B/</link><description>Recent content in 大模型 on 陈少文的网站</description><generator>Hugo -- gohugo.io</generator><language>zh</language><copyright>&amp;copy;2016 - {year}, All Rights Reserved.</copyright><lastBuildDate>Sun, 09 Feb 2025 00:00:00 +0000</lastBuildDate><sy:updatePeriod>weekly</sy:updatePeriod><atom:link href="https://www.chenshaowen.com/tags/%E5%A4%A7%E6%A8%A1%E5%9E%8B/atom.xml" rel="self" type="application/rss+xml"/><item><title>以 Qwen 为例，学习大模型的结构</title><link>https://www.chenshaowen.com/blog/structure-of-large-models-with-qwen.html</link><pubDate>Sun, 09 Feb 2025 00:00:00 +0000</pubDate><atom:modified>Sun, 09 Feb 2025 00:00:00 +0000</atom:modified><guid>https://www.chenshaowen.com/blog/structure-of-large-models-with-qwen.html</guid><description>1. Qwen 模型介绍 2023 年 4 月，阿里巴巴推出 Qwen 的测试版。 2023 年 12 月，阿里巴巴开源了 Qwen 的第一个版本。 2024 年 9 月，阿里巴巴发布了 Qwen2.5。 2025 年 1 月，阿里巴巴发布了 Qwen 2.5-Max。 Qwen 2.5 是 Qwen 大语言模型的目前最新系列。之所以说是系列，是因为在训练完一个预训</description><dc:creator>微信公众号</dc:creator><category>博文</category><category>Qwen</category><category>大模型</category><category>学习</category></item><item><title>什么是 Token</title><link>https://www.chenshaowen.com/blog/what-is-token.html</link><pubDate>Tue, 10 Sep 2024 10:00:00 +0000</pubDate><atom:modified>Tue, 10 Sep 2024 10:00:00 +0000</atom:modified><guid>https://www.chenshaowen.com/blog/what-is-token.html</guid><description>Token 是一个与数据紧密相关的单位，可以用来度量训练模型所需的语料量，还可以用来度量推理时的输入和输出长度。 1. token 是什么 Token 可以是一个完整的单词、子词，甚至是一个字符。在语言模型中，文本被拆分为若干个 token，模型逐一处理这些 token 来生成预测或生成新文</description><dc:creator>微信公众号</dc:creator><category>整理</category><category>机器学习</category><category>大模型</category><category>Token</category><category>什么是</category></item><item><title>什么是 FLOPs</title><link>https://www.chenshaowen.com/blog/what-is-flops.html</link><pubDate>Mon, 09 Sep 2024 10:00:00 +0000</pubDate><atom:modified>Mon, 09 Sep 2024 10:00:00 +0000</atom:modified><guid>https://www.chenshaowen.com/blog/what-is-flops.html</guid><description>1. 关于 FLOPs FLOPs（Floating Point Operations Per Second）指的是每秒执行的浮点数运算次数。 具体地说： 一次浮点加法：如 a + b，被计为一次浮点运算。 一次浮点乘法：如 a * b，也被计为一次浮点运算。 其他基本浮点运算：如除法和平方根，也可以被计为一次浮</description><dc:creator>微信公众号</dc:creator><category>整理</category><category>机器学习</category><category>大模型</category><category>FLOPs</category><category>什么是</category></item><item><title>什么是 PD 分离</title><link>https://www.chenshaowen.com/blog/what-is-pd-separation.html</link><pubDate>Sun, 08 Sep 2024 10:00:00 +0000</pubDate><atom:modified>Sun, 08 Sep 2024 10:00:00 +0000</atom:modified><guid>https://www.chenshaowen.com/blog/what-is-pd-separation.html</guid><description>1. 定义 LLM 推理过程中存在着两个截然不同的阶段，PD 分离就 计算密集型的 Prefill 阶段， LLM 处理所有用户的 input，计算出对应的 KV Cache 显存密集型的 Decode 阶段， 顺序的产生一个个的 token，每次访存只计算一个 token 2. 指标 2.1 prefill 性能评估指标 TTFT（Time To First Toke</description><dc:creator>微信公众号</dc:creator><category>整理</category><category>机器学习</category><category>大模型</category><category>PD</category><category>什么是</category></item><item><title>开发了一个 Copilot 用来处理运维故障</title><link>https://www.chenshaowen.com/blog/develop-a-copilot-to-handle-exceptions.html</link><pubDate>Wed, 14 Aug 2024 00:00:00 +0000</pubDate><atom:modified>Wed, 14 Aug 2024 00:00:00 +0000</atom:modified><guid>https://www.chenshaowen.com/blog/develop-a-copilot-to-handle-exceptions.html</guid><description>本篇内容主要来自内部的一次分享，也是最近工作的一些总结。 1. 常见的故障处理流程 如上图是一次典型的运维异常处理流程。 按照时间线，有如下关键时间点: 发生故障 发现故障 响应故障 定位故障 恢复故障 发生故障到发现故障，指的是被系统检测到，主要涉及到指标的采</description><dc:creator>微信公众号</dc:creator><category>博文</category><category>大模型</category><category>运维</category><category>异常</category><category>故障</category><category>分享</category></item><item><title>模型研发周期中的数据存储</title><link>https://www.chenshaowen.com/blog/data-storage-in-model-development-cycle.html</link><pubDate>Sun, 26 May 2024 00:00:00 +0000</pubDate><atom:modified>Sun, 26 May 2024 00:00:00 +0000</atom:modified><guid>https://www.chenshaowen.com/blog/data-storage-in-model-development-cycle.html</guid><description>1. 基于对象存储的数据交付 如上图，在模型研发过程中，主要涉及三个子平台，分别是: 数据平台 数据平台主要负责数据相关的管理，比如: 数据接入、数据处理，最终生成训练所需的数据。 数据平台将原始数据存储到对象存储中，在处理时，从对象存储中获取数据，进行</description><dc:creator>微信公众号</dc:creator><category>博文</category><category>大模型</category><category>模型</category><category>JuiceFS</category><category>数据存储</category></item><item><title>什么是 MLOps</title><link>https://www.chenshaowen.com/blog/what-is-mlops.html</link><pubDate>Sat, 27 Apr 2024 10:00:00 +0000</pubDate><atom:modified>Sat, 27 Apr 2024 10:00:00 +0000</atom:modified><guid>https://www.chenshaowen.com/blog/what-is-mlops.html</guid><description>1. 什么是 MLOps MLOps 是 Machine Learning Operations 的缩写，描述的是围绕模型研发整个生命周期过程的标准化和工程化。 MLOps 包括以下几个关键步骤: 数据管理，数据的存储、访问、清洗、转换 模型开发，算法开发、模型构建 模型训练与调优，使用数据训练模型，调整超参数优化模型，微调模型 模型评</description><dc:creator>微信公众号</dc:creator><category>整理</category><category>机器学习</category><category>MLOps</category><category>大模型</category><category>研发</category><category>什么是</category></item><item><title>我在给 Ops 工具写 Copilot</title><link>https://www.chenshaowen.com/blog/writing-copilot-for-my-ops-tool.html</link><pubDate>Sat, 23 Sep 2023 00:00:00 +0000</pubDate><atom:modified>Sat, 23 Sep 2023 00:00:00 +0000</atom:modified><guid>https://www.chenshaowen.com/blog/writing-copilot-for-my-ops-tool.html</guid><description>1. 什么是 Ops 工具 https://www.chenshaowen.com/ops/ 是我日常运维最频繁使用的工具之一。 运维机器，我可以复用之前的脚本，批量进行操作。 运维集群，我可以复用之前的脚本，不用登录节点也可以操作机器。 如果遇到新的运维问题，我会马上编写 Task Yaml 对操作进行固化，方便下一次复用。 Ops 的核心操作是</description><dc:creator>微信公众号</dc:creator><category>博文</category><category>大模型</category><category>思考</category><category>API</category><category>文档</category><category>LLM</category><category>Ops</category><category>Copilot</category></item><item><title>使用 CPU 推理 llama 结构的大模型</title><link>https://www.chenshaowen.com/blog/how-to-run-llama-on-cpu.html</link><pubDate>Sat, 16 Sep 2023 00:00:00 +0000</pubDate><atom:modified>Sat, 16 Sep 2023 00:00:00 +0000</atom:modified><guid>https://www.chenshaowen.com/blog/how-to-run-llama-on-cpu.html</guid><description>1. 本地容器运行 启动 LLM 1 docker run --rm -p 8000:8000 shaowenchen/chinese-alpaca-2-7b-gguf:Q2_K 在 http://localhost:8000/docs 页面即可看到接口文档，如下图: 部署一个简单的 Chat UI 这里需要注意的是 OPENAI_API_HOST 参数，需要设置为你的宿主机 IP 地址，而不是 localhost 127.0.0.1，否则无法访问。 1 docker run -e OPENAI_API_HOST=http://{YOUR_HOST_IP}:8000 -e OPENAI_API_KEY=random -p 3000:3000 hubimage/chatbot-ui:main 页面效果如下: 2. K8s 快速部署 部署 LLM 应用 kubectl create</description><dc:creator>微信公众号</dc:creator><category>博文</category><category>CPU</category><category>大模型</category><category>LLM</category><category>推理</category></item><item><title>大模型部署工具 llama.cpp</title><link>https://www.chenshaowen.com/blog/llama-cpp-that-is-a-llm-deployment-tool.html</link><pubDate>Tue, 05 Sep 2023 00:00:00 +0000</pubDate><atom:modified>Tue, 05 Sep 2023 00:00:00 +0000</atom:modified><guid>https://www.chenshaowen.com/blog/llama-cpp-that-is-a-llm-deployment-tool.html</guid><description>1. 大模型部署工具 llama.cpp 大模型的研究分为训练和推理两个部分。训练的过程，实际上就是在寻找模型参数，使得模型的损失函数最小化，推理结果最优化的过程。训练完成之后，模型的参数就固定了，这时候就可以使用模型进行推理，对外提供服务。 llama.cpp 主要解决的是推理过程</description><dc:creator>微信公众号</dc:creator><category>AI</category><category>大模型</category><category>工具</category><category>llama.cpp</category><category>博文</category></item><item><title>transformers 库的使用</title><link>https://www.chenshaowen.com/blog/usage-of-transformers-lib.html</link><pubDate>Tue, 22 Aug 2023 00:00:00 +0000</pubDate><atom:modified>Tue, 22 Aug 2023 00:00:00 +0000</atom:modified><guid>https://www.chenshaowen.com/blog/usage-of-transformers-lib.html</guid><description>transformers 是由 Hugging Face 开发的 Python 库，用于在自然语言处理（NLP）任务中使用和训练预训练的 Transformer 模型。它提供了许多强大的工具和功能，使得处理文本数据和构建 NLP 模型变得更加容易。该库广泛应用于各种 NLP 任务，如文本分类、命名实体识别、问答、文本生成等。 1. transformers 中的 pipeline pipeline 提供</description><dc:creator>微信公众号</dc:creator><category>整理</category><category>Transformer</category><category>AI</category><category>大模型</category><category>NLP</category></item><item><title>Transformer 学习笔记</title><link>https://www.chenshaowen.com/blog/learning-notes-of-transformer.html</link><pubDate>Sun, 20 Aug 2023 00:00:00 +0000</pubDate><atom:modified>Sun, 20 Aug 2023 00:00:00 +0000</atom:modified><guid>https://www.chenshaowen.com/blog/learning-notes-of-transformer.html</guid><description>1. 为什么是 Transformer 全连接的自注意 以往的 RNN 模型，每个单词只能和邻近的单词产生联系，而 Transformer 模型中的 Attention 机制，单词可以和任意位置的单词产生联系，这样就可以捕捉到全局的上下文信息。 没有梯度消失问题 RNN 作用在同一个权值矩阵上，使得其最大的特征值小于 1 时，就会出现</description><dc:creator>微信公众号</dc:creator><category>整理</category><category>Transformer</category><category>AI</category><category>大模型</category></item><item><title>影响使用大模型的技术因素</title><link>https://www.chenshaowen.com/blog/the-key-factors-while-using-large-models.html</link><pubDate>Sat, 19 Aug 2023 00:00:00 +0000</pubDate><atom:modified>Sat, 19 Aug 2023 00:00:00 +0000</atom:modified><guid>https://www.chenshaowen.com/blog/the-key-factors-while-using-large-models.html</guid><description>1. 大模型到底是什么 先请两位大模型回答一下这个问题，看看他们的回答是什么。 Claude 说，大模型本质上是语言知识的概率表达，通过统计学习对语言各层次规律建模，表征语言生成的先验分布，从而具备语言预测生成能力。 ChatGPT 说，大模型本质是深度神经网络通过大量参数和</description><dc:creator>微信公众号</dc:creator><category>整理</category><category>AI</category><category>大模型</category><category>思考</category></item><item><title>AI 基础知识点</title><link>https://www.chenshaowen.com/blog/ai-basic-knowledge.html</link><pubDate>Fri, 18 Aug 2023 11:22:55 +0000</pubDate><atom:modified>Fri, 18 Aug 2023 11:22:55 +0000</atom:modified><guid>https://www.chenshaowen.com/blog/ai-basic-knowledge.html</guid><description>1. 关键字 机器学习(ML) 从数据中自动获取知识的技术 神经网络(NN) 模仿生物神经网络结构和学习机制的模型，是机器学习的分支之一 神经网络的结构包括，输入层、隐藏层、输出层 深度神经网络(DNN) 隐含层常常大于 2 层 DNN 的出众表现源于它使用统计学方法从</description><dc:creator>微信公众号</dc:creator><category>整理</category><category>AI</category><category>LLM</category><category>机器学习</category><category>大模型</category></item></channel></rss>