<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:sy="http://purl.org/rss/1.0/modules/syndication/" xmlns:media="http://search.yahoo.com/mrss/"><channel><title>Transformer on 陈少文的网站</title><link>https://www.chenshaowen.com/tags/transformer/</link><description>Recent content in Transformer on 陈少文的网站</description><generator>Hugo -- gohugo.io</generator><language>zh</language><copyright>&amp;copy;2016 - {year}, All Rights Reserved.</copyright><lastBuildDate>Tue, 22 Aug 2023 00:00:00 +0000</lastBuildDate><sy:updatePeriod>weekly</sy:updatePeriod><atom:link href="https://www.chenshaowen.com/tags/transformer/atom.xml" rel="self" type="application/rss+xml"/><item><title>transformers 库的使用</title><link>https://www.chenshaowen.com/blog/usage-of-transformers-lib.html</link><pubDate>Tue, 22 Aug 2023 00:00:00 +0000</pubDate><atom:modified>Tue, 22 Aug 2023 00:00:00 +0000</atom:modified><guid>https://www.chenshaowen.com/blog/usage-of-transformers-lib.html</guid><description>transformers 是由 Hugging Face 开发的 Python 库，用于在自然语言处理（NLP）任务中使用和训练预训练的 Transformer 模型。它提供了许多强大的工具和功能，使得处理文本数据和构建 NLP 模型变得更加容易。该库广泛应用于各种 NLP 任务，如文本分类、命名实体识别、问答、文本生成等。 1. transformers 中的 pipeline pipeline 提供</description><dc:creator>微信公众号</dc:creator><category>整理</category><category>Transformer</category><category>AI</category><category>大模型</category><category>NLP</category></item><item><title>HuggingFace 的模型和数据操作</title><link>https://www.chenshaowen.com/blog/models-and-datasets-on-huggingface.html</link><pubDate>Mon, 21 Aug 2023 00:00:00 +0000</pubDate><atom:modified>Mon, 21 Aug 2023 00:00:00 +0000</atom:modified><guid>https://www.chenshaowen.com/blog/models-and-datasets-on-huggingface.html</guid><description>HuggingFace 通过提供共享模型 model、数据集 dataset、在线托管 space 等服务，为 AI 研究人员和开发者提供了一个完整的生态。本篇文章将介绍如何使用 HuggingFace 的模型和数据集。 1. 模型操作与使用 1.1 自定义存储目录 1 export HF_HOME=/Volumes/Data/HuggingFace 否则默认在 ~/.cache/huggingface 目录下。 1.2 模型的下载 第一种方法，页面上</description><dc:creator>微信公众号</dc:creator><category>整理</category><category>Transformer</category><category>AI</category><category>模型</category><category>HuggingFace</category><category>数据集</category></item><item><title>Transformer 学习笔记</title><link>https://www.chenshaowen.com/blog/learning-notes-of-transformer.html</link><pubDate>Sun, 20 Aug 2023 00:00:00 +0000</pubDate><atom:modified>Sun, 20 Aug 2023 00:00:00 +0000</atom:modified><guid>https://www.chenshaowen.com/blog/learning-notes-of-transformer.html</guid><description>1. 为什么是 Transformer 全连接的自注意 以往的 RNN 模型，每个单词只能和邻近的单词产生联系，而 Transformer 模型中的 Attention 机制，单词可以和任意位置的单词产生联系，这样就可以捕捉到全局的上下文信息。 没有梯度消失问题 RNN 作用在同一个权值矩阵上，使得其最大的特征值小于 1 时，就会出现</description><dc:creator>微信公众号</dc:creator><category>整理</category><category>Transformer</category><category>AI</category><category>大模型</category></item></channel></rss>