<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:sy="http://purl.org/rss/1.0/modules/syndication/" xmlns:media="http://search.yahoo.com/mrss/"><channel><title>vLLM on 陈少文的网站</title><link>https://www.chenshaowen.com/tags/vllm/</link><description>Recent content in vLLM on 陈少文的网站</description><generator>Hugo -- gohugo.io</generator><language>zh</language><copyright>&amp;copy;2016 - {year}, All Rights Reserved.</copyright><lastBuildDate>Sat, 18 Jan 2025 00:00:00 +0000</lastBuildDate><sy:updatePeriod>weekly</sy:updatePeriod><atom:link href="https://www.chenshaowen.com/tags/vllm/atom.xml" rel="self" type="application/rss+xml"/><item><title>使用 vLLM 进行模型推理</title><link>https://www.chenshaowen.com/blog/use-vllm-for-inference.html</link><pubDate>Sat, 18 Jan 2025 00:00:00 +0000</pubDate><atom:modified>Sat, 18 Jan 2025 00:00:00 +0000</atom:modified><guid>https://www.chenshaowen.com/blog/use-vllm-for-inference.html</guid><description>1. 环境准备 下载 Miniforge 1 wget &amp;#34;https://github.com/conda-forge/miniforge/releases/latest/download/Miniforge3-$(uname)-$(uname -m).sh&amp;#34; 安装 Miniforge 1 bash Miniforge3-$(uname)-$(uname -m).sh 1 2 echo &amp;#34;export PATH=$HOME/miniforge3/bin:$PATH&amp;#34; &amp;gt;&amp;gt; ~/.bashrc source ~/.bashrc 创建环境 1 conda create -n vllm python=3.12 目前 vllm 要求 Python 3.9+ 激活环境 1 conda activate vllm 安装依赖 1 conda install vllm 2. 推理测试 2.1 模型准备 设置模型地址 海外 1 export MODEL_REPO=https://huggingface.co/Qwen/Qwen1.5-1.8B-Chat 国内 1 export MODEL_REPO=https://hf-mirror.com/Qwen/Qwen1.5-1.8B-Chat 下载模型 1 nerdctl run --rm -v ./:/runtime registry.cn-beijing.aliyuncs.com/shaowenchen/git lfs clone $MODEL_REPO 2.2 Offline Batched Inference 这种推理方式适用于离线场景，比</description><dc:creator>微信公众号</dc:creator><category>博文</category><category>vLLM</category><category>AI</category><category>推理</category></item><item><title>使用 vLLM 应用验证推理节点</title><link>https://www.chenshaowen.com/blog/use-vllm-verify-inference-node.html</link><pubDate>Thu, 16 Jan 2025 00:00:00 +0000</pubDate><atom:modified>Thu, 16 Jan 2025 00:00:00 +0000</atom:modified><guid>https://www.chenshaowen.com/blog/use-vllm-verify-inference-node.html</guid><description>1. 制作镜像 为了方便测试，这里将模型文件打包到镜像中。 下载模型 1 2 3 4 git clone https://huggingface.co/Qwen/Qwen1.5-1.8B-Chat cd Qwen1.5-1.8B-Chat &amp;amp;&amp;amp; git lfs pull rm -rf .git cd .. 编写 Dockerfile 1 2 3 4 5 cat &amp;lt;&amp;lt;EOF &amp;gt; Dockerfile FROM vllm/vllm-openai:latest RUN mkdir -p /models/Qwen1.5-1.8B-Chat COPY Qwen1.5-1.8B-Chat/* /models/Qwen1.5-1.8B-Chat EOF 编译镜像 1 nerdctl build --platform=amd64 -t registry-1.docker.io/shaowenchen/demo-vllm-qwen:1.5-1.8b-chat-amd64 . 推送镜像 1 nerdctl push --platform=amd64 registry-1.docker.io/shaowenchen/demo-vllm-qwen:1.5-1.8b-chat-amd64 为了方便国内的集群测试，我将镜像推送到了阿里云的容器镜像服务</description><dc:creator>微信公众号</dc:creator><category>博文</category><category>vLLM</category><category>推理</category><category>AI</category></item></channel></rss>