<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:sy="http://purl.org/rss/1.0/modules/syndication/" xmlns:media="http://search.yahoo.com/mrss/"><channel><title>AI on 陈少文的网站</title><link>https://www.chenshaowen.com/tags/ai/</link><description>Recent content in AI on 陈少文的网站</description><generator>Hugo -- gohugo.io</generator><language>zh</language><copyright>&amp;copy;2016 - {year}, All Rights Reserved.</copyright><lastBuildDate>Mon, 20 Jan 2025 01:00:00 +0000</lastBuildDate><sy:updatePeriod>weekly</sy:updatePeriod><atom:link href="https://www.chenshaowen.com/tags/ai/atom.xml" rel="self" type="application/rss+xml"/><item><title>NVIDIA 环境变量配置</title><link>https://www.chenshaowen.com/blog/nvidia-environment-variable-configuration.html</link><pubDate>Mon, 20 Jan 2025 01:00:00 +0000</pubDate><atom:modified>Mon, 20 Jan 2025 01:00:00 +0000</atom:modified><guid>https://www.chenshaowen.com/blog/nvidia-environment-variable-configuration.html</guid><description>NVIDIA_VISIBLE_DEVICES 指定程序可见的 GPU 设备 1 CUDA_VISIBLE_DEVICES=0,1 可用值: 1,2，以逗号分隔的 GPU UUID 或索引列表 all，所有 GPU none，加载驱动，但无法访问 GPU void，不加载驱动 NVIDIA_DRIVER_CAPABILITIES 控制哪些驱动程序库/二进制文件将被安装在容器内 1 NVIDIA_DRIVER_CAPABILITIES=compute,utility 可用值: compute，CUDA 和 OpenCL 应用程序所需。 co</description><dc:creator>微信公众号</dc:creator><category>博文</category><category>AI</category><category>环境</category><category>NVIDIA</category><category>GPU</category></item><item><title>Ascend 环境变量配置</title><link>https://www.chenshaowen.com/blog/ascend-environment-variable-configuration.html</link><pubDate>Mon, 20 Jan 2025 00:00:00 +0000</pubDate><atom:modified>Mon, 20 Jan 2025 00:00:00 +0000</atom:modified><guid>https://www.chenshaowen.com/blog/ascend-environment-variable-configuration.html</guid><description>HCCL_IF_IP 配置 HCCL 的初始化 root 通信网卡 IP 。 环境变量 HCCL_IF_IP &amp;gt; 环境变量 HCCL_SOCKET_IFNAME &amp;gt; docker/lo 以外网卡(网卡名字典序升序) &amp;gt; docker 网卡 &amp;gt; lo 网卡。 1 export HCCL_IF_IP=10.10.10.1 HCCL_IF_BASE_PORT 指定 Host 网卡起始端口号，配置后系统默认占用以该端口起始的 16 个端口进行集群信息收集，取值范围为[1024,65520] 。 1 export HCCL_IF_BASE_PORT=50000 HCCL_SOCKET_IFNAME HCCL 可通</description><dc:creator>微信公众号</dc:creator><category>博文</category><category>AI</category><category>环境</category><category>Ascend</category><category>NPU</category></item><item><title>Fluid 下的 Juicefs 企业版维护</title><link>https://www.chenshaowen.com/blog/fluid-juicefs-enterprise-maintenance.html</link><pubDate>Sat, 18 Jan 2025 01:00:00 +0000</pubDate><atom:modified>Sat, 18 Jan 2025 01:00:00 +0000</atom:modified><guid>https://www.chenshaowen.com/blog/fluid-juicefs-enterprise-maintenance.html</guid><description>1. 设置环境变量 1 2 export NAMESPACE=xxx export PVC=xxx 2. Dataset 无法就绪 2.1 Fluid 组件问题 1 kubectl -n fluid-system get pod -o wide | grep -v &amp;#34;Running&amp;#34; 可能出现没有正常启动的情况。 2.2 有异常的 Dataset 异常的资源可能导致 Fluid 资源不断重启，需要人工介入删除。 2.3 检查 Worker \ Fuse 副本 worker 副本 1 kubectl -n ${NAMESPACE} get sts -l release=${PVC} 1 kubectl -n ${NAMESPACE} get pod -l release=${PVC},role=juicefs-worker fuse 副本 1 kubectl -n kas-job get ds -l</description><dc:creator>微信公众号</dc:creator><category>博文</category><category>Fluid</category><category>JuiceFS</category><category>AI</category><category>Data</category><category>Lustre</category></item><item><title>使用 vLLM 进行模型推理</title><link>https://www.chenshaowen.com/blog/use-vllm-for-inference.html</link><pubDate>Sat, 18 Jan 2025 00:00:00 +0000</pubDate><atom:modified>Sat, 18 Jan 2025 00:00:00 +0000</atom:modified><guid>https://www.chenshaowen.com/blog/use-vllm-for-inference.html</guid><description>1. 环境准备 下载 Miniforge 1 wget &amp;#34;https://github.com/conda-forge/miniforge/releases/latest/download/Miniforge3-$(uname)-$(uname -m).sh&amp;#34; 安装 Miniforge 1 bash Miniforge3-$(uname)-$(uname -m).sh 1 2 echo &amp;#34;export PATH=$HOME/miniforge3/bin:$PATH&amp;#34; &amp;gt;&amp;gt; ~/.bashrc source ~/.bashrc 创建环境 1 conda create -n vllm python=3.12 目前 vllm 要求 Python 3.9+ 激活环境 1 conda activate vllm 安装依赖 1 conda install vllm 2. 推理测试 2.1 模型准备 设置模型地址 海外 1 export MODEL_REPO=https://huggingface.co/Qwen/Qwen1.5-1.8B-Chat 国内 1 export MODEL_REPO=https://hf-mirror.com/Qwen/Qwen1.5-1.8B-Chat 下载模型 1 nerdctl run --rm -v ./:/runtime registry.cn-beijing.aliyuncs.com/shaowenchen/git lfs clone $MODEL_REPO 2.2 Offline Batched Inference 这种推理方式适用于离线场景，比</description><dc:creator>微信公众号</dc:creator><category>博文</category><category>vLLM</category><category>AI</category><category>推理</category></item><item><title>使用 vLLM 应用验证推理节点</title><link>https://www.chenshaowen.com/blog/use-vllm-verify-inference-node.html</link><pubDate>Thu, 16 Jan 2025 00:00:00 +0000</pubDate><atom:modified>Thu, 16 Jan 2025 00:00:00 +0000</atom:modified><guid>https://www.chenshaowen.com/blog/use-vllm-verify-inference-node.html</guid><description>1. 制作镜像 为了方便测试，这里将模型文件打包到镜像中。 下载模型 1 2 3 4 git clone https://huggingface.co/Qwen/Qwen1.5-1.8B-Chat cd Qwen1.5-1.8B-Chat &amp;amp;&amp;amp; git lfs pull rm -rf .git cd .. 编写 Dockerfile 1 2 3 4 5 cat &amp;lt;&amp;lt;EOF &amp;gt; Dockerfile FROM vllm/vllm-openai:latest RUN mkdir -p /models/Qwen1.5-1.8B-Chat COPY Qwen1.5-1.8B-Chat/* /models/Qwen1.5-1.8B-Chat EOF 编译镜像 1 nerdctl build --platform=amd64 -t registry-1.docker.io/shaowenchen/demo-vllm-qwen:1.5-1.8b-chat-amd64 . 推送镜像 1 nerdctl push --platform=amd64 registry-1.docker.io/shaowenchen/demo-vllm-qwen:1.5-1.8b-chat-amd64 为了方便国内的集群测试，我将镜像推送到了阿里云的容器镜像服务</description><dc:creator>微信公众号</dc:creator><category>博文</category><category>vLLM</category><category>推理</category><category>AI</category></item><item><title>AI 应用开发技术栈</title><link>https://www.chenshaowen.com/blog/ai-application-development-tech-stack.html</link><pubDate>Sun, 12 Jan 2025 00:00:00 +0000</pubDate><atom:modified>Sun, 12 Jan 2025 00:00:00 +0000</atom:modified><guid>https://www.chenshaowen.com/blog/ai-application-development-tech-stack.html</guid><description>Embedding 模型 Embedding 模式将高维度的数据映射到低维度的空间，这样有利于数据的处理和分析。 文本模型 这里有一个排行榜，https://huggingface.co/spaces/mteb/leaderboard 在上面的排行榜中，会给出模型的评分，模型的参数量</description><dc:creator>微信公众号</dc:creator><category>博文</category><category>AI</category><category>应用开发</category></item><item><title>使用 Fluid 和 S3FS 对接 S3 存储及性能测试</title><link>https://www.chenshaowen.com/blog/using-fluid-and-s3fs-to-access-s3-storage-and-performance-testing.html</link><pubDate>Thu, 05 Dec 2024 00:00:00 +0000</pubDate><atom:modified>Thu, 05 Dec 2024 00:00:00 +0000</atom:modified><guid>https://www.chenshaowen.com/blog/using-fluid-and-s3fs-to-access-s3-storage-and-performance-testing.html</guid><description>本文使用的是 Fluid 1.0 版本，高版本的配置文件路径发生了变化，需要根据实际情况调整。 1. 制作镜像 1.1 fluid_config_init.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 #!/usr/bin/env python import json import os rawStr = &amp;#34;&amp;#34; with open(&amp;#34;/etc/fluid/config.json&amp;#34;, &amp;#34;r&amp;#34;) as f: rawStr = f.readlines() rawStr = rawStr[0] script = &amp;#34;&amp;#34;&amp;#34; #!/bin/sh set -ex MNT_TO=$targetPath trap &amp;#34;umount ${MNT_TO}&amp;#34; SIGTERM mkdir -p ${MNT_TO}</description><dc:creator>微信公众号</dc:creator><category>博文</category><category>s3</category><category>Fluid</category><category>JuiceFS</category><category>AI</category><category>Data</category></item><item><title>使用 Fluid 对接 S3 存储及性能测试</title><link>https://www.chenshaowen.com/blog/using-fluid-to-access-s3-storage-and-performance-testing.html</link><pubDate>Wed, 04 Dec 2024 00:00:00 +0000</pubDate><atom:modified>Wed, 04 Dec 2024 00:00:00 +0000</atom:modified><guid>https://www.chenshaowen.com/blog/using-fluid-to-access-s3-storage-and-performance-testing.html</guid><description>1. Jindo 挂载 S3 配置环境变量 1 2 3 4 export ENDPOINT=obs.ap-southeast-3.myhuaweicloud.com export BUCKET= export AK= export SK= 创建凭证 1 2 3 4 5 6 7 8 9 10 kubectl apply -f - &amp;lt;&amp;lt;EOF apiVersion: v1 kind: Secret metadata: name: mys3secret type: Opaque stringData: fs.s3.accessKeyId: ${AK} fs.s3.accessKeySecret: ${SK} EOF 创建 Dataset 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 kubectl apply -f - &amp;lt;&amp;lt;EOF apiVersion: data.fluid.io/v1alpha1 kind: Dataset metadata: name: mys3-jindo spec: mounts: - mountPoint: s3://${BUCKET}/test2/ options: fs.s3.endpoint: ${ENDPOINT} encryptOptions: - name: fs.s3.accessKeyId valueFrom: secretKeyRef: name: mys3secret key: fs.s3.accessKeyId - name: fs.s3.accessKeySecret valueFrom: secretKeyRef: name:</description><dc:creator>微信公众号</dc:creator><category>博文</category><category>s3</category><category>Fluid</category><category>JuiceFS</category><category>AI</category><category>Data</category></item><item><title>使用 TensorBoard 可视化 PyTorch 训练过程</title><link>https://www.chenshaowen.com/blog/using-tensorboard-to-visualize-pytorch-training-process.html</link><pubDate>Sun, 17 Nov 2024 00:00:00 +0000</pubDate><atom:modified>Sun, 17 Nov 2024 00:00:00 +0000</atom:modified><guid>https://www.chenshaowen.com/blog/using-tensorboard-to-visualize-pytorch-training-process.html</guid><description>1. 什么是 TensorBoard TensorBoard 主要是用来监控模型的各种指标的变化，比如 accuracy、loss、各种层的权重分布等。 TensorBoard 是 TensorFlow 的一个可视化工具，支持标量、文本、图像、音频、视频和 Embedding 等多种数据可视化，但是 PyTorch 也可以使用 TensorBoard。 2. 安装 tensorboard 1 pip install tensorboard 3. 使用</description><dc:creator>微信公众号</dc:creator><category>博文</category><category>AI</category><category>PyTorch</category><category>TensorBoard</category><category>训练</category></item><item><title>使用 PyTorch 在 MNIST 数据集训练模型</title><link>https://www.chenshaowen.com/blog/using-pytorch-to-train-model-on-mnist-dataset.html</link><pubDate>Sat, 16 Nov 2024 00:00:00 +0000</pubDate><atom:modified>Sat, 16 Nov 2024 00:00:00 +0000</atom:modified><guid>https://www.chenshaowen.com/blog/using-pytorch-to-train-model-on-mnist-dataset.html</guid><description>1. 创建训练脚本 创建训练脚本 mnist.py，内容如下： 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93</description><dc:creator>微信公众号</dc:creator><category>博文</category><category>AI</category><category>PyTorch</category><category>训练</category><category>容器</category></item><item><title>MPI 通信原语及 Python 编程使用</title><link>https://www.chenshaowen.com/blog/mpi-communication-primitives-and-python-programming.html</link><pubDate>Sun, 10 Nov 2024 00:00:00 +0000</pubDate><atom:modified>Sun, 10 Nov 2024 00:00:00 +0000</atom:modified><guid>https://www.chenshaowen.com/blog/mpi-communication-primitives-and-python-programming.html</guid><description>1. 什么是 MPI MPI，Message Passing Interface 消息传递接口，是一种用于并行计算的通信协议。 MPI 提供了一组标准化的接口，用于在不同的计算节点之间传输数据，广泛应用于科学计算、机器学习、深度学习等领域。 MPI 有多个实现，常用实现有 MPICH 和 OpenMPI。MPICH</description><dc:creator>微信公众号</dc:creator><category>博文</category><category>MPI</category><category>AI</category><category>分布式</category><category>通信</category></item><item><title>常见的几种网络拓扑结构</title><link>https://www.chenshaowen.com/blog/common-network-topology.html</link><pubDate>Wed, 06 Nov 2024 00:00:00 +0000</pubDate><atom:modified>Wed, 06 Nov 2024 00:00:00 +0000</atom:modified><guid>https://www.chenshaowen.com/blog/common-network-topology.html</guid><description>1. Fat-Tree 1985 年 麻省理工学院的 Charles E. Leiserson 发明了 Fat-Tree 胖树网络。如下图，胖树网络是一颗二叉树，从更节点到叶子节点带宽逐步增加。 2008 年 8 月，加州大学圣地亚哥分校的一组计算机科学家发表了一个可扩展的网络架构设计，该设计采用受胖树拓扑启发的拓扑结构，实现了比以前的分</description><dc:creator>微信公众号</dc:creator><category>博文</category><category>AI</category><category>网络</category><category>拓扑结构</category></item><item><title>RDMA 技术</title><link>https://www.chenshaowen.com/blog/rdma-technique.html</link><pubDate>Tue, 05 Nov 2024 00:00:00 +0000</pubDate><atom:modified>Tue, 05 Nov 2024 00:00:00 +0000</atom:modified><guid>https://www.chenshaowen.com/blog/rdma-technique.html</guid><description>1. 什么是 RDMA RDMA(Remote Direct Memory Access，远程直接内存访问)是一种为了解决网络传输中服务器端数据处理延迟而产生的技术。 TCP/IP 传输时，数据经过网络堆栈，再经过网卡发送，接收端接收后，按照序列号组装数据。 DMA 传输时，可以直接在设备和内存之间传输数据，不需要经过网</description><dc:creator>微信公众号</dc:creator><category>博文</category><category>RDMA</category><category>AI</category></item><item><title>InfiniBand 网络及常用命令</title><link>https://www.chenshaowen.com/blog/infiniband-network-and-useful-commands.html</link><pubDate>Sat, 02 Nov 2024 00:00:00 +0000</pubDate><atom:modified>Sat, 02 Nov 2024 00:00:00 +0000</atom:modified><guid>https://www.chenshaowen.com/blog/infiniband-network-and-useful-commands.html</guid><description>1. InfiniBand 网络 InfiniBand(缩写 IB)，是一个用于高性能计算的计算机网络通信标准，它具有极高的吞吐量和极低的延迟，用于计算机与计算机之间的数据互连。InfiniBand 也用作服务器与存储系统之间的直接或交换互连，以及存储系统之间的互连。 InfiniBand</description><dc:creator>微信公众号</dc:creator><category>博文</category><category>InfiniBand</category><category>AI</category></item><item><title>使用 Fluid 对接 OBS 存储及性能测试</title><link>https://www.chenshaowen.com/blog/using-fluid-to-access-obs-storage-and-performance-testing.html</link><pubDate>Tue, 22 Oct 2024 00:00:00 +0000</pubDate><atom:modified>Tue, 22 Oct 2024 00:00:00 +0000</atom:modified><guid>https://www.chenshaowen.com/blog/using-fluid-to-access-obs-storage-and-performance-testing.html</guid><description>1. Jindo 挂载 OBS 配置环境变量 1 2 3 4 export ENDPOINT=obs.cn-north-4.myhuaweicloud.com export BUCKET= export AK= export SK= 创建凭证 1 2 3 4 5 6 7 8 9 10 kubectl apply -f - &amp;lt;&amp;lt;EOF apiVersion: v1 kind: Secret metadata: name: myobssecret type: Opaque stringData: fs.obs.accessKeyId: ${AK} fs.obs.accessKeySecret: ${SK} EOF 创建 Dataset 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 kubectl apply -f - &amp;lt;&amp;lt;EOF apiVersion: data.fluid.io/v1alpha1 kind: Dataset metadata: name: myobs-jindo spec: mounts: - mountPoint: obs://${BUCKET}/test2/ options: fs.obs.endpoint: ${ENDPOINT} encryptOptions: - name: fs.obs.accessKeyId valueFrom: secretKeyRef: name: myobssecret key: fs.obs.accessKeyId - name: fs.obs.accessKeySecret valueFrom: secretKeyRef: name:</description><dc:creator>微信公众号</dc:creator><category>博文</category><category>OBS</category><category>Fluid</category><category>JuiceFS</category><category>AI</category><category>Data</category></item><item><title>GPU 主机如何开启 GDS</title><link>https://www.chenshaowen.com/blog/how-to-enable-gds-on-gpu-host.html</link><pubDate>Wed, 11 Sep 2024 00:00:00 +0000</pubDate><atom:modified>Wed, 11 Sep 2024 00:00:00 +0000</atom:modified><guid>https://www.chenshaowen.com/blog/how-to-enable-gds-on-gpu-host.html</guid><description>1. 什么是 GDS（GPUDirectStorage） GDS 允许 RDMA 网卡直接访问 GPU 内存，有助于增加 GPU 应用读写文件的 IO 带宽，减少 IO 时延，并降低其 CPU 负载。 客户端在开启 GDS 特性后，文件将以 O_DIRECT 方式打开，客户端不会再缓存文件数据。应用层读写文件时，客户端通过 nvidia-fs.ko 将</description><dc:creator>微信公众号</dc:creator><category>博文</category><category>GDS</category><category>GPU</category><category>AI</category></item><item><title>NVIDIA GPU 核心与架构演进史</title><link>https://www.chenshaowen.com/blog/nvidia-gpu-cores-and-architecture-evolution-history.html</link><pubDate>Sun, 25 Aug 2024 00:00:00 +0000</pubDate><atom:modified>Sun, 25 Aug 2024 00:00:00 +0000</atom:modified><guid>https://www.chenshaowen.com/blog/nvidia-gpu-cores-and-architecture-evolution-history.html</guid><description>1. 产品线 GeForce 面向游戏玩家，提供强大的图形处理能力、先进的游戏技术。 常见的有 NVIDIA GTX 系列、高端的 RTX 系列、Titan 系列。 Quadro 面向专业市场，如设计师、工程师、科学家和内容创作者。 常见的有 Quadro P 系列，高端的 Quadro RTX 系列 Tesla 面向数据中心和高性能计算（HPC）市场，</description><dc:creator>微信公众号</dc:creator><category>整理</category><category>AI</category><category>GPU</category><category>NVIDIA</category><category>硬件</category></item><item><title>使用 Volcano 运行 hccl-test</title><link>https://www.chenshaowen.com/blog/use-volcano-to-run-hccl-test.html</link><pubDate>Sun, 11 Aug 2024 00:00:00 +0000</pubDate><atom:modified>Sun, 11 Aug 2024 00:00:00 +0000</atom:modified><guid>https://www.chenshaowen.com/blog/use-volcano-to-run-hccl-test.html</guid><description>1. 制作 hccl-test 镜像 下载依赖包 Python-3.8.18.tgz Ascend-cann-toolkit_8.0.RC2_linux-x86_64.run Ascend-cann-kernels-910b_8.0.RC2_linux.run mpich-3.2.1.tar.gz 如果不方便下载，也可以直接从我打包的镜像中拷贝出来。 编写 Dockerfile 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 FROM ubuntu:22.04 WORKDIR /home RUN sed -i -e &amp;#39;s/^APT/# APT/&amp;#39; -e &amp;#39;s/^DPkg/# DPkg/&amp;#39; /etc/apt/apt.conf.d/docker-clean RUN apt-get update &amp;amp;&amp;amp; apt-get install</description><dc:creator>微信公众号</dc:creator><category>博文</category><category>Volcano</category><category>HCCL</category><category>Ascend</category><category>AI</category><category>测试</category></item><item><title>使用 Volcano 运行 nccl-test</title><link>https://www.chenshaowen.com/blog/use-volcano-to-run-nccl-test.html</link><pubDate>Sun, 11 Aug 2024 00:00:00 +0000</pubDate><atom:modified>Sun, 11 Aug 2024 00:00:00 +0000</atom:modified><guid>https://www.chenshaowen.com/blog/use-volcano-to-run-nccl-test.html</guid><description>1. 制作 nccl-test 镜像 查看 CUDA 版本 1 2 3 nvidia-smi | grep &amp;#34;CUDA Version&amp;#34; | awk &amp;#39;{print $9}&amp;#39; 12.2 编写 Dockerfile 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 cat &amp;gt; Dockerfile &amp;lt;&amp;lt; EOF FROM hubimage/nvidia-cuda:12.1.0-cudnn8-devel-ubuntu22.04 ENV DEBIAN_FRONTEND=noninteractive ARG CONDA_VERSION WORKDIR /workspace ENV DEBIAN_FRONTEND=noninteractive RUN apt-get update &amp;amp;&amp;amp; apt install -y openmpi-bin libopenmpi-dev ssh openssh-server net-tools vim git iputils-ping nfs-common RUN git clone https://github.com/NVIDIA/nccl-tests.git &amp;amp;&amp;amp; \ cd nccl-tests &amp;amp;&amp;amp; \ make MPI=1 MPI_HOME=/usr/lib/x86_64-linux-gnu/openmpi EOF 编译 nccl-test 镜像 1 docker build -t hubimage/nccl-test:12.1.0-ubuntu22.04 -f Dockerfile . 推送 nccl-test 镜像 1 docker push hubimage/nccl-test:12.1.0-ubuntu22.04 2. 运行 Volcano Job 给测试节点打</description><dc:creator>微信公众号</dc:creator><category>博文</category><category>Volcano</category><category>NCCL</category><category>Nvidia</category><category>AI</category><category>测试</category></item><item><title>模型研发过程中的存储系统建设思路</title><link>https://www.chenshaowen.com/blog/some-things-about-storage-system-in-model-development-process.html</link><pubDate>Tue, 23 Jul 2024 00:00:00 +0000</pubDate><atom:modified>Tue, 23 Jul 2024 00:00:00 +0000</atom:modified><guid>https://www.chenshaowen.com/blog/some-things-about-storage-system-in-model-development-process.html</guid><description>本文内容整理自我在一次内部分享的部分内容。 1. 存储系统的核心要素 1.1 安全 对象存储桶的凭证、使用存储 PVC 时的授权、对访问来源的控制，这些都是安全需要关注的问题。 但这些又非常容易被忽视，出了问题就是大问题。 1.2 生命周期管理 存储系统是为业务使用数据服务的</description><dc:creator>微信公众号</dc:creator><category>博文</category><category>AI</category><category>模型数据</category><category>研发流程</category></item><item><title>使用 Fluid 对接 OSS 存储及性能测试</title><link>https://www.chenshaowen.com/blog/using-fluid-to-access-oss-storage-and-performance-testing.html</link><pubDate>Tue, 11 Jun 2024 00:00:00 +0000</pubDate><atom:modified>Tue, 11 Jun 2024 00:00:00 +0000</atom:modified><guid>https://www.chenshaowen.com/blog/using-fluid-to-access-oss-storage-and-performance-testing.html</guid><description>1. Jindo 直接加速 OSS 配置环境变量 1 2 3 4 export ENDPOINT=oss-cn-beijing-internal.aliyuncs.com export BUCKET= export AK= export SK= 创建凭证 1 2 3 4 5 6 7 8 9 10 kubectl apply -f - &amp;lt;&amp;lt;EOF apiVersion: v1 kind: Secret metadata: name: myosssecret type: Opaque stringData: fs.oss.accessKeyId: ${AK} fs.oss.accessKeySecret: ${SK} EOF 创建 Dataset 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 kubectl apply -f - &amp;lt;&amp;lt;EOF apiVersion: data.fluid.io/v1alpha1 kind: Dataset metadata: name: myoss-jindo spec: mounts: - mountPoint: oss://${BUCKET}/test2/ options: fs.oss.endpoint: ${ENDPOINT} encryptOptions: - name: fs.oss.accessKeyId valueFrom: secretKeyRef: name: myosssecret key: fs.oss.accessKeyId - name: fs.oss.accessKeySecret valueFrom:</description><dc:creator>微信公众号</dc:creator><category>博文</category><category>OSS</category><category>Fluid</category><category>JuiceFS</category><category>AI</category><category>Data</category></item><item><title>如何预热 Juicefs 数据</title><link>https://www.chenshaowen.com/blog/how-to-warmup-juicefs-data.html</link><pubDate>Sun, 09 Jun 2024 00:00:00 +0000</pubDate><atom:modified>Sun, 09 Jun 2024 00:00:00 +0000</atom:modified><guid>https://www.chenshaowen.com/blog/how-to-warmup-juicefs-data.html</guid><description>1. 关于 JuiceFS 的缓存 在主机上，预热的缓存是直接放在主机上的。 在集群中，分为两级缓存: Worker，提供集群级别共享的缓存 Fuse，提供仅当前节点级别的缓存 2. 使用 JuiceFS 客户端预热数据 需要注意的是在 Fuse 层预热，仅对当前节点有效，如果需要预热整个集群，需要在</description><dc:creator>微信公众号</dc:creator><category>博文</category><category>Fluid</category><category>JuiceFS</category><category>AI</category><category>Data</category></item><item><title>Ascend NPU 驱动安装</title><link>https://www.chenshaowen.com/blog/ascend-npu-driver-installation.html</link><pubDate>Tue, 28 May 2024 00:00:00 +0000</pubDate><atom:modified>Tue, 28 May 2024 00:00:00 +0000</atom:modified><guid>https://www.chenshaowen.com/blog/ascend-npu-driver-installation.html</guid><description>1. 安装驱动 创建 HwHiAiUser 用户 1 2 groupadd -g 1000 HwHiAiUser useradd -g HwHiAiUser -u 1000 -d /home/HwHiAiUser -m HwHiAiUser -s /bin/bash 添加目录权限 1 2 chown -R HwHiAiUser /usr/local/Ascend chmod -R 755 /usr/local/Ascend 下载驱动、固件 前往 https://www.hiascend.ru/hardware/firmware-drivers/community?product=1&amp;amp;model=30&amp;amp;cann=All&amp;amp;driver=1.0.26.alpha 找到对应的驱动和固件。 1 wget https://ascend-repo.obs.cn-east-2.myhuaweicloud.com/Ascend%20HDK/Ascend%20HDK%2024.1.RC2.2/Ascend-hdk-910b-npu-driver_24.1.rc2.2_linux-x86-64.run 1 wget https://ascend-repo.obs.cn-east-2.myhuaweicloud.com/Ascend%20HDK/Ascend%20HDK%2024.1.RC2.2/Ascend-hdk-910b-npu-firmware_7.3.0.2.220.run 安装驱动 1 bash ./Ascend-hdk-910b-npu-driver_24.1.rc2.2_linux-x86-64.run --full --install-for-all 安装固件 1 bash ./Ascend-hdk-910b-npu-firmware_7.3.0.2.220.run --full 2. 安装 ascend-docker-runtime 下载 ascend-docker-runtime 前往 https://gitee.com/ascend/ascend-docker-runtime/releases/tag/v5.0.0-RC3.2 找到对应架构的下载链接。 1 wget https://gitee.com/ascend/ascend-docker-runtime/releases/download/v5.0.0-RC3.2/Ascend-docker-runtime_5.0.RC3.2_linux-x86_64.run 安装 ascend-docker-runtime 1 bash ./Ascend-docker-runtime_5.0.RC3.2_linux-x86_64.run --install 3.</description><dc:creator>微信公众号</dc:creator><category>博文</category><category>AI</category><category>NPU</category><category>Ascend</category><category>硬件</category><category>驱动</category></item><item><title>Fluid 挂载 S3 为 PVC 以及性能测试</title><link>https://www.chenshaowen.com/blog/fluid-using-s3-as-pvc.html</link><pubDate>Sun, 19 May 2024 00:00:00 +0000</pubDate><atom:modified>Sun, 19 May 2024 00:00:00 +0000</atom:modified><guid>https://www.chenshaowen.com/blog/fluid-using-s3-as-pvc.html</guid><description>1. 创建 Dataset 1 2 3 4 5 6 7 8 9 10 kubectl apply -f - &amp;lt;&amp;lt;EOF apiVersion: v1 kind: Secret metadata: name: my-s3 type: Opaque stringData: aws.accessKeyId: xxx aws.secretKey: xxx EOF 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 kubectl apply -f - &amp;lt;&amp;lt;EOF apiVersion: data.fluid.io/v1alpha1 kind: Dataset metadata: name: my-s3 spec: mounts: - mountPoint: s3://BUCKET/ name: s3 options: alluxio.underfs.s3.endpoint: ks3-cn-beijing-internal.ksyun.com alluxio.underfs.s3.disable.dns.buckets: &amp;#34;false&amp;#34; encryptOptions: - name: aws.accessKeyId valueFrom: secretKeyRef: name: my-s3 key: aws.accessKeyId - name: aws.secretKey valueFrom: secretKeyRef: name: my-s3 key: aws.secretKey accessModes: - ReadWriteMany EOF 2. 创建 Runtime 1 2 3 4 5 6 7 8 9</description><dc:creator>微信公众号</dc:creator><category>博文</category><category>Fluid</category><category>JuiceFS</category><category>AI</category><category>Data</category></item><item><title>Fluid 使用 Lustre Runtime 以及性能测试</title><link>https://www.chenshaowen.com/blog/fluid-using-lustre-runtime-and-performance-testing.html</link><pubDate>Thu, 16 May 2024 00:00:00 +0000</pubDate><atom:modified>Thu, 16 May 2024 00:00:00 +0000</atom:modified><guid>https://www.chenshaowen.com/blog/fluid-using-lustre-runtime-and-performance-testing.html</guid><description>1. 分析 Fluid 挂载 NFS 存储 查看 Fuse Pod 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 kubectl get pod nfs-demo-fuse-f9wg8 -oyaml apiVersion: v1 kind: Pod metadata: generateName: nfs-demo-fuse- spec: containers: - command: - /usr/local/bin/entrypoint.sh env: - name: FLUID_RUNTIME_TYPE value: thin - name: FLUID_RUNTIME_NS value: default - name: FLUID_RUNTIME_NAME value: nfs-demo - name: MOUNT_POINT value: /runtime-mnt/thin/default/nfs-demo/thin-fuse - name: MOUNT_OPTIONS value: ro image: fluidcloudnative/nfs:v0.1 imagePullPolicy: IfNotPresent lifecycle: preStop: exec: command: - sh - -c - umount /runtime-mnt/thin/default/nfs-demo/thin-fuse name: thin-fuse securityContext: privileged: true volumeMounts:</description><dc:creator>微信公众号</dc:creator><category>博文</category><category>Fluid</category><category>JuiceFS</category><category>AI</category><category>Data</category><category>Lustre</category></item><item><title>Fluid 使用 Lustre Runtime 以及性能测试</title><link>https://www.chenshaowen.com/blog/fluid-using-lustre-runtime-and-performance-testing.html</link><pubDate>Thu, 16 May 2024 00:00:00 +0000</pubDate><atom:modified>Thu, 16 May 2024 00:00:00 +0000</atom:modified><guid>https://www.chenshaowen.com/blog/fluid-using-lustre-runtime-and-performance-testing.html</guid><description>1. 打包 Fluid Runtime 镜像 创建 fluid_config_init.py 脚本 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 #!/usr/bin/env python import json rawStr = &amp;#34;&amp;#34; with open(&amp;#34;/etc/fluid/config.json&amp;#34;, &amp;#34;r&amp;#34;) as f: rawStr = f.readlines() rawStr = rawStr[0] script = &amp;#34;&amp;#34;&amp;#34; #!/bin/sh set -ex MNT_FROM=$mountPoint MNT_TO=$targetPath trap &amp;#34;umount ${MNT_TO}&amp;#34; SIGTERM mkdir -p ${MNT_TO} mount -t lustre -o relatime,flock ${MNT_FROM} ${MNT_TO} sleep inf &amp;#34;&amp;#34;&amp;#34; obj = json.loads(rawStr) with open(&amp;#34;mount-lustre.sh&amp;#34;, &amp;#34;w&amp;#34;) as f: f.write(&amp;#39;mountPoint=&amp;#34;%s&amp;#34;\n&amp;#39; % obj[&amp;#34;mounts&amp;#34;][0][&amp;#34;mountPoint&amp;#34;]) f.write(&amp;#39;targetPath=&amp;#34;%s&amp;#34;\n&amp;#39; % obj[&amp;#34;targetPath&amp;#34;]) f.write(script) 只需调整一下 mount 命令即可。 创建启动脚本 entrypoint.sh 1 2 3</description><dc:creator>微信公众号</dc:creator><category>博文</category><category>Fluid</category><category>JuiceFS</category><category>AI</category><category>Data</category><category>Lustre</category></item><item><title>Fluid 使用 NFS Runtime 以及性能测试</title><link>https://www.chenshaowen.com/blog/fluid-using-nfs-runtime-and-performance-testing.html</link><pubDate>Tue, 14 May 2024 00:00:00 +0000</pubDate><atom:modified>Tue, 14 May 2024 00:00:00 +0000</atom:modified><guid>https://www.chenshaowen.com/blog/fluid-using-nfs-runtime-and-performance-testing.html</guid><description>1. 创建 Dataset 1 2 3 4 5 6 7 8 9 10 kubectl apply -f - &amp;lt;&amp;lt;EOF apiVersion: data.fluid.io/v1alpha1 kind: Dataset metadata: name: nfs-demo spec: mounts: - mountPoint: x.x.x.x:/x-x/ name: nfs-demo EOF 2. 创建 Runtime 1 2 3 4 5 6 7 8 9 10 11 12 13 14 kubectl apply -f - &amp;lt;&amp;lt;EOF apiVersion: data.fluid.io/v1alpha1 kind: ThinRuntimeProfile metadata: name: nfs spec: fileSystemType: nfs fuse: image: fluidcloudnative/nfs imageTag: v0.1 imagePullPolicy: IfNotPresent command: - &amp;#34;/usr/local/bin/entrypoint.sh&amp;#34; EOF 1 2 3 4 5 6 7 8 kubectl apply -f - &amp;lt;&amp;lt;EOF apiVersion: data.fluid.io/v1alpha1 kind: ThinRuntime metadata: name: nfs-demo spec: profileName: nfs EOF 3. 创建测试 Pod 1 2 3 4 5 6 7 8 9 10 11 12</description><dc:creator>微信公众号</dc:creator><category>博文</category><category>Fluid</category><category>JuiceFS</category><category>AI</category><category>Data</category></item><item><title>对齐 Ops，使用新思路重写 Ops Copilot 已更新</title><link>https://www.chenshaowen.com/blog/ops-copilot-has-been-updated-using-pipeline-and-llm.html</link><pubDate>Wed, 01 May 2024 00:00:00 +0000</pubDate><atom:modified>Wed, 01 May 2024 00:00:00 +0000</atom:modified><guid>https://www.chenshaowen.com/blog/ops-copilot-has-been-updated-using-pipeline-and-llm.html</guid><description>1. 让 Ops Copilot 成为 Ops Coilot 在 2023 年 09 月，我写过一版 Ops Copilot，也有文章发出 我在给 Ops 工具写 Copilot 。 实现的效果是这样的： 1 2 3 4 5 6 7 8 9 10 Opscli&amp;gt; 打开浏览器 Open a browser and navigate to &amp;#39;https://www.google.com&amp;#39;. ↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓ import webbrowser webbrowser.open(&amp;#39;https://www.google.com&amp;#39;) ↑↑↑↑↑↑↑↑↑↑↑</description><dc:creator>微信公众号</dc:creator><category>博文</category><category>AI</category><category>Copilot</category><category>Agent</category><category>LLM</category></item><item><title>模型并行训练技术</title><link>https://www.chenshaowen.com/blog/model-parallel-training-techniques.html</link><pubDate>Thu, 04 Apr 2024 00:00:00 +0000</pubDate><atom:modified>Thu, 04 Apr 2024 00:00:00 +0000</atom:modified><guid>https://www.chenshaowen.com/blog/model-parallel-training-techniques.html</guid><description>1. 数据并行 训练步骤: master 设备加载模型，并将模型参数复制到每个 worker 设备 master 设备按照 batch 维度划分训练数据，将每个 batch 传递给每个 worker 设备 每个 worker 设备进行训练 master 设备汇总每个 worker 设备的梯度，更新模型参数 master 设备广播模型参数到每个 worker 设备，准备下一个 batch 训练 核心思想: 将训练</description><dc:creator>微信公众号</dc:creator><category>博文</category><category>AI</category><category>训练</category><category>模型</category></item><item><title>常用 AI 基础镜像及启动命令</title><link>https://www.chenshaowen.com/blog/common-ai-base-images-and-run-command.html</link><pubDate>Thu, 28 Mar 2024 00:01:00 +0000</pubDate><atom:modified>Thu, 28 Mar 2024 00:01:00 +0000</atom:modified><guid>https://www.chenshaowen.com/blog/common-ai-base-images-and-run-command.html</guid><description>1. 镜像 Tag 标识的含义 base/cuda: 包括 CUDA 运行时 runtime: 在 base 的基础上，新增了 CUDA math 库和 NCCL、cuDNN 运行时 devel: 在 runtime 的基础上，新增了头文件和用于构建 CUDA 镜像的开发工具，对于多阶段构建特别有用 cuddn: 在上面基础上，新增了 cuDNN 神经网络加速库 py3: Python 3 环境 2. CUDA 镜像 镜像 AMD64 镜像大小 ARM64 镜</description><dc:creator>微信公众号</dc:creator><category>博文</category><category>AI</category><category>镜像</category><category>容器</category></item><item><title>Conda 安装与使用</title><link>https://www.chenshaowen.com/blog/conda-install-and-use.html</link><pubDate>Thu, 28 Mar 2024 00:00:00 +0000</pubDate><atom:modified>Thu, 28 Mar 2024 00:00:00 +0000</atom:modified><guid>https://www.chenshaowen.com/blog/conda-install-and-use.html</guid><description>1. 安装 conda 1 2 3 wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh bash Miniconda3-latest-Linux-x86_64.sh rm -rf Miniconda3-latest-Linux-x86_64.sh 但 Miniconda 不能免费大规模商用，可以使用 Miniforge 平替。 1 2 wget &amp;#34;https://github.com/conda-forge/miniforge/releases/latest/download/Miniforge3-$(uname)-$(uname -m).sh&amp;#34; bash Miniforge3-$(uname)-$(uname -m).sh 2. 修改默认配置 2.1 初始化 Shell 如果不进行初始化，激活环境时会报错 CondaError: Run 'conda init' before 'conda activate' 。 1 2 3 4 5 6 7 8 9 conda init --help usage: conda init [-h] [--all] [--user] [--no-user] [--system] [--reverse] [--json] [-v] [-q] [-d] [SHELLS ...] Initialize conda for shell interaction. positional arguments: SHELLS One or more shells to be</description><dc:creator>微信公众号</dc:creator><category>博文</category><category>Conda</category><category>Python</category><category>Ai</category></item><item><title>Volcano 使用基础</title><link>https://www.chenshaowen.com/blog/the-basic-of-volcano.html</link><pubDate>Sun, 24 Mar 2024 00:00:00 +0000</pubDate><atom:modified>Sun, 24 Mar 2024 00:00:00 +0000</atom:modified><guid>https://www.chenshaowen.com/blog/the-basic-of-volcano.html</guid><description>1. Volcano 简介 Volcano 是华为开源的一个基于 Kubernetes 的资源调度系统，相较于原生的调度器，具有的显著特点有： 支持 gang scheduling 对于批量作业的调度，容易碰到死锁的问题，比如两个作业都需要同时运行 10 个 Pod 才能启动，当两个作业同时提交时，可能都只有部分 Pod 被调度，两个作业都无法正常</description><dc:creator>微信公众号</dc:creator><category>博文</category><category>Volcano</category><category>AI</category><category>Kubernetes</category><category>调度</category></item><item><title>npu-smi 基本使用</title><link>https://www.chenshaowen.com/blog/basic-usage-of-npu-smi.html</link><pubDate>Wed, 20 Mar 2024 00:00:00 +0000</pubDate><atom:modified>Wed, 20 Mar 2024 00:00:00 +0000</atom:modified><guid>https://www.chenshaowen.com/blog/basic-usage-of-npu-smi.html</guid><description>1. 什么是 npu-smi npu-smi 是华为提供的一个命令行工具，专门用于管理和监控华为昇腾（Ascend）系列神经网络处理器（NPU）的状态和性能，似于 NVIDIA 的 nvidia-smi。 2. npu-smi 字段含义 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 npu-smi info +------------------------------------------------------------------------------------------------+ | npu-smi 23.0.2.1 Version: 23.0.2.1 | +---------------------------+---------------+----------------------------------------------------+ | NPU Name |</description><dc:creator>微信公众号</dc:creator><category>博文</category><category>AI</category><category>NPU</category><category>Huawei</category><category>硬件</category></item><item><title>AI 芯片高速互连方案</title><link>https://www.chenshaowen.com/blog/ai-chip-high-speed-connection-solution.html</link><pubDate>Tue, 19 Mar 2024 00:00:00 +0000</pubDate><atom:modified>Tue, 19 Mar 2024 00:00:00 +0000</atom:modified><guid>https://www.chenshaowen.com/blog/ai-chip-high-speed-connection-solution.html</guid><description>最近在研习模型训练相关的基础设施，发现 AI 芯片互连拓扑决定着训练集群任务的调度和资源分配，因此花了一点时间整理了一下常见的 AI 芯片互连方案。 1. 点对点互连 传统的 PCIe 系统下， AI 芯片与 AI 芯片之间的数据传输是通过 PCIe 传输，无法满足大规模数据传输的要求。 1.1 NVLink</description><dc:creator>微信公众号</dc:creator><category>博文</category><category>AI</category><category>芯片</category><category>互连</category></item><item><title>常用 GPU 运维及故障处理</title><link>https://www.chenshaowen.com/blog/common-gpu-operation-and-fault-handling.html</link><pubDate>Mon, 18 Mar 2024 00:00:00 +0000</pubDate><atom:modified>Mon, 18 Mar 2024 00:00:00 +0000</atom:modified><guid>https://www.chenshaowen.com/blog/common-gpu-operation-and-fault-handling.html</guid><description>处理故障时，参考或者记录下的内容，持续更新中 1. XID 错误事件 XID 是 NVIDIA 的错误码，可以通过命令: 1 dmesg -T | grep -i &amp;#34;NVRM: Xid&amp;#34; 或者 1 journalctl --since `date -d &amp;#34;10 days ago&amp;#34; &amp;#34;+%Y-%m-%d&amp;#34;`|grep Xid 根据 XID 可以定位故障，下面是一些常见的 XID 事件 XID 说明 13 Graphics Engine Exception。通常是数组越界、指令错误,小概率是硬件问</description><dc:creator>微信公众号</dc:creator><category>整理</category><category>GPU</category><category>AI</category><category>运维</category></item><item><title>用了一个月，终于找到点写 AI Agent 的思路</title><link>https://www.chenshaowen.com/blog/provide-a-way-to-develop-ai-agent.html</link><pubDate>Sat, 16 Mar 2024 08:01:28 +0000</pubDate><atom:modified>Sat, 16 Mar 2024 08:01:28 +0000</atom:modified><guid>https://www.chenshaowen.com/blog/provide-a-way-to-develop-ai-agent.html</guid><description>1. 不断尝试落地 AI 应用端 基于对运维的认知，我开发了一个开源的运维工具 https://github.com/shaowenchen/ops 。 Ops 工具将运维操作划分为脚本执行、文件分发两类，而运维对象主机和 Kubernetes 集群分别都实现了这两种运维操作。 Ops 对外提供的能力有，Ops Cli 命令行终端，Ops Server 服务端 API 接口，Ops Controller 集群</description><dc:creator>微信公众号</dc:creator><category>博文</category><category>思考</category><category>AI</category><category>Agent</category><category>LLM</category><category>Ops</category></item><item><title>在 Kubernetes 下创建后端为 JuiceFS 的 PVC</title><link>https://www.chenshaowen.com/blog/how-to-quickly-create-juicefs-pvc-in-kubernetes.html</link><pubDate>Thu, 07 Mar 2024 00:00:00 +0000</pubDate><atom:modified>Thu, 07 Mar 2024 00:00:00 +0000</atom:modified><guid>https://www.chenshaowen.com/blog/how-to-quickly-create-juicefs-pvc-in-kubernetes.html</guid><description>本篇主要记录创建 JuiceFS PVC 的脚本，方便快速配置。组件部署可以参考 使用 Fluid 和 JuiceFS 在 Kubernetes 管理数据 。 1. 设置环境变量 桶的配置 1 2 3 4 5 6 export ACCESS_KEY= export SECRET_KEY= export BUCKET= export ENDPOINT=ks3-cn-beijing-internal.ksyun.com export BUCKET_ENPOINT=$BUCKET.$ENDPOINT export PROVIDER=ks3 Workload 的配置 1 2 3 4 5 export NAMESPACE= export PVC_NAME= export NODE_SELECTOR_KEY= export NODE_SELECTOR_VALUE= 镜像的配置 export JUICEFS_IMAGE=juicedata/juicefs-fuse export DEMO_IMAGE=shaowenchen/demo-ubuntu 元数据的配置 如果是 Redis 配置 1 2 3 4 export REDIS_PASSWORD= #ip:port/database export REDIS_ENDPOINT= export META_SERVER=redis://:$REDIS_PASSWORD@$REDIS_ENDPOINT 如</description><dc:creator>微信公众号</dc:creator><category>博文</category><category>AI</category><category>存储</category><category>JuiceFS</category></item><item><title>使用 TensorRT 加速模型推理</title><link>https://www.chenshaowen.com/blog/speeding-up-model-inference-with-tensorrt.html</link><pubDate>Tue, 06 Feb 2024 00:00:00 +0000</pubDate><atom:modified>Tue, 06 Feb 2024 00:00:00 +0000</atom:modified><guid>https://www.chenshaowen.com/blog/speeding-up-model-inference-with-tensorrt.html</guid><description>1. 什么是 TensorRT TensorRT 是一个 C++ 库，主要用在 NVIDIA GPU 进行高性能的推理加速上，提供了 C++ API 和 Python API 用于集成。 TensorRT 支持的主流深度学习框架有: Caffe，TensorRT 可以直接读取 prototxt 格式 TensorFlow，需要将 TensorFlow 的 pb 转换为 uff 格式 PyTorch，需要将 PyTorch 的 pth 格式转</description><dc:creator>微信公众号</dc:creator><category>博文</category><category>AI</category><category>TensorRT</category><category>NVIDIA</category><category>推理</category></item><item><title>Kubernetes 集群中 AI 相关的采集器</title><link>https://www.chenshaowen.com/blog/ai-related-exporters-in-kubernetes.html</link><pubDate>Sun, 04 Feb 2024 00:00:00 +0000</pubDate><atom:modified>Sun, 04 Feb 2024 00:00:00 +0000</atom:modified><guid>https://www.chenshaowen.com/blog/ai-related-exporters-in-kubernetes.html</guid><description>1. dcgm-exporter dcgm-exporter 是 NVIDIA 官方社区提供的 GPU 监控工具。 项目地址 https://github.com/NVIDIA/dcgm-exporter 1.1 安装方式 添加 Helm 镜像仓库 1 helm repo add gpu-helm-charts https://nvidia.github.io/dcgm-exporter/helm-charts 1 helm repo update 安装 1 2 3 4 5 6 helm install dcgm-exporter gpu-helm-charts/dcgm-exporter --namespace monitor --create-namespace \ --set serviceMonitor.enabled=false \ --set image.repository=hubimage/nvidia-dcgm-exporter \ --set image.tag=3.3.3-3.3.0-ubuntu22.04 \ --set nodeSelector.&amp;#34;accelerator\/provider&amp;#34;=nvidia-gpu \ --version 3.3.1 需要给 NVIDIA GPU 节点打上标签 1 kubectl label node &amp;lt;node-name&amp;gt; accelerator/provider=nvidia-gpu 1.2 指标 GPU 利用率 指标名称 指标类型 单位 描述 DCGM_FI_DEV_GPU_UTIL Gauge % GPU 利用率 DCGM_FI_DEV_MEM_COPY_UTIL Gauge</description><dc:creator>微信公众号</dc:creator><category>博文</category><category>Kubernetes</category><category>AI</category><category>指标</category><category>采集</category></item><item><title>容器下使用 Triton Server 和 TensorRT-LLM 进行大模型推理</title><link>https://www.chenshaowen.com/blog/using-triton-server-and-tensorrt-llm-under-container.html</link><pubDate>Sat, 03 Feb 2024 08:05:48 +0000</pubDate><atom:modified>Sat, 03 Feb 2024 08:05:48 +0000</atom:modified><guid>https://www.chenshaowen.com/blog/using-triton-server-and-tensorrt-llm-under-container.html</guid><description>1. TensorRT-LLM 编译模型 1.1 TensorRT-LLM 简介 使用 TensorRT 时，通常需要将模型转换为 ONNX 格式，再将 ONNX 转换为 TensorRT 格式，然后在 TensorRT、Triton Server 中进行推理。 但这个转换过程并不简单，经常会遇到各种报错，需要对模型结构、平台算子有一定的掌握，具备转换和调试能力。而 TensorRT-LLM 的目标</description><dc:creator>微信公众号</dc:creator><category>博文</category><category>AI</category><category>Triton</category><category>TensorRT</category></item><item><title>nvidia-smi 基本使用</title><link>https://www.chenshaowen.com/blog/basic-usage-of-nvidia-smi.html</link><pubDate>Thu, 01 Feb 2024 00:00:00 +0000</pubDate><atom:modified>Thu, 01 Feb 2024 00:00:00 +0000</atom:modified><guid>https://www.chenshaowen.com/blog/basic-usage-of-nvidia-smi.html</guid><description>1. 什么是 nvidia-smi nvidia-smi 全称是 NVIDIA System Management Interface，是 NVIDIA 提供的管理和监控 GPU 的接口。 nvidia-smi 调用的是 NVML。NVML 全称是 NVIDIA Management Library，提供了一组 C API，用于 NVIDIA GPU 监控和管理的库。 1.1 可查询的状态 ECC 错误计数 GPU 利用率 活动计算进程 时钟和 PState 温度和风扇速度 电</description><dc:creator>微信公众号</dc:creator><category>博文</category><category>AI</category><category>GPU</category><category>NVIDIA</category><category>硬件</category></item><item><title>使用 Fluid 和 JuiceFS 在 Kubernetes 管理数据</title><link>https://www.chenshaowen.com/blog/managing-data-in-kubernetes-using-fluid-and-juicefs.html</link><pubDate>Sat, 27 Jan 2024 00:00:00 +0000</pubDate><atom:modified>Sat, 27 Jan 2024 00:00:00 +0000</atom:modified><guid>https://www.chenshaowen.com/blog/managing-data-in-kubernetes-using-fluid-and-juicefs.html</guid><description>1. Fluid 简介 下面是来源于 https://github.com/fluid-cloudnative/fluid 的 Fluid 的架构图: Fluid 抽象了两个概念: Dataset，数据集合，用户视角的抽象 Runtime，数据存储、加速等真实服务的抽象 Fluid 主要解决了传统缓存系统在 Kubernetes 上使用的问题: 通过 CRD 对数据集合 Dataset 进行描述，提供生命周期管理 依赖于 Runtime 后端，</description><dc:creator>微信公众号</dc:creator><category>博文</category><category>Fluid</category><category>JuiceFS</category><category>AI</category><category>Data</category></item><item><title>使用 Dragonfly V2 分发集群的镜像</title><link>https://www.chenshaowen.com/blog/distributing-image-with-dragonfly-v2.html</link><pubDate>Sat, 13 Jan 2024 11:22:55 +0000</pubDate><atom:modified>Sat, 13 Jan 2024 11:22:55 +0000</atom:modified><guid>https://www.chenshaowen.com/blog/distributing-image-with-dragonfly-v2.html</guid><description>1. Dragonfly 简介 Dragonfly 的相关文档在社区 https://d7y.io/zh/docs/ 已经有详细说明。这里只是简单介绍一下，V2 版本的主要组件： Manager，提供 UI 界面、用户管理、集群监控、任务管理等功能 Scheduler，调度 Peer 之间的流量、提供预热等功能 Seed Peer，回源节点，用于从源站（Har</description><dc:creator>微信公众号</dc:creator><category>博文</category><category>AI</category><category>Dragonfly</category><category>Image</category><category>Nydus</category><category>配置</category><category>实践</category></item><item><title>Nydus 懒加载镜像配置与实践</title><link>https://www.chenshaowen.com/blog/nydus-configuration-and-practice.html</link><pubDate>Sat, 06 Jan 2024 11:22:55 +0000</pubDate><atom:modified>Sat, 06 Jan 2024 11:22:55 +0000</atom:modified><guid>https://www.chenshaowen.com/blog/nydus-configuration-and-practice.html</guid><description>据统计容器中的大部分文件不会被使用。根据这一特征，Nydus 自定义了 Rafs 格式的文件系统，实现了镜像文件的按需加载，以解决大镜像导致的启动慢和占用存储的问题。而在 AI 场景下，无论是推理还是训练，镜像常常都是几个 G 起步，甚至几十个 G，Nydus 非常</description><dc:creator>微信公众号</dc:creator><category>博文</category><category>AI</category><category>Nydus</category><category>配置</category><category>实践</category></item><item><title>NVIDIA GPU 驱动安装</title><link>https://www.chenshaowen.com/blog/nvidia-gpu-driver-installation.html</link><pubDate>Thu, 28 Dec 2023 00:00:00 +0000</pubDate><atom:modified>Thu, 28 Dec 2023 00:00:00 +0000</atom:modified><guid>https://www.chenshaowen.com/blog/nvidia-gpu-driver-installation.html</guid><description>1. 安装驱动 1.1 查看系统是否识别显卡 1 2 3 4 lspci | grep -i vga 03:00.0 VGA compatible controller: NVIDIA Corporation GP102 [TITAN X] (rev a1) 0a:00.0 VGA compatible controller: Matrox Electronics Systems Ltd. G200eR2 (rev 01) 识别出显卡为 NVIDIA 的 TITAN X。 1.2 禁用 nouveau 1 lsmod | grep nouveau 如果有输出，说明 nouveau 已经加载，需要禁用。如果没有输出，则可以跳过此操作。 Ubuntu 系统 关闭自动更新 1 sed -i.bak &amp;#39;s/1/0/&amp;#39; /etc/apt/apt.conf.d/10periodic 编辑配置</description><dc:creator>微信公众号</dc:creator><category>博文</category><category>AI</category><category>GPU</category><category>NVIDIA</category><category>硬件</category><category>驱动</category></item><item><title>大模型应用设计与实现指南</title><link>https://www.chenshaowen.com/blog/large-model-application-design-and-implementation-guide.html</link><pubDate>Sat, 23 Dec 2023 11:22:55 +0000</pubDate><atom:modified>Sat, 23 Dec 2023 11:22:55 +0000</atom:modified><guid>https://www.chenshaowen.com/blog/large-model-application-design-and-implementation-guide.html</guid><description>1. 直接使用大模型面临的问题 输出不稳定性 生成式 AI 的特点之一，输出结果的多样性。同样一个问题，问大模型多次，可能会得到不同的答案。 这种输出的不确定性，在对话、创作场景下，会给用户带来惊喜。但在确定性要求比较高的场景下，大模型进入不了采纳阶段。 数</description><dc:creator>微信公众号</dc:creator><category>博文</category><category>LLM</category><category>AI</category><category>最佳实践</category></item><item><title>OpenAI Vs Azure OpenAI API</title><link>https://www.chenshaowen.com/blog/openai-vs-azure-openai-api.html</link><pubDate>Sun, 03 Dec 2023 11:22:55 +0000</pubDate><atom:modified>Sun, 03 Dec 2023 11:22:55 +0000</atom:modified><guid>https://www.chenshaowen.com/blog/openai-vs-azure-openai-api.html</guid><description>由于定价和限制会随时间变化，本文仅供撰写当前时间参考。 1. 术语 RPM (requests per minute) 每分钟请求次数 RPD (requests per day) 每天请求次数 TPM (tokens per minute) 每分钟 Token 数 TPD (tokens per day), 每天 Token 数 在 https://platform.openai.com/tokenizer 可以根据文本查询对应的 token 数。在 https://github.com/openai/tiktoken/blob/main/tiktoken/model.py 可以发现 text-embedding-ada-002 与 gpt-3.5、gpt-4 的词表都是 cl100k_bas</description><dc:creator>微信公众号</dc:creator><category>博文</category><category>OpenAI</category><category>Azure</category><category>API</category><category>AI</category></item><item><title>大模型部署工具 llama.cpp</title><link>https://www.chenshaowen.com/blog/llama-cpp-that-is-a-llm-deployment-tool.html</link><pubDate>Tue, 05 Sep 2023 00:00:00 +0000</pubDate><atom:modified>Tue, 05 Sep 2023 00:00:00 +0000</atom:modified><guid>https://www.chenshaowen.com/blog/llama-cpp-that-is-a-llm-deployment-tool.html</guid><description>1. 大模型部署工具 llama.cpp 大模型的研究分为训练和推理两个部分。训练的过程，实际上就是在寻找模型参数，使得模型的损失函数最小化，推理结果最优化的过程。训练完成之后，模型的参数就固定了，这时候就可以使用模型进行推理，对外提供服务。 llama.cpp 主要解决的是推理过程</description><dc:creator>微信公众号</dc:creator><category>AI</category><category>大模型</category><category>工具</category><category>llama.cpp</category><category>博文</category></item><item><title>transformers 库的使用</title><link>https://www.chenshaowen.com/blog/usage-of-transformers-lib.html</link><pubDate>Tue, 22 Aug 2023 00:00:00 +0000</pubDate><atom:modified>Tue, 22 Aug 2023 00:00:00 +0000</atom:modified><guid>https://www.chenshaowen.com/blog/usage-of-transformers-lib.html</guid><description>transformers 是由 Hugging Face 开发的 Python 库，用于在自然语言处理（NLP）任务中使用和训练预训练的 Transformer 模型。它提供了许多强大的工具和功能，使得处理文本数据和构建 NLP 模型变得更加容易。该库广泛应用于各种 NLP 任务，如文本分类、命名实体识别、问答、文本生成等。 1. transformers 中的 pipeline pipeline 提供</description><dc:creator>微信公众号</dc:creator><category>整理</category><category>Transformer</category><category>AI</category><category>大模型</category><category>NLP</category></item><item><title>HuggingFace 的模型和数据操作</title><link>https://www.chenshaowen.com/blog/models-and-datasets-on-huggingface.html</link><pubDate>Mon, 21 Aug 2023 00:00:00 +0000</pubDate><atom:modified>Mon, 21 Aug 2023 00:00:00 +0000</atom:modified><guid>https://www.chenshaowen.com/blog/models-and-datasets-on-huggingface.html</guid><description>HuggingFace 通过提供共享模型 model、数据集 dataset、在线托管 space 等服务，为 AI 研究人员和开发者提供了一个完整的生态。本篇文章将介绍如何使用 HuggingFace 的模型和数据集。 1. 模型操作与使用 1.1 自定义存储目录 1 export HF_HOME=/Volumes/Data/HuggingFace 否则默认在 ~/.cache/huggingface 目录下。 1.2 模型的下载 第一种方法，页面上</description><dc:creator>微信公众号</dc:creator><category>整理</category><category>Transformer</category><category>AI</category><category>模型</category><category>HuggingFace</category><category>数据集</category></item><item><title>Transformer 学习笔记</title><link>https://www.chenshaowen.com/blog/learning-notes-of-transformer.html</link><pubDate>Sun, 20 Aug 2023 00:00:00 +0000</pubDate><atom:modified>Sun, 20 Aug 2023 00:00:00 +0000</atom:modified><guid>https://www.chenshaowen.com/blog/learning-notes-of-transformer.html</guid><description>1. 为什么是 Transformer 全连接的自注意 以往的 RNN 模型，每个单词只能和邻近的单词产生联系，而 Transformer 模型中的 Attention 机制，单词可以和任意位置的单词产生联系，这样就可以捕捉到全局的上下文信息。 没有梯度消失问题 RNN 作用在同一个权值矩阵上，使得其最大的特征值小于 1 时，就会出现</description><dc:creator>微信公众号</dc:creator><category>整理</category><category>Transformer</category><category>AI</category><category>大模型</category></item><item><title>影响使用大模型的技术因素</title><link>https://www.chenshaowen.com/blog/the-key-factors-while-using-large-models.html</link><pubDate>Sat, 19 Aug 2023 00:00:00 +0000</pubDate><atom:modified>Sat, 19 Aug 2023 00:00:00 +0000</atom:modified><guid>https://www.chenshaowen.com/blog/the-key-factors-while-using-large-models.html</guid><description>1. 大模型到底是什么 先请两位大模型回答一下这个问题，看看他们的回答是什么。 Claude 说，大模型本质上是语言知识的概率表达，通过统计学习对语言各层次规律建模，表征语言生成的先验分布，从而具备语言预测生成能力。 ChatGPT 说，大模型本质是深度神经网络通过大量参数和</description><dc:creator>微信公众号</dc:creator><category>整理</category><category>AI</category><category>大模型</category><category>思考</category></item><item><title>AI 基础知识点</title><link>https://www.chenshaowen.com/blog/ai-basic-knowledge.html</link><pubDate>Fri, 18 Aug 2023 11:22:55 +0000</pubDate><atom:modified>Fri, 18 Aug 2023 11:22:55 +0000</atom:modified><guid>https://www.chenshaowen.com/blog/ai-basic-knowledge.html</guid><description>1. 关键字 机器学习(ML) 从数据中自动获取知识的技术 神经网络(NN) 模仿生物神经网络结构和学习机制的模型，是机器学习的分支之一 神经网络的结构包括，输入层、隐藏层、输出层 深度神经网络(DNN) 隐含层常常大于 2 层 DNN 的出众表现源于它使用统计学方法从</description><dc:creator>微信公众号</dc:creator><category>整理</category><category>AI</category><category>LLM</category><category>机器学习</category><category>大模型</category></item><item><title>使用 OpenAI 和 Langchain 通过对话直接调用函数</title><link>https://www.chenshaowen.com/blog/call-functions-through-dialogue-using-openai-and-langchain.html</link><pubDate>Wed, 16 Aug 2023 00:00:00 +0000</pubDate><atom:modified>Wed, 16 Aug 2023 00:00:00 +0000</atom:modified><guid>https://www.chenshaowen.com/blog/call-functions-through-dialogue-using-openai-and-langchain.html</guid><description>1. 大模型与 Langchain 很多人可能没有机会训练、甚至微调大模型，但对大模型的使用却是未来趋势。那么，我们应该如何拥抱这一变化呢？答案就是 Langchain。 大模型提供的是一种泛而通用的基础能力，目前，我看到的有两种主要落地方式： 基于生成能力的 AIGC，</description><dc:creator>微信公众号</dc:creator><category>博文</category><category>OpenAI</category><category>Langchain</category><category>AI</category><category>集成</category></item><item><title>GitHub Copilot Chat 使用</title><link>https://www.chenshaowen.com/blog/the-practice-of-github-copilot-chat.html</link><pubDate>Tue, 04 Jul 2023 00:00:00 +0000</pubDate><atom:modified>Tue, 04 Jul 2023 00:00:00 +0000</atom:modified><guid>https://www.chenshaowen.com/blog/the-practice-of-github-copilot-chat.html</guid><description>1. 申请使用 GitHub Copilot Chat 申请链接 https://github.com/github-copilot/chat_waitlist_signup/join 申请通过之后，会收到一封邮件: 2. 什么是 VS Code insiders 什么是 VS Code insiders VS Code insiders 是 VS Code 的预览版本，提供一些最新的功能和改进，更新非常频繁。如果有更新强迫症，慎重使用，因为几乎每天都有更新。 VS Code 和 VS Code insiders 的区别 VS Code 的命令行是 code ，logo</description><dc:creator>微信公众号</dc:creator><category>博文</category><category>Copilot</category><category>GitHub</category><category>Chat</category><category>AI</category></item><item><title>如何在无 GPU 的 macOS 上运行 Stable Diffusion</title><link>https://www.chenshaowen.com/blog/how-to-run-stable-diffusion-in-macos-without-gpu.html</link><pubDate>Fri, 10 Feb 2023 00:00:00 +0000</pubDate><atom:modified>Fri, 10 Feb 2023 00:00:00 +0000</atom:modified><guid>https://www.chenshaowen.com/blog/how-to-run-stable-diffusion-in-macos-without-gpu.html</guid><description>1. 运行 Stable Diffusion 推荐配置 内存: 不低于 16 GB DDR4 或 DDR5 存储: 不低于 10 GB 可用空间 GPU: 不低于 6 GB 显存 N 卡 如果硬件达不到要求，也可以使用各种优化 fork 兼容更低配置的硬件，但生成时间会增长。 当前的开发主机配置为: 2.9 GHz 8-Core Intel Core i7 16 GB 2666 MHz DDR4 250 GB SSD 由于没有 GPU，生成图片时，</description><dc:creator>微信公众号</dc:creator><category>博文</category><category>AI</category><category>macOS</category><category>GPU</category></item><item><title>Container 和 AI 是 PaaS 未来的发展方向</title><link>https://www.chenshaowen.com/blog/container-and-ai-are-the-future-directions-in-the-development-of-paas.html</link><pubDate>Sun, 03 Sep 2017 15:57:50 +0000</pubDate><atom:modified>Sun, 03 Sep 2017 15:57:50 +0000</atom:modified><guid>https://www.chenshaowen.com/blog/container-and-ai-are-the-future-directions-in-the-development-of-paas.html</guid><description>1. 关于 PaaS 1.1 什么是 PaaS PaaS 是平台即服务（Platform as a Service）的简称，平台即服务是一种云计算服务，提供运算平台与解决方案服务。PaaS 的出现加快了 SaaS 的发展，尤其是加快了 SaaS 应用的开发速度。比如，SaaS 开发时，使用 PaaS 平台统一提供的登录</description><dc:creator>微信公众号</dc:creator><category>博文</category><category>思考</category><category>云服务</category><category>PaaS</category><category>容器</category><category>AI</category></item></channel></rss>