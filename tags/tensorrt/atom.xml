<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:sy="http://purl.org/rss/1.0/modules/syndication/" xmlns:media="http://search.yahoo.com/mrss/"><channel><title>TensorRT on 陈少文的网站</title><link>https://www.chenshaowen.com/tags/tensorrt/</link><description>Recent content in TensorRT on 陈少文的网站</description><generator>Hugo -- gohugo.io</generator><language>zh</language><copyright>&amp;copy;2016 - {year}, All Rights Reserved.</copyright><lastBuildDate>Tue, 06 Feb 2024 00:00:00 +0000</lastBuildDate><sy:updatePeriod>weekly</sy:updatePeriod><atom:link href="https://www.chenshaowen.com/tags/tensorrt/atom.xml" rel="self" type="application/rss+xml"/><item><title>使用 TensorRT 加速模型推理</title><link>https://www.chenshaowen.com/blog/speeding-up-model-inference-with-tensorrt.html</link><pubDate>Tue, 06 Feb 2024 00:00:00 +0000</pubDate><atom:modified>Tue, 06 Feb 2024 00:00:00 +0000</atom:modified><guid>https://www.chenshaowen.com/blog/speeding-up-model-inference-with-tensorrt.html</guid><description>1. 什么是 TensorRT TensorRT 是一个 C++ 库，主要用在 NVIDIA GPU 进行高性能的推理加速上，提供了 C++ API 和 Python API 用于集成。 TensorRT 支持的主流深度学习框架有: Caffe，TensorRT 可以直接读取 prototxt 格式 TensorFlow，需要将 TensorFlow 的 pb 转换为 uff 格式 PyTorch，需要将 PyTorch 的 pth 格式转</description><dc:creator>微信公众号</dc:creator><category>博文</category><category>AI</category><category>TensorRT</category><category>NVIDIA</category><category>推理</category></item><item><title>容器下使用 Triton Server 和 TensorRT-LLM 进行大模型推理</title><link>https://www.chenshaowen.com/blog/using-triton-server-and-tensorrt-llm-under-container.html</link><pubDate>Sat, 03 Feb 2024 08:05:48 +0000</pubDate><atom:modified>Sat, 03 Feb 2024 08:05:48 +0000</atom:modified><guid>https://www.chenshaowen.com/blog/using-triton-server-and-tensorrt-llm-under-container.html</guid><description>1. TensorRT-LLM 编译模型 1.1 TensorRT-LLM 简介 使用 TensorRT 时，通常需要将模型转换为 ONNX 格式，再将 ONNX 转换为 TensorRT 格式，然后在 TensorRT、Triton Server 中进行推理。 但这个转换过程并不简单，经常会遇到各种报错，需要对模型结构、平台算子有一定的掌握，具备转换和调试能力。而 TensorRT-LLM 的目标</description><dc:creator>微信公众号</dc:creator><category>博文</category><category>AI</category><category>Triton</category><category>TensorRT</category></item></channel></rss>